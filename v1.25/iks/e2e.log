I0128 00:14:08.235741      22 e2e.go:116] Starting e2e run "ba43e6ee-618b-4498-aa9f-cb14cf317c48" on Ginkgo node 1
Jan 28 00:14:08.253: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1674864848 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Jan 28 00:14:08.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 00:14:08.374: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0128 00:14:08.378731      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0128 00:14:08.378731      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jan 28 00:14:08.424: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 28 00:14:08.527: INFO: 36 / 36 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 28 00:14:08.527: INFO: expected 21 pod replicas in namespace 'kube-system', 21 are Running and Ready.
Jan 28 00:14:08.527: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 28 00:14:08.552: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 28 00:14:08.552: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Jan 28 00:14:08.552: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Jan 28 00:14:08.552: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
Jan 28 00:14:08.552: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Jan 28 00:14:08.552: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
Jan 28 00:14:08.552: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Jan 28 00:14:08.552: INFO: e2e test version: v1.25.6
Jan 28 00:14:08.558: INFO: kube-apiserver version: v1.25.6+IKS
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Jan 28 00:14:08.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 00:14:08.574: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.205 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan 28 00:14:08.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 00:14:08.374: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0128 00:14:08.378731      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Jan 28 00:14:08.424: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan 28 00:14:08.527: INFO: 36 / 36 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan 28 00:14:08.527: INFO: expected 21 pod replicas in namespace 'kube-system', 21 are Running and Ready.
    Jan 28 00:14:08.527: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan 28 00:14:08.552: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Jan 28 00:14:08.552: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
    Jan 28 00:14:08.552: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
    Jan 28 00:14:08.552: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
    Jan 28 00:14:08.552: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
    Jan 28 00:14:08.552: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
    Jan 28 00:14:08.552: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
    Jan 28 00:14:08.552: INFO: e2e test version: v1.25.6
    Jan 28 00:14:08.558: INFO: kube-apiserver version: v1.25.6+IKS
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan 28 00:14:08.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 00:14:08.574: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:14:08.603
Jan 28 00:14:08.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename deployment 01/28/23 00:14:08.605
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:14:08.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:14:08.649
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/28/23 00:14:08.678
STEP: waiting for Deployment to be created 01/28/23 00:14:08.693
STEP: waiting for all Replicas to be Ready 01/28/23 00:14:08.698
Jan 28 00:14:08.705: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 28 00:14:08.705: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 28 00:14:08.716: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 28 00:14:08.716: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 28 00:14:08.771: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 28 00:14:08.771: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 28 00:14:08.798: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 28 00:14:08.798: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 28 00:14:22.118: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 28 00:14:22.118: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 28 00:14:22.136: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/28/23 00:14:22.136
W0128 00:14:22.152668      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 28 00:14:22.160: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/28/23 00:14:22.16
Jan 28 00:14:22.167: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0
Jan 28 00:14:22.167: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0
Jan 28 00:14:22.167: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0
Jan 28 00:14:22.167: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0
Jan 28 00:14:22.171: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0
Jan 28 00:14:22.171: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0
Jan 28 00:14:22.171: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0
Jan 28 00:14:22.171: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0
Jan 28 00:14:22.172: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
Jan 28 00:14:22.172: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
Jan 28 00:14:22.173: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
Jan 28 00:14:22.173: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
Jan 28 00:14:22.173: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
Jan 28 00:14:22.173: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
Jan 28 00:14:22.174: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
Jan 28 00:14:22.175: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
Jan 28 00:14:22.199: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
Jan 28 00:14:22.199: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
Jan 28 00:14:22.212: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
Jan 28 00:14:22.212: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
Jan 28 00:14:22.236: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
Jan 28 00:14:22.237: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
Jan 28 00:14:29.185: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
Jan 28 00:14:29.185: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
Jan 28 00:14:29.224: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
STEP: listing Deployments 01/28/23 00:14:29.224
Jan 28 00:14:29.256: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/28/23 00:14:29.256
Jan 28 00:14:29.287: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/28/23 00:14:29.288
Jan 28 00:14:29.318: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 28 00:14:29.318: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 28 00:14:29.329: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 28 00:14:29.356: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 28 00:14:29.390: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 28 00:14:29.390: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 28 00:14:36.205: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 28 00:14:40.245: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jan 28 00:14:40.332: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 28 00:14:40.385: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 28 00:14:51.292: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/28/23 00:14:51.328
STEP: fetching the DeploymentStatus 01/28/23 00:14:51.348
Jan 28 00:14:51.367: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
Jan 28 00:14:51.367: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
Jan 28 00:14:51.367: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
Jan 28 00:14:51.367: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
Jan 28 00:14:51.368: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
Jan 28 00:14:51.368: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
Jan 28 00:14:51.368: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
Jan 28 00:14:51.369: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 3
Jan 28 00:14:51.370: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
Jan 28 00:14:51.370: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
Jan 28 00:14:51.370: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 3
STEP: deleting the Deployment 01/28/23 00:14:51.37
Jan 28 00:14:51.403: INFO: observed event type MODIFIED
Jan 28 00:14:51.404: INFO: observed event type MODIFIED
Jan 28 00:14:51.404: INFO: observed event type MODIFIED
Jan 28 00:14:51.404: INFO: observed event type MODIFIED
Jan 28 00:14:51.405: INFO: observed event type MODIFIED
Jan 28 00:14:51.405: INFO: observed event type MODIFIED
Jan 28 00:14:51.405: INFO: observed event type MODIFIED
Jan 28 00:14:51.406: INFO: observed event type MODIFIED
Jan 28 00:14:51.407: INFO: observed event type MODIFIED
Jan 28 00:14:51.407: INFO: observed event type MODIFIED
Jan 28 00:14:51.408: INFO: observed event type MODIFIED
Jan 28 00:14:51.408: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 28 00:14:51.421: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 28 00:14:51.431: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-2845  24de9ecd-ce97-4762-b7e4-4bdcb3680191 16800 4 2023-01-28 00:14:22 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 03870539-a29b-41bc-8c59-4a519427ec2c 0xc002ad4167 0xc002ad4168}] [] [{kube-controller-manager Update apps/v1 2023-01-28 00:14:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03870539-a29b-41bc-8c59-4a519427ec2c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:14:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ad41f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 28 00:14:51.443: INFO: pod: "test-deployment-54cc775c4b-wnb9w":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-wnb9w test-deployment-54cc775c4b- deployment-2845  c27bbeb0-e050-4bf4-8ec2-c420c94cf7ee 16796 0 2023-01-28 00:14:29 +0000 UTC 2023-01-28 00:14:52 +0000 UTC 0xc002ad4668 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:2d233bc9f11ab2478e7587c410c4e098b9230b68658bd8559358cb308c7da1c9 cni.projectcalico.org/podIP:172.30.12.217/32 cni.projectcalico.org/podIPs:172.30.12.217/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 24de9ecd-ce97-4762-b7e4-4bdcb3680191 0xc002ad46b7 0xc002ad46b8}] [] [{kube-controller-manager Update v1 2023-01-28 00:14:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24de9ecd-ce97-4762-b7e4-4bdcb3680191\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 00:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 00:14:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.217\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2xrrz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2xrrz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:172.30.12.217,StartTime:2023-01-28 00:14:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 00:14:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://83bf2ef10f0865b42d30c3a7de8031aa2ce4f5e836000a7d67010a210be37df0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.12.217,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 28 00:14:51.444: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-2845  59e49d13-35f5-41d8-bb00-caec2d6f78d3 16792 2 2023-01-28 00:14:29 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 03870539-a29b-41bc-8c59-4a519427ec2c 0xc002ad4257 0xc002ad4258}] [] [{kube-controller-manager Update apps/v1 2023-01-28 00:14:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03870539-a29b-41bc-8c59-4a519427ec2c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:14:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ad42e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 28 00:14:51.455: INFO: pod: "test-deployment-7c7d8d58c8-hhgxs":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-hhgxs test-deployment-7c7d8d58c8- deployment-2845  e508b6d0-a9ab-4b7d-9c13-a5edb615a046 16791 0 2023-01-28 00:14:40 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:0ba0bb61ca0eb6637f5a08c8e54a07c65fc890bc36b8b761349ad8565c83b8fd cni.projectcalico.org/podIP:172.30.12.218/32 cni.projectcalico.org/podIPs:172.30.12.218/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 59e49d13-35f5-41d8-bb00-caec2d6f78d3 0xc002a12817 0xc002a12818}] [] [{kube-controller-manager Update v1 2023-01-28 00:14:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59e49d13-35f5-41d8-bb00-caec2d6f78d3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 00:14:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 00:14:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbb7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbb7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:172.30.12.218,StartTime:2023-01-28 00:14:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 00:14:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://96d202388ddd003ad4e90ba07dc778c647422580a88d51e746bac6d96895384a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.12.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 28 00:14:51.456: INFO: pod: "test-deployment-7c7d8d58c8-mkm6j":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-mkm6j test-deployment-7c7d8d58c8- deployment-2845  fee72b4e-29c2-4d3b-9cfb-1aff1d7127b2 16746 0 2023-01-28 00:14:29 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:91dbd836b93f2a97e35ce61535196c55d1c311869ca54061b72722f6df36f485 cni.projectcalico.org/podIP:172.30.185.46/32 cni.projectcalico.org/podIPs:172.30.185.46/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 59e49d13-35f5-41d8-bb00-caec2d6f78d3 0xc002a12a47 0xc002a12a48}] [] [{kube-controller-manager Update v1 2023-01-28 00:14:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59e49d13-35f5-41d8-bb00-caec2d6f78d3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 00:14:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 00:14:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.185.46\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4rxt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4rxt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:172.30.185.46,StartTime:2023-01-28 00:14:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 00:14:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1a0bfaf7fabc877d633d2a7b113a3b5e2479be85812111cf7674ef2224794f29,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.185.46,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 28 00:14:51.457: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-2845  54ee810f-6721-4fb7-93be-2352d8d1a96e 16680 3 2023-01-28 00:14:08 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 03870539-a29b-41bc-8c59-4a519427ec2c 0xc002ad4347 0xc002ad4348}] [] [{kube-controller-manager Update apps/v1 2023-01-28 00:14:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03870539-a29b-41bc-8c59-4a519427ec2c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:14:29 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ad43d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 28 00:14:51.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2845" for this suite. 01/28/23 00:14:51.488
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":1,"skipped":0,"failed":0}
------------------------------
â€¢ [SLOW TEST] [42.904 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:14:08.603
    Jan 28 00:14:08.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename deployment 01/28/23 00:14:08.605
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:14:08.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:14:08.649
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/28/23 00:14:08.678
    STEP: waiting for Deployment to be created 01/28/23 00:14:08.693
    STEP: waiting for all Replicas to be Ready 01/28/23 00:14:08.698
    Jan 28 00:14:08.705: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 28 00:14:08.705: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 28 00:14:08.716: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 28 00:14:08.716: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 28 00:14:08.771: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 28 00:14:08.771: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 28 00:14:08.798: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 28 00:14:08.798: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 28 00:14:22.118: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 28 00:14:22.118: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 28 00:14:22.136: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/28/23 00:14:22.136
    W0128 00:14:22.152668      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 28 00:14:22.160: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/28/23 00:14:22.16
    Jan 28 00:14:22.167: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0
    Jan 28 00:14:22.167: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0
    Jan 28 00:14:22.167: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0
    Jan 28 00:14:22.167: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0
    Jan 28 00:14:22.171: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0
    Jan 28 00:14:22.171: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0
    Jan 28 00:14:22.171: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0
    Jan 28 00:14:22.171: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 0
    Jan 28 00:14:22.172: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
    Jan 28 00:14:22.172: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
    Jan 28 00:14:22.173: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
    Jan 28 00:14:22.173: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
    Jan 28 00:14:22.173: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
    Jan 28 00:14:22.173: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
    Jan 28 00:14:22.174: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
    Jan 28 00:14:22.175: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
    Jan 28 00:14:22.199: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
    Jan 28 00:14:22.199: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
    Jan 28 00:14:22.212: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
    Jan 28 00:14:22.212: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
    Jan 28 00:14:22.236: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
    Jan 28 00:14:22.237: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
    Jan 28 00:14:29.185: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
    Jan 28 00:14:29.185: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
    Jan 28 00:14:29.224: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
    STEP: listing Deployments 01/28/23 00:14:29.224
    Jan 28 00:14:29.256: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/28/23 00:14:29.256
    Jan 28 00:14:29.287: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/28/23 00:14:29.288
    Jan 28 00:14:29.318: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 28 00:14:29.318: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 28 00:14:29.329: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 28 00:14:29.356: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 28 00:14:29.390: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 28 00:14:29.390: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 28 00:14:36.205: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 28 00:14:40.245: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 28 00:14:40.332: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 28 00:14:40.385: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 28 00:14:51.292: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/28/23 00:14:51.328
    STEP: fetching the DeploymentStatus 01/28/23 00:14:51.348
    Jan 28 00:14:51.367: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
    Jan 28 00:14:51.367: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
    Jan 28 00:14:51.367: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
    Jan 28 00:14:51.367: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
    Jan 28 00:14:51.368: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
    Jan 28 00:14:51.368: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 1
    Jan 28 00:14:51.368: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
    Jan 28 00:14:51.369: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 3
    Jan 28 00:14:51.370: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
    Jan 28 00:14:51.370: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 2
    Jan 28 00:14:51.370: INFO: observed Deployment test-deployment in namespace deployment-2845 with ReadyReplicas 3
    STEP: deleting the Deployment 01/28/23 00:14:51.37
    Jan 28 00:14:51.403: INFO: observed event type MODIFIED
    Jan 28 00:14:51.404: INFO: observed event type MODIFIED
    Jan 28 00:14:51.404: INFO: observed event type MODIFIED
    Jan 28 00:14:51.404: INFO: observed event type MODIFIED
    Jan 28 00:14:51.405: INFO: observed event type MODIFIED
    Jan 28 00:14:51.405: INFO: observed event type MODIFIED
    Jan 28 00:14:51.405: INFO: observed event type MODIFIED
    Jan 28 00:14:51.406: INFO: observed event type MODIFIED
    Jan 28 00:14:51.407: INFO: observed event type MODIFIED
    Jan 28 00:14:51.407: INFO: observed event type MODIFIED
    Jan 28 00:14:51.408: INFO: observed event type MODIFIED
    Jan 28 00:14:51.408: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 28 00:14:51.421: INFO: Log out all the ReplicaSets if there is no deployment created
    Jan 28 00:14:51.431: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-2845  24de9ecd-ce97-4762-b7e4-4bdcb3680191 16800 4 2023-01-28 00:14:22 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 03870539-a29b-41bc-8c59-4a519427ec2c 0xc002ad4167 0xc002ad4168}] [] [{kube-controller-manager Update apps/v1 2023-01-28 00:14:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03870539-a29b-41bc-8c59-4a519427ec2c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:14:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ad41f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jan 28 00:14:51.443: INFO: pod: "test-deployment-54cc775c4b-wnb9w":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-wnb9w test-deployment-54cc775c4b- deployment-2845  c27bbeb0-e050-4bf4-8ec2-c420c94cf7ee 16796 0 2023-01-28 00:14:29 +0000 UTC 2023-01-28 00:14:52 +0000 UTC 0xc002ad4668 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:2d233bc9f11ab2478e7587c410c4e098b9230b68658bd8559358cb308c7da1c9 cni.projectcalico.org/podIP:172.30.12.217/32 cni.projectcalico.org/podIPs:172.30.12.217/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 24de9ecd-ce97-4762-b7e4-4bdcb3680191 0xc002ad46b7 0xc002ad46b8}] [] [{kube-controller-manager Update v1 2023-01-28 00:14:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24de9ecd-ce97-4762-b7e4-4bdcb3680191\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 00:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 00:14:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.217\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2xrrz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2xrrz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:172.30.12.217,StartTime:2023-01-28 00:14:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 00:14:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://83bf2ef10f0865b42d30c3a7de8031aa2ce4f5e836000a7d67010a210be37df0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.12.217,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 28 00:14:51.444: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-2845  59e49d13-35f5-41d8-bb00-caec2d6f78d3 16792 2 2023-01-28 00:14:29 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 03870539-a29b-41bc-8c59-4a519427ec2c 0xc002ad4257 0xc002ad4258}] [] [{kube-controller-manager Update apps/v1 2023-01-28 00:14:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03870539-a29b-41bc-8c59-4a519427ec2c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:14:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ad42e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jan 28 00:14:51.455: INFO: pod: "test-deployment-7c7d8d58c8-hhgxs":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-hhgxs test-deployment-7c7d8d58c8- deployment-2845  e508b6d0-a9ab-4b7d-9c13-a5edb615a046 16791 0 2023-01-28 00:14:40 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:0ba0bb61ca0eb6637f5a08c8e54a07c65fc890bc36b8b761349ad8565c83b8fd cni.projectcalico.org/podIP:172.30.12.218/32 cni.projectcalico.org/podIPs:172.30.12.218/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 59e49d13-35f5-41d8-bb00-caec2d6f78d3 0xc002a12817 0xc002a12818}] [] [{kube-controller-manager Update v1 2023-01-28 00:14:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59e49d13-35f5-41d8-bb00-caec2d6f78d3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 00:14:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 00:14:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbb7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbb7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:172.30.12.218,StartTime:2023-01-28 00:14:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 00:14:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://96d202388ddd003ad4e90ba07dc778c647422580a88d51e746bac6d96895384a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.12.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 28 00:14:51.456: INFO: pod: "test-deployment-7c7d8d58c8-mkm6j":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-mkm6j test-deployment-7c7d8d58c8- deployment-2845  fee72b4e-29c2-4d3b-9cfb-1aff1d7127b2 16746 0 2023-01-28 00:14:29 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:91dbd836b93f2a97e35ce61535196c55d1c311869ca54061b72722f6df36f485 cni.projectcalico.org/podIP:172.30.185.46/32 cni.projectcalico.org/podIPs:172.30.185.46/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 59e49d13-35f5-41d8-bb00-caec2d6f78d3 0xc002a12a47 0xc002a12a48}] [] [{kube-controller-manager Update v1 2023-01-28 00:14:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59e49d13-35f5-41d8-bb00-caec2d6f78d3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 00:14:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 00:14:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.185.46\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4rxt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4rxt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:14:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:172.30.185.46,StartTime:2023-01-28 00:14:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 00:14:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1a0bfaf7fabc877d633d2a7b113a3b5e2479be85812111cf7674ef2224794f29,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.185.46,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 28 00:14:51.457: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-2845  54ee810f-6721-4fb7-93be-2352d8d1a96e 16680 3 2023-01-28 00:14:08 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 03870539-a29b-41bc-8c59-4a519427ec2c 0xc002ad4347 0xc002ad4348}] [] [{kube-controller-manager Update apps/v1 2023-01-28 00:14:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03870539-a29b-41bc-8c59-4a519427ec2c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:14:29 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ad43d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 28 00:14:51.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2845" for this suite. 01/28/23 00:14:51.488
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:14:51.511
Jan 28 00:14:51.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename downward-api 01/28/23 00:14:51.515
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:14:51.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:14:51.572
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 01/28/23 00:14:51.583
Jan 28 00:14:51.606: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7e5fdeac-e507-4007-bc76-cf1f8e1c3931" in namespace "downward-api-5743" to be "Succeeded or Failed"
Jan 28 00:14:51.618: INFO: Pod "downwardapi-volume-7e5fdeac-e507-4007-bc76-cf1f8e1c3931": Phase="Pending", Reason="", readiness=false. Elapsed: 11.730827ms
Jan 28 00:14:53.632: INFO: Pod "downwardapi-volume-7e5fdeac-e507-4007-bc76-cf1f8e1c3931": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025775676s
Jan 28 00:14:55.631: INFO: Pod "downwardapi-volume-7e5fdeac-e507-4007-bc76-cf1f8e1c3931": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024496104s
Jan 28 00:14:57.632: INFO: Pod "downwardapi-volume-7e5fdeac-e507-4007-bc76-cf1f8e1c3931": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025662951s
STEP: Saw pod success 01/28/23 00:14:57.632
Jan 28 00:14:57.633: INFO: Pod "downwardapi-volume-7e5fdeac-e507-4007-bc76-cf1f8e1c3931" satisfied condition "Succeeded or Failed"
Jan 28 00:14:57.646: INFO: Trying to get logs from node 10.9.20.72 pod downwardapi-volume-7e5fdeac-e507-4007-bc76-cf1f8e1c3931 container client-container: <nil>
STEP: delete the pod 01/28/23 00:14:57.718
Jan 28 00:14:57.749: INFO: Waiting for pod downwardapi-volume-7e5fdeac-e507-4007-bc76-cf1f8e1c3931 to disappear
Jan 28 00:14:57.759: INFO: Pod downwardapi-volume-7e5fdeac-e507-4007-bc76-cf1f8e1c3931 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 28 00:14:57.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5743" for this suite. 01/28/23 00:14:57.779
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":2,"skipped":3,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.287 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:14:51.511
    Jan 28 00:14:51.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename downward-api 01/28/23 00:14:51.515
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:14:51.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:14:51.572
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 01/28/23 00:14:51.583
    Jan 28 00:14:51.606: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7e5fdeac-e507-4007-bc76-cf1f8e1c3931" in namespace "downward-api-5743" to be "Succeeded or Failed"
    Jan 28 00:14:51.618: INFO: Pod "downwardapi-volume-7e5fdeac-e507-4007-bc76-cf1f8e1c3931": Phase="Pending", Reason="", readiness=false. Elapsed: 11.730827ms
    Jan 28 00:14:53.632: INFO: Pod "downwardapi-volume-7e5fdeac-e507-4007-bc76-cf1f8e1c3931": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025775676s
    Jan 28 00:14:55.631: INFO: Pod "downwardapi-volume-7e5fdeac-e507-4007-bc76-cf1f8e1c3931": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024496104s
    Jan 28 00:14:57.632: INFO: Pod "downwardapi-volume-7e5fdeac-e507-4007-bc76-cf1f8e1c3931": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025662951s
    STEP: Saw pod success 01/28/23 00:14:57.632
    Jan 28 00:14:57.633: INFO: Pod "downwardapi-volume-7e5fdeac-e507-4007-bc76-cf1f8e1c3931" satisfied condition "Succeeded or Failed"
    Jan 28 00:14:57.646: INFO: Trying to get logs from node 10.9.20.72 pod downwardapi-volume-7e5fdeac-e507-4007-bc76-cf1f8e1c3931 container client-container: <nil>
    STEP: delete the pod 01/28/23 00:14:57.718
    Jan 28 00:14:57.749: INFO: Waiting for pod downwardapi-volume-7e5fdeac-e507-4007-bc76-cf1f8e1c3931 to disappear
    Jan 28 00:14:57.759: INFO: Pod downwardapi-volume-7e5fdeac-e507-4007-bc76-cf1f8e1c3931 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 28 00:14:57.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5743" for this suite. 01/28/23 00:14:57.779
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:14:57.805
Jan 28 00:14:57.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename dns 01/28/23 00:14:57.808
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:14:57.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:14:57.853
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/28/23 00:14:57.865
Jan 28 00:14:57.886: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5781  b06b399a-5740-488e-aa2b-ff5d7ad3a303 16917 0 2023-01-28 00:14:57 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-28 00:14:57 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mcc4z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mcc4z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 00:14:57.886: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5781" to be "running and ready"
Jan 28 00:14:57.897: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 10.992755ms
Jan 28 00:14:57.897: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:14:59.912: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025644653s
Jan 28 00:14:59.912: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:15:01.911: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.025086491s
Jan 28 00:15:01.911: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan 28 00:15:01.911: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/28/23 00:15:01.911
Jan 28 00:15:01.911: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5781 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:15:01.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 00:15:01.913: INFO: ExecWithOptions: Clientset creation
Jan 28 00:15:01.913: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-5781/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/28/23 00:15:02.176
Jan 28 00:15:02.176: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5781 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:15:02.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 00:15:02.178: INFO: ExecWithOptions: Clientset creation
Jan 28 00:15:02.178: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-5781/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 28 00:15:02.454: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 28 00:15:02.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5781" for this suite. 01/28/23 00:15:02.51
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":3,"skipped":4,"failed":0}
------------------------------
â€¢ [4.721 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:14:57.805
    Jan 28 00:14:57.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename dns 01/28/23 00:14:57.808
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:14:57.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:14:57.853
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/28/23 00:14:57.865
    Jan 28 00:14:57.886: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5781  b06b399a-5740-488e-aa2b-ff5d7ad3a303 16917 0 2023-01-28 00:14:57 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-28 00:14:57 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mcc4z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mcc4z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 00:14:57.886: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5781" to be "running and ready"
    Jan 28 00:14:57.897: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 10.992755ms
    Jan 28 00:14:57.897: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:14:59.912: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025644653s
    Jan 28 00:14:59.912: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:15:01.911: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.025086491s
    Jan 28 00:15:01.911: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan 28 00:15:01.911: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/28/23 00:15:01.911
    Jan 28 00:15:01.911: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5781 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 00:15:01.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 00:15:01.913: INFO: ExecWithOptions: Clientset creation
    Jan 28 00:15:01.913: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-5781/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/28/23 00:15:02.176
    Jan 28 00:15:02.176: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5781 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 00:15:02.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 00:15:02.178: INFO: ExecWithOptions: Clientset creation
    Jan 28 00:15:02.178: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-5781/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 28 00:15:02.454: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 28 00:15:02.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5781" for this suite. 01/28/23 00:15:02.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:15:02.537
Jan 28 00:15:02.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename emptydir 01/28/23 00:15:02.539
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:15:02.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:15:02.589
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 01/28/23 00:15:02.598
Jan 28 00:15:02.621: INFO: Waiting up to 5m0s for pod "pod-f10d0cf0-5a0e-4492-99aa-ddd6dab7dda0" in namespace "emptydir-4948" to be "Succeeded or Failed"
Jan 28 00:15:02.633: INFO: Pod "pod-f10d0cf0-5a0e-4492-99aa-ddd6dab7dda0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.368425ms
Jan 28 00:15:04.645: INFO: Pod "pod-f10d0cf0-5a0e-4492-99aa-ddd6dab7dda0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024369627s
Jan 28 00:15:06.646: INFO: Pod "pod-f10d0cf0-5a0e-4492-99aa-ddd6dab7dda0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025576427s
Jan 28 00:15:08.646: INFO: Pod "pod-f10d0cf0-5a0e-4492-99aa-ddd6dab7dda0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025350784s
STEP: Saw pod success 01/28/23 00:15:08.646
Jan 28 00:15:08.647: INFO: Pod "pod-f10d0cf0-5a0e-4492-99aa-ddd6dab7dda0" satisfied condition "Succeeded or Failed"
Jan 28 00:15:08.658: INFO: Trying to get logs from node 10.9.20.126 pod pod-f10d0cf0-5a0e-4492-99aa-ddd6dab7dda0 container test-container: <nil>
STEP: delete the pod 01/28/23 00:15:08.75
Jan 28 00:15:08.782: INFO: Waiting for pod pod-f10d0cf0-5a0e-4492-99aa-ddd6dab7dda0 to disappear
Jan 28 00:15:08.794: INFO: Pod pod-f10d0cf0-5a0e-4492-99aa-ddd6dab7dda0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 28 00:15:08.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4948" for this suite. 01/28/23 00:15:08.823
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":4,"skipped":43,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.304 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:15:02.537
    Jan 28 00:15:02.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename emptydir 01/28/23 00:15:02.539
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:15:02.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:15:02.589
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/28/23 00:15:02.598
    Jan 28 00:15:02.621: INFO: Waiting up to 5m0s for pod "pod-f10d0cf0-5a0e-4492-99aa-ddd6dab7dda0" in namespace "emptydir-4948" to be "Succeeded or Failed"
    Jan 28 00:15:02.633: INFO: Pod "pod-f10d0cf0-5a0e-4492-99aa-ddd6dab7dda0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.368425ms
    Jan 28 00:15:04.645: INFO: Pod "pod-f10d0cf0-5a0e-4492-99aa-ddd6dab7dda0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024369627s
    Jan 28 00:15:06.646: INFO: Pod "pod-f10d0cf0-5a0e-4492-99aa-ddd6dab7dda0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025576427s
    Jan 28 00:15:08.646: INFO: Pod "pod-f10d0cf0-5a0e-4492-99aa-ddd6dab7dda0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025350784s
    STEP: Saw pod success 01/28/23 00:15:08.646
    Jan 28 00:15:08.647: INFO: Pod "pod-f10d0cf0-5a0e-4492-99aa-ddd6dab7dda0" satisfied condition "Succeeded or Failed"
    Jan 28 00:15:08.658: INFO: Trying to get logs from node 10.9.20.126 pod pod-f10d0cf0-5a0e-4492-99aa-ddd6dab7dda0 container test-container: <nil>
    STEP: delete the pod 01/28/23 00:15:08.75
    Jan 28 00:15:08.782: INFO: Waiting for pod pod-f10d0cf0-5a0e-4492-99aa-ddd6dab7dda0 to disappear
    Jan 28 00:15:08.794: INFO: Pod pod-f10d0cf0-5a0e-4492-99aa-ddd6dab7dda0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 28 00:15:08.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4948" for this suite. 01/28/23 00:15:08.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:15:08.865
Jan 28 00:15:08.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 00:15:08.868
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:15:08.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:15:08.929
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-b9d6b9a8-2422-4d2d-b554-605ea33d9410 01/28/23 00:15:08.947
STEP: Creating a pod to test consume secrets 01/28/23 00:15:08.962
Jan 28 00:15:08.983: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2792650d-2026-4292-a862-f86366c9ca36" in namespace "projected-5750" to be "Succeeded or Failed"
Jan 28 00:15:08.997: INFO: Pod "pod-projected-secrets-2792650d-2026-4292-a862-f86366c9ca36": Phase="Pending", Reason="", readiness=false. Elapsed: 13.697155ms
Jan 28 00:15:11.010: INFO: Pod "pod-projected-secrets-2792650d-2026-4292-a862-f86366c9ca36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026224602s
Jan 28 00:15:13.010: INFO: Pod "pod-projected-secrets-2792650d-2026-4292-a862-f86366c9ca36": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026937336s
Jan 28 00:15:15.010: INFO: Pod "pod-projected-secrets-2792650d-2026-4292-a862-f86366c9ca36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026483346s
STEP: Saw pod success 01/28/23 00:15:15.01
Jan 28 00:15:15.011: INFO: Pod "pod-projected-secrets-2792650d-2026-4292-a862-f86366c9ca36" satisfied condition "Succeeded or Failed"
Jan 28 00:15:15.022: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-secrets-2792650d-2026-4292-a862-f86366c9ca36 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/28/23 00:15:15.053
Jan 28 00:15:15.084: INFO: Waiting for pod pod-projected-secrets-2792650d-2026-4292-a862-f86366c9ca36 to disappear
Jan 28 00:15:15.095: INFO: Pod pod-projected-secrets-2792650d-2026-4292-a862-f86366c9ca36 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 28 00:15:15.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5750" for this suite. 01/28/23 00:15:15.114
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":5,"skipped":85,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.265 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:15:08.865
    Jan 28 00:15:08.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 00:15:08.868
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:15:08.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:15:08.929
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-b9d6b9a8-2422-4d2d-b554-605ea33d9410 01/28/23 00:15:08.947
    STEP: Creating a pod to test consume secrets 01/28/23 00:15:08.962
    Jan 28 00:15:08.983: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2792650d-2026-4292-a862-f86366c9ca36" in namespace "projected-5750" to be "Succeeded or Failed"
    Jan 28 00:15:08.997: INFO: Pod "pod-projected-secrets-2792650d-2026-4292-a862-f86366c9ca36": Phase="Pending", Reason="", readiness=false. Elapsed: 13.697155ms
    Jan 28 00:15:11.010: INFO: Pod "pod-projected-secrets-2792650d-2026-4292-a862-f86366c9ca36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026224602s
    Jan 28 00:15:13.010: INFO: Pod "pod-projected-secrets-2792650d-2026-4292-a862-f86366c9ca36": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026937336s
    Jan 28 00:15:15.010: INFO: Pod "pod-projected-secrets-2792650d-2026-4292-a862-f86366c9ca36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026483346s
    STEP: Saw pod success 01/28/23 00:15:15.01
    Jan 28 00:15:15.011: INFO: Pod "pod-projected-secrets-2792650d-2026-4292-a862-f86366c9ca36" satisfied condition "Succeeded or Failed"
    Jan 28 00:15:15.022: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-secrets-2792650d-2026-4292-a862-f86366c9ca36 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/28/23 00:15:15.053
    Jan 28 00:15:15.084: INFO: Waiting for pod pod-projected-secrets-2792650d-2026-4292-a862-f86366c9ca36 to disappear
    Jan 28 00:15:15.095: INFO: Pod pod-projected-secrets-2792650d-2026-4292-a862-f86366c9ca36 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 28 00:15:15.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5750" for this suite. 01/28/23 00:15:15.114
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:15:15.133
Jan 28 00:15:15.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename containers 01/28/23 00:15:15.136
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:15:15.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:15:15.184
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 01/28/23 00:15:15.196
Jan 28 00:15:15.215: INFO: Waiting up to 5m0s for pod "client-containers-48728921-d2b1-46bf-a454-a3f1cefa3f72" in namespace "containers-9947" to be "Succeeded or Failed"
Jan 28 00:15:15.228: INFO: Pod "client-containers-48728921-d2b1-46bf-a454-a3f1cefa3f72": Phase="Pending", Reason="", readiness=false. Elapsed: 13.248783ms
Jan 28 00:15:17.240: INFO: Pod "client-containers-48728921-d2b1-46bf-a454-a3f1cefa3f72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025456634s
Jan 28 00:15:19.241: INFO: Pod "client-containers-48728921-d2b1-46bf-a454-a3f1cefa3f72": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026883458s
Jan 28 00:15:21.241: INFO: Pod "client-containers-48728921-d2b1-46bf-a454-a3f1cefa3f72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026271625s
STEP: Saw pod success 01/28/23 00:15:21.241
Jan 28 00:15:21.241: INFO: Pod "client-containers-48728921-d2b1-46bf-a454-a3f1cefa3f72" satisfied condition "Succeeded or Failed"
Jan 28 00:15:21.252: INFO: Trying to get logs from node 10.9.20.126 pod client-containers-48728921-d2b1-46bf-a454-a3f1cefa3f72 container agnhost-container: <nil>
STEP: delete the pod 01/28/23 00:15:21.282
Jan 28 00:15:21.312: INFO: Waiting for pod client-containers-48728921-d2b1-46bf-a454-a3f1cefa3f72 to disappear
Jan 28 00:15:21.322: INFO: Pod client-containers-48728921-d2b1-46bf-a454-a3f1cefa3f72 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 28 00:15:21.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9947" for this suite. 01/28/23 00:15:21.343
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":6,"skipped":89,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.232 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:15:15.133
    Jan 28 00:15:15.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename containers 01/28/23 00:15:15.136
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:15:15.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:15:15.184
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 01/28/23 00:15:15.196
    Jan 28 00:15:15.215: INFO: Waiting up to 5m0s for pod "client-containers-48728921-d2b1-46bf-a454-a3f1cefa3f72" in namespace "containers-9947" to be "Succeeded or Failed"
    Jan 28 00:15:15.228: INFO: Pod "client-containers-48728921-d2b1-46bf-a454-a3f1cefa3f72": Phase="Pending", Reason="", readiness=false. Elapsed: 13.248783ms
    Jan 28 00:15:17.240: INFO: Pod "client-containers-48728921-d2b1-46bf-a454-a3f1cefa3f72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025456634s
    Jan 28 00:15:19.241: INFO: Pod "client-containers-48728921-d2b1-46bf-a454-a3f1cefa3f72": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026883458s
    Jan 28 00:15:21.241: INFO: Pod "client-containers-48728921-d2b1-46bf-a454-a3f1cefa3f72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026271625s
    STEP: Saw pod success 01/28/23 00:15:21.241
    Jan 28 00:15:21.241: INFO: Pod "client-containers-48728921-d2b1-46bf-a454-a3f1cefa3f72" satisfied condition "Succeeded or Failed"
    Jan 28 00:15:21.252: INFO: Trying to get logs from node 10.9.20.126 pod client-containers-48728921-d2b1-46bf-a454-a3f1cefa3f72 container agnhost-container: <nil>
    STEP: delete the pod 01/28/23 00:15:21.282
    Jan 28 00:15:21.312: INFO: Waiting for pod client-containers-48728921-d2b1-46bf-a454-a3f1cefa3f72 to disappear
    Jan 28 00:15:21.322: INFO: Pod client-containers-48728921-d2b1-46bf-a454-a3f1cefa3f72 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 28 00:15:21.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-9947" for this suite. 01/28/23 00:15:21.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:15:21.368
Jan 28 00:15:21.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename container-probe 01/28/23 00:15:21.371
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:15:21.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:15:21.412
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77 in namespace container-probe-32 01/28/23 00:15:21.425
Jan 28 00:15:21.448: INFO: Waiting up to 5m0s for pod "busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77" in namespace "container-probe-32" to be "not pending"
Jan 28 00:15:21.461: INFO: Pod "busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77": Phase="Pending", Reason="", readiness=false. Elapsed: 13.328498ms
Jan 28 00:15:23.473: INFO: Pod "busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024761235s
Jan 28 00:15:25.474: INFO: Pod "busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026590059s
Jan 28 00:15:27.475: INFO: Pod "busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026993733s
Jan 28 00:15:29.474: INFO: Pod "busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77": Phase="Running", Reason="", readiness=true. Elapsed: 8.026271517s
Jan 28 00:15:29.474: INFO: Pod "busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77" satisfied condition "not pending"
Jan 28 00:15:29.474: INFO: Started pod busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77 in namespace container-probe-32
STEP: checking the pod's current state and verifying that restartCount is present 01/28/23 00:15:29.475
Jan 28 00:15:29.486: INFO: Initial restart count of pod busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77 is 0
STEP: deleting the pod 01/28/23 00:19:31.24
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 28 00:19:31.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-32" for this suite. 01/28/23 00:19:31.289
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":7,"skipped":104,"failed":0}
------------------------------
â€¢ [SLOW TEST] [249.939 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:15:21.368
    Jan 28 00:15:21.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename container-probe 01/28/23 00:15:21.371
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:15:21.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:15:21.412
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77 in namespace container-probe-32 01/28/23 00:15:21.425
    Jan 28 00:15:21.448: INFO: Waiting up to 5m0s for pod "busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77" in namespace "container-probe-32" to be "not pending"
    Jan 28 00:15:21.461: INFO: Pod "busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77": Phase="Pending", Reason="", readiness=false. Elapsed: 13.328498ms
    Jan 28 00:15:23.473: INFO: Pod "busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024761235s
    Jan 28 00:15:25.474: INFO: Pod "busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026590059s
    Jan 28 00:15:27.475: INFO: Pod "busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026993733s
    Jan 28 00:15:29.474: INFO: Pod "busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77": Phase="Running", Reason="", readiness=true. Elapsed: 8.026271517s
    Jan 28 00:15:29.474: INFO: Pod "busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77" satisfied condition "not pending"
    Jan 28 00:15:29.474: INFO: Started pod busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77 in namespace container-probe-32
    STEP: checking the pod's current state and verifying that restartCount is present 01/28/23 00:15:29.475
    Jan 28 00:15:29.486: INFO: Initial restart count of pod busybox-b9344041-10ed-4c3b-ba29-1ff63e280d77 is 0
    STEP: deleting the pod 01/28/23 00:19:31.24
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 28 00:19:31.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-32" for this suite. 01/28/23 00:19:31.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:19:31.315
Jan 28 00:19:31.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename watch 01/28/23 00:19:31.317
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:19:31.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:19:31.361
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/28/23 00:19:31.372
STEP: creating a new configmap 01/28/23 00:19:31.377
STEP: modifying the configmap once 01/28/23 00:19:31.39
STEP: closing the watch once it receives two notifications 01/28/23 00:19:31.414
Jan 28 00:19:31.415: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4050  4d70b3b2-9148-4c23-9a58-d3606a9cb7f3 17330 0 2023-01-28 00:19:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-28 00:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 00:19:31.416: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4050  4d70b3b2-9148-4c23-9a58-d3606a9cb7f3 17331 0 2023-01-28 00:19:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-28 00:19:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/28/23 00:19:31.416
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/28/23 00:19:31.439
STEP: deleting the configmap 01/28/23 00:19:31.445
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/28/23 00:19:31.466
Jan 28 00:19:31.467: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4050  4d70b3b2-9148-4c23-9a58-d3606a9cb7f3 17332 0 2023-01-28 00:19:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-28 00:19:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 00:19:31.469: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4050  4d70b3b2-9148-4c23-9a58-d3606a9cb7f3 17333 0 2023-01-28 00:19:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-28 00:19:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 28 00:19:31.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4050" for this suite. 01/28/23 00:19:31.488
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":8,"skipped":128,"failed":0}
------------------------------
â€¢ [0.194 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:19:31.315
    Jan 28 00:19:31.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename watch 01/28/23 00:19:31.317
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:19:31.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:19:31.361
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/28/23 00:19:31.372
    STEP: creating a new configmap 01/28/23 00:19:31.377
    STEP: modifying the configmap once 01/28/23 00:19:31.39
    STEP: closing the watch once it receives two notifications 01/28/23 00:19:31.414
    Jan 28 00:19:31.415: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4050  4d70b3b2-9148-4c23-9a58-d3606a9cb7f3 17330 0 2023-01-28 00:19:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-28 00:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 28 00:19:31.416: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4050  4d70b3b2-9148-4c23-9a58-d3606a9cb7f3 17331 0 2023-01-28 00:19:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-28 00:19:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/28/23 00:19:31.416
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/28/23 00:19:31.439
    STEP: deleting the configmap 01/28/23 00:19:31.445
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/28/23 00:19:31.466
    Jan 28 00:19:31.467: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4050  4d70b3b2-9148-4c23-9a58-d3606a9cb7f3 17332 0 2023-01-28 00:19:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-28 00:19:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 28 00:19:31.469: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4050  4d70b3b2-9148-4c23-9a58-d3606a9cb7f3 17333 0 2023-01-28 00:19:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-28 00:19:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 28 00:19:31.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-4050" for this suite. 01/28/23 00:19:31.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:19:31.511
Jan 28 00:19:31.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename var-expansion 01/28/23 00:19:31.514
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:19:31.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:19:31.563
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 01/28/23 00:19:31.576
STEP: waiting for pod running 01/28/23 00:19:31.597
Jan 28 00:19:31.598: INFO: Waiting up to 2m0s for pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d" in namespace "var-expansion-7598" to be "running"
Jan 28 00:19:31.615: INFO: Pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.472522ms
Jan 28 00:19:33.627: INFO: Pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d": Phase="Running", Reason="", readiness=true. Elapsed: 2.029398012s
Jan 28 00:19:33.627: INFO: Pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d" satisfied condition "running"
STEP: creating a file in subpath 01/28/23 00:19:33.627
Jan 28 00:19:33.639: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7598 PodName:var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:19:33.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 00:19:33.641: INFO: ExecWithOptions: Clientset creation
Jan 28 00:19:33.641: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-7598/pods/var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/28/23 00:19:33.881
Jan 28 00:19:33.894: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7598 PodName:var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:19:33.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 00:19:33.896: INFO: ExecWithOptions: Clientset creation
Jan 28 00:19:33.896: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-7598/pods/var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/28/23 00:19:34.118
Jan 28 00:19:34.653: INFO: Successfully updated pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d"
STEP: waiting for annotated pod running 01/28/23 00:19:34.653
Jan 28 00:19:34.653: INFO: Waiting up to 2m0s for pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d" in namespace "var-expansion-7598" to be "running"
Jan 28 00:19:34.663: INFO: Pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d": Phase="Running", Reason="", readiness=true. Elapsed: 9.617728ms
Jan 28 00:19:34.663: INFO: Pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d" satisfied condition "running"
STEP: deleting the pod gracefully 01/28/23 00:19:34.663
Jan 28 00:19:34.663: INFO: Deleting pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d" in namespace "var-expansion-7598"
Jan 28 00:19:34.684: INFO: Wait up to 5m0s for pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 28 00:20:08.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7598" for this suite. 01/28/23 00:20:08.725
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":9,"skipped":139,"failed":0}
------------------------------
â€¢ [SLOW TEST] [37.235 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:19:31.511
    Jan 28 00:19:31.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename var-expansion 01/28/23 00:19:31.514
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:19:31.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:19:31.563
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 01/28/23 00:19:31.576
    STEP: waiting for pod running 01/28/23 00:19:31.597
    Jan 28 00:19:31.598: INFO: Waiting up to 2m0s for pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d" in namespace "var-expansion-7598" to be "running"
    Jan 28 00:19:31.615: INFO: Pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.472522ms
    Jan 28 00:19:33.627: INFO: Pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d": Phase="Running", Reason="", readiness=true. Elapsed: 2.029398012s
    Jan 28 00:19:33.627: INFO: Pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d" satisfied condition "running"
    STEP: creating a file in subpath 01/28/23 00:19:33.627
    Jan 28 00:19:33.639: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7598 PodName:var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 00:19:33.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 00:19:33.641: INFO: ExecWithOptions: Clientset creation
    Jan 28 00:19:33.641: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-7598/pods/var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/28/23 00:19:33.881
    Jan 28 00:19:33.894: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7598 PodName:var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 00:19:33.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 00:19:33.896: INFO: ExecWithOptions: Clientset creation
    Jan 28 00:19:33.896: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-7598/pods/var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/28/23 00:19:34.118
    Jan 28 00:19:34.653: INFO: Successfully updated pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d"
    STEP: waiting for annotated pod running 01/28/23 00:19:34.653
    Jan 28 00:19:34.653: INFO: Waiting up to 2m0s for pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d" in namespace "var-expansion-7598" to be "running"
    Jan 28 00:19:34.663: INFO: Pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d": Phase="Running", Reason="", readiness=true. Elapsed: 9.617728ms
    Jan 28 00:19:34.663: INFO: Pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d" satisfied condition "running"
    STEP: deleting the pod gracefully 01/28/23 00:19:34.663
    Jan 28 00:19:34.663: INFO: Deleting pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d" in namespace "var-expansion-7598"
    Jan 28 00:19:34.684: INFO: Wait up to 5m0s for pod "var-expansion-f485ae8a-332b-4a60-91aa-23b72410403d" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 28 00:20:08.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7598" for this suite. 01/28/23 00:20:08.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:20:08.757
Jan 28 00:20:08.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename resourcequota 01/28/23 00:20:08.758
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:20:08.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:20:08.809
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 01/28/23 00:20:08.82
STEP: Creating a ResourceQuota 01/28/23 00:20:13.83
STEP: Ensuring resource quota status is calculated 01/28/23 00:20:13.843
STEP: Creating a ReplicaSet 01/28/23 00:20:15.855
STEP: Ensuring resource quota status captures replicaset creation 01/28/23 00:20:15.88
STEP: Deleting a ReplicaSet 01/28/23 00:20:17.893
STEP: Ensuring resource quota status released usage 01/28/23 00:20:17.913
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 28 00:20:19.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9046" for this suite. 01/28/23 00:20:19.944
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":10,"skipped":193,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.228 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:20:08.757
    Jan 28 00:20:08.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename resourcequota 01/28/23 00:20:08.758
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:20:08.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:20:08.809
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 01/28/23 00:20:08.82
    STEP: Creating a ResourceQuota 01/28/23 00:20:13.83
    STEP: Ensuring resource quota status is calculated 01/28/23 00:20:13.843
    STEP: Creating a ReplicaSet 01/28/23 00:20:15.855
    STEP: Ensuring resource quota status captures replicaset creation 01/28/23 00:20:15.88
    STEP: Deleting a ReplicaSet 01/28/23 00:20:17.893
    STEP: Ensuring resource quota status released usage 01/28/23 00:20:17.913
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 28 00:20:19.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9046" for this suite. 01/28/23 00:20:19.944
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:20:19.985
Jan 28 00:20:19.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename webhook 01/28/23 00:20:19.987
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:20:20.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:20:20.035
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/28/23 00:20:20.077
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 00:20:20.388
STEP: Deploying the webhook pod 01/28/23 00:20:20.411
STEP: Wait for the deployment to be ready 01/28/23 00:20:20.438
Jan 28 00:20:20.467: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 00:20:22.503: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/28/23 00:20:24.518
STEP: Verifying the service has paired with the endpoint 01/28/23 00:20:24.551
Jan 28 00:20:25.552: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 01/28/23 00:20:25.564
STEP: Creating a configMap that does not comply to the validation webhook rules 01/28/23 00:20:25.653
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/28/23 00:20:25.705
STEP: Creating a configMap that does not comply to the validation webhook rules 01/28/23 00:20:25.734
STEP: Patching a validating webhook configuration's rules to include the create operation 01/28/23 00:20:25.762
STEP: Creating a configMap that does not comply to the validation webhook rules 01/28/23 00:20:25.789
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 00:20:25.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6969" for this suite. 01/28/23 00:20:25.845
STEP: Destroying namespace "webhook-6969-markers" for this suite. 01/28/23 00:20:25.863
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":11,"skipped":193,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.023 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:20:19.985
    Jan 28 00:20:19.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename webhook 01/28/23 00:20:19.987
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:20:20.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:20:20.035
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/28/23 00:20:20.077
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 00:20:20.388
    STEP: Deploying the webhook pod 01/28/23 00:20:20.411
    STEP: Wait for the deployment to be ready 01/28/23 00:20:20.438
    Jan 28 00:20:20.467: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 28 00:20:22.503: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/28/23 00:20:24.518
    STEP: Verifying the service has paired with the endpoint 01/28/23 00:20:24.551
    Jan 28 00:20:25.552: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 01/28/23 00:20:25.564
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/28/23 00:20:25.653
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/28/23 00:20:25.705
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/28/23 00:20:25.734
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/28/23 00:20:25.762
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/28/23 00:20:25.789
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 00:20:25.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6969" for this suite. 01/28/23 00:20:25.845
    STEP: Destroying namespace "webhook-6969-markers" for this suite. 01/28/23 00:20:25.863
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:20:26.014
Jan 28 00:20:26.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename deployment 01/28/23 00:20:26.016
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:20:26.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:20:26.064
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan 28 00:20:26.105: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 28 00:20:31.122: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/28/23 00:20:31.123
Jan 28 00:20:31.123: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 28 00:20:33.137: INFO: Creating deployment "test-rollover-deployment"
Jan 28 00:20:33.171: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 28 00:20:35.198: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 28 00:20:35.221: INFO: Ensure that both replica sets have 1 created replica
Jan 28 00:20:35.243: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 28 00:20:35.275: INFO: Updating deployment test-rollover-deployment
Jan 28 00:20:35.275: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 28 00:20:37.305: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 28 00:20:37.328: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 28 00:20:37.353: INFO: all replica sets need to contain the pod-template-hash label
Jan 28 00:20:37.353: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 00:20:39.378: INFO: all replica sets need to contain the pod-template-hash label
Jan 28 00:20:39.378: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 00:20:41.379: INFO: all replica sets need to contain the pod-template-hash label
Jan 28 00:20:41.380: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 00:20:43.379: INFO: all replica sets need to contain the pod-template-hash label
Jan 28 00:20:43.379: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 00:20:45.378: INFO: all replica sets need to contain the pod-template-hash label
Jan 28 00:20:45.378: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 00:20:47.382: INFO: all replica sets need to contain the pod-template-hash label
Jan 28 00:20:47.383: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 00:20:49.378: INFO: 
Jan 28 00:20:49.379: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 28 00:20:49.413: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3705  b351e0b8-03a1-4b3c-8c80-88d64a28e795 17630 2 2023-01-28 00:20:33 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-28 00:20:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:20:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035fb928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-28 00:20:33 +0000 UTC,LastTransitionTime:2023-01-28 00:20:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-28 00:20:47 +0000 UTC,LastTransitionTime:2023-01-28 00:20:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 28 00:20:49.425: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-3705  58151b54-90cd-4af6-9e15-f9ac24f57167 17620 2 2023-01-28 00:20:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b351e0b8-03a1-4b3c-8c80-88d64a28e795 0xc0037c6087 0xc0037c6088}] [] [{kube-controller-manager Update apps/v1 2023-01-28 00:20:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b351e0b8-03a1-4b3c-8c80-88d64a28e795\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:20:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037c6138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 28 00:20:49.425: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 28 00:20:49.426: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3705  441aec7a-92ff-4905-b2d2-e1a6220779e7 17629 2 2023-01-28 00:20:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b351e0b8-03a1-4b3c-8c80-88d64a28e795 0xc003619e37 0xc003619e38}] [] [{e2e.test Update apps/v1 2023-01-28 00:20:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:20:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b351e0b8-03a1-4b3c-8c80-88d64a28e795\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:20:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003619ef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 28 00:20:49.426: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-3705  22800543-6f1a-46a3-9297-8bcf7ff2c154 17588 2 2023-01-28 00:20:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b351e0b8-03a1-4b3c-8c80-88d64a28e795 0xc003619f67 0xc003619f68}] [] [{kube-controller-manager Update apps/v1 2023-01-28 00:20:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b351e0b8-03a1-4b3c-8c80-88d64a28e795\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:20:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037c6018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 28 00:20:49.441: INFO: Pod "test-rollover-deployment-6d45fd857b-f5k4v" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-f5k4v test-rollover-deployment-6d45fd857b- deployment-3705  0718adfb-8797-46e7-8a22-23b33ff44477 17609 0 2023-01-28 00:20:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:9707752ae5f5eb4dbb23db500f844ae9a295c040400fabe1eaa3783ff7704dae cni.projectcalico.org/podIP:172.30.12.223/32 cni.projectcalico.org/podIPs:172.30.12.223/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 58151b54-90cd-4af6-9e15-f9ac24f57167 0xc0037c66b7 0xc0037c66b8}] [] [{kube-controller-manager Update v1 2023-01-28 00:20:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58151b54-90cd-4af6-9e15-f9ac24f57167\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 00:20:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 00:20:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ts6bv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ts6bv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:20:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:20:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:20:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:20:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:172.30.12.223,StartTime:2023-01-28 00:20:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 00:20:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://cc97a7a8fc2aa1f3a34df8875e60df73b10428332118a4879b34c946ef557971,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.12.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 28 00:20:49.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3705" for this suite. 01/28/23 00:20:49.46
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":12,"skipped":249,"failed":0}
------------------------------
â€¢ [SLOW TEST] [23.465 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:20:26.014
    Jan 28 00:20:26.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename deployment 01/28/23 00:20:26.016
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:20:26.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:20:26.064
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan 28 00:20:26.105: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan 28 00:20:31.122: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/28/23 00:20:31.123
    Jan 28 00:20:31.123: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan 28 00:20:33.137: INFO: Creating deployment "test-rollover-deployment"
    Jan 28 00:20:33.171: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan 28 00:20:35.198: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan 28 00:20:35.221: INFO: Ensure that both replica sets have 1 created replica
    Jan 28 00:20:35.243: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan 28 00:20:35.275: INFO: Updating deployment test-rollover-deployment
    Jan 28 00:20:35.275: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan 28 00:20:37.305: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan 28 00:20:37.328: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan 28 00:20:37.353: INFO: all replica sets need to contain the pod-template-hash label
    Jan 28 00:20:37.353: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 00:20:39.378: INFO: all replica sets need to contain the pod-template-hash label
    Jan 28 00:20:39.378: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 00:20:41.379: INFO: all replica sets need to contain the pod-template-hash label
    Jan 28 00:20:41.380: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 00:20:43.379: INFO: all replica sets need to contain the pod-template-hash label
    Jan 28 00:20:43.379: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 00:20:45.378: INFO: all replica sets need to contain the pod-template-hash label
    Jan 28 00:20:45.378: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 00:20:47.382: INFO: all replica sets need to contain the pod-template-hash label
    Jan 28 00:20:47.383: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 20, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 20, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 00:20:49.378: INFO: 
    Jan 28 00:20:49.379: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 28 00:20:49.413: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-3705  b351e0b8-03a1-4b3c-8c80-88d64a28e795 17630 2 2023-01-28 00:20:33 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-28 00:20:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:20:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035fb928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-28 00:20:33 +0000 UTC,LastTransitionTime:2023-01-28 00:20:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-28 00:20:47 +0000 UTC,LastTransitionTime:2023-01-28 00:20:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 28 00:20:49.425: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-3705  58151b54-90cd-4af6-9e15-f9ac24f57167 17620 2 2023-01-28 00:20:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b351e0b8-03a1-4b3c-8c80-88d64a28e795 0xc0037c6087 0xc0037c6088}] [] [{kube-controller-manager Update apps/v1 2023-01-28 00:20:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b351e0b8-03a1-4b3c-8c80-88d64a28e795\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:20:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037c6138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 28 00:20:49.425: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan 28 00:20:49.426: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3705  441aec7a-92ff-4905-b2d2-e1a6220779e7 17629 2 2023-01-28 00:20:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b351e0b8-03a1-4b3c-8c80-88d64a28e795 0xc003619e37 0xc003619e38}] [] [{e2e.test Update apps/v1 2023-01-28 00:20:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:20:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b351e0b8-03a1-4b3c-8c80-88d64a28e795\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:20:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003619ef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 28 00:20:49.426: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-3705  22800543-6f1a-46a3-9297-8bcf7ff2c154 17588 2 2023-01-28 00:20:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b351e0b8-03a1-4b3c-8c80-88d64a28e795 0xc003619f67 0xc003619f68}] [] [{kube-controller-manager Update apps/v1 2023-01-28 00:20:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b351e0b8-03a1-4b3c-8c80-88d64a28e795\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:20:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037c6018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 28 00:20:49.441: INFO: Pod "test-rollover-deployment-6d45fd857b-f5k4v" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-f5k4v test-rollover-deployment-6d45fd857b- deployment-3705  0718adfb-8797-46e7-8a22-23b33ff44477 17609 0 2023-01-28 00:20:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:9707752ae5f5eb4dbb23db500f844ae9a295c040400fabe1eaa3783ff7704dae cni.projectcalico.org/podIP:172.30.12.223/32 cni.projectcalico.org/podIPs:172.30.12.223/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 58151b54-90cd-4af6-9e15-f9ac24f57167 0xc0037c66b7 0xc0037c66b8}] [] [{kube-controller-manager Update v1 2023-01-28 00:20:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58151b54-90cd-4af6-9e15-f9ac24f57167\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 00:20:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 00:20:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ts6bv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ts6bv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:20:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:20:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:20:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:20:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:172.30.12.223,StartTime:2023-01-28 00:20:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 00:20:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://cc97a7a8fc2aa1f3a34df8875e60df73b10428332118a4879b34c946ef557971,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.12.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 28 00:20:49.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3705" for this suite. 01/28/23 00:20:49.46
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:20:49.484
Jan 28 00:20:49.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 00:20:49.486
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:20:49.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:20:49.535
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-345790f1-1559-4633-b38c-75c2a6b8c321 01/28/23 00:20:49.547
STEP: Creating a pod to test consume secrets 01/28/23 00:20:49.561
Jan 28 00:20:49.584: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-28375799-a87b-4216-ad81-0a75ae6e0bdc" in namespace "projected-9888" to be "Succeeded or Failed"
Jan 28 00:20:49.596: INFO: Pod "pod-projected-secrets-28375799-a87b-4216-ad81-0a75ae6e0bdc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.123124ms
Jan 28 00:20:51.610: INFO: Pod "pod-projected-secrets-28375799-a87b-4216-ad81-0a75ae6e0bdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025316361s
Jan 28 00:20:53.609: INFO: Pod "pod-projected-secrets-28375799-a87b-4216-ad81-0a75ae6e0bdc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024671721s
Jan 28 00:20:55.611: INFO: Pod "pod-projected-secrets-28375799-a87b-4216-ad81-0a75ae6e0bdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02653663s
STEP: Saw pod success 01/28/23 00:20:55.611
Jan 28 00:20:55.611: INFO: Pod "pod-projected-secrets-28375799-a87b-4216-ad81-0a75ae6e0bdc" satisfied condition "Succeeded or Failed"
Jan 28 00:20:55.623: INFO: Trying to get logs from node 10.9.20.72 pod pod-projected-secrets-28375799-a87b-4216-ad81-0a75ae6e0bdc container projected-secret-volume-test: <nil>
STEP: delete the pod 01/28/23 00:20:55.704
Jan 28 00:20:55.752: INFO: Waiting for pod pod-projected-secrets-28375799-a87b-4216-ad81-0a75ae6e0bdc to disappear
Jan 28 00:20:55.763: INFO: Pod pod-projected-secrets-28375799-a87b-4216-ad81-0a75ae6e0bdc no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 28 00:20:55.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9888" for this suite. 01/28/23 00:20:55.78
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":13,"skipped":252,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.315 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:20:49.484
    Jan 28 00:20:49.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 00:20:49.486
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:20:49.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:20:49.535
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-345790f1-1559-4633-b38c-75c2a6b8c321 01/28/23 00:20:49.547
    STEP: Creating a pod to test consume secrets 01/28/23 00:20:49.561
    Jan 28 00:20:49.584: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-28375799-a87b-4216-ad81-0a75ae6e0bdc" in namespace "projected-9888" to be "Succeeded or Failed"
    Jan 28 00:20:49.596: INFO: Pod "pod-projected-secrets-28375799-a87b-4216-ad81-0a75ae6e0bdc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.123124ms
    Jan 28 00:20:51.610: INFO: Pod "pod-projected-secrets-28375799-a87b-4216-ad81-0a75ae6e0bdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025316361s
    Jan 28 00:20:53.609: INFO: Pod "pod-projected-secrets-28375799-a87b-4216-ad81-0a75ae6e0bdc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024671721s
    Jan 28 00:20:55.611: INFO: Pod "pod-projected-secrets-28375799-a87b-4216-ad81-0a75ae6e0bdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02653663s
    STEP: Saw pod success 01/28/23 00:20:55.611
    Jan 28 00:20:55.611: INFO: Pod "pod-projected-secrets-28375799-a87b-4216-ad81-0a75ae6e0bdc" satisfied condition "Succeeded or Failed"
    Jan 28 00:20:55.623: INFO: Trying to get logs from node 10.9.20.72 pod pod-projected-secrets-28375799-a87b-4216-ad81-0a75ae6e0bdc container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/28/23 00:20:55.704
    Jan 28 00:20:55.752: INFO: Waiting for pod pod-projected-secrets-28375799-a87b-4216-ad81-0a75ae6e0bdc to disappear
    Jan 28 00:20:55.763: INFO: Pod pod-projected-secrets-28375799-a87b-4216-ad81-0a75ae6e0bdc no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 28 00:20:55.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9888" for this suite. 01/28/23 00:20:55.78
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:20:55.805
Jan 28 00:20:55.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename services 01/28/23 00:20:55.808
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:20:55.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:20:55.851
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-6905 01/28/23 00:20:55.861
Jan 28 00:20:55.881: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6905" to be "running and ready"
Jan 28 00:20:55.893: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 11.994357ms
Jan 28 00:20:55.893: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:20:57.908: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.026869439s
Jan 28 00:20:57.908: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 28 00:20:57.908: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan 28 00:20:57.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6905 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 28 00:20:58.294: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 28 00:20:58.294: INFO: stdout: "iptables"
Jan 28 00:20:58.294: INFO: proxyMode: iptables
Jan 28 00:20:58.321: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 28 00:20:58.331: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-6905 01/28/23 00:20:58.331
STEP: creating replication controller affinity-clusterip-timeout in namespace services-6905 01/28/23 00:20:58.363
I0128 00:20:58.379545      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-6905, replica count: 3
I0128 00:21:01.430468      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0128 00:21:04.431547      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0128 00:21:07.433287      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0128 00:21:10.434332      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 00:21:10.455: INFO: Creating new exec pod
Jan 28 00:21:10.468: INFO: Waiting up to 5m0s for pod "execpod-affinitysn57l" in namespace "services-6905" to be "running"
Jan 28 00:21:10.479: INFO: Pod "execpod-affinitysn57l": Phase="Pending", Reason="", readiness=false. Elapsed: 10.78656ms
Jan 28 00:21:12.492: INFO: Pod "execpod-affinitysn57l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023688533s
Jan 28 00:21:14.491: INFO: Pod "execpod-affinitysn57l": Phase="Running", Reason="", readiness=true. Elapsed: 4.022953634s
Jan 28 00:21:14.491: INFO: Pod "execpod-affinitysn57l" satisfied condition "running"
Jan 28 00:21:15.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6905 exec execpod-affinitysn57l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan 28 00:21:15.868: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-timeout 80\n+ echo hostName\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan 28 00:21:15.868: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 00:21:15.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6905 exec execpod-affinitysn57l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.89.92 80'
Jan 28 00:21:16.240: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.89.92 80\nConnection to 172.21.89.92 80 port [tcp/http] succeeded!\n"
Jan 28 00:21:16.240: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 00:21:16.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6905 exec execpod-affinitysn57l -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.89.92:80/ ; done'
Jan 28 00:21:16.668: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n"
Jan 28 00:21:16.668: INFO: stdout: "\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6"
Jan 28 00:21:16.668: INFO: Received response from host: affinity-clusterip-timeout-h65s6
Jan 28 00:21:16.668: INFO: Received response from host: affinity-clusterip-timeout-h65s6
Jan 28 00:21:16.668: INFO: Received response from host: affinity-clusterip-timeout-h65s6
Jan 28 00:21:16.668: INFO: Received response from host: affinity-clusterip-timeout-h65s6
Jan 28 00:21:16.668: INFO: Received response from host: affinity-clusterip-timeout-h65s6
Jan 28 00:21:16.668: INFO: Received response from host: affinity-clusterip-timeout-h65s6
Jan 28 00:21:16.668: INFO: Received response from host: affinity-clusterip-timeout-h65s6
Jan 28 00:21:16.669: INFO: Received response from host: affinity-clusterip-timeout-h65s6
Jan 28 00:21:16.669: INFO: Received response from host: affinity-clusterip-timeout-h65s6
Jan 28 00:21:16.669: INFO: Received response from host: affinity-clusterip-timeout-h65s6
Jan 28 00:21:16.669: INFO: Received response from host: affinity-clusterip-timeout-h65s6
Jan 28 00:21:16.669: INFO: Received response from host: affinity-clusterip-timeout-h65s6
Jan 28 00:21:16.669: INFO: Received response from host: affinity-clusterip-timeout-h65s6
Jan 28 00:21:16.669: INFO: Received response from host: affinity-clusterip-timeout-h65s6
Jan 28 00:21:16.669: INFO: Received response from host: affinity-clusterip-timeout-h65s6
Jan 28 00:21:16.669: INFO: Received response from host: affinity-clusterip-timeout-h65s6
Jan 28 00:21:16.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6905 exec execpod-affinitysn57l -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.89.92:80/'
Jan 28 00:21:17.028: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n"
Jan 28 00:21:17.028: INFO: stdout: "affinity-clusterip-timeout-h65s6"
Jan 28 00:21:37.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6905 exec execpod-affinitysn57l -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.89.92:80/'
Jan 28 00:21:37.383: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n"
Jan 28 00:21:37.383: INFO: stdout: "affinity-clusterip-timeout-2h5kq"
Jan 28 00:21:37.383: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-6905, will wait for the garbage collector to delete the pods 01/28/23 00:21:37.417
Jan 28 00:21:37.496: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 16.715385ms
Jan 28 00:21:37.596: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.161561ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 28 00:21:40.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6905" for this suite. 01/28/23 00:21:40.48
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":14,"skipped":268,"failed":0}
------------------------------
â€¢ [SLOW TEST] [44.691 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:20:55.805
    Jan 28 00:20:55.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename services 01/28/23 00:20:55.808
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:20:55.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:20:55.851
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-6905 01/28/23 00:20:55.861
    Jan 28 00:20:55.881: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6905" to be "running and ready"
    Jan 28 00:20:55.893: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 11.994357ms
    Jan 28 00:20:55.893: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:20:57.908: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.026869439s
    Jan 28 00:20:57.908: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan 28 00:20:57.908: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan 28 00:20:57.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6905 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan 28 00:20:58.294: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan 28 00:20:58.294: INFO: stdout: "iptables"
    Jan 28 00:20:58.294: INFO: proxyMode: iptables
    Jan 28 00:20:58.321: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan 28 00:20:58.331: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-6905 01/28/23 00:20:58.331
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-6905 01/28/23 00:20:58.363
    I0128 00:20:58.379545      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-6905, replica count: 3
    I0128 00:21:01.430468      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0128 00:21:04.431547      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0128 00:21:07.433287      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0128 00:21:10.434332      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 28 00:21:10.455: INFO: Creating new exec pod
    Jan 28 00:21:10.468: INFO: Waiting up to 5m0s for pod "execpod-affinitysn57l" in namespace "services-6905" to be "running"
    Jan 28 00:21:10.479: INFO: Pod "execpod-affinitysn57l": Phase="Pending", Reason="", readiness=false. Elapsed: 10.78656ms
    Jan 28 00:21:12.492: INFO: Pod "execpod-affinitysn57l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023688533s
    Jan 28 00:21:14.491: INFO: Pod "execpod-affinitysn57l": Phase="Running", Reason="", readiness=true. Elapsed: 4.022953634s
    Jan 28 00:21:14.491: INFO: Pod "execpod-affinitysn57l" satisfied condition "running"
    Jan 28 00:21:15.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6905 exec execpod-affinitysn57l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Jan 28 00:21:15.868: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-timeout 80\n+ echo hostName\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Jan 28 00:21:15.868: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 00:21:15.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6905 exec execpod-affinitysn57l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.89.92 80'
    Jan 28 00:21:16.240: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.89.92 80\nConnection to 172.21.89.92 80 port [tcp/http] succeeded!\n"
    Jan 28 00:21:16.240: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 00:21:16.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6905 exec execpod-affinitysn57l -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.89.92:80/ ; done'
    Jan 28 00:21:16.668: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n"
    Jan 28 00:21:16.668: INFO: stdout: "\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6\naffinity-clusterip-timeout-h65s6"
    Jan 28 00:21:16.668: INFO: Received response from host: affinity-clusterip-timeout-h65s6
    Jan 28 00:21:16.668: INFO: Received response from host: affinity-clusterip-timeout-h65s6
    Jan 28 00:21:16.668: INFO: Received response from host: affinity-clusterip-timeout-h65s6
    Jan 28 00:21:16.668: INFO: Received response from host: affinity-clusterip-timeout-h65s6
    Jan 28 00:21:16.668: INFO: Received response from host: affinity-clusterip-timeout-h65s6
    Jan 28 00:21:16.668: INFO: Received response from host: affinity-clusterip-timeout-h65s6
    Jan 28 00:21:16.668: INFO: Received response from host: affinity-clusterip-timeout-h65s6
    Jan 28 00:21:16.669: INFO: Received response from host: affinity-clusterip-timeout-h65s6
    Jan 28 00:21:16.669: INFO: Received response from host: affinity-clusterip-timeout-h65s6
    Jan 28 00:21:16.669: INFO: Received response from host: affinity-clusterip-timeout-h65s6
    Jan 28 00:21:16.669: INFO: Received response from host: affinity-clusterip-timeout-h65s6
    Jan 28 00:21:16.669: INFO: Received response from host: affinity-clusterip-timeout-h65s6
    Jan 28 00:21:16.669: INFO: Received response from host: affinity-clusterip-timeout-h65s6
    Jan 28 00:21:16.669: INFO: Received response from host: affinity-clusterip-timeout-h65s6
    Jan 28 00:21:16.669: INFO: Received response from host: affinity-clusterip-timeout-h65s6
    Jan 28 00:21:16.669: INFO: Received response from host: affinity-clusterip-timeout-h65s6
    Jan 28 00:21:16.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6905 exec execpod-affinitysn57l -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.89.92:80/'
    Jan 28 00:21:17.028: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n"
    Jan 28 00:21:17.028: INFO: stdout: "affinity-clusterip-timeout-h65s6"
    Jan 28 00:21:37.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6905 exec execpod-affinitysn57l -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.89.92:80/'
    Jan 28 00:21:37.383: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.89.92:80/\n"
    Jan 28 00:21:37.383: INFO: stdout: "affinity-clusterip-timeout-2h5kq"
    Jan 28 00:21:37.383: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-6905, will wait for the garbage collector to delete the pods 01/28/23 00:21:37.417
    Jan 28 00:21:37.496: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 16.715385ms
    Jan 28 00:21:37.596: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.161561ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 28 00:21:40.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6905" for this suite. 01/28/23 00:21:40.48
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:21:40.502
Jan 28 00:21:40.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename replication-controller 01/28/23 00:21:40.505
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:21:40.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:21:40.551
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 01/28/23 00:21:40.575
STEP: waiting for RC to be added 01/28/23 00:21:40.594
STEP: waiting for available Replicas 01/28/23 00:21:40.595
STEP: patching ReplicationController 01/28/23 00:21:53.387
STEP: waiting for RC to be modified 01/28/23 00:21:53.404
STEP: patching ReplicationController status 01/28/23 00:21:53.404
STEP: waiting for RC to be modified 01/28/23 00:21:53.418
STEP: waiting for available Replicas 01/28/23 00:21:53.419
STEP: fetching ReplicationController status 01/28/23 00:21:53.43
STEP: patching ReplicationController scale 01/28/23 00:21:53.442
STEP: waiting for RC to be modified 01/28/23 00:21:53.458
STEP: waiting for ReplicationController's scale to be the max amount 01/28/23 00:21:53.458
STEP: fetching ReplicationController; ensuring that it's patched 01/28/23 00:22:06.224
STEP: updating ReplicationController status 01/28/23 00:22:06.235
STEP: waiting for RC to be modified 01/28/23 00:22:06.249
STEP: listing all ReplicationControllers 01/28/23 00:22:06.25
STEP: checking that ReplicationController has expected values 01/28/23 00:22:06.26
STEP: deleting ReplicationControllers by collection 01/28/23 00:22:06.261
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/28/23 00:22:06.288
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 28 00:22:06.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1949" for this suite. 01/28/23 00:22:06.539
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":15,"skipped":297,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.056 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:21:40.502
    Jan 28 00:21:40.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename replication-controller 01/28/23 00:21:40.505
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:21:40.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:21:40.551
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 01/28/23 00:21:40.575
    STEP: waiting for RC to be added 01/28/23 00:21:40.594
    STEP: waiting for available Replicas 01/28/23 00:21:40.595
    STEP: patching ReplicationController 01/28/23 00:21:53.387
    STEP: waiting for RC to be modified 01/28/23 00:21:53.404
    STEP: patching ReplicationController status 01/28/23 00:21:53.404
    STEP: waiting for RC to be modified 01/28/23 00:21:53.418
    STEP: waiting for available Replicas 01/28/23 00:21:53.419
    STEP: fetching ReplicationController status 01/28/23 00:21:53.43
    STEP: patching ReplicationController scale 01/28/23 00:21:53.442
    STEP: waiting for RC to be modified 01/28/23 00:21:53.458
    STEP: waiting for ReplicationController's scale to be the max amount 01/28/23 00:21:53.458
    STEP: fetching ReplicationController; ensuring that it's patched 01/28/23 00:22:06.224
    STEP: updating ReplicationController status 01/28/23 00:22:06.235
    STEP: waiting for RC to be modified 01/28/23 00:22:06.249
    STEP: listing all ReplicationControllers 01/28/23 00:22:06.25
    STEP: checking that ReplicationController has expected values 01/28/23 00:22:06.26
    STEP: deleting ReplicationControllers by collection 01/28/23 00:22:06.261
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/28/23 00:22:06.288
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 28 00:22:06.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1949" for this suite. 01/28/23 00:22:06.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:22:06.574
Jan 28 00:22:06.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename container-lifecycle-hook 01/28/23 00:22:06.577
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:22:06.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:22:06.636
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/28/23 00:22:06.694
Jan 28 00:22:06.716: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6919" to be "running and ready"
Jan 28 00:22:06.730: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 13.621018ms
Jan 28 00:22:06.730: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:22:08.742: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.026359294s
Jan 28 00:22:08.742: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 28 00:22:08.742: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 01/28/23 00:22:08.752
Jan 28 00:22:08.765: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6919" to be "running and ready"
Jan 28 00:22:08.775: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.001803ms
Jan 28 00:22:08.775: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:22:10.788: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.02300213s
Jan 28 00:22:10.789: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan 28 00:22:10.789: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/28/23 00:22:10.801
STEP: delete the pod with lifecycle hook 01/28/23 00:22:10.83
Jan 28 00:22:10.853: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 28 00:22:10.863: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 28 00:22:12.864: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 28 00:22:12.874: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 28 00:22:14.865: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 28 00:22:14.877: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 28 00:22:14.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6919" for this suite. 01/28/23 00:22:14.896
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":16,"skipped":314,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.342 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:22:06.574
    Jan 28 00:22:06.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/28/23 00:22:06.577
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:22:06.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:22:06.636
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/28/23 00:22:06.694
    Jan 28 00:22:06.716: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6919" to be "running and ready"
    Jan 28 00:22:06.730: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 13.621018ms
    Jan 28 00:22:06.730: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:22:08.742: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.026359294s
    Jan 28 00:22:08.742: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 28 00:22:08.742: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 01/28/23 00:22:08.752
    Jan 28 00:22:08.765: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6919" to be "running and ready"
    Jan 28 00:22:08.775: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.001803ms
    Jan 28 00:22:08.775: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:22:10.788: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.02300213s
    Jan 28 00:22:10.789: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan 28 00:22:10.789: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/28/23 00:22:10.801
    STEP: delete the pod with lifecycle hook 01/28/23 00:22:10.83
    Jan 28 00:22:10.853: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 28 00:22:10.863: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 28 00:22:12.864: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 28 00:22:12.874: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 28 00:22:14.865: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 28 00:22:14.877: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 28 00:22:14.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6919" for this suite. 01/28/23 00:22:14.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:22:14.935
Jan 28 00:22:14.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename job 01/28/23 00:22:14.936
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:22:14.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:22:14.982
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 01/28/23 00:22:14.992
STEP: Ensuring active pods == parallelism 01/28/23 00:22:15.007
STEP: Orphaning one of the Job's Pods 01/28/23 00:22:25.019
Jan 28 00:22:25.558: INFO: Successfully updated pod "adopt-release-6qqh4"
STEP: Checking that the Job readopts the Pod 01/28/23 00:22:25.558
Jan 28 00:22:25.559: INFO: Waiting up to 15m0s for pod "adopt-release-6qqh4" in namespace "job-52" to be "adopted"
Jan 28 00:22:25.570: INFO: Pod "adopt-release-6qqh4": Phase="Running", Reason="", readiness=true. Elapsed: 10.592706ms
Jan 28 00:22:27.584: INFO: Pod "adopt-release-6qqh4": Phase="Running", Reason="", readiness=true. Elapsed: 2.024521428s
Jan 28 00:22:27.584: INFO: Pod "adopt-release-6qqh4" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/28/23 00:22:27.584
Jan 28 00:22:28.121: INFO: Successfully updated pod "adopt-release-6qqh4"
STEP: Checking that the Job releases the Pod 01/28/23 00:22:28.122
Jan 28 00:22:28.123: INFO: Waiting up to 15m0s for pod "adopt-release-6qqh4" in namespace "job-52" to be "released"
Jan 28 00:22:28.134: INFO: Pod "adopt-release-6qqh4": Phase="Running", Reason="", readiness=true. Elapsed: 11.073835ms
Jan 28 00:22:30.146: INFO: Pod "adopt-release-6qqh4": Phase="Running", Reason="", readiness=true. Elapsed: 2.023289723s
Jan 28 00:22:30.146: INFO: Pod "adopt-release-6qqh4" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 28 00:22:30.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-52" for this suite. 01/28/23 00:22:30.165
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":17,"skipped":359,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.249 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:22:14.935
    Jan 28 00:22:14.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename job 01/28/23 00:22:14.936
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:22:14.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:22:14.982
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 01/28/23 00:22:14.992
    STEP: Ensuring active pods == parallelism 01/28/23 00:22:15.007
    STEP: Orphaning one of the Job's Pods 01/28/23 00:22:25.019
    Jan 28 00:22:25.558: INFO: Successfully updated pod "adopt-release-6qqh4"
    STEP: Checking that the Job readopts the Pod 01/28/23 00:22:25.558
    Jan 28 00:22:25.559: INFO: Waiting up to 15m0s for pod "adopt-release-6qqh4" in namespace "job-52" to be "adopted"
    Jan 28 00:22:25.570: INFO: Pod "adopt-release-6qqh4": Phase="Running", Reason="", readiness=true. Elapsed: 10.592706ms
    Jan 28 00:22:27.584: INFO: Pod "adopt-release-6qqh4": Phase="Running", Reason="", readiness=true. Elapsed: 2.024521428s
    Jan 28 00:22:27.584: INFO: Pod "adopt-release-6qqh4" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/28/23 00:22:27.584
    Jan 28 00:22:28.121: INFO: Successfully updated pod "adopt-release-6qqh4"
    STEP: Checking that the Job releases the Pod 01/28/23 00:22:28.122
    Jan 28 00:22:28.123: INFO: Waiting up to 15m0s for pod "adopt-release-6qqh4" in namespace "job-52" to be "released"
    Jan 28 00:22:28.134: INFO: Pod "adopt-release-6qqh4": Phase="Running", Reason="", readiness=true. Elapsed: 11.073835ms
    Jan 28 00:22:30.146: INFO: Pod "adopt-release-6qqh4": Phase="Running", Reason="", readiness=true. Elapsed: 2.023289723s
    Jan 28 00:22:30.146: INFO: Pod "adopt-release-6qqh4" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 28 00:22:30.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-52" for this suite. 01/28/23 00:22:30.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:22:30.192
Jan 28 00:22:30.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename webhook 01/28/23 00:22:30.197
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:22:30.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:22:30.242
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/28/23 00:22:30.285
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 00:22:30.891
STEP: Deploying the webhook pod 01/28/23 00:22:30.912
STEP: Wait for the deployment to be ready 01/28/23 00:22:30.94
Jan 28 00:22:30.966: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 00:22:33.004: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 22, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 22, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 22, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 22, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/28/23 00:22:35.025
STEP: Verifying the service has paired with the endpoint 01/28/23 00:22:35.056
Jan 28 00:22:36.057: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Jan 28 00:22:36.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-688-crds.webhook.example.com via the AdmissionRegistration API 01/28/23 00:22:36.595
STEP: Creating a custom resource that should be mutated by the webhook 01/28/23 00:22:36.727
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 00:22:39.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7118" for this suite. 01/28/23 00:22:39.44
STEP: Destroying namespace "webhook-7118-markers" for this suite. 01/28/23 00:22:39.472
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":18,"skipped":401,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.408 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:22:30.192
    Jan 28 00:22:30.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename webhook 01/28/23 00:22:30.197
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:22:30.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:22:30.242
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/28/23 00:22:30.285
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 00:22:30.891
    STEP: Deploying the webhook pod 01/28/23 00:22:30.912
    STEP: Wait for the deployment to be ready 01/28/23 00:22:30.94
    Jan 28 00:22:30.966: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 28 00:22:33.004: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 22, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 22, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 22, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 22, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/28/23 00:22:35.025
    STEP: Verifying the service has paired with the endpoint 01/28/23 00:22:35.056
    Jan 28 00:22:36.057: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Jan 28 00:22:36.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-688-crds.webhook.example.com via the AdmissionRegistration API 01/28/23 00:22:36.595
    STEP: Creating a custom resource that should be mutated by the webhook 01/28/23 00:22:36.727
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 00:22:39.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7118" for this suite. 01/28/23 00:22:39.44
    STEP: Destroying namespace "webhook-7118-markers" for this suite. 01/28/23 00:22:39.472
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:22:39.611
Jan 28 00:22:39.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename daemonsets 01/28/23 00:22:39.613
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:22:39.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:22:39.653
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 01/28/23 00:22:39.733
STEP: Check that daemon pods launch on every node of the cluster. 01/28/23 00:22:39.748
Jan 28 00:22:39.777: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:22:39.777: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 00:22:40.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:22:40.807: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 00:22:41.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:22:41.809: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
Jan 28 00:22:42.806: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:22:42.806: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
Jan 28 00:22:43.811: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:22:43.811: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
Jan 28 00:22:44.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:22:44.808: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
Jan 28 00:22:45.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:22:45.809: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
Jan 28 00:22:46.806: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:22:46.807: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
Jan 28 00:22:47.810: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:22:47.810: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
Jan 28 00:22:48.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:22:48.809: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
Jan 28 00:22:49.839: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:22:49.839: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
Jan 28 00:22:50.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:22:50.807: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
Jan 28 00:22:51.808: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 28 00:22:51.809: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 01/28/23 00:22:51.821
STEP: DeleteCollection of the DaemonSets 01/28/23 00:22:51.838
STEP: Verify that ReplicaSets have been deleted 01/28/23 00:22:51.868
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jan 28 00:22:51.930: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18326"},"items":null}

Jan 28 00:22:51.944: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18326"},"items":[{"metadata":{"name":"daemon-set-4tk62","generateName":"daemon-set-","namespace":"daemonsets-5171","uid":"8e4f72df-e3e9-436a-bdd8-de097e84b0ad","resourceVersion":"18323","creationTimestamp":"2023-01-28T00:22:39Z","deletionTimestamp":"2023-01-28T00:23:21Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"625dcaad64dacf1845ec9b4a6ce1b01a9c9e693a492ec9658add08391f1b3535","cni.projectcalico.org/podIP":"172.30.84.53/32","cni.projectcalico.org/podIPs":"172.30.84.53/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"725e9b8b-3578-4fd9-9358-06073f77a96c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:22:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"725e9b8b-3578-4fd9-9358-06073f77a96c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:22:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:22:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.84.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-tr8gd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-tr8gd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.9.20.75","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.9.20.75"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:50Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:50Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:39Z"}],"hostIP":"10.9.20.75","podIP":"172.30.84.53","podIPs":[{"ip":"172.30.84.53"}],"startTime":"2023-01-28T00:22:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-28T00:22:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://3d2c8efaf9ab311baf2c22463f760c19ef3f419ff6653e2014d5bba206447d3b","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-m48c4","generateName":"daemon-set-","namespace":"daemonsets-5171","uid":"ee4c839b-0cc6-4a4c-a5d8-05b3dfcfd97e","resourceVersion":"18321","creationTimestamp":"2023-01-28T00:22:39Z","deletionTimestamp":"2023-01-28T00:23:21Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"5db6d455240139e7e468272d103b7a7ff89ecc64d828187f2f8f15b0f872fdcd","cni.projectcalico.org/podIP":"172.30.185.60/32","cni.projectcalico.org/podIPs":"172.30.185.60/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"725e9b8b-3578-4fd9-9358-06073f77a96c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:22:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"725e9b8b-3578-4fd9-9358-06073f77a96c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:22:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:22:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.185.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-x7nf9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-x7nf9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.9.20.72","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.9.20.72"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:39Z"}],"hostIP":"10.9.20.72","podIP":"172.30.185.60","podIPs":[{"ip":"172.30.185.60"}],"startTime":"2023-01-28T00:22:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-28T00:22:41Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://16f21736e7558bba670f299a7d5f32c6ee85d95eacb91694d54afef161aabcd6","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-w7669","generateName":"daemon-set-","namespace":"daemonsets-5171","uid":"75dc8bee-6c73-4fd6-99af-8e5495ccbbe3","resourceVersion":"18324","creationTimestamp":"2023-01-28T00:22:39Z","deletionTimestamp":"2023-01-28T00:23:21Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d7cfb6bf0d4b58440bd30fe89e6e55d3a49c49f7ed14921575e413e5c756a733","cni.projectcalico.org/podIP":"172.30.12.228/32","cni.projectcalico.org/podIPs":"172.30.12.228/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"725e9b8b-3578-4fd9-9358-06073f77a96c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:22:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"725e9b8b-3578-4fd9-9358-06073f77a96c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:22:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:22:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-skhk4","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-skhk4","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.9.20.126","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.9.20.126"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:39Z"}],"hostIP":"10.9.20.126","podIP":"172.30.12.228","podIPs":[{"ip":"172.30.12.228"}],"startTime":"2023-01-28T00:22:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-28T00:22:41Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://da36a12b190748e43cfe125969c1d7143b8349aa228383dfbc69f3788f420992","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 28 00:22:52.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5171" for this suite. 01/28/23 00:22:52.025
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":19,"skipped":420,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.434 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:22:39.611
    Jan 28 00:22:39.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename daemonsets 01/28/23 00:22:39.613
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:22:39.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:22:39.653
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 01/28/23 00:22:39.733
    STEP: Check that daemon pods launch on every node of the cluster. 01/28/23 00:22:39.748
    Jan 28 00:22:39.777: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 00:22:39.777: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 00:22:40.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 00:22:40.807: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 00:22:41.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 28 00:22:41.809: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
    Jan 28 00:22:42.806: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 28 00:22:42.806: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
    Jan 28 00:22:43.811: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 28 00:22:43.811: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
    Jan 28 00:22:44.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 28 00:22:44.808: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
    Jan 28 00:22:45.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 28 00:22:45.809: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
    Jan 28 00:22:46.806: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 28 00:22:46.807: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
    Jan 28 00:22:47.810: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 28 00:22:47.810: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
    Jan 28 00:22:48.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 28 00:22:48.809: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
    Jan 28 00:22:49.839: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 28 00:22:49.839: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
    Jan 28 00:22:50.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 28 00:22:50.807: INFO: Node 10.9.20.75 is running 0 daemon pod, expected 1
    Jan 28 00:22:51.808: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 28 00:22:51.809: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 01/28/23 00:22:51.821
    STEP: DeleteCollection of the DaemonSets 01/28/23 00:22:51.838
    STEP: Verify that ReplicaSets have been deleted 01/28/23 00:22:51.868
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Jan 28 00:22:51.930: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18326"},"items":null}

    Jan 28 00:22:51.944: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18326"},"items":[{"metadata":{"name":"daemon-set-4tk62","generateName":"daemon-set-","namespace":"daemonsets-5171","uid":"8e4f72df-e3e9-436a-bdd8-de097e84b0ad","resourceVersion":"18323","creationTimestamp":"2023-01-28T00:22:39Z","deletionTimestamp":"2023-01-28T00:23:21Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"625dcaad64dacf1845ec9b4a6ce1b01a9c9e693a492ec9658add08391f1b3535","cni.projectcalico.org/podIP":"172.30.84.53/32","cni.projectcalico.org/podIPs":"172.30.84.53/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"725e9b8b-3578-4fd9-9358-06073f77a96c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:22:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"725e9b8b-3578-4fd9-9358-06073f77a96c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:22:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:22:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.84.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-tr8gd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-tr8gd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.9.20.75","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.9.20.75"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:50Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:50Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:39Z"}],"hostIP":"10.9.20.75","podIP":"172.30.84.53","podIPs":[{"ip":"172.30.84.53"}],"startTime":"2023-01-28T00:22:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-28T00:22:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://3d2c8efaf9ab311baf2c22463f760c19ef3f419ff6653e2014d5bba206447d3b","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-m48c4","generateName":"daemon-set-","namespace":"daemonsets-5171","uid":"ee4c839b-0cc6-4a4c-a5d8-05b3dfcfd97e","resourceVersion":"18321","creationTimestamp":"2023-01-28T00:22:39Z","deletionTimestamp":"2023-01-28T00:23:21Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"5db6d455240139e7e468272d103b7a7ff89ecc64d828187f2f8f15b0f872fdcd","cni.projectcalico.org/podIP":"172.30.185.60/32","cni.projectcalico.org/podIPs":"172.30.185.60/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"725e9b8b-3578-4fd9-9358-06073f77a96c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:22:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"725e9b8b-3578-4fd9-9358-06073f77a96c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:22:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:22:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.185.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-x7nf9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-x7nf9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.9.20.72","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.9.20.72"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:39Z"}],"hostIP":"10.9.20.72","podIP":"172.30.185.60","podIPs":[{"ip":"172.30.185.60"}],"startTime":"2023-01-28T00:22:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-28T00:22:41Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://16f21736e7558bba670f299a7d5f32c6ee85d95eacb91694d54afef161aabcd6","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-w7669","generateName":"daemon-set-","namespace":"daemonsets-5171","uid":"75dc8bee-6c73-4fd6-99af-8e5495ccbbe3","resourceVersion":"18324","creationTimestamp":"2023-01-28T00:22:39Z","deletionTimestamp":"2023-01-28T00:23:21Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d7cfb6bf0d4b58440bd30fe89e6e55d3a49c49f7ed14921575e413e5c756a733","cni.projectcalico.org/podIP":"172.30.12.228/32","cni.projectcalico.org/podIPs":"172.30.12.228/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"725e9b8b-3578-4fd9-9358-06073f77a96c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:22:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"725e9b8b-3578-4fd9-9358-06073f77a96c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:22:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:22:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-skhk4","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-skhk4","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.9.20.126","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.9.20.126"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:22:39Z"}],"hostIP":"10.9.20.126","podIP":"172.30.12.228","podIPs":[{"ip":"172.30.12.228"}],"startTime":"2023-01-28T00:22:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-28T00:22:41Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://da36a12b190748e43cfe125969c1d7143b8349aa228383dfbc69f3788f420992","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 00:22:52.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5171" for this suite. 01/28/23 00:22:52.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:22:52.047
Jan 28 00:22:52.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename emptydir 01/28/23 00:22:52.048
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:22:52.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:22:52.091
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 01/28/23 00:22:52.104
Jan 28 00:22:52.125: INFO: Waiting up to 5m0s for pod "pod-83a0bfb2-9655-4c1a-a5f7-4217b11e93a5" in namespace "emptydir-7731" to be "Succeeded or Failed"
Jan 28 00:22:52.137: INFO: Pod "pod-83a0bfb2-9655-4c1a-a5f7-4217b11e93a5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.152356ms
Jan 28 00:22:54.152: INFO: Pod "pod-83a0bfb2-9655-4c1a-a5f7-4217b11e93a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026696934s
Jan 28 00:22:56.152: INFO: Pod "pod-83a0bfb2-9655-4c1a-a5f7-4217b11e93a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027108828s
Jan 28 00:22:58.150: INFO: Pod "pod-83a0bfb2-9655-4c1a-a5f7-4217b11e93a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02504995s
STEP: Saw pod success 01/28/23 00:22:58.15
Jan 28 00:22:58.151: INFO: Pod "pod-83a0bfb2-9655-4c1a-a5f7-4217b11e93a5" satisfied condition "Succeeded or Failed"
Jan 28 00:22:58.163: INFO: Trying to get logs from node 10.9.20.126 pod pod-83a0bfb2-9655-4c1a-a5f7-4217b11e93a5 container test-container: <nil>
STEP: delete the pod 01/28/23 00:22:58.245
Jan 28 00:22:58.276: INFO: Waiting for pod pod-83a0bfb2-9655-4c1a-a5f7-4217b11e93a5 to disappear
Jan 28 00:22:58.286: INFO: Pod pod-83a0bfb2-9655-4c1a-a5f7-4217b11e93a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 28 00:22:58.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7731" for this suite. 01/28/23 00:22:58.304
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":20,"skipped":438,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.275 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:22:52.047
    Jan 28 00:22:52.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename emptydir 01/28/23 00:22:52.048
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:22:52.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:22:52.091
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/28/23 00:22:52.104
    Jan 28 00:22:52.125: INFO: Waiting up to 5m0s for pod "pod-83a0bfb2-9655-4c1a-a5f7-4217b11e93a5" in namespace "emptydir-7731" to be "Succeeded or Failed"
    Jan 28 00:22:52.137: INFO: Pod "pod-83a0bfb2-9655-4c1a-a5f7-4217b11e93a5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.152356ms
    Jan 28 00:22:54.152: INFO: Pod "pod-83a0bfb2-9655-4c1a-a5f7-4217b11e93a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026696934s
    Jan 28 00:22:56.152: INFO: Pod "pod-83a0bfb2-9655-4c1a-a5f7-4217b11e93a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027108828s
    Jan 28 00:22:58.150: INFO: Pod "pod-83a0bfb2-9655-4c1a-a5f7-4217b11e93a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02504995s
    STEP: Saw pod success 01/28/23 00:22:58.15
    Jan 28 00:22:58.151: INFO: Pod "pod-83a0bfb2-9655-4c1a-a5f7-4217b11e93a5" satisfied condition "Succeeded or Failed"
    Jan 28 00:22:58.163: INFO: Trying to get logs from node 10.9.20.126 pod pod-83a0bfb2-9655-4c1a-a5f7-4217b11e93a5 container test-container: <nil>
    STEP: delete the pod 01/28/23 00:22:58.245
    Jan 28 00:22:58.276: INFO: Waiting for pod pod-83a0bfb2-9655-4c1a-a5f7-4217b11e93a5 to disappear
    Jan 28 00:22:58.286: INFO: Pod pod-83a0bfb2-9655-4c1a-a5f7-4217b11e93a5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 28 00:22:58.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7731" for this suite. 01/28/23 00:22:58.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:22:58.327
Jan 28 00:22:58.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 00:22:58.33
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:22:58.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:22:58.374
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-89e2ad5d-8803-4200-b848-25f97fb55f55 01/28/23 00:22:58.386
STEP: Creating a pod to test consume configMaps 01/28/23 00:22:58.399
Jan 28 00:22:58.420: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bf4a07b2-32a2-4fdb-b572-ec1d20a441ff" in namespace "projected-3044" to be "Succeeded or Failed"
Jan 28 00:22:58.431: INFO: Pod "pod-projected-configmaps-bf4a07b2-32a2-4fdb-b572-ec1d20a441ff": Phase="Pending", Reason="", readiness=false. Elapsed: 11.193909ms
Jan 28 00:23:00.444: INFO: Pod "pod-projected-configmaps-bf4a07b2-32a2-4fdb-b572-ec1d20a441ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024017176s
Jan 28 00:23:02.444: INFO: Pod "pod-projected-configmaps-bf4a07b2-32a2-4fdb-b572-ec1d20a441ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023685374s
Jan 28 00:23:04.445: INFO: Pod "pod-projected-configmaps-bf4a07b2-32a2-4fdb-b572-ec1d20a441ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024783017s
STEP: Saw pod success 01/28/23 00:23:04.445
Jan 28 00:23:04.445: INFO: Pod "pod-projected-configmaps-bf4a07b2-32a2-4fdb-b572-ec1d20a441ff" satisfied condition "Succeeded or Failed"
Jan 28 00:23:04.458: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-configmaps-bf4a07b2-32a2-4fdb-b572-ec1d20a441ff container agnhost-container: <nil>
STEP: delete the pod 01/28/23 00:23:04.488
Jan 28 00:23:04.514: INFO: Waiting for pod pod-projected-configmaps-bf4a07b2-32a2-4fdb-b572-ec1d20a441ff to disappear
Jan 28 00:23:04.526: INFO: Pod pod-projected-configmaps-bf4a07b2-32a2-4fdb-b572-ec1d20a441ff no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 28 00:23:04.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3044" for this suite. 01/28/23 00:23:04.544
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":21,"skipped":444,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.236 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:22:58.327
    Jan 28 00:22:58.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 00:22:58.33
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:22:58.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:22:58.374
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-89e2ad5d-8803-4200-b848-25f97fb55f55 01/28/23 00:22:58.386
    STEP: Creating a pod to test consume configMaps 01/28/23 00:22:58.399
    Jan 28 00:22:58.420: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bf4a07b2-32a2-4fdb-b572-ec1d20a441ff" in namespace "projected-3044" to be "Succeeded or Failed"
    Jan 28 00:22:58.431: INFO: Pod "pod-projected-configmaps-bf4a07b2-32a2-4fdb-b572-ec1d20a441ff": Phase="Pending", Reason="", readiness=false. Elapsed: 11.193909ms
    Jan 28 00:23:00.444: INFO: Pod "pod-projected-configmaps-bf4a07b2-32a2-4fdb-b572-ec1d20a441ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024017176s
    Jan 28 00:23:02.444: INFO: Pod "pod-projected-configmaps-bf4a07b2-32a2-4fdb-b572-ec1d20a441ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023685374s
    Jan 28 00:23:04.445: INFO: Pod "pod-projected-configmaps-bf4a07b2-32a2-4fdb-b572-ec1d20a441ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024783017s
    STEP: Saw pod success 01/28/23 00:23:04.445
    Jan 28 00:23:04.445: INFO: Pod "pod-projected-configmaps-bf4a07b2-32a2-4fdb-b572-ec1d20a441ff" satisfied condition "Succeeded or Failed"
    Jan 28 00:23:04.458: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-configmaps-bf4a07b2-32a2-4fdb-b572-ec1d20a441ff container agnhost-container: <nil>
    STEP: delete the pod 01/28/23 00:23:04.488
    Jan 28 00:23:04.514: INFO: Waiting for pod pod-projected-configmaps-bf4a07b2-32a2-4fdb-b572-ec1d20a441ff to disappear
    Jan 28 00:23:04.526: INFO: Pod pod-projected-configmaps-bf4a07b2-32a2-4fdb-b572-ec1d20a441ff no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 28 00:23:04.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3044" for this suite. 01/28/23 00:23:04.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:23:04.572
Jan 28 00:23:04.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename statefulset 01/28/23 00:23:04.574
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:23:04.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:23:04.619
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6092 01/28/23 00:23:04.631
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-6092 01/28/23 00:23:04.645
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6092 01/28/23 00:23:04.666
Jan 28 00:23:04.677: INFO: Found 0 stateful pods, waiting for 1
Jan 28 00:23:14.689: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/28/23 00:23:14.689
Jan 28 00:23:14.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-6092 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 00:23:15.155: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 00:23:15.155: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 00:23:15.155: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 28 00:23:15.165: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 28 00:23:25.178: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 28 00:23:25.178: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 00:23:25.279: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jan 28 00:23:25.279: INFO: ss-0  10.9.20.126  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:04 +0000 UTC  }]
Jan 28 00:23:25.279: INFO: 
Jan 28 00:23:25.279: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 28 00:23:26.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98560181s
Jan 28 00:23:27.306: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.972800623s
Jan 28 00:23:28.318: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.958867508s
Jan 28 00:23:29.330: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.947247583s
Jan 28 00:23:30.344: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.933455659s
Jan 28 00:23:31.361: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.921178005s
Jan 28 00:23:32.389: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.904012669s
Jan 28 00:23:33.401: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.875545355s
Jan 28 00:23:34.415: INFO: Verifying statefulset ss doesn't scale past 3 for another 863.055267ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6092 01/28/23 00:23:35.415
Jan 28 00:23:35.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-6092 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 28 00:23:35.806: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 28 00:23:35.806: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 28 00:23:35.806: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 28 00:23:35.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-6092 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 28 00:23:36.163: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 28 00:23:36.163: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 28 00:23:36.163: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 28 00:23:36.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-6092 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 28 00:23:36.547: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 28 00:23:36.548: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 28 00:23:36.548: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 28 00:23:36.560: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 00:23:36.560: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 00:23:36.560: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/28/23 00:23:36.56
Jan 28 00:23:36.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-6092 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 00:23:36.919: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 00:23:36.919: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 00:23:36.919: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 28 00:23:36.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-6092 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 00:23:37.320: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 00:23:37.321: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 00:23:37.321: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 28 00:23:37.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-6092 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 00:23:37.646: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 00:23:37.646: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 00:23:37.646: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 28 00:23:37.646: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 00:23:37.663: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 28 00:23:47.695: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 28 00:23:47.696: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 28 00:23:47.696: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 28 00:23:47.736: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jan 28 00:23:47.736: INFO: ss-0  10.9.20.126  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:04 +0000 UTC  }]
Jan 28 00:23:47.736: INFO: ss-1  10.9.20.72   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:25 +0000 UTC  }]
Jan 28 00:23:47.736: INFO: ss-2  10.9.20.75   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:25 +0000 UTC  }]
Jan 28 00:23:47.736: INFO: 
Jan 28 00:23:47.736: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 28 00:23:48.748: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jan 28 00:23:48.748: INFO: ss-0  10.9.20.126  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:04 +0000 UTC  }]
Jan 28 00:23:48.748: INFO: ss-1  10.9.20.72   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:25 +0000 UTC  }]
Jan 28 00:23:48.748: INFO: ss-2  10.9.20.75   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:25 +0000 UTC  }]
Jan 28 00:23:48.748: INFO: 
Jan 28 00:23:48.748: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 28 00:23:49.765: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jan 28 00:23:49.765: INFO: ss-0  10.9.20.126  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:04 +0000 UTC  }]
Jan 28 00:23:49.765: INFO: 
Jan 28 00:23:49.765: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 28 00:23:50.788: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.958891665s
Jan 28 00:23:51.826: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.935693571s
Jan 28 00:23:52.837: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.897701209s
Jan 28 00:23:53.848: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.886939626s
Jan 28 00:23:54.859: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.875679393s
Jan 28 00:23:55.870: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.86461847s
Jan 28 00:23:56.881: INFO: Verifying statefulset ss doesn't scale past 0 for another 853.957638ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6092 01/28/23 00:23:57.881
Jan 28 00:23:57.895: INFO: Scaling statefulset ss to 0
Jan 28 00:23:57.927: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 28 00:23:57.962: INFO: Deleting all statefulset in ns statefulset-6092
Jan 28 00:23:57.998: INFO: Scaling statefulset ss to 0
Jan 28 00:23:58.079: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 00:23:58.089: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 28 00:23:58.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6092" for this suite. 01/28/23 00:23:58.156
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":22,"skipped":477,"failed":0}
------------------------------
â€¢ [SLOW TEST] [53.632 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:23:04.572
    Jan 28 00:23:04.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename statefulset 01/28/23 00:23:04.574
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:23:04.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:23:04.619
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6092 01/28/23 00:23:04.631
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-6092 01/28/23 00:23:04.645
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6092 01/28/23 00:23:04.666
    Jan 28 00:23:04.677: INFO: Found 0 stateful pods, waiting for 1
    Jan 28 00:23:14.689: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/28/23 00:23:14.689
    Jan 28 00:23:14.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-6092 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 28 00:23:15.155: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 28 00:23:15.155: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 28 00:23:15.155: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 28 00:23:15.165: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 28 00:23:25.178: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 28 00:23:25.178: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 28 00:23:25.279: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
    Jan 28 00:23:25.279: INFO: ss-0  10.9.20.126  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:04 +0000 UTC  }]
    Jan 28 00:23:25.279: INFO: 
    Jan 28 00:23:25.279: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan 28 00:23:26.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98560181s
    Jan 28 00:23:27.306: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.972800623s
    Jan 28 00:23:28.318: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.958867508s
    Jan 28 00:23:29.330: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.947247583s
    Jan 28 00:23:30.344: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.933455659s
    Jan 28 00:23:31.361: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.921178005s
    Jan 28 00:23:32.389: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.904012669s
    Jan 28 00:23:33.401: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.875545355s
    Jan 28 00:23:34.415: INFO: Verifying statefulset ss doesn't scale past 3 for another 863.055267ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6092 01/28/23 00:23:35.415
    Jan 28 00:23:35.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-6092 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 28 00:23:35.806: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 28 00:23:35.806: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 28 00:23:35.806: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 28 00:23:35.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-6092 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 28 00:23:36.163: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 28 00:23:36.163: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 28 00:23:36.163: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 28 00:23:36.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-6092 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 28 00:23:36.547: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 28 00:23:36.548: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 28 00:23:36.548: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 28 00:23:36.560: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 28 00:23:36.560: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 28 00:23:36.560: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/28/23 00:23:36.56
    Jan 28 00:23:36.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-6092 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 28 00:23:36.919: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 28 00:23:36.919: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 28 00:23:36.919: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 28 00:23:36.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-6092 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 28 00:23:37.320: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 28 00:23:37.321: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 28 00:23:37.321: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 28 00:23:37.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-6092 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 28 00:23:37.646: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 28 00:23:37.646: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 28 00:23:37.646: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 28 00:23:37.646: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 28 00:23:37.663: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jan 28 00:23:47.695: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 28 00:23:47.696: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 28 00:23:47.696: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 28 00:23:47.736: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
    Jan 28 00:23:47.736: INFO: ss-0  10.9.20.126  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:04 +0000 UTC  }]
    Jan 28 00:23:47.736: INFO: ss-1  10.9.20.72   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:25 +0000 UTC  }]
    Jan 28 00:23:47.736: INFO: ss-2  10.9.20.75   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:25 +0000 UTC  }]
    Jan 28 00:23:47.736: INFO: 
    Jan 28 00:23:47.736: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 28 00:23:48.748: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
    Jan 28 00:23:48.748: INFO: ss-0  10.9.20.126  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:04 +0000 UTC  }]
    Jan 28 00:23:48.748: INFO: ss-1  10.9.20.72   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:25 +0000 UTC  }]
    Jan 28 00:23:48.748: INFO: ss-2  10.9.20.75   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:25 +0000 UTC  }]
    Jan 28 00:23:48.748: INFO: 
    Jan 28 00:23:48.748: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 28 00:23:49.765: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
    Jan 28 00:23:49.765: INFO: ss-0  10.9.20.126  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:23:04 +0000 UTC  }]
    Jan 28 00:23:49.765: INFO: 
    Jan 28 00:23:49.765: INFO: StatefulSet ss has not reached scale 0, at 1
    Jan 28 00:23:50.788: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.958891665s
    Jan 28 00:23:51.826: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.935693571s
    Jan 28 00:23:52.837: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.897701209s
    Jan 28 00:23:53.848: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.886939626s
    Jan 28 00:23:54.859: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.875679393s
    Jan 28 00:23:55.870: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.86461847s
    Jan 28 00:23:56.881: INFO: Verifying statefulset ss doesn't scale past 0 for another 853.957638ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6092 01/28/23 00:23:57.881
    Jan 28 00:23:57.895: INFO: Scaling statefulset ss to 0
    Jan 28 00:23:57.927: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 28 00:23:57.962: INFO: Deleting all statefulset in ns statefulset-6092
    Jan 28 00:23:57.998: INFO: Scaling statefulset ss to 0
    Jan 28 00:23:58.079: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 28 00:23:58.089: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 28 00:23:58.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6092" for this suite. 01/28/23 00:23:58.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:23:58.219
Jan 28 00:23:58.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename podtemplate 01/28/23 00:23:58.22
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:23:58.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:23:58.262
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/28/23 00:23:58.273
Jan 28 00:23:58.286: INFO: created test-podtemplate-1
Jan 28 00:23:58.303: INFO: created test-podtemplate-2
Jan 28 00:23:58.316: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/28/23 00:23:58.316
STEP: delete collection of pod templates 01/28/23 00:23:58.326
Jan 28 00:23:58.326: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/28/23 00:23:58.371
Jan 28 00:23:58.372: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 28 00:23:58.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1973" for this suite. 01/28/23 00:23:58.398
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":23,"skipped":488,"failed":0}
------------------------------
â€¢ [0.225 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:23:58.219
    Jan 28 00:23:58.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename podtemplate 01/28/23 00:23:58.22
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:23:58.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:23:58.262
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/28/23 00:23:58.273
    Jan 28 00:23:58.286: INFO: created test-podtemplate-1
    Jan 28 00:23:58.303: INFO: created test-podtemplate-2
    Jan 28 00:23:58.316: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/28/23 00:23:58.316
    STEP: delete collection of pod templates 01/28/23 00:23:58.326
    Jan 28 00:23:58.326: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/28/23 00:23:58.371
    Jan 28 00:23:58.372: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 28 00:23:58.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-1973" for this suite. 01/28/23 00:23:58.398
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:23:58.445
Jan 28 00:23:58.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename disruption 01/28/23 00:23:58.448
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:23:58.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:23:58.565
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 01/28/23 00:23:58.575
STEP: Waiting for the pdb to be processed 01/28/23 00:23:58.589
STEP: First trying to evict a pod which shouldn't be evictable 01/28/23 00:24:00.661
STEP: Waiting for all pods to be running 01/28/23 00:24:00.661
Jan 28 00:24:00.702: INFO: pods: 0 < 3
Jan 28 00:24:02.714: INFO: running pods: 0 < 3
Jan 28 00:24:04.716: INFO: running pods: 2 < 3
Jan 28 00:24:06.716: INFO: running pods: 2 < 3
STEP: locating a running pod 01/28/23 00:24:08.721
STEP: Updating the pdb to allow a pod to be evicted 01/28/23 00:24:08.796
STEP: Waiting for the pdb to be processed 01/28/23 00:24:08.818
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/28/23 00:24:10.866
STEP: Waiting for all pods to be running 01/28/23 00:24:10.867
STEP: Waiting for the pdb to observed all healthy pods 01/28/23 00:24:10.878
STEP: Patching the pdb to disallow a pod to be evicted 01/28/23 00:24:10.928
STEP: Waiting for the pdb to be processed 01/28/23 00:24:10.963
STEP: Waiting for all pods to be running 01/28/23 00:24:10.973
Jan 28 00:24:11.016: INFO: running pods: 2 < 3
Jan 28 00:24:13.028: INFO: running pods: 2 < 3
STEP: locating a running pod 01/28/23 00:24:15.027
STEP: Deleting the pdb to allow a pod to be evicted 01/28/23 00:24:15.058
STEP: Waiting for the pdb to be deleted 01/28/23 00:24:15.076
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/28/23 00:24:15.086
STEP: Waiting for all pods to be running 01/28/23 00:24:15.086
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 28 00:24:15.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1565" for this suite. 01/28/23 00:24:15.158
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":24,"skipped":488,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.747 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:23:58.445
    Jan 28 00:23:58.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename disruption 01/28/23 00:23:58.448
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:23:58.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:23:58.565
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 01/28/23 00:23:58.575
    STEP: Waiting for the pdb to be processed 01/28/23 00:23:58.589
    STEP: First trying to evict a pod which shouldn't be evictable 01/28/23 00:24:00.661
    STEP: Waiting for all pods to be running 01/28/23 00:24:00.661
    Jan 28 00:24:00.702: INFO: pods: 0 < 3
    Jan 28 00:24:02.714: INFO: running pods: 0 < 3
    Jan 28 00:24:04.716: INFO: running pods: 2 < 3
    Jan 28 00:24:06.716: INFO: running pods: 2 < 3
    STEP: locating a running pod 01/28/23 00:24:08.721
    STEP: Updating the pdb to allow a pod to be evicted 01/28/23 00:24:08.796
    STEP: Waiting for the pdb to be processed 01/28/23 00:24:08.818
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/28/23 00:24:10.866
    STEP: Waiting for all pods to be running 01/28/23 00:24:10.867
    STEP: Waiting for the pdb to observed all healthy pods 01/28/23 00:24:10.878
    STEP: Patching the pdb to disallow a pod to be evicted 01/28/23 00:24:10.928
    STEP: Waiting for the pdb to be processed 01/28/23 00:24:10.963
    STEP: Waiting for all pods to be running 01/28/23 00:24:10.973
    Jan 28 00:24:11.016: INFO: running pods: 2 < 3
    Jan 28 00:24:13.028: INFO: running pods: 2 < 3
    STEP: locating a running pod 01/28/23 00:24:15.027
    STEP: Deleting the pdb to allow a pod to be evicted 01/28/23 00:24:15.058
    STEP: Waiting for the pdb to be deleted 01/28/23 00:24:15.076
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/28/23 00:24:15.086
    STEP: Waiting for all pods to be running 01/28/23 00:24:15.086
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 28 00:24:15.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1565" for this suite. 01/28/23 00:24:15.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:24:15.193
Jan 28 00:24:15.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename webhook 01/28/23 00:24:15.197
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:24:15.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:24:15.238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/28/23 00:24:15.284
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 00:24:15.686
STEP: Deploying the webhook pod 01/28/23 00:24:15.706
STEP: Wait for the deployment to be ready 01/28/23 00:24:15.753
Jan 28 00:24:15.781: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 00:24:17.818: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 24, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 24, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 24, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 24, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/28/23 00:24:19.83
STEP: Verifying the service has paired with the endpoint 01/28/23 00:24:19.884
Jan 28 00:24:20.885: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Jan 28 00:24:20.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/28/23 00:24:21.428
STEP: Creating a custom resource that should be denied by the webhook 01/28/23 00:24:21.503
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/28/23 00:24:23.587
STEP: Updating the custom resource with disallowed data should be denied 01/28/23 00:24:23.639
STEP: Deleting the custom resource should be denied 01/28/23 00:24:23.674
STEP: Remove the offending key and value from the custom resource data 01/28/23 00:24:23.703
STEP: Deleting the updated custom resource should be successful 01/28/23 00:24:23.739
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 00:24:24.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4825" for this suite. 01/28/23 00:24:24.339
STEP: Destroying namespace "webhook-4825-markers" for this suite. 01/28/23 00:24:24.359
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":25,"skipped":504,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.349 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:24:15.193
    Jan 28 00:24:15.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename webhook 01/28/23 00:24:15.197
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:24:15.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:24:15.238
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/28/23 00:24:15.284
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 00:24:15.686
    STEP: Deploying the webhook pod 01/28/23 00:24:15.706
    STEP: Wait for the deployment to be ready 01/28/23 00:24:15.753
    Jan 28 00:24:15.781: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 28 00:24:17.818: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 24, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 24, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 24, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 24, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/28/23 00:24:19.83
    STEP: Verifying the service has paired with the endpoint 01/28/23 00:24:19.884
    Jan 28 00:24:20.885: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Jan 28 00:24:20.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/28/23 00:24:21.428
    STEP: Creating a custom resource that should be denied by the webhook 01/28/23 00:24:21.503
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/28/23 00:24:23.587
    STEP: Updating the custom resource with disallowed data should be denied 01/28/23 00:24:23.639
    STEP: Deleting the custom resource should be denied 01/28/23 00:24:23.674
    STEP: Remove the offending key and value from the custom resource data 01/28/23 00:24:23.703
    STEP: Deleting the updated custom resource should be successful 01/28/23 00:24:23.739
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 00:24:24.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4825" for this suite. 01/28/23 00:24:24.339
    STEP: Destroying namespace "webhook-4825-markers" for this suite. 01/28/23 00:24:24.359
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:24:24.547
Jan 28 00:24:24.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename daemonsets 01/28/23 00:24:24.55
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:24:24.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:24:24.605
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 01/28/23 00:24:24.715
STEP: Check that daemon pods launch on every node of the cluster. 01/28/23 00:24:24.757
Jan 28 00:24:24.794: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:24:24.794: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 00:24:25.824: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:24:25.827: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 00:24:26.824: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 28 00:24:26.824: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 00:24:27.832: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 28 00:24:27.832: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 01/28/23 00:24:27.845
Jan 28 00:24:27.857: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/28/23 00:24:27.857
Jan 28 00:24:27.891: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/28/23 00:24:27.892
Jan 28 00:24:27.898: INFO: Observed &DaemonSet event: ADDED
Jan 28 00:24:27.899: INFO: Observed &DaemonSet event: MODIFIED
Jan 28 00:24:27.899: INFO: Observed &DaemonSet event: MODIFIED
Jan 28 00:24:27.900: INFO: Observed &DaemonSet event: MODIFIED
Jan 28 00:24:27.900: INFO: Observed &DaemonSet event: MODIFIED
Jan 28 00:24:27.901: INFO: Found daemon set daemon-set in namespace daemonsets-714 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 28 00:24:27.901: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/28/23 00:24:27.901
STEP: watching for the daemon set status to be patched 01/28/23 00:24:27.917
Jan 28 00:24:27.923: INFO: Observed &DaemonSet event: ADDED
Jan 28 00:24:27.923: INFO: Observed &DaemonSet event: MODIFIED
Jan 28 00:24:27.924: INFO: Observed &DaemonSet event: MODIFIED
Jan 28 00:24:27.925: INFO: Observed &DaemonSet event: MODIFIED
Jan 28 00:24:27.925: INFO: Observed &DaemonSet event: MODIFIED
Jan 28 00:24:27.926: INFO: Observed daemon set daemon-set in namespace daemonsets-714 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 28 00:24:27.926: INFO: Observed &DaemonSet event: MODIFIED
Jan 28 00:24:27.927: INFO: Found daemon set daemon-set in namespace daemonsets-714 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 28 00:24:27.927: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/28/23 00:24:27.939
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-714, will wait for the garbage collector to delete the pods 01/28/23 00:24:27.94
Jan 28 00:24:28.024: INFO: Deleting DaemonSet.extensions daemon-set took: 21.287374ms
Jan 28 00:24:28.125: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.203749ms
Jan 28 00:24:30.436: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:24:30.436: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 28 00:24:30.449: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19090"},"items":null}

Jan 28 00:24:30.458: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19091"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 28 00:24:30.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-714" for this suite. 01/28/23 00:24:30.529
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":26,"skipped":527,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.005 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:24:24.547
    Jan 28 00:24:24.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename daemonsets 01/28/23 00:24:24.55
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:24:24.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:24:24.605
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 01/28/23 00:24:24.715
    STEP: Check that daemon pods launch on every node of the cluster. 01/28/23 00:24:24.757
    Jan 28 00:24:24.794: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 00:24:24.794: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 00:24:25.824: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 00:24:25.827: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 00:24:26.824: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 28 00:24:26.824: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 00:24:27.832: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 28 00:24:27.832: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 01/28/23 00:24:27.845
    Jan 28 00:24:27.857: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/28/23 00:24:27.857
    Jan 28 00:24:27.891: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/28/23 00:24:27.892
    Jan 28 00:24:27.898: INFO: Observed &DaemonSet event: ADDED
    Jan 28 00:24:27.899: INFO: Observed &DaemonSet event: MODIFIED
    Jan 28 00:24:27.899: INFO: Observed &DaemonSet event: MODIFIED
    Jan 28 00:24:27.900: INFO: Observed &DaemonSet event: MODIFIED
    Jan 28 00:24:27.900: INFO: Observed &DaemonSet event: MODIFIED
    Jan 28 00:24:27.901: INFO: Found daemon set daemon-set in namespace daemonsets-714 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 28 00:24:27.901: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/28/23 00:24:27.901
    STEP: watching for the daemon set status to be patched 01/28/23 00:24:27.917
    Jan 28 00:24:27.923: INFO: Observed &DaemonSet event: ADDED
    Jan 28 00:24:27.923: INFO: Observed &DaemonSet event: MODIFIED
    Jan 28 00:24:27.924: INFO: Observed &DaemonSet event: MODIFIED
    Jan 28 00:24:27.925: INFO: Observed &DaemonSet event: MODIFIED
    Jan 28 00:24:27.925: INFO: Observed &DaemonSet event: MODIFIED
    Jan 28 00:24:27.926: INFO: Observed daemon set daemon-set in namespace daemonsets-714 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 28 00:24:27.926: INFO: Observed &DaemonSet event: MODIFIED
    Jan 28 00:24:27.927: INFO: Found daemon set daemon-set in namespace daemonsets-714 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan 28 00:24:27.927: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/28/23 00:24:27.939
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-714, will wait for the garbage collector to delete the pods 01/28/23 00:24:27.94
    Jan 28 00:24:28.024: INFO: Deleting DaemonSet.extensions daemon-set took: 21.287374ms
    Jan 28 00:24:28.125: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.203749ms
    Jan 28 00:24:30.436: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 00:24:30.436: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 28 00:24:30.449: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19090"},"items":null}

    Jan 28 00:24:30.458: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19091"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 00:24:30.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-714" for this suite. 01/28/23 00:24:30.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:24:30.562
Jan 28 00:24:30.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename events 01/28/23 00:24:30.565
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:24:30.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:24:30.615
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/28/23 00:24:30.625
STEP: get a list of Events with a label in the current namespace 01/28/23 00:24:30.665
STEP: delete a list of events 01/28/23 00:24:30.677
Jan 28 00:24:30.678: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/28/23 00:24:30.741
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan 28 00:24:30.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8961" for this suite. 01/28/23 00:24:30.778
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":27,"skipped":535,"failed":0}
------------------------------
â€¢ [0.232 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:24:30.562
    Jan 28 00:24:30.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename events 01/28/23 00:24:30.565
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:24:30.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:24:30.615
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/28/23 00:24:30.625
    STEP: get a list of Events with a label in the current namespace 01/28/23 00:24:30.665
    STEP: delete a list of events 01/28/23 00:24:30.677
    Jan 28 00:24:30.678: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/28/23 00:24:30.741
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan 28 00:24:30.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-8961" for this suite. 01/28/23 00:24:30.778
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:24:30.794
Jan 28 00:24:30.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename gc 01/28/23 00:24:30.796
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:24:30.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:24:30.842
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/28/23 00:24:30.871
STEP: delete the rc 01/28/23 00:24:35.904
STEP: wait for the rc to be deleted 01/28/23 00:24:35.927
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/28/23 00:24:40.94
STEP: Gathering metrics 01/28/23 00:25:10.986
W0128 00:25:11.021746      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 28 00:25:11.021: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 28 00:25:11.021: INFO: Deleting pod "simpletest.rc-2dcp7" in namespace "gc-7898"
Jan 28 00:25:11.054: INFO: Deleting pod "simpletest.rc-2h2xs" in namespace "gc-7898"
Jan 28 00:25:11.086: INFO: Deleting pod "simpletest.rc-2kxrk" in namespace "gc-7898"
Jan 28 00:25:11.112: INFO: Deleting pod "simpletest.rc-2rl8h" in namespace "gc-7898"
Jan 28 00:25:11.144: INFO: Deleting pod "simpletest.rc-45cs7" in namespace "gc-7898"
Jan 28 00:25:11.172: INFO: Deleting pod "simpletest.rc-4gp67" in namespace "gc-7898"
Jan 28 00:25:11.218: INFO: Deleting pod "simpletest.rc-4p5tw" in namespace "gc-7898"
Jan 28 00:25:11.247: INFO: Deleting pod "simpletest.rc-4t87b" in namespace "gc-7898"
Jan 28 00:25:11.271: INFO: Deleting pod "simpletest.rc-522k7" in namespace "gc-7898"
Jan 28 00:25:11.316: INFO: Deleting pod "simpletest.rc-5g7nh" in namespace "gc-7898"
Jan 28 00:25:11.345: INFO: Deleting pod "simpletest.rc-5gncb" in namespace "gc-7898"
Jan 28 00:25:11.381: INFO: Deleting pod "simpletest.rc-5kmfj" in namespace "gc-7898"
Jan 28 00:25:11.405: INFO: Deleting pod "simpletest.rc-5v8qq" in namespace "gc-7898"
Jan 28 00:25:11.430: INFO: Deleting pod "simpletest.rc-5wrdd" in namespace "gc-7898"
Jan 28 00:25:11.460: INFO: Deleting pod "simpletest.rc-64rcb" in namespace "gc-7898"
Jan 28 00:25:11.481: INFO: Deleting pod "simpletest.rc-68hqd" in namespace "gc-7898"
Jan 28 00:25:11.508: INFO: Deleting pod "simpletest.rc-6fh8d" in namespace "gc-7898"
Jan 28 00:25:11.531: INFO: Deleting pod "simpletest.rc-6g7cx" in namespace "gc-7898"
Jan 28 00:25:11.554: INFO: Deleting pod "simpletest.rc-6kpvq" in namespace "gc-7898"
Jan 28 00:25:11.584: INFO: Deleting pod "simpletest.rc-6z7dc" in namespace "gc-7898"
Jan 28 00:25:11.608: INFO: Deleting pod "simpletest.rc-75chr" in namespace "gc-7898"
Jan 28 00:25:11.643: INFO: Deleting pod "simpletest.rc-78j8j" in namespace "gc-7898"
Jan 28 00:25:11.684: INFO: Deleting pod "simpletest.rc-7dbhq" in namespace "gc-7898"
Jan 28 00:25:11.717: INFO: Deleting pod "simpletest.rc-7htjc" in namespace "gc-7898"
Jan 28 00:25:11.753: INFO: Deleting pod "simpletest.rc-7kjjl" in namespace "gc-7898"
Jan 28 00:25:11.780: INFO: Deleting pod "simpletest.rc-7pdc4" in namespace "gc-7898"
Jan 28 00:25:11.809: INFO: Deleting pod "simpletest.rc-7vdn9" in namespace "gc-7898"
Jan 28 00:25:11.856: INFO: Deleting pod "simpletest.rc-8rcgh" in namespace "gc-7898"
Jan 28 00:25:11.883: INFO: Deleting pod "simpletest.rc-8rnt6" in namespace "gc-7898"
Jan 28 00:25:11.912: INFO: Deleting pod "simpletest.rc-8tbwc" in namespace "gc-7898"
Jan 28 00:25:11.936: INFO: Deleting pod "simpletest.rc-8vcbz" in namespace "gc-7898"
Jan 28 00:25:11.973: INFO: Deleting pod "simpletest.rc-9dwlq" in namespace "gc-7898"
Jan 28 00:25:12.009: INFO: Deleting pod "simpletest.rc-9jszm" in namespace "gc-7898"
Jan 28 00:25:12.030: INFO: Deleting pod "simpletest.rc-9kh6k" in namespace "gc-7898"
Jan 28 00:25:12.054: INFO: Deleting pod "simpletest.rc-9wvq6" in namespace "gc-7898"
Jan 28 00:25:12.080: INFO: Deleting pod "simpletest.rc-b4dzj" in namespace "gc-7898"
Jan 28 00:25:12.106: INFO: Deleting pod "simpletest.rc-bvnqv" in namespace "gc-7898"
Jan 28 00:25:12.131: INFO: Deleting pod "simpletest.rc-c8g5d" in namespace "gc-7898"
Jan 28 00:25:12.175: INFO: Deleting pod "simpletest.rc-cjbbf" in namespace "gc-7898"
Jan 28 00:25:12.202: INFO: Deleting pod "simpletest.rc-cmsj6" in namespace "gc-7898"
Jan 28 00:25:12.220: INFO: Deleting pod "simpletest.rc-crdtg" in namespace "gc-7898"
Jan 28 00:25:12.240: INFO: Deleting pod "simpletest.rc-dgthz" in namespace "gc-7898"
Jan 28 00:25:12.268: INFO: Deleting pod "simpletest.rc-dwsl4" in namespace "gc-7898"
Jan 28 00:25:12.302: INFO: Deleting pod "simpletest.rc-dxwv2" in namespace "gc-7898"
Jan 28 00:25:12.323: INFO: Deleting pod "simpletest.rc-f9pmt" in namespace "gc-7898"
Jan 28 00:25:12.344: INFO: Deleting pod "simpletest.rc-fvwlr" in namespace "gc-7898"
Jan 28 00:25:12.384: INFO: Deleting pod "simpletest.rc-g9l4g" in namespace "gc-7898"
Jan 28 00:25:12.403: INFO: Deleting pod "simpletest.rc-gfqsf" in namespace "gc-7898"
Jan 28 00:25:12.417: INFO: Deleting pod "simpletest.rc-gkxkb" in namespace "gc-7898"
Jan 28 00:25:12.433: INFO: Deleting pod "simpletest.rc-gx75c" in namespace "gc-7898"
Jan 28 00:25:12.464: INFO: Deleting pod "simpletest.rc-gztb8" in namespace "gc-7898"
Jan 28 00:25:12.487: INFO: Deleting pod "simpletest.rc-h8csn" in namespace "gc-7898"
Jan 28 00:25:12.516: INFO: Deleting pod "simpletest.rc-hfbwq" in namespace "gc-7898"
Jan 28 00:25:12.542: INFO: Deleting pod "simpletest.rc-hgvbc" in namespace "gc-7898"
Jan 28 00:25:12.566: INFO: Deleting pod "simpletest.rc-hk27f" in namespace "gc-7898"
Jan 28 00:25:12.602: INFO: Deleting pod "simpletest.rc-hn47s" in namespace "gc-7898"
Jan 28 00:25:12.632: INFO: Deleting pod "simpletest.rc-hqgnc" in namespace "gc-7898"
Jan 28 00:25:12.657: INFO: Deleting pod "simpletest.rc-hzpfc" in namespace "gc-7898"
Jan 28 00:25:12.677: INFO: Deleting pod "simpletest.rc-jbw26" in namespace "gc-7898"
Jan 28 00:25:12.697: INFO: Deleting pod "simpletest.rc-jgjtn" in namespace "gc-7898"
Jan 28 00:25:12.720: INFO: Deleting pod "simpletest.rc-jlprz" in namespace "gc-7898"
Jan 28 00:25:12.741: INFO: Deleting pod "simpletest.rc-jxhxs" in namespace "gc-7898"
Jan 28 00:25:12.760: INFO: Deleting pod "simpletest.rc-k5g8k" in namespace "gc-7898"
Jan 28 00:25:12.778: INFO: Deleting pod "simpletest.rc-kxfz9" in namespace "gc-7898"
Jan 28 00:25:12.800: INFO: Deleting pod "simpletest.rc-lhgmc" in namespace "gc-7898"
Jan 28 00:25:12.820: INFO: Deleting pod "simpletest.rc-m7pv7" in namespace "gc-7898"
Jan 28 00:25:12.842: INFO: Deleting pod "simpletest.rc-mjpdh" in namespace "gc-7898"
Jan 28 00:25:12.871: INFO: Deleting pod "simpletest.rc-n52nx" in namespace "gc-7898"
Jan 28 00:25:12.904: INFO: Deleting pod "simpletest.rc-nr96q" in namespace "gc-7898"
Jan 28 00:25:12.931: INFO: Deleting pod "simpletest.rc-nw9h8" in namespace "gc-7898"
Jan 28 00:25:12.952: INFO: Deleting pod "simpletest.rc-p7cmt" in namespace "gc-7898"
Jan 28 00:25:12.981: INFO: Deleting pod "simpletest.rc-p7lgc" in namespace "gc-7898"
Jan 28 00:25:13.009: INFO: Deleting pod "simpletest.rc-p8bgt" in namespace "gc-7898"
Jan 28 00:25:13.033: INFO: Deleting pod "simpletest.rc-phlnc" in namespace "gc-7898"
Jan 28 00:25:13.055: INFO: Deleting pod "simpletest.rc-prwxf" in namespace "gc-7898"
Jan 28 00:25:13.078: INFO: Deleting pod "simpletest.rc-ps5ms" in namespace "gc-7898"
Jan 28 00:25:13.109: INFO: Deleting pod "simpletest.rc-pvggz" in namespace "gc-7898"
Jan 28 00:25:13.140: INFO: Deleting pod "simpletest.rc-qnsjf" in namespace "gc-7898"
Jan 28 00:25:13.161: INFO: Deleting pod "simpletest.rc-qs4d4" in namespace "gc-7898"
Jan 28 00:25:13.186: INFO: Deleting pod "simpletest.rc-rnd49" in namespace "gc-7898"
Jan 28 00:25:13.212: INFO: Deleting pod "simpletest.rc-rrjt8" in namespace "gc-7898"
Jan 28 00:25:13.238: INFO: Deleting pod "simpletest.rc-rvtj7" in namespace "gc-7898"
Jan 28 00:25:13.257: INFO: Deleting pod "simpletest.rc-s7b9h" in namespace "gc-7898"
Jan 28 00:25:13.279: INFO: Deleting pod "simpletest.rc-spnsj" in namespace "gc-7898"
Jan 28 00:25:13.301: INFO: Deleting pod "simpletest.rc-tdg7d" in namespace "gc-7898"
Jan 28 00:25:13.333: INFO: Deleting pod "simpletest.rc-tq8vz" in namespace "gc-7898"
Jan 28 00:25:13.357: INFO: Deleting pod "simpletest.rc-twjmk" in namespace "gc-7898"
Jan 28 00:25:13.386: INFO: Deleting pod "simpletest.rc-vm8td" in namespace "gc-7898"
Jan 28 00:25:13.403: INFO: Deleting pod "simpletest.rc-vv5v7" in namespace "gc-7898"
Jan 28 00:25:13.446: INFO: Deleting pod "simpletest.rc-w2rz5" in namespace "gc-7898"
Jan 28 00:25:13.462: INFO: Deleting pod "simpletest.rc-w2vrz" in namespace "gc-7898"
Jan 28 00:25:13.485: INFO: Deleting pod "simpletest.rc-w5nhk" in namespace "gc-7898"
Jan 28 00:25:13.511: INFO: Deleting pod "simpletest.rc-w7vdj" in namespace "gc-7898"
Jan 28 00:25:13.529: INFO: Deleting pod "simpletest.rc-xmxhn" in namespace "gc-7898"
Jan 28 00:25:13.566: INFO: Deleting pod "simpletest.rc-xrfwn" in namespace "gc-7898"
Jan 28 00:25:13.584: INFO: Deleting pod "simpletest.rc-z68sd" in namespace "gc-7898"
Jan 28 00:25:13.604: INFO: Deleting pod "simpletest.rc-zbg55" in namespace "gc-7898"
Jan 28 00:25:13.623: INFO: Deleting pod "simpletest.rc-zg2g2" in namespace "gc-7898"
Jan 28 00:25:13.643: INFO: Deleting pod "simpletest.rc-zsjk4" in namespace "gc-7898"
Jan 28 00:25:13.666: INFO: Deleting pod "simpletest.rc-zzdkm" in namespace "gc-7898"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 28 00:25:13.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7898" for this suite. 01/28/23 00:25:13.705
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":28,"skipped":535,"failed":0}
------------------------------
â€¢ [SLOW TEST] [42.931 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:24:30.794
    Jan 28 00:24:30.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename gc 01/28/23 00:24:30.796
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:24:30.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:24:30.842
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/28/23 00:24:30.871
    STEP: delete the rc 01/28/23 00:24:35.904
    STEP: wait for the rc to be deleted 01/28/23 00:24:35.927
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/28/23 00:24:40.94
    STEP: Gathering metrics 01/28/23 00:25:10.986
    W0128 00:25:11.021746      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 28 00:25:11.021: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 28 00:25:11.021: INFO: Deleting pod "simpletest.rc-2dcp7" in namespace "gc-7898"
    Jan 28 00:25:11.054: INFO: Deleting pod "simpletest.rc-2h2xs" in namespace "gc-7898"
    Jan 28 00:25:11.086: INFO: Deleting pod "simpletest.rc-2kxrk" in namespace "gc-7898"
    Jan 28 00:25:11.112: INFO: Deleting pod "simpletest.rc-2rl8h" in namespace "gc-7898"
    Jan 28 00:25:11.144: INFO: Deleting pod "simpletest.rc-45cs7" in namespace "gc-7898"
    Jan 28 00:25:11.172: INFO: Deleting pod "simpletest.rc-4gp67" in namespace "gc-7898"
    Jan 28 00:25:11.218: INFO: Deleting pod "simpletest.rc-4p5tw" in namespace "gc-7898"
    Jan 28 00:25:11.247: INFO: Deleting pod "simpletest.rc-4t87b" in namespace "gc-7898"
    Jan 28 00:25:11.271: INFO: Deleting pod "simpletest.rc-522k7" in namespace "gc-7898"
    Jan 28 00:25:11.316: INFO: Deleting pod "simpletest.rc-5g7nh" in namespace "gc-7898"
    Jan 28 00:25:11.345: INFO: Deleting pod "simpletest.rc-5gncb" in namespace "gc-7898"
    Jan 28 00:25:11.381: INFO: Deleting pod "simpletest.rc-5kmfj" in namespace "gc-7898"
    Jan 28 00:25:11.405: INFO: Deleting pod "simpletest.rc-5v8qq" in namespace "gc-7898"
    Jan 28 00:25:11.430: INFO: Deleting pod "simpletest.rc-5wrdd" in namespace "gc-7898"
    Jan 28 00:25:11.460: INFO: Deleting pod "simpletest.rc-64rcb" in namespace "gc-7898"
    Jan 28 00:25:11.481: INFO: Deleting pod "simpletest.rc-68hqd" in namespace "gc-7898"
    Jan 28 00:25:11.508: INFO: Deleting pod "simpletest.rc-6fh8d" in namespace "gc-7898"
    Jan 28 00:25:11.531: INFO: Deleting pod "simpletest.rc-6g7cx" in namespace "gc-7898"
    Jan 28 00:25:11.554: INFO: Deleting pod "simpletest.rc-6kpvq" in namespace "gc-7898"
    Jan 28 00:25:11.584: INFO: Deleting pod "simpletest.rc-6z7dc" in namespace "gc-7898"
    Jan 28 00:25:11.608: INFO: Deleting pod "simpletest.rc-75chr" in namespace "gc-7898"
    Jan 28 00:25:11.643: INFO: Deleting pod "simpletest.rc-78j8j" in namespace "gc-7898"
    Jan 28 00:25:11.684: INFO: Deleting pod "simpletest.rc-7dbhq" in namespace "gc-7898"
    Jan 28 00:25:11.717: INFO: Deleting pod "simpletest.rc-7htjc" in namespace "gc-7898"
    Jan 28 00:25:11.753: INFO: Deleting pod "simpletest.rc-7kjjl" in namespace "gc-7898"
    Jan 28 00:25:11.780: INFO: Deleting pod "simpletest.rc-7pdc4" in namespace "gc-7898"
    Jan 28 00:25:11.809: INFO: Deleting pod "simpletest.rc-7vdn9" in namespace "gc-7898"
    Jan 28 00:25:11.856: INFO: Deleting pod "simpletest.rc-8rcgh" in namespace "gc-7898"
    Jan 28 00:25:11.883: INFO: Deleting pod "simpletest.rc-8rnt6" in namespace "gc-7898"
    Jan 28 00:25:11.912: INFO: Deleting pod "simpletest.rc-8tbwc" in namespace "gc-7898"
    Jan 28 00:25:11.936: INFO: Deleting pod "simpletest.rc-8vcbz" in namespace "gc-7898"
    Jan 28 00:25:11.973: INFO: Deleting pod "simpletest.rc-9dwlq" in namespace "gc-7898"
    Jan 28 00:25:12.009: INFO: Deleting pod "simpletest.rc-9jszm" in namespace "gc-7898"
    Jan 28 00:25:12.030: INFO: Deleting pod "simpletest.rc-9kh6k" in namespace "gc-7898"
    Jan 28 00:25:12.054: INFO: Deleting pod "simpletest.rc-9wvq6" in namespace "gc-7898"
    Jan 28 00:25:12.080: INFO: Deleting pod "simpletest.rc-b4dzj" in namespace "gc-7898"
    Jan 28 00:25:12.106: INFO: Deleting pod "simpletest.rc-bvnqv" in namespace "gc-7898"
    Jan 28 00:25:12.131: INFO: Deleting pod "simpletest.rc-c8g5d" in namespace "gc-7898"
    Jan 28 00:25:12.175: INFO: Deleting pod "simpletest.rc-cjbbf" in namespace "gc-7898"
    Jan 28 00:25:12.202: INFO: Deleting pod "simpletest.rc-cmsj6" in namespace "gc-7898"
    Jan 28 00:25:12.220: INFO: Deleting pod "simpletest.rc-crdtg" in namespace "gc-7898"
    Jan 28 00:25:12.240: INFO: Deleting pod "simpletest.rc-dgthz" in namespace "gc-7898"
    Jan 28 00:25:12.268: INFO: Deleting pod "simpletest.rc-dwsl4" in namespace "gc-7898"
    Jan 28 00:25:12.302: INFO: Deleting pod "simpletest.rc-dxwv2" in namespace "gc-7898"
    Jan 28 00:25:12.323: INFO: Deleting pod "simpletest.rc-f9pmt" in namespace "gc-7898"
    Jan 28 00:25:12.344: INFO: Deleting pod "simpletest.rc-fvwlr" in namespace "gc-7898"
    Jan 28 00:25:12.384: INFO: Deleting pod "simpletest.rc-g9l4g" in namespace "gc-7898"
    Jan 28 00:25:12.403: INFO: Deleting pod "simpletest.rc-gfqsf" in namespace "gc-7898"
    Jan 28 00:25:12.417: INFO: Deleting pod "simpletest.rc-gkxkb" in namespace "gc-7898"
    Jan 28 00:25:12.433: INFO: Deleting pod "simpletest.rc-gx75c" in namespace "gc-7898"
    Jan 28 00:25:12.464: INFO: Deleting pod "simpletest.rc-gztb8" in namespace "gc-7898"
    Jan 28 00:25:12.487: INFO: Deleting pod "simpletest.rc-h8csn" in namespace "gc-7898"
    Jan 28 00:25:12.516: INFO: Deleting pod "simpletest.rc-hfbwq" in namespace "gc-7898"
    Jan 28 00:25:12.542: INFO: Deleting pod "simpletest.rc-hgvbc" in namespace "gc-7898"
    Jan 28 00:25:12.566: INFO: Deleting pod "simpletest.rc-hk27f" in namespace "gc-7898"
    Jan 28 00:25:12.602: INFO: Deleting pod "simpletest.rc-hn47s" in namespace "gc-7898"
    Jan 28 00:25:12.632: INFO: Deleting pod "simpletest.rc-hqgnc" in namespace "gc-7898"
    Jan 28 00:25:12.657: INFO: Deleting pod "simpletest.rc-hzpfc" in namespace "gc-7898"
    Jan 28 00:25:12.677: INFO: Deleting pod "simpletest.rc-jbw26" in namespace "gc-7898"
    Jan 28 00:25:12.697: INFO: Deleting pod "simpletest.rc-jgjtn" in namespace "gc-7898"
    Jan 28 00:25:12.720: INFO: Deleting pod "simpletest.rc-jlprz" in namespace "gc-7898"
    Jan 28 00:25:12.741: INFO: Deleting pod "simpletest.rc-jxhxs" in namespace "gc-7898"
    Jan 28 00:25:12.760: INFO: Deleting pod "simpletest.rc-k5g8k" in namespace "gc-7898"
    Jan 28 00:25:12.778: INFO: Deleting pod "simpletest.rc-kxfz9" in namespace "gc-7898"
    Jan 28 00:25:12.800: INFO: Deleting pod "simpletest.rc-lhgmc" in namespace "gc-7898"
    Jan 28 00:25:12.820: INFO: Deleting pod "simpletest.rc-m7pv7" in namespace "gc-7898"
    Jan 28 00:25:12.842: INFO: Deleting pod "simpletest.rc-mjpdh" in namespace "gc-7898"
    Jan 28 00:25:12.871: INFO: Deleting pod "simpletest.rc-n52nx" in namespace "gc-7898"
    Jan 28 00:25:12.904: INFO: Deleting pod "simpletest.rc-nr96q" in namespace "gc-7898"
    Jan 28 00:25:12.931: INFO: Deleting pod "simpletest.rc-nw9h8" in namespace "gc-7898"
    Jan 28 00:25:12.952: INFO: Deleting pod "simpletest.rc-p7cmt" in namespace "gc-7898"
    Jan 28 00:25:12.981: INFO: Deleting pod "simpletest.rc-p7lgc" in namespace "gc-7898"
    Jan 28 00:25:13.009: INFO: Deleting pod "simpletest.rc-p8bgt" in namespace "gc-7898"
    Jan 28 00:25:13.033: INFO: Deleting pod "simpletest.rc-phlnc" in namespace "gc-7898"
    Jan 28 00:25:13.055: INFO: Deleting pod "simpletest.rc-prwxf" in namespace "gc-7898"
    Jan 28 00:25:13.078: INFO: Deleting pod "simpletest.rc-ps5ms" in namespace "gc-7898"
    Jan 28 00:25:13.109: INFO: Deleting pod "simpletest.rc-pvggz" in namespace "gc-7898"
    Jan 28 00:25:13.140: INFO: Deleting pod "simpletest.rc-qnsjf" in namespace "gc-7898"
    Jan 28 00:25:13.161: INFO: Deleting pod "simpletest.rc-qs4d4" in namespace "gc-7898"
    Jan 28 00:25:13.186: INFO: Deleting pod "simpletest.rc-rnd49" in namespace "gc-7898"
    Jan 28 00:25:13.212: INFO: Deleting pod "simpletest.rc-rrjt8" in namespace "gc-7898"
    Jan 28 00:25:13.238: INFO: Deleting pod "simpletest.rc-rvtj7" in namespace "gc-7898"
    Jan 28 00:25:13.257: INFO: Deleting pod "simpletest.rc-s7b9h" in namespace "gc-7898"
    Jan 28 00:25:13.279: INFO: Deleting pod "simpletest.rc-spnsj" in namespace "gc-7898"
    Jan 28 00:25:13.301: INFO: Deleting pod "simpletest.rc-tdg7d" in namespace "gc-7898"
    Jan 28 00:25:13.333: INFO: Deleting pod "simpletest.rc-tq8vz" in namespace "gc-7898"
    Jan 28 00:25:13.357: INFO: Deleting pod "simpletest.rc-twjmk" in namespace "gc-7898"
    Jan 28 00:25:13.386: INFO: Deleting pod "simpletest.rc-vm8td" in namespace "gc-7898"
    Jan 28 00:25:13.403: INFO: Deleting pod "simpletest.rc-vv5v7" in namespace "gc-7898"
    Jan 28 00:25:13.446: INFO: Deleting pod "simpletest.rc-w2rz5" in namespace "gc-7898"
    Jan 28 00:25:13.462: INFO: Deleting pod "simpletest.rc-w2vrz" in namespace "gc-7898"
    Jan 28 00:25:13.485: INFO: Deleting pod "simpletest.rc-w5nhk" in namespace "gc-7898"
    Jan 28 00:25:13.511: INFO: Deleting pod "simpletest.rc-w7vdj" in namespace "gc-7898"
    Jan 28 00:25:13.529: INFO: Deleting pod "simpletest.rc-xmxhn" in namespace "gc-7898"
    Jan 28 00:25:13.566: INFO: Deleting pod "simpletest.rc-xrfwn" in namespace "gc-7898"
    Jan 28 00:25:13.584: INFO: Deleting pod "simpletest.rc-z68sd" in namespace "gc-7898"
    Jan 28 00:25:13.604: INFO: Deleting pod "simpletest.rc-zbg55" in namespace "gc-7898"
    Jan 28 00:25:13.623: INFO: Deleting pod "simpletest.rc-zg2g2" in namespace "gc-7898"
    Jan 28 00:25:13.643: INFO: Deleting pod "simpletest.rc-zsjk4" in namespace "gc-7898"
    Jan 28 00:25:13.666: INFO: Deleting pod "simpletest.rc-zzdkm" in namespace "gc-7898"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 28 00:25:13.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7898" for this suite. 01/28/23 00:25:13.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:25:13.73
Jan 28 00:25:13.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename replicaset 01/28/23 00:25:13.731
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:25:13.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:25:13.777
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan 28 00:25:13.827: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 28 00:25:18.836: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/28/23 00:25:18.836
STEP: Scaling up "test-rs" replicaset  01/28/23 00:25:18.836
Jan 28 00:25:18.866: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/28/23 00:25:18.866
W0128 00:25:18.894364      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 28 00:25:18.898: INFO: observed ReplicaSet test-rs in namespace replicaset-4930 with ReadyReplicas 1, AvailableReplicas 1
Jan 28 00:25:18.954: INFO: observed ReplicaSet test-rs in namespace replicaset-4930 with ReadyReplicas 1, AvailableReplicas 1
Jan 28 00:25:19.008: INFO: observed ReplicaSet test-rs in namespace replicaset-4930 with ReadyReplicas 1, AvailableReplicas 1
Jan 28 00:25:19.022: INFO: observed ReplicaSet test-rs in namespace replicaset-4930 with ReadyReplicas 1, AvailableReplicas 1
Jan 28 00:25:21.325: INFO: observed ReplicaSet test-rs in namespace replicaset-4930 with ReadyReplicas 2, AvailableReplicas 2
Jan 28 00:25:21.583: INFO: observed Replicaset test-rs in namespace replicaset-4930 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 28 00:25:21.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4930" for this suite. 01/28/23 00:25:21.598
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":29,"skipped":568,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.883 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:25:13.73
    Jan 28 00:25:13.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename replicaset 01/28/23 00:25:13.731
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:25:13.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:25:13.777
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan 28 00:25:13.827: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 28 00:25:18.836: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/28/23 00:25:18.836
    STEP: Scaling up "test-rs" replicaset  01/28/23 00:25:18.836
    Jan 28 00:25:18.866: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/28/23 00:25:18.866
    W0128 00:25:18.894364      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 28 00:25:18.898: INFO: observed ReplicaSet test-rs in namespace replicaset-4930 with ReadyReplicas 1, AvailableReplicas 1
    Jan 28 00:25:18.954: INFO: observed ReplicaSet test-rs in namespace replicaset-4930 with ReadyReplicas 1, AvailableReplicas 1
    Jan 28 00:25:19.008: INFO: observed ReplicaSet test-rs in namespace replicaset-4930 with ReadyReplicas 1, AvailableReplicas 1
    Jan 28 00:25:19.022: INFO: observed ReplicaSet test-rs in namespace replicaset-4930 with ReadyReplicas 1, AvailableReplicas 1
    Jan 28 00:25:21.325: INFO: observed ReplicaSet test-rs in namespace replicaset-4930 with ReadyReplicas 2, AvailableReplicas 2
    Jan 28 00:25:21.583: INFO: observed Replicaset test-rs in namespace replicaset-4930 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 28 00:25:21.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4930" for this suite. 01/28/23 00:25:21.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:25:21.63
Jan 28 00:25:21.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename resourcequota 01/28/23 00:25:21.632
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:25:21.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:25:21.685
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 01/28/23 00:25:21.695
STEP: Creating a ResourceQuota 01/28/23 00:25:26.709
STEP: Ensuring resource quota status is calculated 01/28/23 00:25:26.729
STEP: Creating a ReplicationController 01/28/23 00:25:28.745
STEP: Ensuring resource quota status captures replication controller creation 01/28/23 00:25:28.782
STEP: Deleting a ReplicationController 01/28/23 00:25:30.801
STEP: Ensuring resource quota status released usage 01/28/23 00:25:30.824
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 28 00:25:32.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6639" for this suite. 01/28/23 00:25:32.849
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":30,"skipped":670,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.236 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:25:21.63
    Jan 28 00:25:21.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename resourcequota 01/28/23 00:25:21.632
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:25:21.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:25:21.685
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 01/28/23 00:25:21.695
    STEP: Creating a ResourceQuota 01/28/23 00:25:26.709
    STEP: Ensuring resource quota status is calculated 01/28/23 00:25:26.729
    STEP: Creating a ReplicationController 01/28/23 00:25:28.745
    STEP: Ensuring resource quota status captures replication controller creation 01/28/23 00:25:28.782
    STEP: Deleting a ReplicationController 01/28/23 00:25:30.801
    STEP: Ensuring resource quota status released usage 01/28/23 00:25:30.824
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 28 00:25:32.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6639" for this suite. 01/28/23 00:25:32.849
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:25:32.881
Jan 28 00:25:32.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename crd-publish-openapi 01/28/23 00:25:32.883
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:25:32.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:25:32.931
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Jan 28 00:25:32.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/28/23 00:25:37.383
Jan 28 00:25:37.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-1521 --namespace=crd-publish-openapi-1521 create -f -'
Jan 28 00:25:38.235: INFO: stderr: ""
Jan 28 00:25:38.236: INFO: stdout: "e2e-test-crd-publish-openapi-2923-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 28 00:25:38.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-1521 --namespace=crd-publish-openapi-1521 delete e2e-test-crd-publish-openapi-2923-crds test-cr'
Jan 28 00:25:38.369: INFO: stderr: ""
Jan 28 00:25:38.369: INFO: stdout: "e2e-test-crd-publish-openapi-2923-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 28 00:25:38.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-1521 --namespace=crd-publish-openapi-1521 apply -f -'
Jan 28 00:25:39.174: INFO: stderr: ""
Jan 28 00:25:39.174: INFO: stdout: "e2e-test-crd-publish-openapi-2923-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 28 00:25:39.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-1521 --namespace=crd-publish-openapi-1521 delete e2e-test-crd-publish-openapi-2923-crds test-cr'
Jan 28 00:25:39.309: INFO: stderr: ""
Jan 28 00:25:39.309: INFO: stdout: "e2e-test-crd-publish-openapi-2923-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/28/23 00:25:39.309
Jan 28 00:25:39.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-1521 explain e2e-test-crd-publish-openapi-2923-crds'
Jan 28 00:25:39.576: INFO: stderr: ""
Jan 28 00:25:39.576: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2923-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 00:25:43.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1521" for this suite. 01/28/23 00:25:43.23
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":31,"skipped":710,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.368 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:25:32.881
    Jan 28 00:25:32.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename crd-publish-openapi 01/28/23 00:25:32.883
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:25:32.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:25:32.931
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Jan 28 00:25:32.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/28/23 00:25:37.383
    Jan 28 00:25:37.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-1521 --namespace=crd-publish-openapi-1521 create -f -'
    Jan 28 00:25:38.235: INFO: stderr: ""
    Jan 28 00:25:38.236: INFO: stdout: "e2e-test-crd-publish-openapi-2923-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 28 00:25:38.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-1521 --namespace=crd-publish-openapi-1521 delete e2e-test-crd-publish-openapi-2923-crds test-cr'
    Jan 28 00:25:38.369: INFO: stderr: ""
    Jan 28 00:25:38.369: INFO: stdout: "e2e-test-crd-publish-openapi-2923-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan 28 00:25:38.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-1521 --namespace=crd-publish-openapi-1521 apply -f -'
    Jan 28 00:25:39.174: INFO: stderr: ""
    Jan 28 00:25:39.174: INFO: stdout: "e2e-test-crd-publish-openapi-2923-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 28 00:25:39.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-1521 --namespace=crd-publish-openapi-1521 delete e2e-test-crd-publish-openapi-2923-crds test-cr'
    Jan 28 00:25:39.309: INFO: stderr: ""
    Jan 28 00:25:39.309: INFO: stdout: "e2e-test-crd-publish-openapi-2923-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/28/23 00:25:39.309
    Jan 28 00:25:39.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-1521 explain e2e-test-crd-publish-openapi-2923-crds'
    Jan 28 00:25:39.576: INFO: stderr: ""
    Jan 28 00:25:39.576: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2923-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 00:25:43.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1521" for this suite. 01/28/23 00:25:43.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:25:43.26
Jan 28 00:25:43.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename job 01/28/23 00:25:43.263
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:25:43.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:25:43.311
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 01/28/23 00:25:43.322
STEP: Ensuring job reaches completions 01/28/23 00:25:43.338
STEP: Ensuring pods with index for job exist 01/28/23 00:25:53.354
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 28 00:25:53.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-237" for this suite. 01/28/23 00:25:53.386
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":32,"skipped":716,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.154 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:25:43.26
    Jan 28 00:25:43.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename job 01/28/23 00:25:43.263
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:25:43.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:25:43.311
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 01/28/23 00:25:43.322
    STEP: Ensuring job reaches completions 01/28/23 00:25:43.338
    STEP: Ensuring pods with index for job exist 01/28/23 00:25:53.354
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 28 00:25:53.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-237" for this suite. 01/28/23 00:25:53.386
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:25:53.418
Jan 28 00:25:53.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename watch 01/28/23 00:25:53.421
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:25:53.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:25:53.466
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/28/23 00:25:53.478
STEP: starting a background goroutine to produce watch events 01/28/23 00:25:53.489
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/28/23 00:25:53.489
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 28 00:25:56.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5647" for this suite. 01/28/23 00:25:56.287
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":33,"skipped":723,"failed":0}
------------------------------
â€¢ [2.921 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:25:53.418
    Jan 28 00:25:53.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename watch 01/28/23 00:25:53.421
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:25:53.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:25:53.466
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/28/23 00:25:53.478
    STEP: starting a background goroutine to produce watch events 01/28/23 00:25:53.489
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/28/23 00:25:53.489
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 28 00:25:56.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5647" for this suite. 01/28/23 00:25:56.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:25:56.347
Jan 28 00:25:56.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename sched-pred 01/28/23 00:25:56.349
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:25:56.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:25:56.405
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 28 00:25:56.416: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 28 00:25:56.449: INFO: Waiting for terminating namespaces to be deleted...
Jan 28 00:25:56.462: INFO: 
Logging pods the apiserver thinks is on node 10.9.20.126 before test
Jan 28 00:25:56.500: INFO: indexed-job-1-2wm2z from job-237 started at 2023-01-28 00:25:43 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.500: INFO: 	Container c ready: false, restart count 0
Jan 28 00:25:56.500: INFO: indexed-job-2-grldd from job-237 started at 2023-01-28 00:25:48 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.500: INFO: 	Container c ready: false, restart count 0
Jan 28 00:25:56.500: INFO: calico-node-xl9f8 from kube-system started at 2023-01-27 22:05:01 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.500: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 00:25:56.500: INFO: calico-typha-677688fdc5-twxlr from kube-system started at 2023-01-27 22:05:32 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.500: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 00:25:56.500: INFO: coredns-6754846f95-9ck4t from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.500: INFO: 	Container coredns ready: true, restart count 0
Jan 28 00:25:56.501: INFO: ibm-keepalived-watcher-b94td from kube-system started at 2023-01-27 22:05:01 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.501: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 00:25:56.501: INFO: ibm-master-proxy-static-10.9.20.126 from kube-system started at 2023-01-27 22:04:49 +0000 UTC (2 container statuses recorded)
Jan 28 00:25:56.501: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 00:25:56.501: INFO: 	Container pause ready: true, restart count 0
Jan 28 00:25:56.501: INFO: ibmcloud-block-storage-driver-lw66t from kube-system started at 2023-01-27 22:05:09 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.501: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 00:25:56.501: INFO: konnectivity-agent-65zgm from kube-system started at 2023-01-27 22:15:24 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.501: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 00:25:56.501: INFO: metrics-server-9cdf87dc6-4ptjv from kube-system started at 2023-01-27 22:51:34 +0000 UTC (3 container statuses recorded)
Jan 28 00:25:56.501: INFO: 	Container config-watcher ready: true, restart count 0
Jan 28 00:25:56.501: INFO: 	Container metrics-server ready: true, restart count 0
Jan 28 00:25:56.501: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 28 00:25:56.501: INFO: public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-vvqqx from kube-system started at 2023-01-27 22:09:22 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.501: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 28 00:25:56.501: INFO: sonobuoy from sonobuoy started at 2023-01-28 00:13:47 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.501: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 28 00:25:56.501: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-qbzm7 from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
Jan 28 00:25:56.501: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:25:56.501: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 28 00:25:56.501: INFO: 
Logging pods the apiserver thinks is on node 10.9.20.72 before test
Jan 28 00:25:56.539: INFO: ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-2gg2p from ibm-system started at 2023-01-27 22:09:35 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.539: INFO: 	Container ibm-cloud-provider-ip-163-69-70-149 ready: true, restart count 0
Jan 28 00:25:56.539: INFO: indexed-job-0-7wb2m from job-237 started at 2023-01-28 00:25:43 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.539: INFO: 	Container c ready: false, restart count 0
Jan 28 00:25:56.539: INFO: indexed-job-3-l2knj from job-237 started at 2023-01-28 00:25:48 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.539: INFO: 	Container c ready: false, restart count 0
Jan 28 00:25:56.539: INFO: calico-node-4bc72 from kube-system started at 2023-01-27 22:04:57 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.539: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 00:25:56.539: INFO: calico-typha-677688fdc5-6nr4v from kube-system started at 2023-01-27 22:05:47 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.539: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 00:25:56.539: INFO: coredns-6754846f95-b686l from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.539: INFO: 	Container coredns ready: true, restart count 0
Jan 28 00:25:56.539: INFO: ibm-keepalived-watcher-gstl5 from kube-system started at 2023-01-27 22:04:57 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.539: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 00:25:56.539: INFO: ibm-master-proxy-static-10.9.20.72 from kube-system started at 2023-01-27 22:04:44 +0000 UTC (2 container statuses recorded)
Jan 28 00:25:56.539: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 00:25:56.539: INFO: 	Container pause ready: true, restart count 0
Jan 28 00:25:56.539: INFO: ibmcloud-block-storage-driver-sktn2 from kube-system started at 2023-01-27 22:05:04 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.539: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 00:25:56.539: INFO: ingress-cluster-healthcheck-655c49644b-dmz5q from kube-system started at 2023-01-27 22:06:59 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.539: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Jan 28 00:25:56.539: INFO: konnectivity-agent-zbbtb from kube-system started at 2023-01-27 22:15:21 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.539: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 00:25:56.539: INFO: metrics-server-9cdf87dc6-n44dc from kube-system started at 2023-01-27 22:51:34 +0000 UTC (3 container statuses recorded)
Jan 28 00:25:56.539: INFO: 	Container config-watcher ready: true, restart count 0
Jan 28 00:25:56.539: INFO: 	Container metrics-server ready: true, restart count 0
Jan 28 00:25:56.539: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 28 00:25:56.539: INFO: public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-ztg7f from kube-system started at 2023-01-27 22:09:22 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.539: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 28 00:25:56.539: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-tm2wf from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
Jan 28 00:25:56.539: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:25:56.539: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 28 00:25:56.539: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-01-27 22:07:40 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.539: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jan 28 00:25:56.539: INFO: 
Logging pods the apiserver thinks is on node 10.9.20.75 before test
Jan 28 00:25:56.579: INFO: rs-b9qd7 from disruption-1565 started at 2023-01-28 00:24:15 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.579: INFO: 	Container donothing ready: false, restart count 0
Jan 28 00:25:56.579: INFO: ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-9qs6m from ibm-system started at 2023-01-27 22:09:35 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.580: INFO: 	Container ibm-cloud-provider-ip-163-69-70-149 ready: true, restart count 0
Jan 28 00:25:56.580: INFO: calico-kube-controllers-5754dfd4dd-2fl6z from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.580: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 28 00:25:56.580: INFO: calico-node-6pnwh from kube-system started at 2023-01-27 22:04:45 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.580: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 00:25:56.580: INFO: calico-typha-677688fdc5-s2khq from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.580: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 00:25:56.580: INFO: coredns-6754846f95-r75xk from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.581: INFO: 	Container coredns ready: true, restart count 0
Jan 28 00:25:56.581: INFO: coredns-autoscaler-669cf746f6-b9s85 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.581: INFO: 	Container autoscaler ready: true, restart count 0
Jan 28 00:25:56.581: INFO: dashboard-metrics-scraper-c964d5594-76f8h from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.581: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 28 00:25:56.581: INFO: ibm-file-plugin-7dd6c48b68-kn5ff from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.581: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jan 28 00:25:56.581: INFO: ibm-keepalived-watcher-gj6b6 from kube-system started at 2023-01-27 22:04:45 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.581: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 00:25:56.581: INFO: ibm-master-proxy-static-10.9.20.75 from kube-system started at 2023-01-27 22:04:42 +0000 UTC (2 container statuses recorded)
Jan 28 00:25:56.581: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 00:25:56.581: INFO: 	Container pause ready: true, restart count 0
Jan 28 00:25:56.581: INFO: ibm-storage-watcher-746995c8c9-7mmhc from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.581: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jan 28 00:25:56.581: INFO: ibmcloud-block-storage-driver-jgsbr from kube-system started at 2023-01-27 22:04:51 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.581: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 00:25:56.581: INFO: ibmcloud-block-storage-plugin-697cd846b-rglpr from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.581: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jan 28 00:25:56.581: INFO: konnectivity-agent-95p2n from kube-system started at 2023-01-27 22:15:28 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.581: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 00:25:56.581: INFO: kubernetes-dashboard-55c4d56798-7r7j7 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.582: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 28 00:25:56.582: INFO: snapshot-controller-c5c6dddff-25l94 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.582: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 28 00:25:56.582: INFO: snapshot-controller-c5c6dddff-5xzjc from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.582: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 28 00:25:56.582: INFO: snapshot-controller-c5c6dddff-pzr2k from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:25:56.582: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 28 00:25:56.582: INFO: sonobuoy-e2e-job-70551213e7cc4fe2 from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
Jan 28 00:25:56.582: INFO: 	Container e2e ready: true, restart count 0
Jan 28 00:25:56.582: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:25:56.582: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-jcljc from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
Jan 28 00:25:56.582: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:25:56.582: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/28/23 00:25:56.582
Jan 28 00:25:56.605: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4001" to be "running"
Jan 28 00:25:56.617: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 11.636141ms
Jan 28 00:25:58.629: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023285127s
Jan 28 00:26:00.629: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.023866801s
Jan 28 00:26:00.629: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/28/23 00:26:00.64
STEP: Trying to apply a random label on the found node. 01/28/23 00:26:00.684
STEP: verifying the node has the label kubernetes.io/e2e-afc9dcc5-1f23-4dd3-a485-6f1594d1e5af 95 01/28/23 00:26:00.706
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/28/23 00:26:00.72
Jan 28 00:26:00.733: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-4001" to be "not pending"
Jan 28 00:26:00.744: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.378584ms
Jan 28 00:26:02.755: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021771899s
Jan 28 00:26:04.755: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.02119039s
Jan 28 00:26:04.755: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.9.20.126 on the node which pod4 resides and expect not scheduled 01/28/23 00:26:04.755
Jan 28 00:26:04.769: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-4001" to be "not pending"
Jan 28 00:26:04.780: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.536877ms
Jan 28 00:26:06.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023113268s
Jan 28 00:26:08.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022848052s
Jan 28 00:26:10.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022415532s
Jan 28 00:26:12.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021962875s
Jan 28 00:26:14.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024481903s
Jan 28 00:26:16.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02512177s
Jan 28 00:26:18.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.024563956s
Jan 28 00:26:20.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.023378953s
Jan 28 00:26:22.789: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.020574838s
Jan 28 00:26:24.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.023588818s
Jan 28 00:26:26.796: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.026966867s
Jan 28 00:26:28.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.022951183s
Jan 28 00:26:30.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.023654814s
Jan 28 00:26:32.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.02368054s
Jan 28 00:26:34.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.024621835s
Jan 28 00:26:36.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.023220113s
Jan 28 00:26:38.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.022622521s
Jan 28 00:26:40.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.025437265s
Jan 28 00:26:42.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.023167513s
Jan 28 00:26:44.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.024154523s
Jan 28 00:26:46.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.022901987s
Jan 28 00:26:48.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.022888382s
Jan 28 00:26:50.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.023757168s
Jan 28 00:26:52.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.024140805s
Jan 28 00:26:54.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.021939307s
Jan 28 00:26:56.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.024567531s
Jan 28 00:26:58.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.022917386s
Jan 28 00:27:00.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.025053877s
Jan 28 00:27:02.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.023199147s
Jan 28 00:27:04.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.022761472s
Jan 28 00:27:06.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.023251543s
Jan 28 00:27:08.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.023622087s
Jan 28 00:27:10.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.023964871s
Jan 28 00:27:12.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.025160197s
Jan 28 00:27:14.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.023376755s
Jan 28 00:27:16.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.025163256s
Jan 28 00:27:18.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.022424814s
Jan 28 00:27:20.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.02366529s
Jan 28 00:27:22.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.023311843s
Jan 28 00:27:24.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.023027808s
Jan 28 00:27:26.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.023447585s
Jan 28 00:27:28.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.022281825s
Jan 28 00:27:30.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.021824121s
Jan 28 00:27:32.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.023229076s
Jan 28 00:27:34.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.023638241s
Jan 28 00:27:36.799: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.030141335s
Jan 28 00:27:38.797: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.028323241s
Jan 28 00:27:40.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.025282436s
Jan 28 00:27:42.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.024356367s
Jan 28 00:27:44.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.025869313s
Jan 28 00:27:46.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.023514848s
Jan 28 00:27:48.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.023307904s
Jan 28 00:27:50.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.02446719s
Jan 28 00:27:52.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.023403598s
Jan 28 00:27:54.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.021785428s
Jan 28 00:27:56.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.022925695s
Jan 28 00:27:58.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.022954819s
Jan 28 00:28:00.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.024276264s
Jan 28 00:28:02.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.024669526s
Jan 28 00:28:04.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022890617s
Jan 28 00:28:06.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.022676414s
Jan 28 00:28:08.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.022105687s
Jan 28 00:28:10.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.022503371s
Jan 28 00:28:12.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.023240188s
Jan 28 00:28:14.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.045867256s
Jan 28 00:28:16.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.024963116s
Jan 28 00:28:18.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.022263773s
Jan 28 00:28:20.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.026160832s
Jan 28 00:28:22.801: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.032112639s
Jan 28 00:28:24.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.02535447s
Jan 28 00:28:26.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.026039906s
Jan 28 00:28:28.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.021771965s
Jan 28 00:28:30.798: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.029285507s
Jan 28 00:28:32.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.026531059s
Jan 28 00:28:34.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.024850284s
Jan 28 00:28:36.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.023854466s
Jan 28 00:28:38.789: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.020579351s
Jan 28 00:28:40.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.026049936s
Jan 28 00:28:42.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.02616542s
Jan 28 00:28:44.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.02527434s
Jan 28 00:28:46.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.025424115s
Jan 28 00:28:48.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.025899134s
Jan 28 00:28:50.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.025862561s
Jan 28 00:28:52.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.025460437s
Jan 28 00:28:54.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.025025392s
Jan 28 00:28:56.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.026229554s
Jan 28 00:28:58.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.022750201s
Jan 28 00:29:00.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.025995826s
Jan 28 00:29:02.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.025273647s
Jan 28 00:29:04.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.024874428s
Jan 28 00:29:06.796: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.026931698s
Jan 28 00:29:08.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.024456723s
Jan 28 00:29:10.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.025238449s
Jan 28 00:29:12.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.025797229s
Jan 28 00:29:14.797: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.028685488s
Jan 28 00:29:16.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.025041559s
Jan 28 00:29:18.790: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.021274826s
Jan 28 00:29:20.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.02665601s
Jan 28 00:29:22.797: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.02777855s
Jan 28 00:29:24.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.024742122s
Jan 28 00:29:26.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.024720606s
Jan 28 00:29:28.790: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.021256091s
Jan 28 00:29:30.807: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.037994875s
Jan 28 00:29:32.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.023288916s
Jan 28 00:29:34.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.022979044s
Jan 28 00:29:36.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.025075516s
Jan 28 00:29:38.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.021798998s
Jan 28 00:29:40.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.023326393s
Jan 28 00:29:42.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.024026696s
Jan 28 00:29:44.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.026303923s
Jan 28 00:29:46.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.026677519s
Jan 28 00:29:48.807: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.038105386s
Jan 28 00:29:50.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.024186484s
Jan 28 00:29:52.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.0248151s
Jan 28 00:29:54.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.025045139s
Jan 28 00:29:56.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.024674318s
Jan 28 00:29:58.826: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.05696305s
Jan 28 00:30:00.797: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.027762554s
Jan 28 00:30:02.800: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.031499343s
Jan 28 00:30:04.799: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.029751899s
Jan 28 00:30:06.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.025062994s
Jan 28 00:30:08.790: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.02119521s
Jan 28 00:30:10.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.021923201s
Jan 28 00:30:12.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.023924364s
Jan 28 00:30:14.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.025469288s
Jan 28 00:30:16.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.026299741s
Jan 28 00:30:18.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.02337129s
Jan 28 00:30:20.796: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.027215378s
Jan 28 00:30:22.797: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.028305112s
Jan 28 00:30:24.798: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.029475369s
Jan 28 00:30:26.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.02544871s
Jan 28 00:30:28.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.024832258s
Jan 28 00:30:30.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.024244039s
Jan 28 00:30:32.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.025414959s
Jan 28 00:30:34.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.025754608s
Jan 28 00:30:36.796: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.027180186s
Jan 28 00:30:38.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.023886066s
Jan 28 00:30:40.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.02664314s
Jan 28 00:30:42.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.025625817s
Jan 28 00:30:44.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.026070703s
Jan 28 00:30:46.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.0254451s
Jan 28 00:30:48.790: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.021364855s
Jan 28 00:30:50.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.026472321s
Jan 28 00:30:52.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.025159449s
Jan 28 00:30:54.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.024733257s
Jan 28 00:30:56.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.02634762s
Jan 28 00:30:58.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.022708403s
Jan 28 00:31:00.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.024819488s
Jan 28 00:31:02.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.025967622s
Jan 28 00:31:04.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.025376137s
Jan 28 00:31:04.807: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.038537979s
STEP: removing the label kubernetes.io/e2e-afc9dcc5-1f23-4dd3-a485-6f1594d1e5af off the node 10.9.20.126 01/28/23 00:31:04.807
STEP: verifying the node doesn't have the label kubernetes.io/e2e-afc9dcc5-1f23-4dd3-a485-6f1594d1e5af 01/28/23 00:31:04.855
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 28 00:31:04.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4001" for this suite. 01/28/23 00:31:04.883
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":34,"skipped":735,"failed":0}
------------------------------
â€¢ [SLOW TEST] [308.554 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:25:56.347
    Jan 28 00:25:56.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename sched-pred 01/28/23 00:25:56.349
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:25:56.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:25:56.405
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 28 00:25:56.416: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 28 00:25:56.449: INFO: Waiting for terminating namespaces to be deleted...
    Jan 28 00:25:56.462: INFO: 
    Logging pods the apiserver thinks is on node 10.9.20.126 before test
    Jan 28 00:25:56.500: INFO: indexed-job-1-2wm2z from job-237 started at 2023-01-28 00:25:43 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.500: INFO: 	Container c ready: false, restart count 0
    Jan 28 00:25:56.500: INFO: indexed-job-2-grldd from job-237 started at 2023-01-28 00:25:48 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.500: INFO: 	Container c ready: false, restart count 0
    Jan 28 00:25:56.500: INFO: calico-node-xl9f8 from kube-system started at 2023-01-27 22:05:01 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.500: INFO: 	Container calico-node ready: true, restart count 0
    Jan 28 00:25:56.500: INFO: calico-typha-677688fdc5-twxlr from kube-system started at 2023-01-27 22:05:32 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.500: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 28 00:25:56.500: INFO: coredns-6754846f95-9ck4t from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.500: INFO: 	Container coredns ready: true, restart count 0
    Jan 28 00:25:56.501: INFO: ibm-keepalived-watcher-b94td from kube-system started at 2023-01-27 22:05:01 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.501: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 28 00:25:56.501: INFO: ibm-master-proxy-static-10.9.20.126 from kube-system started at 2023-01-27 22:04:49 +0000 UTC (2 container statuses recorded)
    Jan 28 00:25:56.501: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 28 00:25:56.501: INFO: 	Container pause ready: true, restart count 0
    Jan 28 00:25:56.501: INFO: ibmcloud-block-storage-driver-lw66t from kube-system started at 2023-01-27 22:05:09 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.501: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 28 00:25:56.501: INFO: konnectivity-agent-65zgm from kube-system started at 2023-01-27 22:15:24 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.501: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 28 00:25:56.501: INFO: metrics-server-9cdf87dc6-4ptjv from kube-system started at 2023-01-27 22:51:34 +0000 UTC (3 container statuses recorded)
    Jan 28 00:25:56.501: INFO: 	Container config-watcher ready: true, restart count 0
    Jan 28 00:25:56.501: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 28 00:25:56.501: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan 28 00:25:56.501: INFO: public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-vvqqx from kube-system started at 2023-01-27 22:09:22 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.501: INFO: 	Container nginx-ingress ready: true, restart count 0
    Jan 28 00:25:56.501: INFO: sonobuoy from sonobuoy started at 2023-01-28 00:13:47 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.501: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 28 00:25:56.501: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-qbzm7 from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
    Jan 28 00:25:56.501: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 28 00:25:56.501: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 28 00:25:56.501: INFO: 
    Logging pods the apiserver thinks is on node 10.9.20.72 before test
    Jan 28 00:25:56.539: INFO: ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-2gg2p from ibm-system started at 2023-01-27 22:09:35 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.539: INFO: 	Container ibm-cloud-provider-ip-163-69-70-149 ready: true, restart count 0
    Jan 28 00:25:56.539: INFO: indexed-job-0-7wb2m from job-237 started at 2023-01-28 00:25:43 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.539: INFO: 	Container c ready: false, restart count 0
    Jan 28 00:25:56.539: INFO: indexed-job-3-l2knj from job-237 started at 2023-01-28 00:25:48 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.539: INFO: 	Container c ready: false, restart count 0
    Jan 28 00:25:56.539: INFO: calico-node-4bc72 from kube-system started at 2023-01-27 22:04:57 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.539: INFO: 	Container calico-node ready: true, restart count 0
    Jan 28 00:25:56.539: INFO: calico-typha-677688fdc5-6nr4v from kube-system started at 2023-01-27 22:05:47 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.539: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 28 00:25:56.539: INFO: coredns-6754846f95-b686l from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.539: INFO: 	Container coredns ready: true, restart count 0
    Jan 28 00:25:56.539: INFO: ibm-keepalived-watcher-gstl5 from kube-system started at 2023-01-27 22:04:57 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.539: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 28 00:25:56.539: INFO: ibm-master-proxy-static-10.9.20.72 from kube-system started at 2023-01-27 22:04:44 +0000 UTC (2 container statuses recorded)
    Jan 28 00:25:56.539: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 28 00:25:56.539: INFO: 	Container pause ready: true, restart count 0
    Jan 28 00:25:56.539: INFO: ibmcloud-block-storage-driver-sktn2 from kube-system started at 2023-01-27 22:05:04 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.539: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 28 00:25:56.539: INFO: ingress-cluster-healthcheck-655c49644b-dmz5q from kube-system started at 2023-01-27 22:06:59 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.539: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
    Jan 28 00:25:56.539: INFO: konnectivity-agent-zbbtb from kube-system started at 2023-01-27 22:15:21 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.539: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 28 00:25:56.539: INFO: metrics-server-9cdf87dc6-n44dc from kube-system started at 2023-01-27 22:51:34 +0000 UTC (3 container statuses recorded)
    Jan 28 00:25:56.539: INFO: 	Container config-watcher ready: true, restart count 0
    Jan 28 00:25:56.539: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 28 00:25:56.539: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan 28 00:25:56.539: INFO: public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-ztg7f from kube-system started at 2023-01-27 22:09:22 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.539: INFO: 	Container nginx-ingress ready: true, restart count 0
    Jan 28 00:25:56.539: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-tm2wf from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
    Jan 28 00:25:56.539: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 28 00:25:56.539: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 28 00:25:56.539: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-01-27 22:07:40 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.539: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    Jan 28 00:25:56.539: INFO: 
    Logging pods the apiserver thinks is on node 10.9.20.75 before test
    Jan 28 00:25:56.579: INFO: rs-b9qd7 from disruption-1565 started at 2023-01-28 00:24:15 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.579: INFO: 	Container donothing ready: false, restart count 0
    Jan 28 00:25:56.579: INFO: ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-9qs6m from ibm-system started at 2023-01-27 22:09:35 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.580: INFO: 	Container ibm-cloud-provider-ip-163-69-70-149 ready: true, restart count 0
    Jan 28 00:25:56.580: INFO: calico-kube-controllers-5754dfd4dd-2fl6z from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.580: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 28 00:25:56.580: INFO: calico-node-6pnwh from kube-system started at 2023-01-27 22:04:45 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.580: INFO: 	Container calico-node ready: true, restart count 0
    Jan 28 00:25:56.580: INFO: calico-typha-677688fdc5-s2khq from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.580: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 28 00:25:56.580: INFO: coredns-6754846f95-r75xk from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.581: INFO: 	Container coredns ready: true, restart count 0
    Jan 28 00:25:56.581: INFO: coredns-autoscaler-669cf746f6-b9s85 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.581: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 28 00:25:56.581: INFO: dashboard-metrics-scraper-c964d5594-76f8h from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.581: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Jan 28 00:25:56.581: INFO: ibm-file-plugin-7dd6c48b68-kn5ff from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.581: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Jan 28 00:25:56.581: INFO: ibm-keepalived-watcher-gj6b6 from kube-system started at 2023-01-27 22:04:45 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.581: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 28 00:25:56.581: INFO: ibm-master-proxy-static-10.9.20.75 from kube-system started at 2023-01-27 22:04:42 +0000 UTC (2 container statuses recorded)
    Jan 28 00:25:56.581: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 28 00:25:56.581: INFO: 	Container pause ready: true, restart count 0
    Jan 28 00:25:56.581: INFO: ibm-storage-watcher-746995c8c9-7mmhc from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.581: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Jan 28 00:25:56.581: INFO: ibmcloud-block-storage-driver-jgsbr from kube-system started at 2023-01-27 22:04:51 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.581: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 28 00:25:56.581: INFO: ibmcloud-block-storage-plugin-697cd846b-rglpr from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.581: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Jan 28 00:25:56.581: INFO: konnectivity-agent-95p2n from kube-system started at 2023-01-27 22:15:28 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.581: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 28 00:25:56.581: INFO: kubernetes-dashboard-55c4d56798-7r7j7 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.582: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Jan 28 00:25:56.582: INFO: snapshot-controller-c5c6dddff-25l94 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.582: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 28 00:25:56.582: INFO: snapshot-controller-c5c6dddff-5xzjc from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.582: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 28 00:25:56.582: INFO: snapshot-controller-c5c6dddff-pzr2k from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:25:56.582: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 28 00:25:56.582: INFO: sonobuoy-e2e-job-70551213e7cc4fe2 from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
    Jan 28 00:25:56.582: INFO: 	Container e2e ready: true, restart count 0
    Jan 28 00:25:56.582: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 28 00:25:56.582: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-jcljc from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
    Jan 28 00:25:56.582: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 28 00:25:56.582: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/28/23 00:25:56.582
    Jan 28 00:25:56.605: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4001" to be "running"
    Jan 28 00:25:56.617: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 11.636141ms
    Jan 28 00:25:58.629: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023285127s
    Jan 28 00:26:00.629: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.023866801s
    Jan 28 00:26:00.629: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/28/23 00:26:00.64
    STEP: Trying to apply a random label on the found node. 01/28/23 00:26:00.684
    STEP: verifying the node has the label kubernetes.io/e2e-afc9dcc5-1f23-4dd3-a485-6f1594d1e5af 95 01/28/23 00:26:00.706
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/28/23 00:26:00.72
    Jan 28 00:26:00.733: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-4001" to be "not pending"
    Jan 28 00:26:00.744: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.378584ms
    Jan 28 00:26:02.755: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021771899s
    Jan 28 00:26:04.755: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.02119039s
    Jan 28 00:26:04.755: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.9.20.126 on the node which pod4 resides and expect not scheduled 01/28/23 00:26:04.755
    Jan 28 00:26:04.769: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-4001" to be "not pending"
    Jan 28 00:26:04.780: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.536877ms
    Jan 28 00:26:06.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023113268s
    Jan 28 00:26:08.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022848052s
    Jan 28 00:26:10.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022415532s
    Jan 28 00:26:12.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021962875s
    Jan 28 00:26:14.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024481903s
    Jan 28 00:26:16.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02512177s
    Jan 28 00:26:18.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.024563956s
    Jan 28 00:26:20.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.023378953s
    Jan 28 00:26:22.789: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.020574838s
    Jan 28 00:26:24.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.023588818s
    Jan 28 00:26:26.796: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.026966867s
    Jan 28 00:26:28.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.022951183s
    Jan 28 00:26:30.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.023654814s
    Jan 28 00:26:32.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.02368054s
    Jan 28 00:26:34.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.024621835s
    Jan 28 00:26:36.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.023220113s
    Jan 28 00:26:38.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.022622521s
    Jan 28 00:26:40.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.025437265s
    Jan 28 00:26:42.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.023167513s
    Jan 28 00:26:44.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.024154523s
    Jan 28 00:26:46.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.022901987s
    Jan 28 00:26:48.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.022888382s
    Jan 28 00:26:50.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.023757168s
    Jan 28 00:26:52.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.024140805s
    Jan 28 00:26:54.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.021939307s
    Jan 28 00:26:56.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.024567531s
    Jan 28 00:26:58.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.022917386s
    Jan 28 00:27:00.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.025053877s
    Jan 28 00:27:02.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.023199147s
    Jan 28 00:27:04.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.022761472s
    Jan 28 00:27:06.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.023251543s
    Jan 28 00:27:08.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.023622087s
    Jan 28 00:27:10.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.023964871s
    Jan 28 00:27:12.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.025160197s
    Jan 28 00:27:14.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.023376755s
    Jan 28 00:27:16.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.025163256s
    Jan 28 00:27:18.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.022424814s
    Jan 28 00:27:20.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.02366529s
    Jan 28 00:27:22.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.023311843s
    Jan 28 00:27:24.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.023027808s
    Jan 28 00:27:26.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.023447585s
    Jan 28 00:27:28.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.022281825s
    Jan 28 00:27:30.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.021824121s
    Jan 28 00:27:32.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.023229076s
    Jan 28 00:27:34.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.023638241s
    Jan 28 00:27:36.799: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.030141335s
    Jan 28 00:27:38.797: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.028323241s
    Jan 28 00:27:40.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.025282436s
    Jan 28 00:27:42.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.024356367s
    Jan 28 00:27:44.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.025869313s
    Jan 28 00:27:46.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.023514848s
    Jan 28 00:27:48.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.023307904s
    Jan 28 00:27:50.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.02446719s
    Jan 28 00:27:52.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.023403598s
    Jan 28 00:27:54.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.021785428s
    Jan 28 00:27:56.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.022925695s
    Jan 28 00:27:58.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.022954819s
    Jan 28 00:28:00.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.024276264s
    Jan 28 00:28:02.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.024669526s
    Jan 28 00:28:04.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022890617s
    Jan 28 00:28:06.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.022676414s
    Jan 28 00:28:08.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.022105687s
    Jan 28 00:28:10.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.022503371s
    Jan 28 00:28:12.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.023240188s
    Jan 28 00:28:14.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.045867256s
    Jan 28 00:28:16.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.024963116s
    Jan 28 00:28:18.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.022263773s
    Jan 28 00:28:20.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.026160832s
    Jan 28 00:28:22.801: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.032112639s
    Jan 28 00:28:24.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.02535447s
    Jan 28 00:28:26.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.026039906s
    Jan 28 00:28:28.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.021771965s
    Jan 28 00:28:30.798: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.029285507s
    Jan 28 00:28:32.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.026531059s
    Jan 28 00:28:34.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.024850284s
    Jan 28 00:28:36.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.023854466s
    Jan 28 00:28:38.789: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.020579351s
    Jan 28 00:28:40.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.026049936s
    Jan 28 00:28:42.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.02616542s
    Jan 28 00:28:44.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.02527434s
    Jan 28 00:28:46.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.025424115s
    Jan 28 00:28:48.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.025899134s
    Jan 28 00:28:50.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.025862561s
    Jan 28 00:28:52.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.025460437s
    Jan 28 00:28:54.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.025025392s
    Jan 28 00:28:56.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.026229554s
    Jan 28 00:28:58.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.022750201s
    Jan 28 00:29:00.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.025995826s
    Jan 28 00:29:02.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.025273647s
    Jan 28 00:29:04.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.024874428s
    Jan 28 00:29:06.796: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.026931698s
    Jan 28 00:29:08.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.024456723s
    Jan 28 00:29:10.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.025238449s
    Jan 28 00:29:12.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.025797229s
    Jan 28 00:29:14.797: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.028685488s
    Jan 28 00:29:16.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.025041559s
    Jan 28 00:29:18.790: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.021274826s
    Jan 28 00:29:20.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.02665601s
    Jan 28 00:29:22.797: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.02777855s
    Jan 28 00:29:24.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.024742122s
    Jan 28 00:29:26.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.024720606s
    Jan 28 00:29:28.790: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.021256091s
    Jan 28 00:29:30.807: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.037994875s
    Jan 28 00:29:32.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.023288916s
    Jan 28 00:29:34.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.022979044s
    Jan 28 00:29:36.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.025075516s
    Jan 28 00:29:38.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.021798998s
    Jan 28 00:29:40.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.023326393s
    Jan 28 00:29:42.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.024026696s
    Jan 28 00:29:44.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.026303923s
    Jan 28 00:29:46.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.026677519s
    Jan 28 00:29:48.807: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.038105386s
    Jan 28 00:29:50.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.024186484s
    Jan 28 00:29:52.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.0248151s
    Jan 28 00:29:54.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.025045139s
    Jan 28 00:29:56.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.024674318s
    Jan 28 00:29:58.826: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.05696305s
    Jan 28 00:30:00.797: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.027762554s
    Jan 28 00:30:02.800: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.031499343s
    Jan 28 00:30:04.799: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.029751899s
    Jan 28 00:30:06.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.025062994s
    Jan 28 00:30:08.790: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.02119521s
    Jan 28 00:30:10.791: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.021923201s
    Jan 28 00:30:12.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.023924364s
    Jan 28 00:30:14.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.025469288s
    Jan 28 00:30:16.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.026299741s
    Jan 28 00:30:18.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.02337129s
    Jan 28 00:30:20.796: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.027215378s
    Jan 28 00:30:22.797: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.028305112s
    Jan 28 00:30:24.798: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.029475369s
    Jan 28 00:30:26.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.02544871s
    Jan 28 00:30:28.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.024832258s
    Jan 28 00:30:30.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.024244039s
    Jan 28 00:30:32.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.025414959s
    Jan 28 00:30:34.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.025754608s
    Jan 28 00:30:36.796: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.027180186s
    Jan 28 00:30:38.793: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.023886066s
    Jan 28 00:30:40.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.02664314s
    Jan 28 00:30:42.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.025625817s
    Jan 28 00:30:44.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.026070703s
    Jan 28 00:30:46.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.0254451s
    Jan 28 00:30:48.790: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.021364855s
    Jan 28 00:30:50.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.026472321s
    Jan 28 00:30:52.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.025159449s
    Jan 28 00:30:54.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.024733257s
    Jan 28 00:30:56.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.02634762s
    Jan 28 00:30:58.792: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.022708403s
    Jan 28 00:31:00.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.024819488s
    Jan 28 00:31:02.795: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.025967622s
    Jan 28 00:31:04.794: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.025376137s
    Jan 28 00:31:04.807: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.038537979s
    STEP: removing the label kubernetes.io/e2e-afc9dcc5-1f23-4dd3-a485-6f1594d1e5af off the node 10.9.20.126 01/28/23 00:31:04.807
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-afc9dcc5-1f23-4dd3-a485-6f1594d1e5af 01/28/23 00:31:04.855
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 00:31:04.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-4001" for this suite. 01/28/23 00:31:04.883
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:31:04.903
Jan 28 00:31:04.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename subpath 01/28/23 00:31:04.906
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:31:04.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:31:04.964
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/28/23 00:31:04.978
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-6qqv 01/28/23 00:31:05.006
STEP: Creating a pod to test atomic-volume-subpath 01/28/23 00:31:05.006
Jan 28 00:31:05.026: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6qqv" in namespace "subpath-5360" to be "Succeeded or Failed"
Jan 28 00:31:05.035: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Pending", Reason="", readiness=false. Elapsed: 8.894126ms
Jan 28 00:31:07.051: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024415049s
Jan 28 00:31:09.049: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 4.022071085s
Jan 28 00:31:11.050: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 6.023325239s
Jan 28 00:31:13.051: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 8.024047553s
Jan 28 00:31:15.049: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 10.022109017s
Jan 28 00:31:17.049: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 12.022632638s
Jan 28 00:31:19.051: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 14.024067519s
Jan 28 00:31:21.049: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 16.022646886s
Jan 28 00:31:23.051: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 18.024507304s
Jan 28 00:31:25.049: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 20.022722616s
Jan 28 00:31:27.048: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 22.021881049s
Jan 28 00:31:29.051: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=false. Elapsed: 24.024056085s
Jan 28 00:31:31.050: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.023198054s
STEP: Saw pod success 01/28/23 00:31:31.05
Jan 28 00:31:31.050: INFO: Pod "pod-subpath-test-configmap-6qqv" satisfied condition "Succeeded or Failed"
Jan 28 00:31:31.062: INFO: Trying to get logs from node 10.9.20.72 pod pod-subpath-test-configmap-6qqv container test-container-subpath-configmap-6qqv: <nil>
STEP: delete the pod 01/28/23 00:31:31.148
Jan 28 00:31:31.174: INFO: Waiting for pod pod-subpath-test-configmap-6qqv to disappear
Jan 28 00:31:31.183: INFO: Pod pod-subpath-test-configmap-6qqv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-6qqv 01/28/23 00:31:31.183
Jan 28 00:31:31.183: INFO: Deleting pod "pod-subpath-test-configmap-6qqv" in namespace "subpath-5360"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 28 00:31:31.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5360" for this suite. 01/28/23 00:31:31.207
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":35,"skipped":746,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.318 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:31:04.903
    Jan 28 00:31:04.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename subpath 01/28/23 00:31:04.906
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:31:04.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:31:04.964
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/28/23 00:31:04.978
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-6qqv 01/28/23 00:31:05.006
    STEP: Creating a pod to test atomic-volume-subpath 01/28/23 00:31:05.006
    Jan 28 00:31:05.026: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6qqv" in namespace "subpath-5360" to be "Succeeded or Failed"
    Jan 28 00:31:05.035: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Pending", Reason="", readiness=false. Elapsed: 8.894126ms
    Jan 28 00:31:07.051: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024415049s
    Jan 28 00:31:09.049: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 4.022071085s
    Jan 28 00:31:11.050: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 6.023325239s
    Jan 28 00:31:13.051: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 8.024047553s
    Jan 28 00:31:15.049: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 10.022109017s
    Jan 28 00:31:17.049: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 12.022632638s
    Jan 28 00:31:19.051: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 14.024067519s
    Jan 28 00:31:21.049: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 16.022646886s
    Jan 28 00:31:23.051: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 18.024507304s
    Jan 28 00:31:25.049: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 20.022722616s
    Jan 28 00:31:27.048: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=true. Elapsed: 22.021881049s
    Jan 28 00:31:29.051: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Running", Reason="", readiness=false. Elapsed: 24.024056085s
    Jan 28 00:31:31.050: INFO: Pod "pod-subpath-test-configmap-6qqv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.023198054s
    STEP: Saw pod success 01/28/23 00:31:31.05
    Jan 28 00:31:31.050: INFO: Pod "pod-subpath-test-configmap-6qqv" satisfied condition "Succeeded or Failed"
    Jan 28 00:31:31.062: INFO: Trying to get logs from node 10.9.20.72 pod pod-subpath-test-configmap-6qqv container test-container-subpath-configmap-6qqv: <nil>
    STEP: delete the pod 01/28/23 00:31:31.148
    Jan 28 00:31:31.174: INFO: Waiting for pod pod-subpath-test-configmap-6qqv to disappear
    Jan 28 00:31:31.183: INFO: Pod pod-subpath-test-configmap-6qqv no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-6qqv 01/28/23 00:31:31.183
    Jan 28 00:31:31.183: INFO: Deleting pod "pod-subpath-test-configmap-6qqv" in namespace "subpath-5360"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 28 00:31:31.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-5360" for this suite. 01/28/23 00:31:31.207
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:31:31.221
Jan 28 00:31:31.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 00:31:31.224
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:31:31.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:31:31.281
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 01/28/23 00:31:31.297
Jan 28 00:31:31.334: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd98f28a-e493-4cda-af94-52fd4cfc3787" in namespace "projected-6593" to be "Succeeded or Failed"
Jan 28 00:31:31.344: INFO: Pod "downwardapi-volume-cd98f28a-e493-4cda-af94-52fd4cfc3787": Phase="Pending", Reason="", readiness=false. Elapsed: 10.46031ms
Jan 28 00:31:33.359: INFO: Pod "downwardapi-volume-cd98f28a-e493-4cda-af94-52fd4cfc3787": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025546458s
Jan 28 00:31:35.366: INFO: Pod "downwardapi-volume-cd98f28a-e493-4cda-af94-52fd4cfc3787": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032619133s
Jan 28 00:31:37.360: INFO: Pod "downwardapi-volume-cd98f28a-e493-4cda-af94-52fd4cfc3787": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026275449s
STEP: Saw pod success 01/28/23 00:31:37.36
Jan 28 00:31:37.360: INFO: Pod "downwardapi-volume-cd98f28a-e493-4cda-af94-52fd4cfc3787" satisfied condition "Succeeded or Failed"
Jan 28 00:31:37.374: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-cd98f28a-e493-4cda-af94-52fd4cfc3787 container client-container: <nil>
STEP: delete the pod 01/28/23 00:31:37.45
Jan 28 00:31:37.475: INFO: Waiting for pod downwardapi-volume-cd98f28a-e493-4cda-af94-52fd4cfc3787 to disappear
Jan 28 00:31:37.488: INFO: Pod downwardapi-volume-cd98f28a-e493-4cda-af94-52fd4cfc3787 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 28 00:31:37.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6593" for this suite. 01/28/23 00:31:37.505
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":36,"skipped":746,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.301 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:31:31.221
    Jan 28 00:31:31.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 00:31:31.224
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:31:31.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:31:31.281
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 01/28/23 00:31:31.297
    Jan 28 00:31:31.334: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd98f28a-e493-4cda-af94-52fd4cfc3787" in namespace "projected-6593" to be "Succeeded or Failed"
    Jan 28 00:31:31.344: INFO: Pod "downwardapi-volume-cd98f28a-e493-4cda-af94-52fd4cfc3787": Phase="Pending", Reason="", readiness=false. Elapsed: 10.46031ms
    Jan 28 00:31:33.359: INFO: Pod "downwardapi-volume-cd98f28a-e493-4cda-af94-52fd4cfc3787": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025546458s
    Jan 28 00:31:35.366: INFO: Pod "downwardapi-volume-cd98f28a-e493-4cda-af94-52fd4cfc3787": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032619133s
    Jan 28 00:31:37.360: INFO: Pod "downwardapi-volume-cd98f28a-e493-4cda-af94-52fd4cfc3787": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026275449s
    STEP: Saw pod success 01/28/23 00:31:37.36
    Jan 28 00:31:37.360: INFO: Pod "downwardapi-volume-cd98f28a-e493-4cda-af94-52fd4cfc3787" satisfied condition "Succeeded or Failed"
    Jan 28 00:31:37.374: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-cd98f28a-e493-4cda-af94-52fd4cfc3787 container client-container: <nil>
    STEP: delete the pod 01/28/23 00:31:37.45
    Jan 28 00:31:37.475: INFO: Waiting for pod downwardapi-volume-cd98f28a-e493-4cda-af94-52fd4cfc3787 to disappear
    Jan 28 00:31:37.488: INFO: Pod downwardapi-volume-cd98f28a-e493-4cda-af94-52fd4cfc3787 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 28 00:31:37.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6593" for this suite. 01/28/23 00:31:37.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:31:37.525
Jan 28 00:31:37.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename webhook 01/28/23 00:31:37.528
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:31:37.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:31:37.59
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/28/23 00:31:37.646
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 00:31:37.914
STEP: Deploying the webhook pod 01/28/23 00:31:37.941
STEP: Wait for the deployment to be ready 01/28/23 00:31:37.975
Jan 28 00:31:38.004: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 28 00:31:40.049: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 31, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 31, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 31, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 31, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/28/23 00:31:42.066
STEP: Verifying the service has paired with the endpoint 01/28/23 00:31:42.099
Jan 28 00:31:43.099: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Jan 28 00:31:43.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4489-crds.webhook.example.com via the AdmissionRegistration API 01/28/23 00:31:43.661
STEP: Creating a custom resource while v1 is storage version 01/28/23 00:31:43.745
STEP: Patching Custom Resource Definition to set v2 as storage 01/28/23 00:31:45.906
STEP: Patching the custom resource while v2 is storage version 01/28/23 00:31:45.948
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 00:31:46.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6937" for this suite. 01/28/23 00:31:46.641
STEP: Destroying namespace "webhook-6937-markers" for this suite. 01/28/23 00:31:46.659
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":37,"skipped":755,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.312 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:31:37.525
    Jan 28 00:31:37.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename webhook 01/28/23 00:31:37.528
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:31:37.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:31:37.59
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/28/23 00:31:37.646
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 00:31:37.914
    STEP: Deploying the webhook pod 01/28/23 00:31:37.941
    STEP: Wait for the deployment to be ready 01/28/23 00:31:37.975
    Jan 28 00:31:38.004: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 28 00:31:40.049: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 31, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 31, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 31, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 31, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/28/23 00:31:42.066
    STEP: Verifying the service has paired with the endpoint 01/28/23 00:31:42.099
    Jan 28 00:31:43.099: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Jan 28 00:31:43.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4489-crds.webhook.example.com via the AdmissionRegistration API 01/28/23 00:31:43.661
    STEP: Creating a custom resource while v1 is storage version 01/28/23 00:31:43.745
    STEP: Patching Custom Resource Definition to set v2 as storage 01/28/23 00:31:45.906
    STEP: Patching the custom resource while v2 is storage version 01/28/23 00:31:45.948
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 00:31:46.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6937" for this suite. 01/28/23 00:31:46.641
    STEP: Destroying namespace "webhook-6937-markers" for this suite. 01/28/23 00:31:46.659
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:31:46.839
Jan 28 00:31:46.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename pods 01/28/23 00:31:46.843
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:31:46.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:31:46.893
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Jan 28 00:31:46.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: creating the pod 01/28/23 00:31:46.906
STEP: submitting the pod to kubernetes 01/28/23 00:31:46.906
Jan 28 00:31:46.922: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88" in namespace "pods-8095" to be "running and ready"
Jan 28 00:31:46.934: INFO: Pod "pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88": Phase="Pending", Reason="", readiness=false. Elapsed: 11.312467ms
Jan 28 00:31:46.934: INFO: The phase of Pod pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:31:48.954: INFO: Pod "pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031678739s
Jan 28 00:31:48.954: INFO: The phase of Pod pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:31:50.950: INFO: Pod "pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88": Phase="Running", Reason="", readiness=true. Elapsed: 4.027864813s
Jan 28 00:31:50.950: INFO: The phase of Pod pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88 is Running (Ready = true)
Jan 28 00:31:50.950: INFO: Pod "pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 28 00:31:51.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8095" for this suite. 01/28/23 00:31:51.184
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":38,"skipped":757,"failed":0}
------------------------------
â€¢ [4.366 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:31:46.839
    Jan 28 00:31:46.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename pods 01/28/23 00:31:46.843
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:31:46.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:31:46.893
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Jan 28 00:31:46.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: creating the pod 01/28/23 00:31:46.906
    STEP: submitting the pod to kubernetes 01/28/23 00:31:46.906
    Jan 28 00:31:46.922: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88" in namespace "pods-8095" to be "running and ready"
    Jan 28 00:31:46.934: INFO: Pod "pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88": Phase="Pending", Reason="", readiness=false. Elapsed: 11.312467ms
    Jan 28 00:31:46.934: INFO: The phase of Pod pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:31:48.954: INFO: Pod "pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031678739s
    Jan 28 00:31:48.954: INFO: The phase of Pod pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:31:50.950: INFO: Pod "pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88": Phase="Running", Reason="", readiness=true. Elapsed: 4.027864813s
    Jan 28 00:31:50.950: INFO: The phase of Pod pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88 is Running (Ready = true)
    Jan 28 00:31:50.950: INFO: Pod "pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 28 00:31:51.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8095" for this suite. 01/28/23 00:31:51.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:31:51.207
Jan 28 00:31:51.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename sched-pred 01/28/23 00:31:51.209
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:31:51.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:31:51.271
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 28 00:31:51.285: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 28 00:31:51.316: INFO: Waiting for terminating namespaces to be deleted...
Jan 28 00:31:51.329: INFO: 
Logging pods the apiserver thinks is on node 10.9.20.126 before test
Jan 28 00:31:51.357: INFO: calico-node-xl9f8 from kube-system started at 2023-01-27 22:05:01 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.357: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 00:31:51.357: INFO: calico-typha-677688fdc5-twxlr from kube-system started at 2023-01-27 22:05:32 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.357: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 00:31:51.357: INFO: coredns-6754846f95-9ck4t from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.357: INFO: 	Container coredns ready: true, restart count 0
Jan 28 00:31:51.357: INFO: ibm-keepalived-watcher-b94td from kube-system started at 2023-01-27 22:05:01 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.357: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 00:31:51.357: INFO: ibm-master-proxy-static-10.9.20.126 from kube-system started at 2023-01-27 22:04:49 +0000 UTC (2 container statuses recorded)
Jan 28 00:31:51.357: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 00:31:51.357: INFO: 	Container pause ready: true, restart count 0
Jan 28 00:31:51.358: INFO: ibmcloud-block-storage-driver-lw66t from kube-system started at 2023-01-27 22:05:09 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.358: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 00:31:51.358: INFO: konnectivity-agent-65zgm from kube-system started at 2023-01-27 22:15:24 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.358: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 00:31:51.358: INFO: metrics-server-9cdf87dc6-4ptjv from kube-system started at 2023-01-27 22:51:34 +0000 UTC (3 container statuses recorded)
Jan 28 00:31:51.358: INFO: 	Container config-watcher ready: true, restart count 0
Jan 28 00:31:51.358: INFO: 	Container metrics-server ready: true, restart count 0
Jan 28 00:31:51.358: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 28 00:31:51.358: INFO: public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-vvqqx from kube-system started at 2023-01-27 22:09:22 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.358: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 28 00:31:51.358: INFO: pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88 from pods-8095 started at 2023-01-28 00:31:46 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.358: INFO: 	Container main ready: true, restart count 0
Jan 28 00:31:51.358: INFO: sonobuoy from sonobuoy started at 2023-01-28 00:13:47 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.358: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 28 00:31:51.358: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-qbzm7 from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
Jan 28 00:31:51.358: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:31:51.358: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 28 00:31:51.358: INFO: 
Logging pods the apiserver thinks is on node 10.9.20.72 before test
Jan 28 00:31:51.385: INFO: ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-2gg2p from ibm-system started at 2023-01-27 22:09:35 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.385: INFO: 	Container ibm-cloud-provider-ip-163-69-70-149 ready: true, restart count 0
Jan 28 00:31:51.385: INFO: calico-node-4bc72 from kube-system started at 2023-01-27 22:04:57 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.385: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 00:31:51.385: INFO: calico-typha-677688fdc5-6nr4v from kube-system started at 2023-01-27 22:05:47 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.385: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 00:31:51.385: INFO: coredns-6754846f95-b686l from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.385: INFO: 	Container coredns ready: true, restart count 0
Jan 28 00:31:51.385: INFO: ibm-keepalived-watcher-gstl5 from kube-system started at 2023-01-27 22:04:57 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.385: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 00:31:51.385: INFO: ibm-master-proxy-static-10.9.20.72 from kube-system started at 2023-01-27 22:04:44 +0000 UTC (2 container statuses recorded)
Jan 28 00:31:51.385: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 00:31:51.385: INFO: 	Container pause ready: true, restart count 0
Jan 28 00:31:51.385: INFO: ibmcloud-block-storage-driver-sktn2 from kube-system started at 2023-01-27 22:05:04 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.385: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 00:31:51.385: INFO: ingress-cluster-healthcheck-655c49644b-dmz5q from kube-system started at 2023-01-27 22:06:59 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.385: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Jan 28 00:31:51.385: INFO: konnectivity-agent-zbbtb from kube-system started at 2023-01-27 22:15:21 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.385: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 00:31:51.385: INFO: metrics-server-9cdf87dc6-n44dc from kube-system started at 2023-01-27 22:51:34 +0000 UTC (3 container statuses recorded)
Jan 28 00:31:51.385: INFO: 	Container config-watcher ready: true, restart count 0
Jan 28 00:31:51.385: INFO: 	Container metrics-server ready: true, restart count 0
Jan 28 00:31:51.386: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 28 00:31:51.386: INFO: public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-ztg7f from kube-system started at 2023-01-27 22:09:22 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.386: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 28 00:31:51.386: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-tm2wf from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
Jan 28 00:31:51.386: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:31:51.386: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 28 00:31:51.386: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-01-27 22:07:40 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.386: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jan 28 00:31:51.386: INFO: 
Logging pods the apiserver thinks is on node 10.9.20.75 before test
Jan 28 00:31:51.416: INFO: ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-9qs6m from ibm-system started at 2023-01-27 22:09:35 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.416: INFO: 	Container ibm-cloud-provider-ip-163-69-70-149 ready: true, restart count 0
Jan 28 00:31:51.416: INFO: calico-kube-controllers-5754dfd4dd-2fl6z from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.416: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 28 00:31:51.416: INFO: calico-node-6pnwh from kube-system started at 2023-01-27 22:04:45 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.416: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 00:31:51.416: INFO: calico-typha-677688fdc5-s2khq from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.416: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 00:31:51.416: INFO: coredns-6754846f95-r75xk from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.416: INFO: 	Container coredns ready: true, restart count 0
Jan 28 00:31:51.416: INFO: coredns-autoscaler-669cf746f6-b9s85 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.416: INFO: 	Container autoscaler ready: true, restart count 0
Jan 28 00:31:51.416: INFO: dashboard-metrics-scraper-c964d5594-76f8h from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.416: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 28 00:31:51.416: INFO: ibm-file-plugin-7dd6c48b68-kn5ff from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.416: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jan 28 00:31:51.416: INFO: ibm-keepalived-watcher-gj6b6 from kube-system started at 2023-01-27 22:04:45 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.416: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 00:31:51.417: INFO: ibm-master-proxy-static-10.9.20.75 from kube-system started at 2023-01-27 22:04:42 +0000 UTC (2 container statuses recorded)
Jan 28 00:31:51.417: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 00:31:51.417: INFO: 	Container pause ready: true, restart count 0
Jan 28 00:31:51.417: INFO: ibm-storage-watcher-746995c8c9-7mmhc from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.417: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jan 28 00:31:51.417: INFO: ibmcloud-block-storage-driver-jgsbr from kube-system started at 2023-01-27 22:04:51 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.417: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 00:31:51.417: INFO: ibmcloud-block-storage-plugin-697cd846b-rglpr from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.417: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jan 28 00:31:51.417: INFO: konnectivity-agent-95p2n from kube-system started at 2023-01-27 22:15:28 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.417: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 00:31:51.417: INFO: kubernetes-dashboard-55c4d56798-7r7j7 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.417: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 28 00:31:51.417: INFO: snapshot-controller-c5c6dddff-25l94 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.417: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 28 00:31:51.417: INFO: snapshot-controller-c5c6dddff-5xzjc from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.417: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 28 00:31:51.417: INFO: snapshot-controller-c5c6dddff-pzr2k from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:31:51.417: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 28 00:31:51.417: INFO: sonobuoy-e2e-job-70551213e7cc4fe2 from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
Jan 28 00:31:51.417: INFO: 	Container e2e ready: true, restart count 0
Jan 28 00:31:51.417: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:31:51.417: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-jcljc from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
Jan 28 00:31:51.417: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:31:51.417: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node 10.9.20.126 01/28/23 00:31:51.478
STEP: verifying the node has the label node 10.9.20.72 01/28/23 00:31:51.525
STEP: verifying the node has the label node 10.9.20.75 01/28/23 00:31:51.563
Jan 28 00:31:51.634: INFO: Pod ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-2gg2p requesting resource cpu=5m on Node 10.9.20.72
Jan 28 00:31:51.634: INFO: Pod ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-9qs6m requesting resource cpu=5m on Node 10.9.20.75
Jan 28 00:31:51.634: INFO: Pod calico-kube-controllers-5754dfd4dd-2fl6z requesting resource cpu=10m on Node 10.9.20.75
Jan 28 00:31:51.634: INFO: Pod calico-node-4bc72 requesting resource cpu=250m on Node 10.9.20.72
Jan 28 00:31:51.634: INFO: Pod calico-node-6pnwh requesting resource cpu=250m on Node 10.9.20.75
Jan 28 00:31:51.634: INFO: Pod calico-node-xl9f8 requesting resource cpu=250m on Node 10.9.20.126
Jan 28 00:31:51.634: INFO: Pod calico-typha-677688fdc5-6nr4v requesting resource cpu=250m on Node 10.9.20.72
Jan 28 00:31:51.635: INFO: Pod calico-typha-677688fdc5-s2khq requesting resource cpu=250m on Node 10.9.20.75
Jan 28 00:31:51.635: INFO: Pod calico-typha-677688fdc5-twxlr requesting resource cpu=250m on Node 10.9.20.126
Jan 28 00:31:51.635: INFO: Pod coredns-6754846f95-9ck4t requesting resource cpu=100m on Node 10.9.20.126
Jan 28 00:31:51.635: INFO: Pod coredns-6754846f95-b686l requesting resource cpu=100m on Node 10.9.20.72
Jan 28 00:31:51.635: INFO: Pod coredns-6754846f95-r75xk requesting resource cpu=100m on Node 10.9.20.75
Jan 28 00:31:51.635: INFO: Pod coredns-autoscaler-669cf746f6-b9s85 requesting resource cpu=1m on Node 10.9.20.75
Jan 28 00:31:51.635: INFO: Pod dashboard-metrics-scraper-c964d5594-76f8h requesting resource cpu=1m on Node 10.9.20.75
Jan 28 00:31:51.635: INFO: Pod ibm-file-plugin-7dd6c48b68-kn5ff requesting resource cpu=50m on Node 10.9.20.75
Jan 28 00:31:51.635: INFO: Pod ibm-keepalived-watcher-b94td requesting resource cpu=5m on Node 10.9.20.126
Jan 28 00:31:51.635: INFO: Pod ibm-keepalived-watcher-gj6b6 requesting resource cpu=5m on Node 10.9.20.75
Jan 28 00:31:51.635: INFO: Pod ibm-keepalived-watcher-gstl5 requesting resource cpu=5m on Node 10.9.20.72
Jan 28 00:31:51.635: INFO: Pod ibm-master-proxy-static-10.9.20.126 requesting resource cpu=25m on Node 10.9.20.126
Jan 28 00:31:51.635: INFO: Pod ibm-master-proxy-static-10.9.20.72 requesting resource cpu=25m on Node 10.9.20.72
Jan 28 00:31:51.635: INFO: Pod ibm-master-proxy-static-10.9.20.75 requesting resource cpu=25m on Node 10.9.20.75
Jan 28 00:31:51.635: INFO: Pod ibm-storage-watcher-746995c8c9-7mmhc requesting resource cpu=50m on Node 10.9.20.75
Jan 28 00:31:51.635: INFO: Pod ibmcloud-block-storage-driver-jgsbr requesting resource cpu=50m on Node 10.9.20.75
Jan 28 00:31:51.635: INFO: Pod ibmcloud-block-storage-driver-lw66t requesting resource cpu=50m on Node 10.9.20.126
Jan 28 00:31:51.635: INFO: Pod ibmcloud-block-storage-driver-sktn2 requesting resource cpu=50m on Node 10.9.20.72
Jan 28 00:31:51.635: INFO: Pod ibmcloud-block-storage-plugin-697cd846b-rglpr requesting resource cpu=50m on Node 10.9.20.75
Jan 28 00:31:51.635: INFO: Pod ingress-cluster-healthcheck-655c49644b-dmz5q requesting resource cpu=10m on Node 10.9.20.72
Jan 28 00:31:51.635: INFO: Pod konnectivity-agent-65zgm requesting resource cpu=10m on Node 10.9.20.126
Jan 28 00:31:51.635: INFO: Pod konnectivity-agent-95p2n requesting resource cpu=10m on Node 10.9.20.75
Jan 28 00:31:51.635: INFO: Pod konnectivity-agent-zbbtb requesting resource cpu=10m on Node 10.9.20.72
Jan 28 00:31:51.635: INFO: Pod kubernetes-dashboard-55c4d56798-7r7j7 requesting resource cpu=50m on Node 10.9.20.75
Jan 28 00:31:51.635: INFO: Pod metrics-server-9cdf87dc6-4ptjv requesting resource cpu=126m on Node 10.9.20.126
Jan 28 00:31:51.635: INFO: Pod metrics-server-9cdf87dc6-n44dc requesting resource cpu=126m on Node 10.9.20.72
Jan 28 00:31:51.635: INFO: Pod public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-vvqqx requesting resource cpu=20m on Node 10.9.20.126
Jan 28 00:31:51.635: INFO: Pod public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-ztg7f requesting resource cpu=20m on Node 10.9.20.72
Jan 28 00:31:51.635: INFO: Pod snapshot-controller-c5c6dddff-25l94 requesting resource cpu=10m on Node 10.9.20.75
Jan 28 00:31:51.635: INFO: Pod snapshot-controller-c5c6dddff-5xzjc requesting resource cpu=10m on Node 10.9.20.75
Jan 28 00:31:51.635: INFO: Pod snapshot-controller-c5c6dddff-pzr2k requesting resource cpu=10m on Node 10.9.20.75
Jan 28 00:31:51.635: INFO: Pod pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88 requesting resource cpu=0m on Node 10.9.20.126
Jan 28 00:31:51.635: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.9.20.126
Jan 28 00:31:51.635: INFO: Pod sonobuoy-e2e-job-70551213e7cc4fe2 requesting resource cpu=0m on Node 10.9.20.75
Jan 28 00:31:51.635: INFO: Pod sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-jcljc requesting resource cpu=0m on Node 10.9.20.75
Jan 28 00:31:51.635: INFO: Pod sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-qbzm7 requesting resource cpu=0m on Node 10.9.20.126
Jan 28 00:31:51.635: INFO: Pod sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-tm2wf requesting resource cpu=0m on Node 10.9.20.72
Jan 28 00:31:51.635: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.9.20.72
STEP: Starting Pods to consume most of the cluster CPU. 01/28/23 00:31:51.635
Jan 28 00:31:51.635: INFO: Creating a pod which consumes cpu=2141m on Node 10.9.20.72
Jan 28 00:31:51.661: INFO: Creating a pod which consumes cpu=2081m on Node 10.9.20.75
Jan 28 00:31:51.675: INFO: Creating a pod which consumes cpu=2151m on Node 10.9.20.126
Jan 28 00:31:51.689: INFO: Waiting up to 5m0s for pod "filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf" in namespace "sched-pred-851" to be "running"
Jan 28 00:31:51.700: INFO: Pod "filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.166878ms
Jan 28 00:31:53.710: INFO: Pod "filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf": Phase="Running", Reason="", readiness=true. Elapsed: 2.0210267s
Jan 28 00:31:53.711: INFO: Pod "filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf" satisfied condition "running"
Jan 28 00:31:53.711: INFO: Waiting up to 5m0s for pod "filler-pod-058ee827-529b-48fc-abca-b5145ac93d25" in namespace "sched-pred-851" to be "running"
Jan 28 00:31:53.721: INFO: Pod "filler-pod-058ee827-529b-48fc-abca-b5145ac93d25": Phase="Running", Reason="", readiness=true. Elapsed: 9.379364ms
Jan 28 00:31:53.721: INFO: Pod "filler-pod-058ee827-529b-48fc-abca-b5145ac93d25" satisfied condition "running"
Jan 28 00:31:53.721: INFO: Waiting up to 5m0s for pod "filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2" in namespace "sched-pred-851" to be "running"
Jan 28 00:31:53.733: INFO: Pod "filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2": Phase="Running", Reason="", readiness=true. Elapsed: 11.337064ms
Jan 28 00:31:53.733: INFO: Pod "filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/28/23 00:31:53.733
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-058ee827-529b-48fc-abca-b5145ac93d25.173e51ac41b1f030], Reason = [Scheduled], Message = [Successfully assigned sched-pred-851/filler-pod-058ee827-529b-48fc-abca-b5145ac93d25 to 10.9.20.75] 01/28/23 00:31:53.749
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-058ee827-529b-48fc-abca-b5145ac93d25.173e51ac8dd0627c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/28/23 00:31:53.749
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-058ee827-529b-48fc-abca-b5145ac93d25.173e51ac90e7261a], Reason = [Created], Message = [Created container filler-pod-058ee827-529b-48fc-abca-b5145ac93d25] 01/28/23 00:31:53.749
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-058ee827-529b-48fc-abca-b5145ac93d25.173e51ac9dad1b5f], Reason = [Started], Message = [Started container filler-pod-058ee827-529b-48fc-abca-b5145ac93d25] 01/28/23 00:31:53.749
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf.173e51ac40e25a5c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-851/filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf to 10.9.20.72] 01/28/23 00:31:53.75
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf.173e51ac8a456eb4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/28/23 00:31:53.75
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf.173e51ac8e00bb66], Reason = [Created], Message = [Created container filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf] 01/28/23 00:31:53.75
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf.173e51ac9d445bef], Reason = [Started], Message = [Started container filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf] 01/28/23 00:31:53.75
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2.173e51ac41c8a8db], Reason = [Scheduled], Message = [Successfully assigned sched-pred-851/filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2 to 10.9.20.126] 01/28/23 00:31:53.75
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2.173e51ac8ac051ba], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/28/23 00:31:53.75
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2.173e51ac8d9cd468], Reason = [Created], Message = [Created container filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2] 01/28/23 00:31:53.75
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2.173e51ac9a9c8d23], Reason = [Started], Message = [Started container filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2] 01/28/23 00:31:53.75
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.173e51acbcb7a1b0], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 01/28/23 00:31:53.782
STEP: removing the label node off the node 10.9.20.72 01/28/23 00:31:54.789
STEP: verifying the node doesn't have the label node 01/28/23 00:31:54.831
STEP: removing the label node off the node 10.9.20.75 01/28/23 00:31:54.844
STEP: verifying the node doesn't have the label node 01/28/23 00:31:54.882
STEP: removing the label node off the node 10.9.20.126 01/28/23 00:31:54.894
STEP: verifying the node doesn't have the label node 01/28/23 00:31:54.937
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 28 00:31:54.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-851" for this suite. 01/28/23 00:31:54.965
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":39,"skipped":767,"failed":0}
------------------------------
â€¢ [3.773 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:31:51.207
    Jan 28 00:31:51.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename sched-pred 01/28/23 00:31:51.209
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:31:51.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:31:51.271
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 28 00:31:51.285: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 28 00:31:51.316: INFO: Waiting for terminating namespaces to be deleted...
    Jan 28 00:31:51.329: INFO: 
    Logging pods the apiserver thinks is on node 10.9.20.126 before test
    Jan 28 00:31:51.357: INFO: calico-node-xl9f8 from kube-system started at 2023-01-27 22:05:01 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.357: INFO: 	Container calico-node ready: true, restart count 0
    Jan 28 00:31:51.357: INFO: calico-typha-677688fdc5-twxlr from kube-system started at 2023-01-27 22:05:32 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.357: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 28 00:31:51.357: INFO: coredns-6754846f95-9ck4t from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.357: INFO: 	Container coredns ready: true, restart count 0
    Jan 28 00:31:51.357: INFO: ibm-keepalived-watcher-b94td from kube-system started at 2023-01-27 22:05:01 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.357: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 28 00:31:51.357: INFO: ibm-master-proxy-static-10.9.20.126 from kube-system started at 2023-01-27 22:04:49 +0000 UTC (2 container statuses recorded)
    Jan 28 00:31:51.357: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 28 00:31:51.357: INFO: 	Container pause ready: true, restart count 0
    Jan 28 00:31:51.358: INFO: ibmcloud-block-storage-driver-lw66t from kube-system started at 2023-01-27 22:05:09 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.358: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 28 00:31:51.358: INFO: konnectivity-agent-65zgm from kube-system started at 2023-01-27 22:15:24 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.358: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 28 00:31:51.358: INFO: metrics-server-9cdf87dc6-4ptjv from kube-system started at 2023-01-27 22:51:34 +0000 UTC (3 container statuses recorded)
    Jan 28 00:31:51.358: INFO: 	Container config-watcher ready: true, restart count 0
    Jan 28 00:31:51.358: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 28 00:31:51.358: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan 28 00:31:51.358: INFO: public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-vvqqx from kube-system started at 2023-01-27 22:09:22 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.358: INFO: 	Container nginx-ingress ready: true, restart count 0
    Jan 28 00:31:51.358: INFO: pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88 from pods-8095 started at 2023-01-28 00:31:46 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.358: INFO: 	Container main ready: true, restart count 0
    Jan 28 00:31:51.358: INFO: sonobuoy from sonobuoy started at 2023-01-28 00:13:47 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.358: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 28 00:31:51.358: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-qbzm7 from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
    Jan 28 00:31:51.358: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 28 00:31:51.358: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 28 00:31:51.358: INFO: 
    Logging pods the apiserver thinks is on node 10.9.20.72 before test
    Jan 28 00:31:51.385: INFO: ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-2gg2p from ibm-system started at 2023-01-27 22:09:35 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.385: INFO: 	Container ibm-cloud-provider-ip-163-69-70-149 ready: true, restart count 0
    Jan 28 00:31:51.385: INFO: calico-node-4bc72 from kube-system started at 2023-01-27 22:04:57 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.385: INFO: 	Container calico-node ready: true, restart count 0
    Jan 28 00:31:51.385: INFO: calico-typha-677688fdc5-6nr4v from kube-system started at 2023-01-27 22:05:47 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.385: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 28 00:31:51.385: INFO: coredns-6754846f95-b686l from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.385: INFO: 	Container coredns ready: true, restart count 0
    Jan 28 00:31:51.385: INFO: ibm-keepalived-watcher-gstl5 from kube-system started at 2023-01-27 22:04:57 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.385: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 28 00:31:51.385: INFO: ibm-master-proxy-static-10.9.20.72 from kube-system started at 2023-01-27 22:04:44 +0000 UTC (2 container statuses recorded)
    Jan 28 00:31:51.385: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 28 00:31:51.385: INFO: 	Container pause ready: true, restart count 0
    Jan 28 00:31:51.385: INFO: ibmcloud-block-storage-driver-sktn2 from kube-system started at 2023-01-27 22:05:04 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.385: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 28 00:31:51.385: INFO: ingress-cluster-healthcheck-655c49644b-dmz5q from kube-system started at 2023-01-27 22:06:59 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.385: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
    Jan 28 00:31:51.385: INFO: konnectivity-agent-zbbtb from kube-system started at 2023-01-27 22:15:21 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.385: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 28 00:31:51.385: INFO: metrics-server-9cdf87dc6-n44dc from kube-system started at 2023-01-27 22:51:34 +0000 UTC (3 container statuses recorded)
    Jan 28 00:31:51.385: INFO: 	Container config-watcher ready: true, restart count 0
    Jan 28 00:31:51.385: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 28 00:31:51.386: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan 28 00:31:51.386: INFO: public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-ztg7f from kube-system started at 2023-01-27 22:09:22 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.386: INFO: 	Container nginx-ingress ready: true, restart count 0
    Jan 28 00:31:51.386: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-tm2wf from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
    Jan 28 00:31:51.386: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 28 00:31:51.386: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 28 00:31:51.386: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-01-27 22:07:40 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.386: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    Jan 28 00:31:51.386: INFO: 
    Logging pods the apiserver thinks is on node 10.9.20.75 before test
    Jan 28 00:31:51.416: INFO: ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-9qs6m from ibm-system started at 2023-01-27 22:09:35 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.416: INFO: 	Container ibm-cloud-provider-ip-163-69-70-149 ready: true, restart count 0
    Jan 28 00:31:51.416: INFO: calico-kube-controllers-5754dfd4dd-2fl6z from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.416: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 28 00:31:51.416: INFO: calico-node-6pnwh from kube-system started at 2023-01-27 22:04:45 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.416: INFO: 	Container calico-node ready: true, restart count 0
    Jan 28 00:31:51.416: INFO: calico-typha-677688fdc5-s2khq from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.416: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 28 00:31:51.416: INFO: coredns-6754846f95-r75xk from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.416: INFO: 	Container coredns ready: true, restart count 0
    Jan 28 00:31:51.416: INFO: coredns-autoscaler-669cf746f6-b9s85 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.416: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 28 00:31:51.416: INFO: dashboard-metrics-scraper-c964d5594-76f8h from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.416: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Jan 28 00:31:51.416: INFO: ibm-file-plugin-7dd6c48b68-kn5ff from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.416: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Jan 28 00:31:51.416: INFO: ibm-keepalived-watcher-gj6b6 from kube-system started at 2023-01-27 22:04:45 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.416: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 28 00:31:51.417: INFO: ibm-master-proxy-static-10.9.20.75 from kube-system started at 2023-01-27 22:04:42 +0000 UTC (2 container statuses recorded)
    Jan 28 00:31:51.417: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 28 00:31:51.417: INFO: 	Container pause ready: true, restart count 0
    Jan 28 00:31:51.417: INFO: ibm-storage-watcher-746995c8c9-7mmhc from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.417: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Jan 28 00:31:51.417: INFO: ibmcloud-block-storage-driver-jgsbr from kube-system started at 2023-01-27 22:04:51 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.417: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 28 00:31:51.417: INFO: ibmcloud-block-storage-plugin-697cd846b-rglpr from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.417: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Jan 28 00:31:51.417: INFO: konnectivity-agent-95p2n from kube-system started at 2023-01-27 22:15:28 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.417: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 28 00:31:51.417: INFO: kubernetes-dashboard-55c4d56798-7r7j7 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.417: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Jan 28 00:31:51.417: INFO: snapshot-controller-c5c6dddff-25l94 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.417: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 28 00:31:51.417: INFO: snapshot-controller-c5c6dddff-5xzjc from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.417: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 28 00:31:51.417: INFO: snapshot-controller-c5c6dddff-pzr2k from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:31:51.417: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 28 00:31:51.417: INFO: sonobuoy-e2e-job-70551213e7cc4fe2 from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
    Jan 28 00:31:51.417: INFO: 	Container e2e ready: true, restart count 0
    Jan 28 00:31:51.417: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 28 00:31:51.417: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-jcljc from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
    Jan 28 00:31:51.417: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 28 00:31:51.417: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node 10.9.20.126 01/28/23 00:31:51.478
    STEP: verifying the node has the label node 10.9.20.72 01/28/23 00:31:51.525
    STEP: verifying the node has the label node 10.9.20.75 01/28/23 00:31:51.563
    Jan 28 00:31:51.634: INFO: Pod ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-2gg2p requesting resource cpu=5m on Node 10.9.20.72
    Jan 28 00:31:51.634: INFO: Pod ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-9qs6m requesting resource cpu=5m on Node 10.9.20.75
    Jan 28 00:31:51.634: INFO: Pod calico-kube-controllers-5754dfd4dd-2fl6z requesting resource cpu=10m on Node 10.9.20.75
    Jan 28 00:31:51.634: INFO: Pod calico-node-4bc72 requesting resource cpu=250m on Node 10.9.20.72
    Jan 28 00:31:51.634: INFO: Pod calico-node-6pnwh requesting resource cpu=250m on Node 10.9.20.75
    Jan 28 00:31:51.634: INFO: Pod calico-node-xl9f8 requesting resource cpu=250m on Node 10.9.20.126
    Jan 28 00:31:51.634: INFO: Pod calico-typha-677688fdc5-6nr4v requesting resource cpu=250m on Node 10.9.20.72
    Jan 28 00:31:51.635: INFO: Pod calico-typha-677688fdc5-s2khq requesting resource cpu=250m on Node 10.9.20.75
    Jan 28 00:31:51.635: INFO: Pod calico-typha-677688fdc5-twxlr requesting resource cpu=250m on Node 10.9.20.126
    Jan 28 00:31:51.635: INFO: Pod coredns-6754846f95-9ck4t requesting resource cpu=100m on Node 10.9.20.126
    Jan 28 00:31:51.635: INFO: Pod coredns-6754846f95-b686l requesting resource cpu=100m on Node 10.9.20.72
    Jan 28 00:31:51.635: INFO: Pod coredns-6754846f95-r75xk requesting resource cpu=100m on Node 10.9.20.75
    Jan 28 00:31:51.635: INFO: Pod coredns-autoscaler-669cf746f6-b9s85 requesting resource cpu=1m on Node 10.9.20.75
    Jan 28 00:31:51.635: INFO: Pod dashboard-metrics-scraper-c964d5594-76f8h requesting resource cpu=1m on Node 10.9.20.75
    Jan 28 00:31:51.635: INFO: Pod ibm-file-plugin-7dd6c48b68-kn5ff requesting resource cpu=50m on Node 10.9.20.75
    Jan 28 00:31:51.635: INFO: Pod ibm-keepalived-watcher-b94td requesting resource cpu=5m on Node 10.9.20.126
    Jan 28 00:31:51.635: INFO: Pod ibm-keepalived-watcher-gj6b6 requesting resource cpu=5m on Node 10.9.20.75
    Jan 28 00:31:51.635: INFO: Pod ibm-keepalived-watcher-gstl5 requesting resource cpu=5m on Node 10.9.20.72
    Jan 28 00:31:51.635: INFO: Pod ibm-master-proxy-static-10.9.20.126 requesting resource cpu=25m on Node 10.9.20.126
    Jan 28 00:31:51.635: INFO: Pod ibm-master-proxy-static-10.9.20.72 requesting resource cpu=25m on Node 10.9.20.72
    Jan 28 00:31:51.635: INFO: Pod ibm-master-proxy-static-10.9.20.75 requesting resource cpu=25m on Node 10.9.20.75
    Jan 28 00:31:51.635: INFO: Pod ibm-storage-watcher-746995c8c9-7mmhc requesting resource cpu=50m on Node 10.9.20.75
    Jan 28 00:31:51.635: INFO: Pod ibmcloud-block-storage-driver-jgsbr requesting resource cpu=50m on Node 10.9.20.75
    Jan 28 00:31:51.635: INFO: Pod ibmcloud-block-storage-driver-lw66t requesting resource cpu=50m on Node 10.9.20.126
    Jan 28 00:31:51.635: INFO: Pod ibmcloud-block-storage-driver-sktn2 requesting resource cpu=50m on Node 10.9.20.72
    Jan 28 00:31:51.635: INFO: Pod ibmcloud-block-storage-plugin-697cd846b-rglpr requesting resource cpu=50m on Node 10.9.20.75
    Jan 28 00:31:51.635: INFO: Pod ingress-cluster-healthcheck-655c49644b-dmz5q requesting resource cpu=10m on Node 10.9.20.72
    Jan 28 00:31:51.635: INFO: Pod konnectivity-agent-65zgm requesting resource cpu=10m on Node 10.9.20.126
    Jan 28 00:31:51.635: INFO: Pod konnectivity-agent-95p2n requesting resource cpu=10m on Node 10.9.20.75
    Jan 28 00:31:51.635: INFO: Pod konnectivity-agent-zbbtb requesting resource cpu=10m on Node 10.9.20.72
    Jan 28 00:31:51.635: INFO: Pod kubernetes-dashboard-55c4d56798-7r7j7 requesting resource cpu=50m on Node 10.9.20.75
    Jan 28 00:31:51.635: INFO: Pod metrics-server-9cdf87dc6-4ptjv requesting resource cpu=126m on Node 10.9.20.126
    Jan 28 00:31:51.635: INFO: Pod metrics-server-9cdf87dc6-n44dc requesting resource cpu=126m on Node 10.9.20.72
    Jan 28 00:31:51.635: INFO: Pod public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-vvqqx requesting resource cpu=20m on Node 10.9.20.126
    Jan 28 00:31:51.635: INFO: Pod public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-ztg7f requesting resource cpu=20m on Node 10.9.20.72
    Jan 28 00:31:51.635: INFO: Pod snapshot-controller-c5c6dddff-25l94 requesting resource cpu=10m on Node 10.9.20.75
    Jan 28 00:31:51.635: INFO: Pod snapshot-controller-c5c6dddff-5xzjc requesting resource cpu=10m on Node 10.9.20.75
    Jan 28 00:31:51.635: INFO: Pod snapshot-controller-c5c6dddff-pzr2k requesting resource cpu=10m on Node 10.9.20.75
    Jan 28 00:31:51.635: INFO: Pod pod-exec-websocket-e9a36e27-30fc-4ace-8774-4815bf491a88 requesting resource cpu=0m on Node 10.9.20.126
    Jan 28 00:31:51.635: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.9.20.126
    Jan 28 00:31:51.635: INFO: Pod sonobuoy-e2e-job-70551213e7cc4fe2 requesting resource cpu=0m on Node 10.9.20.75
    Jan 28 00:31:51.635: INFO: Pod sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-jcljc requesting resource cpu=0m on Node 10.9.20.75
    Jan 28 00:31:51.635: INFO: Pod sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-qbzm7 requesting resource cpu=0m on Node 10.9.20.126
    Jan 28 00:31:51.635: INFO: Pod sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-tm2wf requesting resource cpu=0m on Node 10.9.20.72
    Jan 28 00:31:51.635: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.9.20.72
    STEP: Starting Pods to consume most of the cluster CPU. 01/28/23 00:31:51.635
    Jan 28 00:31:51.635: INFO: Creating a pod which consumes cpu=2141m on Node 10.9.20.72
    Jan 28 00:31:51.661: INFO: Creating a pod which consumes cpu=2081m on Node 10.9.20.75
    Jan 28 00:31:51.675: INFO: Creating a pod which consumes cpu=2151m on Node 10.9.20.126
    Jan 28 00:31:51.689: INFO: Waiting up to 5m0s for pod "filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf" in namespace "sched-pred-851" to be "running"
    Jan 28 00:31:51.700: INFO: Pod "filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.166878ms
    Jan 28 00:31:53.710: INFO: Pod "filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf": Phase="Running", Reason="", readiness=true. Elapsed: 2.0210267s
    Jan 28 00:31:53.711: INFO: Pod "filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf" satisfied condition "running"
    Jan 28 00:31:53.711: INFO: Waiting up to 5m0s for pod "filler-pod-058ee827-529b-48fc-abca-b5145ac93d25" in namespace "sched-pred-851" to be "running"
    Jan 28 00:31:53.721: INFO: Pod "filler-pod-058ee827-529b-48fc-abca-b5145ac93d25": Phase="Running", Reason="", readiness=true. Elapsed: 9.379364ms
    Jan 28 00:31:53.721: INFO: Pod "filler-pod-058ee827-529b-48fc-abca-b5145ac93d25" satisfied condition "running"
    Jan 28 00:31:53.721: INFO: Waiting up to 5m0s for pod "filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2" in namespace "sched-pred-851" to be "running"
    Jan 28 00:31:53.733: INFO: Pod "filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2": Phase="Running", Reason="", readiness=true. Elapsed: 11.337064ms
    Jan 28 00:31:53.733: INFO: Pod "filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/28/23 00:31:53.733
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-058ee827-529b-48fc-abca-b5145ac93d25.173e51ac41b1f030], Reason = [Scheduled], Message = [Successfully assigned sched-pred-851/filler-pod-058ee827-529b-48fc-abca-b5145ac93d25 to 10.9.20.75] 01/28/23 00:31:53.749
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-058ee827-529b-48fc-abca-b5145ac93d25.173e51ac8dd0627c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/28/23 00:31:53.749
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-058ee827-529b-48fc-abca-b5145ac93d25.173e51ac90e7261a], Reason = [Created], Message = [Created container filler-pod-058ee827-529b-48fc-abca-b5145ac93d25] 01/28/23 00:31:53.749
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-058ee827-529b-48fc-abca-b5145ac93d25.173e51ac9dad1b5f], Reason = [Started], Message = [Started container filler-pod-058ee827-529b-48fc-abca-b5145ac93d25] 01/28/23 00:31:53.749
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf.173e51ac40e25a5c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-851/filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf to 10.9.20.72] 01/28/23 00:31:53.75
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf.173e51ac8a456eb4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/28/23 00:31:53.75
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf.173e51ac8e00bb66], Reason = [Created], Message = [Created container filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf] 01/28/23 00:31:53.75
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf.173e51ac9d445bef], Reason = [Started], Message = [Started container filler-pod-67f6387e-5f77-41fd-ae4f-119c6f811bdf] 01/28/23 00:31:53.75
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2.173e51ac41c8a8db], Reason = [Scheduled], Message = [Successfully assigned sched-pred-851/filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2 to 10.9.20.126] 01/28/23 00:31:53.75
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2.173e51ac8ac051ba], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/28/23 00:31:53.75
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2.173e51ac8d9cd468], Reason = [Created], Message = [Created container filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2] 01/28/23 00:31:53.75
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2.173e51ac9a9c8d23], Reason = [Started], Message = [Started container filler-pod-6d5a7cd5-8c9f-4618-8551-8d97245295b2] 01/28/23 00:31:53.75
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.173e51acbcb7a1b0], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 01/28/23 00:31:53.782
    STEP: removing the label node off the node 10.9.20.72 01/28/23 00:31:54.789
    STEP: verifying the node doesn't have the label node 01/28/23 00:31:54.831
    STEP: removing the label node off the node 10.9.20.75 01/28/23 00:31:54.844
    STEP: verifying the node doesn't have the label node 01/28/23 00:31:54.882
    STEP: removing the label node off the node 10.9.20.126 01/28/23 00:31:54.894
    STEP: verifying the node doesn't have the label node 01/28/23 00:31:54.937
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 00:31:54.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-851" for this suite. 01/28/23 00:31:54.965
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:31:54.99
Jan 28 00:31:54.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename configmap 01/28/23 00:31:54.992
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:31:55.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:31:55.049
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-9302b1c6-0d57-4012-a4ce-c8d66fe5c3fd 01/28/23 00:31:55.078
STEP: Creating configMap with name cm-test-opt-upd-3e11647a-ccf7-43dc-a970-2556480da987 01/28/23 00:31:55.096
STEP: Creating the pod 01/28/23 00:31:55.111
Jan 28 00:31:55.134: INFO: Waiting up to 5m0s for pod "pod-configmaps-274a51ef-066a-47b1-88b7-b516c5e33356" in namespace "configmap-2889" to be "running and ready"
Jan 28 00:31:55.146: INFO: Pod "pod-configmaps-274a51ef-066a-47b1-88b7-b516c5e33356": Phase="Pending", Reason="", readiness=false. Elapsed: 11.563456ms
Jan 28 00:31:55.146: INFO: The phase of Pod pod-configmaps-274a51ef-066a-47b1-88b7-b516c5e33356 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:31:57.162: INFO: Pod "pod-configmaps-274a51ef-066a-47b1-88b7-b516c5e33356": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027177193s
Jan 28 00:31:57.162: INFO: The phase of Pod pod-configmaps-274a51ef-066a-47b1-88b7-b516c5e33356 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:31:59.161: INFO: Pod "pod-configmaps-274a51ef-066a-47b1-88b7-b516c5e33356": Phase="Running", Reason="", readiness=true. Elapsed: 4.026695773s
Jan 28 00:31:59.161: INFO: The phase of Pod pod-configmaps-274a51ef-066a-47b1-88b7-b516c5e33356 is Running (Ready = true)
Jan 28 00:31:59.162: INFO: Pod "pod-configmaps-274a51ef-066a-47b1-88b7-b516c5e33356" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-9302b1c6-0d57-4012-a4ce-c8d66fe5c3fd 01/28/23 00:31:59.254
STEP: Updating configmap cm-test-opt-upd-3e11647a-ccf7-43dc-a970-2556480da987 01/28/23 00:31:59.268
STEP: Creating configMap with name cm-test-opt-create-922a49eb-ce2e-4638-8619-636b1a3e13af 01/28/23 00:31:59.281
STEP: waiting to observe update in volume 01/28/23 00:31:59.293
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 28 00:32:03.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2889" for this suite. 01/28/23 00:32:03.436
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":40,"skipped":784,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.466 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:31:54.99
    Jan 28 00:31:54.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename configmap 01/28/23 00:31:54.992
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:31:55.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:31:55.049
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-9302b1c6-0d57-4012-a4ce-c8d66fe5c3fd 01/28/23 00:31:55.078
    STEP: Creating configMap with name cm-test-opt-upd-3e11647a-ccf7-43dc-a970-2556480da987 01/28/23 00:31:55.096
    STEP: Creating the pod 01/28/23 00:31:55.111
    Jan 28 00:31:55.134: INFO: Waiting up to 5m0s for pod "pod-configmaps-274a51ef-066a-47b1-88b7-b516c5e33356" in namespace "configmap-2889" to be "running and ready"
    Jan 28 00:31:55.146: INFO: Pod "pod-configmaps-274a51ef-066a-47b1-88b7-b516c5e33356": Phase="Pending", Reason="", readiness=false. Elapsed: 11.563456ms
    Jan 28 00:31:55.146: INFO: The phase of Pod pod-configmaps-274a51ef-066a-47b1-88b7-b516c5e33356 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:31:57.162: INFO: Pod "pod-configmaps-274a51ef-066a-47b1-88b7-b516c5e33356": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027177193s
    Jan 28 00:31:57.162: INFO: The phase of Pod pod-configmaps-274a51ef-066a-47b1-88b7-b516c5e33356 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:31:59.161: INFO: Pod "pod-configmaps-274a51ef-066a-47b1-88b7-b516c5e33356": Phase="Running", Reason="", readiness=true. Elapsed: 4.026695773s
    Jan 28 00:31:59.161: INFO: The phase of Pod pod-configmaps-274a51ef-066a-47b1-88b7-b516c5e33356 is Running (Ready = true)
    Jan 28 00:31:59.162: INFO: Pod "pod-configmaps-274a51ef-066a-47b1-88b7-b516c5e33356" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-9302b1c6-0d57-4012-a4ce-c8d66fe5c3fd 01/28/23 00:31:59.254
    STEP: Updating configmap cm-test-opt-upd-3e11647a-ccf7-43dc-a970-2556480da987 01/28/23 00:31:59.268
    STEP: Creating configMap with name cm-test-opt-create-922a49eb-ce2e-4638-8619-636b1a3e13af 01/28/23 00:31:59.281
    STEP: waiting to observe update in volume 01/28/23 00:31:59.293
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 28 00:32:03.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2889" for this suite. 01/28/23 00:32:03.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:32:03.462
Jan 28 00:32:03.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename certificates 01/28/23 00:32:03.464
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:32:03.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:32:03.53
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/28/23 00:32:04.046
STEP: getting /apis/certificates.k8s.io 01/28/23 00:32:04.059
STEP: getting /apis/certificates.k8s.io/v1 01/28/23 00:32:04.065
STEP: creating 01/28/23 00:32:04.073
STEP: getting 01/28/23 00:32:04.12
STEP: listing 01/28/23 00:32:04.134
STEP: watching 01/28/23 00:32:04.146
Jan 28 00:32:04.146: INFO: starting watch
STEP: patching 01/28/23 00:32:04.153
STEP: updating 01/28/23 00:32:04.178
Jan 28 00:32:04.197: INFO: waiting for watch events with expected annotations
Jan 28 00:32:04.197: INFO: saw patched and updated annotations
STEP: getting /approval 01/28/23 00:32:04.197
STEP: patching /approval 01/28/23 00:32:04.207
STEP: updating /approval 01/28/23 00:32:04.223
STEP: getting /status 01/28/23 00:32:04.237
STEP: patching /status 01/28/23 00:32:04.247
STEP: updating /status 01/28/23 00:32:04.268
STEP: deleting 01/28/23 00:32:04.284
STEP: deleting a collection 01/28/23 00:32:04.321
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 00:32:04.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-7815" for this suite. 01/28/23 00:32:04.373
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":41,"skipped":817,"failed":0}
------------------------------
â€¢ [0.927 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:32:03.462
    Jan 28 00:32:03.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename certificates 01/28/23 00:32:03.464
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:32:03.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:32:03.53
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/28/23 00:32:04.046
    STEP: getting /apis/certificates.k8s.io 01/28/23 00:32:04.059
    STEP: getting /apis/certificates.k8s.io/v1 01/28/23 00:32:04.065
    STEP: creating 01/28/23 00:32:04.073
    STEP: getting 01/28/23 00:32:04.12
    STEP: listing 01/28/23 00:32:04.134
    STEP: watching 01/28/23 00:32:04.146
    Jan 28 00:32:04.146: INFO: starting watch
    STEP: patching 01/28/23 00:32:04.153
    STEP: updating 01/28/23 00:32:04.178
    Jan 28 00:32:04.197: INFO: waiting for watch events with expected annotations
    Jan 28 00:32:04.197: INFO: saw patched and updated annotations
    STEP: getting /approval 01/28/23 00:32:04.197
    STEP: patching /approval 01/28/23 00:32:04.207
    STEP: updating /approval 01/28/23 00:32:04.223
    STEP: getting /status 01/28/23 00:32:04.237
    STEP: patching /status 01/28/23 00:32:04.247
    STEP: updating /status 01/28/23 00:32:04.268
    STEP: deleting 01/28/23 00:32:04.284
    STEP: deleting a collection 01/28/23 00:32:04.321
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 00:32:04.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-7815" for this suite. 01/28/23 00:32:04.373
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:32:04.391
Jan 28 00:32:04.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename tables 01/28/23 00:32:04.393
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:32:04.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:32:04.453
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Jan 28 00:32:04.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8079" for this suite. 01/28/23 00:32:04.493
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":42,"skipped":829,"failed":0}
------------------------------
â€¢ [0.121 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:32:04.391
    Jan 28 00:32:04.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename tables 01/28/23 00:32:04.393
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:32:04.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:32:04.453
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Jan 28 00:32:04.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-8079" for this suite. 01/28/23 00:32:04.493
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:32:04.522
Jan 28 00:32:04.522: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename crd-publish-openapi 01/28/23 00:32:04.524
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:32:04.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:32:04.584
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/28/23 00:32:04.598
Jan 28 00:32:04.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 00:32:07.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 00:32:20.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6612" for this suite. 01/28/23 00:32:20.146
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":43,"skipped":830,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.641 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:32:04.522
    Jan 28 00:32:04.522: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename crd-publish-openapi 01/28/23 00:32:04.524
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:32:04.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:32:04.584
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/28/23 00:32:04.598
    Jan 28 00:32:04.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 00:32:07.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 00:32:20.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6612" for this suite. 01/28/23 00:32:20.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:32:20.177
Jan 28 00:32:20.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename resourcequota 01/28/23 00:32:20.178
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:32:20.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:32:20.237
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 01/28/23 00:32:20.252
STEP: Creating a ResourceQuota 01/28/23 00:32:25.269
STEP: Ensuring resource quota status is calculated 01/28/23 00:32:25.289
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 28 00:32:27.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2646" for this suite. 01/28/23 00:32:27.326
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":44,"skipped":861,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.167 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:32:20.177
    Jan 28 00:32:20.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename resourcequota 01/28/23 00:32:20.178
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:32:20.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:32:20.237
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 01/28/23 00:32:20.252
    STEP: Creating a ResourceQuota 01/28/23 00:32:25.269
    STEP: Ensuring resource quota status is calculated 01/28/23 00:32:25.289
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 28 00:32:27.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2646" for this suite. 01/28/23 00:32:27.326
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:32:27.348
Jan 28 00:32:27.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename webhook 01/28/23 00:32:27.35
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:32:27.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:32:27.416
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/28/23 00:32:27.469
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 00:32:27.884
STEP: Deploying the webhook pod 01/28/23 00:32:27.913
STEP: Wait for the deployment to be ready 01/28/23 00:32:27.948
Jan 28 00:32:27.978: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/28/23 00:32:30.033
STEP: Verifying the service has paired with the endpoint 01/28/23 00:32:30.07
Jan 28 00:32:31.071: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/28/23 00:32:31.086
STEP: Registering slow webhook via the AdmissionRegistration API 01/28/23 00:32:31.087
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/28/23 00:32:31.156
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/28/23 00:32:32.188
STEP: Registering slow webhook via the AdmissionRegistration API 01/28/23 00:32:32.188
STEP: Having no error when timeout is longer than webhook latency 01/28/23 00:32:33.286
STEP: Registering slow webhook via the AdmissionRegistration API 01/28/23 00:32:33.287
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/28/23 00:32:38.436
STEP: Registering slow webhook via the AdmissionRegistration API 01/28/23 00:32:38.436
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 00:32:43.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1989" for this suite. 01/28/23 00:32:43.545
STEP: Destroying namespace "webhook-1989-markers" for this suite. 01/28/23 00:32:43.559
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":45,"skipped":864,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.352 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:32:27.348
    Jan 28 00:32:27.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename webhook 01/28/23 00:32:27.35
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:32:27.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:32:27.416
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/28/23 00:32:27.469
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 00:32:27.884
    STEP: Deploying the webhook pod 01/28/23 00:32:27.913
    STEP: Wait for the deployment to be ready 01/28/23 00:32:27.948
    Jan 28 00:32:27.978: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/28/23 00:32:30.033
    STEP: Verifying the service has paired with the endpoint 01/28/23 00:32:30.07
    Jan 28 00:32:31.071: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/28/23 00:32:31.086
    STEP: Registering slow webhook via the AdmissionRegistration API 01/28/23 00:32:31.087
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/28/23 00:32:31.156
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/28/23 00:32:32.188
    STEP: Registering slow webhook via the AdmissionRegistration API 01/28/23 00:32:32.188
    STEP: Having no error when timeout is longer than webhook latency 01/28/23 00:32:33.286
    STEP: Registering slow webhook via the AdmissionRegistration API 01/28/23 00:32:33.287
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/28/23 00:32:38.436
    STEP: Registering slow webhook via the AdmissionRegistration API 01/28/23 00:32:38.436
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 00:32:43.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1989" for this suite. 01/28/23 00:32:43.545
    STEP: Destroying namespace "webhook-1989-markers" for this suite. 01/28/23 00:32:43.559
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:32:43.704
Jan 28 00:32:43.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename resourcequota 01/28/23 00:32:43.709
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:32:43.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:32:43.765
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 01/28/23 00:32:43.777
STEP: Getting a ResourceQuota 01/28/23 00:32:43.791
STEP: Listing all ResourceQuotas with LabelSelector 01/28/23 00:32:43.807
STEP: Patching the ResourceQuota 01/28/23 00:32:43.821
STEP: Deleting a Collection of ResourceQuotas 01/28/23 00:32:43.842
STEP: Verifying the deleted ResourceQuota 01/28/23 00:32:43.873
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 28 00:32:43.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-387" for this suite. 01/28/23 00:32:43.907
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":46,"skipped":869,"failed":0}
------------------------------
â€¢ [0.223 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:32:43.704
    Jan 28 00:32:43.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename resourcequota 01/28/23 00:32:43.709
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:32:43.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:32:43.765
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 01/28/23 00:32:43.777
    STEP: Getting a ResourceQuota 01/28/23 00:32:43.791
    STEP: Listing all ResourceQuotas with LabelSelector 01/28/23 00:32:43.807
    STEP: Patching the ResourceQuota 01/28/23 00:32:43.821
    STEP: Deleting a Collection of ResourceQuotas 01/28/23 00:32:43.842
    STEP: Verifying the deleted ResourceQuota 01/28/23 00:32:43.873
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 28 00:32:43.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-387" for this suite. 01/28/23 00:32:43.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:32:43.933
Jan 28 00:32:43.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename pod-network-test 01/28/23 00:32:43.936
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:32:43.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:32:44.021
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-2090 01/28/23 00:32:44.032
STEP: creating a selector 01/28/23 00:32:44.033
STEP: Creating the service pods in kubernetes 01/28/23 00:32:44.033
Jan 28 00:32:44.033: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 28 00:32:44.102: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2090" to be "running and ready"
Jan 28 00:32:44.112: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.681913ms
Jan 28 00:32:44.112: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:32:46.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.023187147s
Jan 28 00:32:46.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 00:32:48.130: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.027472992s
Jan 28 00:32:48.135: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 00:32:50.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.023155495s
Jan 28 00:32:50.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 00:32:52.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.023159288s
Jan 28 00:32:52.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 00:32:54.137: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.034625411s
Jan 28 00:32:54.137: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 00:32:56.125: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.022602693s
Jan 28 00:32:56.125: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 00:32:58.127: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.024295499s
Jan 28 00:32:58.127: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 00:33:00.128: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.025755599s
Jan 28 00:33:00.128: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 00:33:02.137: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.034389753s
Jan 28 00:33:02.137: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 00:33:04.127: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.024603637s
Jan 28 00:33:04.127: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 00:33:06.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.02413838s
Jan 28 00:33:06.127: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 28 00:33:06.127: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 28 00:33:06.138: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2090" to be "running and ready"
Jan 28 00:33:06.152: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.114994ms
Jan 28 00:33:06.153: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 28 00:33:06.153: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 28 00:33:06.163: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2090" to be "running and ready"
Jan 28 00:33:06.178: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 14.434964ms
Jan 28 00:33:06.178: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 28 00:33:06.178: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/28/23 00:33:06.186
Jan 28 00:33:06.214: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2090" to be "running"
Jan 28 00:33:06.224: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.456533ms
Jan 28 00:33:08.238: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.023851324s
Jan 28 00:33:08.238: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 28 00:33:08.259: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2090" to be "running"
Jan 28 00:33:08.271: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 12.518984ms
Jan 28 00:33:08.272: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 28 00:33:08.284: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 28 00:33:08.284: INFO: Going to poll 172.30.12.219 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan 28 00:33:08.295: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.12.219:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2090 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:33:08.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 00:33:08.296: INFO: ExecWithOptions: Clientset creation
Jan 28 00:33:08.296: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2090/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.12.219%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 28 00:33:08.510: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 28 00:33:08.510: INFO: Going to poll 172.30.185.48 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan 28 00:33:08.523: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.185.48:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2090 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:33:08.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 00:33:08.525: INFO: ExecWithOptions: Clientset creation
Jan 28 00:33:08.525: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2090/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.185.48%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 28 00:33:08.731: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 28 00:33:08.731: INFO: Going to poll 172.30.84.43 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan 28 00:33:08.743: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.84.43:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2090 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:33:08.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 00:33:08.745: INFO: ExecWithOptions: Clientset creation
Jan 28 00:33:08.745: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2090/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.84.43%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 28 00:33:08.959: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 28 00:33:08.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2090" for this suite. 01/28/23 00:33:08.974
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":47,"skipped":888,"failed":0}
------------------------------
â€¢ [SLOW TEST] [25.067 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:32:43.933
    Jan 28 00:32:43.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename pod-network-test 01/28/23 00:32:43.936
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:32:43.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:32:44.021
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-2090 01/28/23 00:32:44.032
    STEP: creating a selector 01/28/23 00:32:44.033
    STEP: Creating the service pods in kubernetes 01/28/23 00:32:44.033
    Jan 28 00:32:44.033: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 28 00:32:44.102: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2090" to be "running and ready"
    Jan 28 00:32:44.112: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.681913ms
    Jan 28 00:32:44.112: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:32:46.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.023187147s
    Jan 28 00:32:46.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 00:32:48.130: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.027472992s
    Jan 28 00:32:48.135: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 00:32:50.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.023155495s
    Jan 28 00:32:50.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 00:32:52.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.023159288s
    Jan 28 00:32:52.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 00:32:54.137: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.034625411s
    Jan 28 00:32:54.137: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 00:32:56.125: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.022602693s
    Jan 28 00:32:56.125: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 00:32:58.127: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.024295499s
    Jan 28 00:32:58.127: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 00:33:00.128: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.025755599s
    Jan 28 00:33:00.128: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 00:33:02.137: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.034389753s
    Jan 28 00:33:02.137: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 00:33:04.127: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.024603637s
    Jan 28 00:33:04.127: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 00:33:06.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.02413838s
    Jan 28 00:33:06.127: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 28 00:33:06.127: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 28 00:33:06.138: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2090" to be "running and ready"
    Jan 28 00:33:06.152: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.114994ms
    Jan 28 00:33:06.153: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 28 00:33:06.153: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 28 00:33:06.163: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2090" to be "running and ready"
    Jan 28 00:33:06.178: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 14.434964ms
    Jan 28 00:33:06.178: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 28 00:33:06.178: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/28/23 00:33:06.186
    Jan 28 00:33:06.214: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2090" to be "running"
    Jan 28 00:33:06.224: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.456533ms
    Jan 28 00:33:08.238: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.023851324s
    Jan 28 00:33:08.238: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 28 00:33:08.259: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2090" to be "running"
    Jan 28 00:33:08.271: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 12.518984ms
    Jan 28 00:33:08.272: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 28 00:33:08.284: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan 28 00:33:08.284: INFO: Going to poll 172.30.12.219 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan 28 00:33:08.295: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.12.219:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2090 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 00:33:08.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 00:33:08.296: INFO: ExecWithOptions: Clientset creation
    Jan 28 00:33:08.296: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2090/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.12.219%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 28 00:33:08.510: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 28 00:33:08.510: INFO: Going to poll 172.30.185.48 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan 28 00:33:08.523: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.185.48:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2090 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 00:33:08.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 00:33:08.525: INFO: ExecWithOptions: Clientset creation
    Jan 28 00:33:08.525: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2090/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.185.48%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 28 00:33:08.731: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan 28 00:33:08.731: INFO: Going to poll 172.30.84.43 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan 28 00:33:08.743: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.84.43:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2090 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 00:33:08.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 00:33:08.745: INFO: ExecWithOptions: Clientset creation
    Jan 28 00:33:08.745: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2090/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.84.43%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 28 00:33:08.959: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 28 00:33:08.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-2090" for this suite. 01/28/23 00:33:08.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:33:09.004
Jan 28 00:33:09.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename pods 01/28/23 00:33:09.006
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:33:09.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:33:09.06
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 01/28/23 00:33:09.075
Jan 28 00:33:09.098: INFO: created test-pod-1
Jan 28 00:33:09.114: INFO: created test-pod-2
Jan 28 00:33:09.135: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/28/23 00:33:09.136
Jan 28 00:33:09.136: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1390' to be running and ready
Jan 28 00:33:09.177: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 28 00:33:09.177: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 28 00:33:09.177: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 28 00:33:09.177: INFO: 0 / 3 pods in namespace 'pods-1390' are running and ready (0 seconds elapsed)
Jan 28 00:33:09.177: INFO: expected 0 pod replicas in namespace 'pods-1390', 0 are Running and Ready.
Jan 28 00:33:09.177: INFO: POD         NODE         PHASE    GRACE  CONDITIONS
Jan 28 00:33:09.177: INFO: test-pod-1  10.9.20.72   Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:33:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:33:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:33:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:33:09 +0000 UTC  }]
Jan 28 00:33:09.177: INFO: test-pod-2  10.9.20.126  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:33:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:33:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:33:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:33:09 +0000 UTC  }]
Jan 28 00:33:09.177: INFO: test-pod-3  10.9.20.75   Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:33:09 +0000 UTC  }]
Jan 28 00:33:09.177: INFO: 
Jan 28 00:33:11.220: INFO: 3 / 3 pods in namespace 'pods-1390' are running and ready (2 seconds elapsed)
Jan 28 00:33:11.220: INFO: expected 0 pod replicas in namespace 'pods-1390', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/28/23 00:33:11.266
Jan 28 00:33:11.278: INFO: Pod quantity 3 is different from expected quantity 0
Jan 28 00:33:12.294: INFO: Pod quantity 3 is different from expected quantity 0
Jan 28 00:33:13.292: INFO: Pod quantity 3 is different from expected quantity 0
Jan 28 00:33:14.291: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 28 00:33:15.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1390" for this suite. 01/28/23 00:33:15.308
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":48,"skipped":895,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.324 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:33:09.004
    Jan 28 00:33:09.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename pods 01/28/23 00:33:09.006
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:33:09.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:33:09.06
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 01/28/23 00:33:09.075
    Jan 28 00:33:09.098: INFO: created test-pod-1
    Jan 28 00:33:09.114: INFO: created test-pod-2
    Jan 28 00:33:09.135: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/28/23 00:33:09.136
    Jan 28 00:33:09.136: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1390' to be running and ready
    Jan 28 00:33:09.177: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 28 00:33:09.177: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 28 00:33:09.177: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 28 00:33:09.177: INFO: 0 / 3 pods in namespace 'pods-1390' are running and ready (0 seconds elapsed)
    Jan 28 00:33:09.177: INFO: expected 0 pod replicas in namespace 'pods-1390', 0 are Running and Ready.
    Jan 28 00:33:09.177: INFO: POD         NODE         PHASE    GRACE  CONDITIONS
    Jan 28 00:33:09.177: INFO: test-pod-1  10.9.20.72   Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:33:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:33:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:33:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:33:09 +0000 UTC  }]
    Jan 28 00:33:09.177: INFO: test-pod-2  10.9.20.126  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:33:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:33:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:33:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:33:09 +0000 UTC  }]
    Jan 28 00:33:09.177: INFO: test-pod-3  10.9.20.75   Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:33:09 +0000 UTC  }]
    Jan 28 00:33:09.177: INFO: 
    Jan 28 00:33:11.220: INFO: 3 / 3 pods in namespace 'pods-1390' are running and ready (2 seconds elapsed)
    Jan 28 00:33:11.220: INFO: expected 0 pod replicas in namespace 'pods-1390', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/28/23 00:33:11.266
    Jan 28 00:33:11.278: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 28 00:33:12.294: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 28 00:33:13.292: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 28 00:33:14.291: INFO: Pod quantity 1 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 28 00:33:15.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1390" for this suite. 01/28/23 00:33:15.308
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:33:15.33
Jan 28 00:33:15.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename job 01/28/23 00:33:15.333
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:33:15.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:33:15.43
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 01/28/23 00:33:15.442
STEP: Ensuring active pods == parallelism 01/28/23 00:33:15.477
STEP: delete a job 01/28/23 00:33:19.489
STEP: deleting Job.batch foo in namespace job-7402, will wait for the garbage collector to delete the pods 01/28/23 00:33:19.489
Jan 28 00:33:19.587: INFO: Deleting Job.batch foo took: 35.439648ms
Jan 28 00:33:19.688: INFO: Terminating Job.batch foo pods took: 100.526074ms
STEP: Ensuring job was deleted 01/28/23 00:33:51.289
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 28 00:33:51.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7402" for this suite. 01/28/23 00:33:51.323
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":49,"skipped":895,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.011 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:33:15.33
    Jan 28 00:33:15.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename job 01/28/23 00:33:15.333
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:33:15.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:33:15.43
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 01/28/23 00:33:15.442
    STEP: Ensuring active pods == parallelism 01/28/23 00:33:15.477
    STEP: delete a job 01/28/23 00:33:19.489
    STEP: deleting Job.batch foo in namespace job-7402, will wait for the garbage collector to delete the pods 01/28/23 00:33:19.489
    Jan 28 00:33:19.587: INFO: Deleting Job.batch foo took: 35.439648ms
    Jan 28 00:33:19.688: INFO: Terminating Job.batch foo pods took: 100.526074ms
    STEP: Ensuring job was deleted 01/28/23 00:33:51.289
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 28 00:33:51.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7402" for this suite. 01/28/23 00:33:51.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:33:51.345
Jan 28 00:33:51.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename emptydir-wrapper 01/28/23 00:33:51.348
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:33:51.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:33:51.404
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan 28 00:33:51.472: INFO: Waiting up to 5m0s for pod "pod-secrets-ed74305b-ce09-49d4-9b73-f8bb12436a46" in namespace "emptydir-wrapper-1289" to be "running and ready"
Jan 28 00:33:51.483: INFO: Pod "pod-secrets-ed74305b-ce09-49d4-9b73-f8bb12436a46": Phase="Pending", Reason="", readiness=false. Elapsed: 10.768072ms
Jan 28 00:33:51.483: INFO: The phase of Pod pod-secrets-ed74305b-ce09-49d4-9b73-f8bb12436a46 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:33:53.495: INFO: Pod "pod-secrets-ed74305b-ce09-49d4-9b73-f8bb12436a46": Phase="Running", Reason="", readiness=true. Elapsed: 2.02267112s
Jan 28 00:33:53.495: INFO: The phase of Pod pod-secrets-ed74305b-ce09-49d4-9b73-f8bb12436a46 is Running (Ready = true)
Jan 28 00:33:53.495: INFO: Pod "pod-secrets-ed74305b-ce09-49d4-9b73-f8bb12436a46" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/28/23 00:33:53.505
STEP: Cleaning up the configmap 01/28/23 00:33:53.519
STEP: Cleaning up the pod 01/28/23 00:33:53.533
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan 28 00:33:53.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1289" for this suite. 01/28/23 00:33:53.57
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":50,"skipped":909,"failed":0}
------------------------------
â€¢ [2.238 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:33:51.345
    Jan 28 00:33:51.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename emptydir-wrapper 01/28/23 00:33:51.348
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:33:51.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:33:51.404
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan 28 00:33:51.472: INFO: Waiting up to 5m0s for pod "pod-secrets-ed74305b-ce09-49d4-9b73-f8bb12436a46" in namespace "emptydir-wrapper-1289" to be "running and ready"
    Jan 28 00:33:51.483: INFO: Pod "pod-secrets-ed74305b-ce09-49d4-9b73-f8bb12436a46": Phase="Pending", Reason="", readiness=false. Elapsed: 10.768072ms
    Jan 28 00:33:51.483: INFO: The phase of Pod pod-secrets-ed74305b-ce09-49d4-9b73-f8bb12436a46 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:33:53.495: INFO: Pod "pod-secrets-ed74305b-ce09-49d4-9b73-f8bb12436a46": Phase="Running", Reason="", readiness=true. Elapsed: 2.02267112s
    Jan 28 00:33:53.495: INFO: The phase of Pod pod-secrets-ed74305b-ce09-49d4-9b73-f8bb12436a46 is Running (Ready = true)
    Jan 28 00:33:53.495: INFO: Pod "pod-secrets-ed74305b-ce09-49d4-9b73-f8bb12436a46" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/28/23 00:33:53.505
    STEP: Cleaning up the configmap 01/28/23 00:33:53.519
    STEP: Cleaning up the pod 01/28/23 00:33:53.533
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan 28 00:33:53.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-1289" for this suite. 01/28/23 00:33:53.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:33:53.597
Jan 28 00:33:53.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename pods 01/28/23 00:33:53.599
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:33:53.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:33:53.655
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 01/28/23 00:33:53.668
STEP: submitting the pod to kubernetes 01/28/23 00:33:53.669
Jan 28 00:33:53.696: INFO: Waiting up to 5m0s for pod "pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879" in namespace "pods-7826" to be "running and ready"
Jan 28 00:33:53.708: INFO: Pod "pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879": Phase="Pending", Reason="", readiness=false. Elapsed: 11.127476ms
Jan 28 00:33:53.708: INFO: The phase of Pod pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:33:55.719: INFO: Pod "pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021992556s
Jan 28 00:33:55.719: INFO: The phase of Pod pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:33:57.722: INFO: Pod "pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879": Phase="Running", Reason="", readiness=true. Elapsed: 4.02517652s
Jan 28 00:33:57.722: INFO: The phase of Pod pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879 is Running (Ready = true)
Jan 28 00:33:57.722: INFO: Pod "pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/28/23 00:33:57.734
STEP: updating the pod 01/28/23 00:33:57.746
Jan 28 00:33:58.279: INFO: Successfully updated pod "pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879"
Jan 28 00:33:58.279: INFO: Waiting up to 5m0s for pod "pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879" in namespace "pods-7826" to be "running"
Jan 28 00:33:58.289: INFO: Pod "pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879": Phase="Running", Reason="", readiness=true. Elapsed: 9.542493ms
Jan 28 00:33:58.289: INFO: Pod "pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/28/23 00:33:58.289
Jan 28 00:33:58.299: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 28 00:33:58.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7826" for this suite. 01/28/23 00:33:58.314
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":51,"skipped":964,"failed":0}
------------------------------
â€¢ [4.735 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:33:53.597
    Jan 28 00:33:53.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename pods 01/28/23 00:33:53.599
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:33:53.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:33:53.655
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 01/28/23 00:33:53.668
    STEP: submitting the pod to kubernetes 01/28/23 00:33:53.669
    Jan 28 00:33:53.696: INFO: Waiting up to 5m0s for pod "pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879" in namespace "pods-7826" to be "running and ready"
    Jan 28 00:33:53.708: INFO: Pod "pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879": Phase="Pending", Reason="", readiness=false. Elapsed: 11.127476ms
    Jan 28 00:33:53.708: INFO: The phase of Pod pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:33:55.719: INFO: Pod "pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021992556s
    Jan 28 00:33:55.719: INFO: The phase of Pod pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:33:57.722: INFO: Pod "pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879": Phase="Running", Reason="", readiness=true. Elapsed: 4.02517652s
    Jan 28 00:33:57.722: INFO: The phase of Pod pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879 is Running (Ready = true)
    Jan 28 00:33:57.722: INFO: Pod "pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/28/23 00:33:57.734
    STEP: updating the pod 01/28/23 00:33:57.746
    Jan 28 00:33:58.279: INFO: Successfully updated pod "pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879"
    Jan 28 00:33:58.279: INFO: Waiting up to 5m0s for pod "pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879" in namespace "pods-7826" to be "running"
    Jan 28 00:33:58.289: INFO: Pod "pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879": Phase="Running", Reason="", readiness=true. Elapsed: 9.542493ms
    Jan 28 00:33:58.289: INFO: Pod "pod-update-2d94a212-bd3f-4061-8cec-d7f74903e879" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/28/23 00:33:58.289
    Jan 28 00:33:58.299: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 28 00:33:58.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7826" for this suite. 01/28/23 00:33:58.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:33:58.335
Jan 28 00:33:58.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename secrets 01/28/23 00:33:58.338
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:33:58.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:33:58.391
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-38266511-8252-49bb-af6e-8dae0faa68bd 01/28/23 00:33:58.405
STEP: Creating a pod to test consume secrets 01/28/23 00:33:58.419
Jan 28 00:33:58.445: INFO: Waiting up to 5m0s for pod "pod-secrets-c90d6bc7-4dbe-4400-9745-befc2563e8ba" in namespace "secrets-5082" to be "Succeeded or Failed"
Jan 28 00:33:58.459: INFO: Pod "pod-secrets-c90d6bc7-4dbe-4400-9745-befc2563e8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 13.936705ms
Jan 28 00:34:00.476: INFO: Pod "pod-secrets-c90d6bc7-4dbe-4400-9745-befc2563e8ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.03144655s
Jan 28 00:34:02.471: INFO: Pod "pod-secrets-c90d6bc7-4dbe-4400-9745-befc2563e8ba": Phase="Running", Reason="", readiness=false. Elapsed: 4.025995665s
Jan 28 00:34:04.475: INFO: Pod "pod-secrets-c90d6bc7-4dbe-4400-9745-befc2563e8ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029905446s
STEP: Saw pod success 01/28/23 00:34:04.475
Jan 28 00:34:04.475: INFO: Pod "pod-secrets-c90d6bc7-4dbe-4400-9745-befc2563e8ba" satisfied condition "Succeeded or Failed"
Jan 28 00:34:04.486: INFO: Trying to get logs from node 10.9.20.72 pod pod-secrets-c90d6bc7-4dbe-4400-9745-befc2563e8ba container secret-volume-test: <nil>
STEP: delete the pod 01/28/23 00:34:04.56
Jan 28 00:34:04.591: INFO: Waiting for pod pod-secrets-c90d6bc7-4dbe-4400-9745-befc2563e8ba to disappear
Jan 28 00:34:04.601: INFO: Pod pod-secrets-c90d6bc7-4dbe-4400-9745-befc2563e8ba no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 28 00:34:04.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5082" for this suite. 01/28/23 00:34:04.618
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":52,"skipped":971,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.298 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:33:58.335
    Jan 28 00:33:58.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename secrets 01/28/23 00:33:58.338
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:33:58.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:33:58.391
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-38266511-8252-49bb-af6e-8dae0faa68bd 01/28/23 00:33:58.405
    STEP: Creating a pod to test consume secrets 01/28/23 00:33:58.419
    Jan 28 00:33:58.445: INFO: Waiting up to 5m0s for pod "pod-secrets-c90d6bc7-4dbe-4400-9745-befc2563e8ba" in namespace "secrets-5082" to be "Succeeded or Failed"
    Jan 28 00:33:58.459: INFO: Pod "pod-secrets-c90d6bc7-4dbe-4400-9745-befc2563e8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 13.936705ms
    Jan 28 00:34:00.476: INFO: Pod "pod-secrets-c90d6bc7-4dbe-4400-9745-befc2563e8ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.03144655s
    Jan 28 00:34:02.471: INFO: Pod "pod-secrets-c90d6bc7-4dbe-4400-9745-befc2563e8ba": Phase="Running", Reason="", readiness=false. Elapsed: 4.025995665s
    Jan 28 00:34:04.475: INFO: Pod "pod-secrets-c90d6bc7-4dbe-4400-9745-befc2563e8ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029905446s
    STEP: Saw pod success 01/28/23 00:34:04.475
    Jan 28 00:34:04.475: INFO: Pod "pod-secrets-c90d6bc7-4dbe-4400-9745-befc2563e8ba" satisfied condition "Succeeded or Failed"
    Jan 28 00:34:04.486: INFO: Trying to get logs from node 10.9.20.72 pod pod-secrets-c90d6bc7-4dbe-4400-9745-befc2563e8ba container secret-volume-test: <nil>
    STEP: delete the pod 01/28/23 00:34:04.56
    Jan 28 00:34:04.591: INFO: Waiting for pod pod-secrets-c90d6bc7-4dbe-4400-9745-befc2563e8ba to disappear
    Jan 28 00:34:04.601: INFO: Pod pod-secrets-c90d6bc7-4dbe-4400-9745-befc2563e8ba no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 28 00:34:04.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5082" for this suite. 01/28/23 00:34:04.618
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:34:04.634
Jan 28 00:34:04.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename webhook 01/28/23 00:34:04.636
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:34:04.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:34:04.708
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/28/23 00:34:04.758
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 00:34:05.088
STEP: Deploying the webhook pod 01/28/23 00:34:05.117
STEP: Wait for the deployment to be ready 01/28/23 00:34:05.151
Jan 28 00:34:05.181: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 00:34:07.227: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 34, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 34, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 34, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 34, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/28/23 00:34:09.243
STEP: Verifying the service has paired with the endpoint 01/28/23 00:34:09.274
Jan 28 00:34:10.274: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/28/23 00:34:10.288
STEP: create a configmap that should be updated by the webhook 01/28/23 00:34:10.367
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 00:34:10.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2579" for this suite. 01/28/23 00:34:10.51
STEP: Destroying namespace "webhook-2579-markers" for this suite. 01/28/23 00:34:10.528
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":53,"skipped":975,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.022 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:34:04.634
    Jan 28 00:34:04.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename webhook 01/28/23 00:34:04.636
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:34:04.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:34:04.708
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/28/23 00:34:04.758
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 00:34:05.088
    STEP: Deploying the webhook pod 01/28/23 00:34:05.117
    STEP: Wait for the deployment to be ready 01/28/23 00:34:05.151
    Jan 28 00:34:05.181: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 28 00:34:07.227: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 34, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 34, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 34, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 34, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/28/23 00:34:09.243
    STEP: Verifying the service has paired with the endpoint 01/28/23 00:34:09.274
    Jan 28 00:34:10.274: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/28/23 00:34:10.288
    STEP: create a configmap that should be updated by the webhook 01/28/23 00:34:10.367
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 00:34:10.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2579" for this suite. 01/28/23 00:34:10.51
    STEP: Destroying namespace "webhook-2579-markers" for this suite. 01/28/23 00:34:10.528
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:34:10.663
Jan 28 00:34:10.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename webhook 01/28/23 00:34:10.665
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:34:10.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:34:10.705
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/28/23 00:34:10.749
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 00:34:11.173
STEP: Deploying the webhook pod 01/28/23 00:34:11.192
STEP: Wait for the deployment to be ready 01/28/23 00:34:11.235
Jan 28 00:34:11.262: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 00:34:13.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 34, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 34, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 34, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 34, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/28/23 00:34:15.311
STEP: Verifying the service has paired with the endpoint 01/28/23 00:34:15.341
Jan 28 00:34:16.342: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Jan 28 00:34:16.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-283-crds.webhook.example.com via the AdmissionRegistration API 01/28/23 00:34:16.893
STEP: Creating a custom resource that should be mutated by the webhook 01/28/23 00:34:16.969
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 00:34:19.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9565" for this suite. 01/28/23 00:34:19.706
STEP: Destroying namespace "webhook-9565-markers" for this suite. 01/28/23 00:34:19.722
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":54,"skipped":1052,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.197 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:34:10.663
    Jan 28 00:34:10.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename webhook 01/28/23 00:34:10.665
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:34:10.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:34:10.705
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/28/23 00:34:10.749
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 00:34:11.173
    STEP: Deploying the webhook pod 01/28/23 00:34:11.192
    STEP: Wait for the deployment to be ready 01/28/23 00:34:11.235
    Jan 28 00:34:11.262: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 28 00:34:13.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 34, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 34, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 34, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 34, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/28/23 00:34:15.311
    STEP: Verifying the service has paired with the endpoint 01/28/23 00:34:15.341
    Jan 28 00:34:16.342: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Jan 28 00:34:16.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-283-crds.webhook.example.com via the AdmissionRegistration API 01/28/23 00:34:16.893
    STEP: Creating a custom resource that should be mutated by the webhook 01/28/23 00:34:16.969
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 00:34:19.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9565" for this suite. 01/28/23 00:34:19.706
    STEP: Destroying namespace "webhook-9565-markers" for this suite. 01/28/23 00:34:19.722
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:34:19.863
Jan 28 00:34:19.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename pods 01/28/23 00:34:19.866
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:34:19.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:34:19.913
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 01/28/23 00:34:19.95
STEP: watching for Pod to be ready 01/28/23 00:34:19.973
Jan 28 00:34:19.979: INFO: observed Pod pod-test in namespace pods-3889 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 28 00:34:20.003: INFO: observed Pod pod-test in namespace pods-3889 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:19 +0000 UTC  }]
Jan 28 00:34:20.034: INFO: observed Pod pod-test in namespace pods-3889 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:19 +0000 UTC  }]
Jan 28 00:34:21.201: INFO: observed Pod pod-test in namespace pods-3889 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:19 +0000 UTC  }]
Jan 28 00:34:22.294: INFO: Found Pod pod-test in namespace pods-3889 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:19 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/28/23 00:34:22.308
STEP: getting the Pod and ensuring that it's patched 01/28/23 00:34:22.329
STEP: replacing the Pod's status Ready condition to False 01/28/23 00:34:22.34
STEP: check the Pod again to ensure its Ready conditions are False 01/28/23 00:34:22.367
STEP: deleting the Pod via a Collection with a LabelSelector 01/28/23 00:34:22.367
STEP: watching for the Pod to be deleted 01/28/23 00:34:22.393
Jan 28 00:34:22.399: INFO: observed event type MODIFIED
Jan 28 00:34:24.309: INFO: observed event type MODIFIED
Jan 28 00:34:24.675: INFO: observed event type MODIFIED
Jan 28 00:34:25.309: INFO: observed event type MODIFIED
Jan 28 00:34:25.330: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 28 00:34:25.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3889" for this suite. 01/28/23 00:34:25.366
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":55,"skipped":1064,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.520 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:34:19.863
    Jan 28 00:34:19.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename pods 01/28/23 00:34:19.866
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:34:19.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:34:19.913
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 01/28/23 00:34:19.95
    STEP: watching for Pod to be ready 01/28/23 00:34:19.973
    Jan 28 00:34:19.979: INFO: observed Pod pod-test in namespace pods-3889 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan 28 00:34:20.003: INFO: observed Pod pod-test in namespace pods-3889 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:19 +0000 UTC  }]
    Jan 28 00:34:20.034: INFO: observed Pod pod-test in namespace pods-3889 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:19 +0000 UTC  }]
    Jan 28 00:34:21.201: INFO: observed Pod pod-test in namespace pods-3889 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:19 +0000 UTC  }]
    Jan 28 00:34:22.294: INFO: Found Pod pod-test in namespace pods-3889 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:34:19 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/28/23 00:34:22.308
    STEP: getting the Pod and ensuring that it's patched 01/28/23 00:34:22.329
    STEP: replacing the Pod's status Ready condition to False 01/28/23 00:34:22.34
    STEP: check the Pod again to ensure its Ready conditions are False 01/28/23 00:34:22.367
    STEP: deleting the Pod via a Collection with a LabelSelector 01/28/23 00:34:22.367
    STEP: watching for the Pod to be deleted 01/28/23 00:34:22.393
    Jan 28 00:34:22.399: INFO: observed event type MODIFIED
    Jan 28 00:34:24.309: INFO: observed event type MODIFIED
    Jan 28 00:34:24.675: INFO: observed event type MODIFIED
    Jan 28 00:34:25.309: INFO: observed event type MODIFIED
    Jan 28 00:34:25.330: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 28 00:34:25.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3889" for this suite. 01/28/23 00:34:25.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:34:25.39
Jan 28 00:34:25.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubectl 01/28/23 00:34:25.392
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:34:25.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:34:25.438
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/28/23 00:34:25.45
Jan 28 00:34:25.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6842 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 28 00:34:25.568: INFO: stderr: ""
Jan 28 00:34:25.568: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/28/23 00:34:25.568
STEP: verifying the pod e2e-test-httpd-pod was created 01/28/23 00:34:30.62
Jan 28 00:34:30.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6842 get pod e2e-test-httpd-pod -o json'
Jan 28 00:34:30.740: INFO: stderr: ""
Jan 28 00:34:30.740: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"241d9fa156e8bf2f4f33137c26c749120a8d136dd009109d231a0f1bbc404db4\",\n            \"cni.projectcalico.org/podIP\": \"172.30.185.56/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.185.56/32\"\n        },\n        \"creationTimestamp\": \"2023-01-28T00:34:25Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6842\",\n        \"resourceVersion\": \"23524\",\n        \"uid\": \"80baa73c-92f5-4aa0-a0a0-6133b6ad389b\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-x7mcs\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.9.20.72\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-x7mcs\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-28T00:34:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-28T00:34:27Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-28T00:34:27Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-28T00:34:25Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://f03b17fd3d14faeba43526cfe26cc3500391d8f6912066975da50a1c7d79bff1\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-28T00:34:27Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.9.20.72\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.185.56\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.185.56\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-28T00:34:25Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/28/23 00:34:30.74
Jan 28 00:34:30.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6842 replace -f -'
Jan 28 00:34:31.831: INFO: stderr: ""
Jan 28 00:34:31.831: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/28/23 00:34:31.831
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Jan 28 00:34:31.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6842 delete pods e2e-test-httpd-pod'
Jan 28 00:34:33.517: INFO: stderr: ""
Jan 28 00:34:33.517: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 28 00:34:33.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6842" for this suite. 01/28/23 00:34:33.534
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":56,"skipped":1108,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.161 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:34:25.39
    Jan 28 00:34:25.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubectl 01/28/23 00:34:25.392
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:34:25.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:34:25.438
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/28/23 00:34:25.45
    Jan 28 00:34:25.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6842 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 28 00:34:25.568: INFO: stderr: ""
    Jan 28 00:34:25.568: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/28/23 00:34:25.568
    STEP: verifying the pod e2e-test-httpd-pod was created 01/28/23 00:34:30.62
    Jan 28 00:34:30.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6842 get pod e2e-test-httpd-pod -o json'
    Jan 28 00:34:30.740: INFO: stderr: ""
    Jan 28 00:34:30.740: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"241d9fa156e8bf2f4f33137c26c749120a8d136dd009109d231a0f1bbc404db4\",\n            \"cni.projectcalico.org/podIP\": \"172.30.185.56/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.185.56/32\"\n        },\n        \"creationTimestamp\": \"2023-01-28T00:34:25Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6842\",\n        \"resourceVersion\": \"23524\",\n        \"uid\": \"80baa73c-92f5-4aa0-a0a0-6133b6ad389b\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-x7mcs\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.9.20.72\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-x7mcs\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-28T00:34:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-28T00:34:27Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-28T00:34:27Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-28T00:34:25Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://f03b17fd3d14faeba43526cfe26cc3500391d8f6912066975da50a1c7d79bff1\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-28T00:34:27Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.9.20.72\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.185.56\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.185.56\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-28T00:34:25Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/28/23 00:34:30.74
    Jan 28 00:34:30.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6842 replace -f -'
    Jan 28 00:34:31.831: INFO: stderr: ""
    Jan 28 00:34:31.831: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/28/23 00:34:31.831
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Jan 28 00:34:31.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6842 delete pods e2e-test-httpd-pod'
    Jan 28 00:34:33.517: INFO: stderr: ""
    Jan 28 00:34:33.517: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 28 00:34:33.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6842" for this suite. 01/28/23 00:34:33.534
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:34:33.553
Jan 28 00:34:33.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 00:34:33.556
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:34:33.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:34:33.597
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 01/28/23 00:34:33.609
Jan 28 00:34:33.631: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81711f80-fab8-462f-b79b-a994eedc6b0b" in namespace "projected-2559" to be "Succeeded or Failed"
Jan 28 00:34:33.642: INFO: Pod "downwardapi-volume-81711f80-fab8-462f-b79b-a994eedc6b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.945732ms
Jan 28 00:34:35.654: INFO: Pod "downwardapi-volume-81711f80-fab8-462f-b79b-a994eedc6b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022877414s
Jan 28 00:34:37.654: INFO: Pod "downwardapi-volume-81711f80-fab8-462f-b79b-a994eedc6b0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022706943s
STEP: Saw pod success 01/28/23 00:34:37.654
Jan 28 00:34:37.654: INFO: Pod "downwardapi-volume-81711f80-fab8-462f-b79b-a994eedc6b0b" satisfied condition "Succeeded or Failed"
Jan 28 00:34:37.666: INFO: Trying to get logs from node 10.9.20.72 pod downwardapi-volume-81711f80-fab8-462f-b79b-a994eedc6b0b container client-container: <nil>
STEP: delete the pod 01/28/23 00:34:37.732
Jan 28 00:34:37.764: INFO: Waiting for pod downwardapi-volume-81711f80-fab8-462f-b79b-a994eedc6b0b to disappear
Jan 28 00:34:37.774: INFO: Pod downwardapi-volume-81711f80-fab8-462f-b79b-a994eedc6b0b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 28 00:34:37.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2559" for this suite. 01/28/23 00:34:37.79
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":57,"skipped":1111,"failed":0}
------------------------------
â€¢ [4.255 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:34:33.553
    Jan 28 00:34:33.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 00:34:33.556
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:34:33.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:34:33.597
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 01/28/23 00:34:33.609
    Jan 28 00:34:33.631: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81711f80-fab8-462f-b79b-a994eedc6b0b" in namespace "projected-2559" to be "Succeeded or Failed"
    Jan 28 00:34:33.642: INFO: Pod "downwardapi-volume-81711f80-fab8-462f-b79b-a994eedc6b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.945732ms
    Jan 28 00:34:35.654: INFO: Pod "downwardapi-volume-81711f80-fab8-462f-b79b-a994eedc6b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022877414s
    Jan 28 00:34:37.654: INFO: Pod "downwardapi-volume-81711f80-fab8-462f-b79b-a994eedc6b0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022706943s
    STEP: Saw pod success 01/28/23 00:34:37.654
    Jan 28 00:34:37.654: INFO: Pod "downwardapi-volume-81711f80-fab8-462f-b79b-a994eedc6b0b" satisfied condition "Succeeded or Failed"
    Jan 28 00:34:37.666: INFO: Trying to get logs from node 10.9.20.72 pod downwardapi-volume-81711f80-fab8-462f-b79b-a994eedc6b0b container client-container: <nil>
    STEP: delete the pod 01/28/23 00:34:37.732
    Jan 28 00:34:37.764: INFO: Waiting for pod downwardapi-volume-81711f80-fab8-462f-b79b-a994eedc6b0b to disappear
    Jan 28 00:34:37.774: INFO: Pod downwardapi-volume-81711f80-fab8-462f-b79b-a994eedc6b0b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 28 00:34:37.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2559" for this suite. 01/28/23 00:34:37.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:34:37.815
Jan 28 00:34:37.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 00:34:37.816
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:34:37.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:34:37.859
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 01/28/23 00:34:37.87
Jan 28 00:34:37.893: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a54caabb-1097-4e21-819c-d2ef7cd9ce6a" in namespace "projected-6508" to be "Succeeded or Failed"
Jan 28 00:34:37.904: INFO: Pod "downwardapi-volume-a54caabb-1097-4e21-819c-d2ef7cd9ce6a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.963603ms
Jan 28 00:34:39.916: INFO: Pod "downwardapi-volume-a54caabb-1097-4e21-819c-d2ef7cd9ce6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023072919s
Jan 28 00:34:41.920: INFO: Pod "downwardapi-volume-a54caabb-1097-4e21-819c-d2ef7cd9ce6a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027014356s
Jan 28 00:34:43.916: INFO: Pod "downwardapi-volume-a54caabb-1097-4e21-819c-d2ef7cd9ce6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023152487s
STEP: Saw pod success 01/28/23 00:34:43.916
Jan 28 00:34:43.917: INFO: Pod "downwardapi-volume-a54caabb-1097-4e21-819c-d2ef7cd9ce6a" satisfied condition "Succeeded or Failed"
Jan 28 00:34:43.927: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-a54caabb-1097-4e21-819c-d2ef7cd9ce6a container client-container: <nil>
STEP: delete the pod 01/28/23 00:34:43.989
Jan 28 00:34:44.020: INFO: Waiting for pod downwardapi-volume-a54caabb-1097-4e21-819c-d2ef7cd9ce6a to disappear
Jan 28 00:34:44.044: INFO: Pod downwardapi-volume-a54caabb-1097-4e21-819c-d2ef7cd9ce6a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 28 00:34:44.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6508" for this suite. 01/28/23 00:34:44.059
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":58,"skipped":1135,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.261 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:34:37.815
    Jan 28 00:34:37.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 00:34:37.816
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:34:37.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:34:37.859
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 01/28/23 00:34:37.87
    Jan 28 00:34:37.893: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a54caabb-1097-4e21-819c-d2ef7cd9ce6a" in namespace "projected-6508" to be "Succeeded or Failed"
    Jan 28 00:34:37.904: INFO: Pod "downwardapi-volume-a54caabb-1097-4e21-819c-d2ef7cd9ce6a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.963603ms
    Jan 28 00:34:39.916: INFO: Pod "downwardapi-volume-a54caabb-1097-4e21-819c-d2ef7cd9ce6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023072919s
    Jan 28 00:34:41.920: INFO: Pod "downwardapi-volume-a54caabb-1097-4e21-819c-d2ef7cd9ce6a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027014356s
    Jan 28 00:34:43.916: INFO: Pod "downwardapi-volume-a54caabb-1097-4e21-819c-d2ef7cd9ce6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023152487s
    STEP: Saw pod success 01/28/23 00:34:43.916
    Jan 28 00:34:43.917: INFO: Pod "downwardapi-volume-a54caabb-1097-4e21-819c-d2ef7cd9ce6a" satisfied condition "Succeeded or Failed"
    Jan 28 00:34:43.927: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-a54caabb-1097-4e21-819c-d2ef7cd9ce6a container client-container: <nil>
    STEP: delete the pod 01/28/23 00:34:43.989
    Jan 28 00:34:44.020: INFO: Waiting for pod downwardapi-volume-a54caabb-1097-4e21-819c-d2ef7cd9ce6a to disappear
    Jan 28 00:34:44.044: INFO: Pod downwardapi-volume-a54caabb-1097-4e21-819c-d2ef7cd9ce6a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 28 00:34:44.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6508" for this suite. 01/28/23 00:34:44.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:34:44.08
Jan 28 00:34:44.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename var-expansion 01/28/23 00:34:44.082
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:34:44.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:34:44.132
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 01/28/23 00:34:44.142
Jan 28 00:34:44.161: INFO: Waiting up to 2m0s for pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139" in namespace "var-expansion-9312" to be "running"
Jan 28 00:34:44.172: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 10.555725ms
Jan 28 00:34:46.186: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025003608s
Jan 28 00:34:48.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022324023s
Jan 28 00:34:50.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02236622s
Jan 28 00:34:52.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022821618s
Jan 28 00:34:54.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022367323s
Jan 28 00:34:56.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 12.022737418s
Jan 28 00:34:58.187: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026340013s
Jan 28 00:35:00.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 16.023455098s
Jan 28 00:35:02.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 18.022483122s
Jan 28 00:35:04.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 20.022240408s
Jan 28 00:35:06.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 22.021645467s
Jan 28 00:35:08.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 24.02371511s
Jan 28 00:35:10.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 26.022023883s
Jan 28 00:35:12.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 28.023317447s
Jan 28 00:35:14.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 30.022974324s
Jan 28 00:35:16.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 32.02221032s
Jan 28 00:35:18.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 34.023771086s
Jan 28 00:35:20.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 36.022100892s
Jan 28 00:35:22.214: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 38.053502942s
Jan 28 00:35:24.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 40.02382269s
Jan 28 00:35:26.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 42.022984045s
Jan 28 00:35:28.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 44.023112119s
Jan 28 00:35:30.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 46.023741868s
Jan 28 00:35:32.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 48.024330158s
Jan 28 00:35:34.187: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 50.026079806s
Jan 28 00:35:36.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 52.023396764s
Jan 28 00:35:38.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 54.022227651s
Jan 28 00:35:40.187: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 56.02559539s
Jan 28 00:35:42.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 58.023301288s
Jan 28 00:35:44.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.023310844s
Jan 28 00:35:46.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.024087233s
Jan 28 00:35:48.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.023166163s
Jan 28 00:35:50.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.024369446s
Jan 28 00:35:52.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.024297577s
Jan 28 00:35:54.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.022566244s
Jan 28 00:35:56.210: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.048756493s
Jan 28 00:35:58.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.021688078s
Jan 28 00:36:00.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.022028193s
Jan 28 00:36:02.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.021569647s
Jan 28 00:36:04.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.024089263s
Jan 28 00:36:06.182: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.020981582s
Jan 28 00:36:08.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.023396012s
Jan 28 00:36:10.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.022512791s
Jan 28 00:36:12.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.023621293s
Jan 28 00:36:14.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.022691538s
Jan 28 00:36:16.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.022879516s
Jan 28 00:36:18.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.023273434s
Jan 28 00:36:20.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.02407524s
Jan 28 00:36:22.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.022471924s
Jan 28 00:36:24.186: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.024939506s
Jan 28 00:36:26.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.022848942s
Jan 28 00:36:28.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.023438512s
Jan 28 00:36:30.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.023390219s
Jan 28 00:36:32.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.023614797s
Jan 28 00:36:34.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.022333265s
Jan 28 00:36:36.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.023779798s
Jan 28 00:36:38.187: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.025893904s
Jan 28 00:36:40.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.024278713s
Jan 28 00:36:42.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.024332106s
Jan 28 00:36:44.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022550447s
Jan 28 00:36:44.196: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.034681271s
STEP: updating the pod 01/28/23 00:36:44.196
Jan 28 00:36:44.725: INFO: Successfully updated pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139"
STEP: waiting for pod running 01/28/23 00:36:44.726
Jan 28 00:36:44.726: INFO: Waiting up to 2m0s for pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139" in namespace "var-expansion-9312" to be "running"
Jan 28 00:36:44.737: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 10.118814ms
Jan 28 00:36:46.749: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Running", Reason="", readiness=true. Elapsed: 2.022756554s
Jan 28 00:36:46.749: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139" satisfied condition "running"
STEP: deleting the pod gracefully 01/28/23 00:36:46.749
Jan 28 00:36:46.750: INFO: Deleting pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139" in namespace "var-expansion-9312"
Jan 28 00:36:46.769: INFO: Wait up to 5m0s for pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 28 00:37:18.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9312" for this suite. 01/28/23 00:37:18.809
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":59,"skipped":1157,"failed":0}
------------------------------
â€¢ [SLOW TEST] [154.747 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:34:44.08
    Jan 28 00:34:44.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename var-expansion 01/28/23 00:34:44.082
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:34:44.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:34:44.132
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 01/28/23 00:34:44.142
    Jan 28 00:34:44.161: INFO: Waiting up to 2m0s for pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139" in namespace "var-expansion-9312" to be "running"
    Jan 28 00:34:44.172: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 10.555725ms
    Jan 28 00:34:46.186: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025003608s
    Jan 28 00:34:48.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022324023s
    Jan 28 00:34:50.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02236622s
    Jan 28 00:34:52.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022821618s
    Jan 28 00:34:54.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022367323s
    Jan 28 00:34:56.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 12.022737418s
    Jan 28 00:34:58.187: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026340013s
    Jan 28 00:35:00.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 16.023455098s
    Jan 28 00:35:02.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 18.022483122s
    Jan 28 00:35:04.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 20.022240408s
    Jan 28 00:35:06.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 22.021645467s
    Jan 28 00:35:08.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 24.02371511s
    Jan 28 00:35:10.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 26.022023883s
    Jan 28 00:35:12.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 28.023317447s
    Jan 28 00:35:14.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 30.022974324s
    Jan 28 00:35:16.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 32.02221032s
    Jan 28 00:35:18.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 34.023771086s
    Jan 28 00:35:20.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 36.022100892s
    Jan 28 00:35:22.214: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 38.053502942s
    Jan 28 00:35:24.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 40.02382269s
    Jan 28 00:35:26.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 42.022984045s
    Jan 28 00:35:28.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 44.023112119s
    Jan 28 00:35:30.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 46.023741868s
    Jan 28 00:35:32.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 48.024330158s
    Jan 28 00:35:34.187: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 50.026079806s
    Jan 28 00:35:36.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 52.023396764s
    Jan 28 00:35:38.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 54.022227651s
    Jan 28 00:35:40.187: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 56.02559539s
    Jan 28 00:35:42.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 58.023301288s
    Jan 28 00:35:44.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.023310844s
    Jan 28 00:35:46.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.024087233s
    Jan 28 00:35:48.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.023166163s
    Jan 28 00:35:50.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.024369446s
    Jan 28 00:35:52.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.024297577s
    Jan 28 00:35:54.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.022566244s
    Jan 28 00:35:56.210: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.048756493s
    Jan 28 00:35:58.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.021688078s
    Jan 28 00:36:00.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.022028193s
    Jan 28 00:36:02.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.021569647s
    Jan 28 00:36:04.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.024089263s
    Jan 28 00:36:06.182: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.020981582s
    Jan 28 00:36:08.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.023396012s
    Jan 28 00:36:10.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.022512791s
    Jan 28 00:36:12.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.023621293s
    Jan 28 00:36:14.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.022691538s
    Jan 28 00:36:16.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.022879516s
    Jan 28 00:36:18.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.023273434s
    Jan 28 00:36:20.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.02407524s
    Jan 28 00:36:22.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.022471924s
    Jan 28 00:36:24.186: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.024939506s
    Jan 28 00:36:26.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.022848942s
    Jan 28 00:36:28.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.023438512s
    Jan 28 00:36:30.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.023390219s
    Jan 28 00:36:32.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.023614797s
    Jan 28 00:36:34.183: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.022333265s
    Jan 28 00:36:36.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.023779798s
    Jan 28 00:36:38.187: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.025893904s
    Jan 28 00:36:40.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.024278713s
    Jan 28 00:36:42.185: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.024332106s
    Jan 28 00:36:44.184: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022550447s
    Jan 28 00:36:44.196: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.034681271s
    STEP: updating the pod 01/28/23 00:36:44.196
    Jan 28 00:36:44.725: INFO: Successfully updated pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139"
    STEP: waiting for pod running 01/28/23 00:36:44.726
    Jan 28 00:36:44.726: INFO: Waiting up to 2m0s for pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139" in namespace "var-expansion-9312" to be "running"
    Jan 28 00:36:44.737: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Pending", Reason="", readiness=false. Elapsed: 10.118814ms
    Jan 28 00:36:46.749: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139": Phase="Running", Reason="", readiness=true. Elapsed: 2.022756554s
    Jan 28 00:36:46.749: INFO: Pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139" satisfied condition "running"
    STEP: deleting the pod gracefully 01/28/23 00:36:46.749
    Jan 28 00:36:46.750: INFO: Deleting pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139" in namespace "var-expansion-9312"
    Jan 28 00:36:46.769: INFO: Wait up to 5m0s for pod "var-expansion-0b685f1d-8d41-437a-ba73-6d7687bfb139" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 28 00:37:18.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9312" for this suite. 01/28/23 00:37:18.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:37:18.836
Jan 28 00:37:18.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename cronjob 01/28/23 00:37:18.838
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:37:18.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:37:18.895
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/28/23 00:37:18.908
STEP: creating 01/28/23 00:37:18.909
STEP: getting 01/28/23 00:37:18.952
STEP: listing 01/28/23 00:37:18.963
STEP: watching 01/28/23 00:37:18.975
Jan 28 00:37:18.975: INFO: starting watch
STEP: cluster-wide listing 01/28/23 00:37:18.98
STEP: cluster-wide watching 01/28/23 00:37:18.992
Jan 28 00:37:18.992: INFO: starting watch
STEP: patching 01/28/23 00:37:18.997
STEP: updating 01/28/23 00:37:19.015
Jan 28 00:37:19.040: INFO: waiting for watch events with expected annotations
Jan 28 00:37:19.040: INFO: saw patched and updated annotations
STEP: patching /status 01/28/23 00:37:19.04
STEP: updating /status 01/28/23 00:37:19.056
STEP: get /status 01/28/23 00:37:19.083
STEP: deleting 01/28/23 00:37:19.094
STEP: deleting a collection 01/28/23 00:37:19.135
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 28 00:37:19.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1027" for this suite. 01/28/23 00:37:19.186
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":60,"skipped":1213,"failed":0}
------------------------------
â€¢ [0.370 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:37:18.836
    Jan 28 00:37:18.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename cronjob 01/28/23 00:37:18.838
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:37:18.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:37:18.895
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/28/23 00:37:18.908
    STEP: creating 01/28/23 00:37:18.909
    STEP: getting 01/28/23 00:37:18.952
    STEP: listing 01/28/23 00:37:18.963
    STEP: watching 01/28/23 00:37:18.975
    Jan 28 00:37:18.975: INFO: starting watch
    STEP: cluster-wide listing 01/28/23 00:37:18.98
    STEP: cluster-wide watching 01/28/23 00:37:18.992
    Jan 28 00:37:18.992: INFO: starting watch
    STEP: patching 01/28/23 00:37:18.997
    STEP: updating 01/28/23 00:37:19.015
    Jan 28 00:37:19.040: INFO: waiting for watch events with expected annotations
    Jan 28 00:37:19.040: INFO: saw patched and updated annotations
    STEP: patching /status 01/28/23 00:37:19.04
    STEP: updating /status 01/28/23 00:37:19.056
    STEP: get /status 01/28/23 00:37:19.083
    STEP: deleting 01/28/23 00:37:19.094
    STEP: deleting a collection 01/28/23 00:37:19.135
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 28 00:37:19.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-1027" for this suite. 01/28/23 00:37:19.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:37:19.212
Jan 28 00:37:19.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename statefulset 01/28/23 00:37:19.214
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:37:19.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:37:19.26
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2489 01/28/23 00:37:19.272
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 01/28/23 00:37:19.285
Jan 28 00:37:19.312: INFO: Found 0 stateful pods, waiting for 3
Jan 28 00:37:29.326: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 00:37:29.326: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 00:37:29.327: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/28/23 00:37:29.362
Jan 28 00:37:29.397: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/28/23 00:37:29.397
STEP: Not applying an update when the partition is greater than the number of replicas 01/28/23 00:37:39.445
STEP: Performing a canary update 01/28/23 00:37:39.445
Jan 28 00:37:39.480: INFO: Updating stateful set ss2
Jan 28 00:37:39.505: INFO: Waiting for Pod statefulset-2489/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 01/28/23 00:37:49.538
Jan 28 00:37:49.620: INFO: Found 2 stateful pods, waiting for 3
Jan 28 00:37:59.634: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 00:37:59.634: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 00:37:59.634: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/28/23 00:37:59.66
Jan 28 00:37:59.697: INFO: Updating stateful set ss2
Jan 28 00:37:59.739: INFO: Waiting for Pod statefulset-2489/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jan 28 00:38:09.806: INFO: Updating stateful set ss2
Jan 28 00:38:09.829: INFO: Waiting for StatefulSet statefulset-2489/ss2 to complete update
Jan 28 00:38:09.830: INFO: Waiting for Pod statefulset-2489/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jan 28 00:38:19.858: INFO: Waiting for StatefulSet statefulset-2489/ss2 to complete update
Jan 28 00:38:29.855: INFO: Waiting for StatefulSet statefulset-2489/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 28 00:38:39.855: INFO: Deleting all statefulset in ns statefulset-2489
Jan 28 00:38:39.868: INFO: Scaling statefulset ss2 to 0
Jan 28 00:38:49.923: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 00:38:49.934: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 28 00:38:49.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2489" for this suite. 01/28/23 00:38:50.006
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":61,"skipped":1228,"failed":0}
------------------------------
â€¢ [SLOW TEST] [90.818 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:37:19.212
    Jan 28 00:37:19.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename statefulset 01/28/23 00:37:19.214
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:37:19.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:37:19.26
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2489 01/28/23 00:37:19.272
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 01/28/23 00:37:19.285
    Jan 28 00:37:19.312: INFO: Found 0 stateful pods, waiting for 3
    Jan 28 00:37:29.326: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 28 00:37:29.326: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 28 00:37:29.327: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/28/23 00:37:29.362
    Jan 28 00:37:29.397: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/28/23 00:37:29.397
    STEP: Not applying an update when the partition is greater than the number of replicas 01/28/23 00:37:39.445
    STEP: Performing a canary update 01/28/23 00:37:39.445
    Jan 28 00:37:39.480: INFO: Updating stateful set ss2
    Jan 28 00:37:39.505: INFO: Waiting for Pod statefulset-2489/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 01/28/23 00:37:49.538
    Jan 28 00:37:49.620: INFO: Found 2 stateful pods, waiting for 3
    Jan 28 00:37:59.634: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 28 00:37:59.634: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 28 00:37:59.634: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/28/23 00:37:59.66
    Jan 28 00:37:59.697: INFO: Updating stateful set ss2
    Jan 28 00:37:59.739: INFO: Waiting for Pod statefulset-2489/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jan 28 00:38:09.806: INFO: Updating stateful set ss2
    Jan 28 00:38:09.829: INFO: Waiting for StatefulSet statefulset-2489/ss2 to complete update
    Jan 28 00:38:09.830: INFO: Waiting for Pod statefulset-2489/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jan 28 00:38:19.858: INFO: Waiting for StatefulSet statefulset-2489/ss2 to complete update
    Jan 28 00:38:29.855: INFO: Waiting for StatefulSet statefulset-2489/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 28 00:38:39.855: INFO: Deleting all statefulset in ns statefulset-2489
    Jan 28 00:38:39.868: INFO: Scaling statefulset ss2 to 0
    Jan 28 00:38:49.923: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 28 00:38:49.934: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 28 00:38:49.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2489" for this suite. 01/28/23 00:38:50.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:38:50.039
Jan 28 00:38:50.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename containers 01/28/23 00:38:50.041
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:38:50.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:38:50.101
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 01/28/23 00:38:50.113
Jan 28 00:38:50.133: INFO: Waiting up to 5m0s for pod "client-containers-2c9f7974-cccc-4792-a4db-0316daf62673" in namespace "containers-2765" to be "Succeeded or Failed"
Jan 28 00:38:50.144: INFO: Pod "client-containers-2c9f7974-cccc-4792-a4db-0316daf62673": Phase="Pending", Reason="", readiness=false. Elapsed: 10.564692ms
Jan 28 00:38:52.155: INFO: Pod "client-containers-2c9f7974-cccc-4792-a4db-0316daf62673": Phase="Running", Reason="", readiness=true. Elapsed: 2.021886915s
Jan 28 00:38:54.154: INFO: Pod "client-containers-2c9f7974-cccc-4792-a4db-0316daf62673": Phase="Running", Reason="", readiness=false. Elapsed: 4.020739446s
Jan 28 00:38:56.154: INFO: Pod "client-containers-2c9f7974-cccc-4792-a4db-0316daf62673": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021136537s
STEP: Saw pod success 01/28/23 00:38:56.154
Jan 28 00:38:56.155: INFO: Pod "client-containers-2c9f7974-cccc-4792-a4db-0316daf62673" satisfied condition "Succeeded or Failed"
Jan 28 00:38:56.165: INFO: Trying to get logs from node 10.9.20.72 pod client-containers-2c9f7974-cccc-4792-a4db-0316daf62673 container agnhost-container: <nil>
STEP: delete the pod 01/28/23 00:38:56.248
Jan 28 00:38:56.275: INFO: Waiting for pod client-containers-2c9f7974-cccc-4792-a4db-0316daf62673 to disappear
Jan 28 00:38:56.285: INFO: Pod client-containers-2c9f7974-cccc-4792-a4db-0316daf62673 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 28 00:38:56.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2765" for this suite. 01/28/23 00:38:56.3
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":62,"skipped":1244,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.282 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:38:50.039
    Jan 28 00:38:50.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename containers 01/28/23 00:38:50.041
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:38:50.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:38:50.101
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 01/28/23 00:38:50.113
    Jan 28 00:38:50.133: INFO: Waiting up to 5m0s for pod "client-containers-2c9f7974-cccc-4792-a4db-0316daf62673" in namespace "containers-2765" to be "Succeeded or Failed"
    Jan 28 00:38:50.144: INFO: Pod "client-containers-2c9f7974-cccc-4792-a4db-0316daf62673": Phase="Pending", Reason="", readiness=false. Elapsed: 10.564692ms
    Jan 28 00:38:52.155: INFO: Pod "client-containers-2c9f7974-cccc-4792-a4db-0316daf62673": Phase="Running", Reason="", readiness=true. Elapsed: 2.021886915s
    Jan 28 00:38:54.154: INFO: Pod "client-containers-2c9f7974-cccc-4792-a4db-0316daf62673": Phase="Running", Reason="", readiness=false. Elapsed: 4.020739446s
    Jan 28 00:38:56.154: INFO: Pod "client-containers-2c9f7974-cccc-4792-a4db-0316daf62673": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021136537s
    STEP: Saw pod success 01/28/23 00:38:56.154
    Jan 28 00:38:56.155: INFO: Pod "client-containers-2c9f7974-cccc-4792-a4db-0316daf62673" satisfied condition "Succeeded or Failed"
    Jan 28 00:38:56.165: INFO: Trying to get logs from node 10.9.20.72 pod client-containers-2c9f7974-cccc-4792-a4db-0316daf62673 container agnhost-container: <nil>
    STEP: delete the pod 01/28/23 00:38:56.248
    Jan 28 00:38:56.275: INFO: Waiting for pod client-containers-2c9f7974-cccc-4792-a4db-0316daf62673 to disappear
    Jan 28 00:38:56.285: INFO: Pod client-containers-2c9f7974-cccc-4792-a4db-0316daf62673 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 28 00:38:56.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-2765" for this suite. 01/28/23 00:38:56.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:38:56.329
Jan 28 00:38:56.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename replicaset 01/28/23 00:38:56.332
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:38:56.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:38:56.378
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan 28 00:38:56.390: INFO: Creating ReplicaSet my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7
Jan 28 00:38:56.416: INFO: Pod name my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7: Found 0 pods out of 1
Jan 28 00:39:01.431: INFO: Pod name my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7: Found 1 pods out of 1
Jan 28 00:39:01.431: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7" is running
Jan 28 00:39:01.431: INFO: Waiting up to 5m0s for pod "my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7-sxzmz" in namespace "replicaset-290" to be "running"
Jan 28 00:39:01.442: INFO: Pod "my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7-sxzmz": Phase="Running", Reason="", readiness=true. Elapsed: 11.419235ms
Jan 28 00:39:01.442: INFO: Pod "my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7-sxzmz" satisfied condition "running"
Jan 28 00:39:01.442: INFO: Pod "my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7-sxzmz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 00:38:56 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 00:38:58 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 00:38:58 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 00:38:56 +0000 UTC Reason: Message:}])
Jan 28 00:39:01.442: INFO: Trying to dial the pod
Jan 28 00:39:06.512: INFO: Controller my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7: Got expected result from replica 1 [my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7-sxzmz]: "my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7-sxzmz", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 28 00:39:06.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-290" for this suite. 01/28/23 00:39:06.528
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":63,"skipped":1282,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.217 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:38:56.329
    Jan 28 00:38:56.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename replicaset 01/28/23 00:38:56.332
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:38:56.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:38:56.378
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan 28 00:38:56.390: INFO: Creating ReplicaSet my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7
    Jan 28 00:38:56.416: INFO: Pod name my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7: Found 0 pods out of 1
    Jan 28 00:39:01.431: INFO: Pod name my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7: Found 1 pods out of 1
    Jan 28 00:39:01.431: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7" is running
    Jan 28 00:39:01.431: INFO: Waiting up to 5m0s for pod "my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7-sxzmz" in namespace "replicaset-290" to be "running"
    Jan 28 00:39:01.442: INFO: Pod "my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7-sxzmz": Phase="Running", Reason="", readiness=true. Elapsed: 11.419235ms
    Jan 28 00:39:01.442: INFO: Pod "my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7-sxzmz" satisfied condition "running"
    Jan 28 00:39:01.442: INFO: Pod "my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7-sxzmz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 00:38:56 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 00:38:58 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 00:38:58 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 00:38:56 +0000 UTC Reason: Message:}])
    Jan 28 00:39:01.442: INFO: Trying to dial the pod
    Jan 28 00:39:06.512: INFO: Controller my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7: Got expected result from replica 1 [my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7-sxzmz]: "my-hostname-basic-2b6eff50-f15f-47df-a757-2cf25f0e90f7-sxzmz", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 28 00:39:06.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-290" for this suite. 01/28/23 00:39:06.528
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:39:06.547
Jan 28 00:39:06.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename cronjob 01/28/23 00:39:06.55
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:39:06.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:39:06.603
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/28/23 00:39:06.614
STEP: Ensuring a job is scheduled 01/28/23 00:39:06.628
STEP: Ensuring exactly one is scheduled 01/28/23 00:40:00.639
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/28/23 00:40:00.651
STEP: Ensuring the job is replaced with a new one 01/28/23 00:40:00.662
STEP: Removing cronjob 01/28/23 00:41:00.677
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 28 00:41:00.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6951" for this suite. 01/28/23 00:41:00.718
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":64,"skipped":1287,"failed":0}
------------------------------
â€¢ [SLOW TEST] [114.189 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:39:06.547
    Jan 28 00:39:06.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename cronjob 01/28/23 00:39:06.55
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:39:06.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:39:06.603
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/28/23 00:39:06.614
    STEP: Ensuring a job is scheduled 01/28/23 00:39:06.628
    STEP: Ensuring exactly one is scheduled 01/28/23 00:40:00.639
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/28/23 00:40:00.651
    STEP: Ensuring the job is replaced with a new one 01/28/23 00:40:00.662
    STEP: Removing cronjob 01/28/23 00:41:00.677
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 28 00:41:00.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6951" for this suite. 01/28/23 00:41:00.718
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:41:00.749
Jan 28 00:41:00.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename watch 01/28/23 00:41:00.752
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:41:00.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:41:00.798
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/28/23 00:41:00.81
STEP: creating a new configmap 01/28/23 00:41:00.815
STEP: modifying the configmap once 01/28/23 00:41:00.83
STEP: changing the label value of the configmap 01/28/23 00:41:00.853
STEP: Expecting to observe a delete notification for the watched object 01/28/23 00:41:00.875
Jan 28 00:41:00.876: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2398  69c58a58-1407-43a0-ae8f-18a0c304a91a 24520 0 2023-01-28 00:41:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-28 00:41:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 00:41:00.877: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2398  69c58a58-1407-43a0-ae8f-18a0c304a91a 24521 0 2023-01-28 00:41:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-28 00:41:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 00:41:00.877: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2398  69c58a58-1407-43a0-ae8f-18a0c304a91a 24522 0 2023-01-28 00:41:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-28 00:41:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/28/23 00:41:00.877
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/28/23 00:41:00.9
STEP: changing the label value of the configmap back 01/28/23 00:41:10.901
STEP: modifying the configmap a third time 01/28/23 00:41:10.926
STEP: deleting the configmap 01/28/23 00:41:10.949
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/28/23 00:41:10.965
Jan 28 00:41:10.966: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2398  69c58a58-1407-43a0-ae8f-18a0c304a91a 24553 0 2023-01-28 00:41:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-28 00:41:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 00:41:10.966: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2398  69c58a58-1407-43a0-ae8f-18a0c304a91a 24554 0 2023-01-28 00:41:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-28 00:41:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 00:41:10.966: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2398  69c58a58-1407-43a0-ae8f-18a0c304a91a 24555 0 2023-01-28 00:41:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-28 00:41:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 28 00:41:10.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2398" for this suite. 01/28/23 00:41:10.982
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":65,"skipped":1292,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.251 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:41:00.749
    Jan 28 00:41:00.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename watch 01/28/23 00:41:00.752
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:41:00.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:41:00.798
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/28/23 00:41:00.81
    STEP: creating a new configmap 01/28/23 00:41:00.815
    STEP: modifying the configmap once 01/28/23 00:41:00.83
    STEP: changing the label value of the configmap 01/28/23 00:41:00.853
    STEP: Expecting to observe a delete notification for the watched object 01/28/23 00:41:00.875
    Jan 28 00:41:00.876: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2398  69c58a58-1407-43a0-ae8f-18a0c304a91a 24520 0 2023-01-28 00:41:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-28 00:41:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 28 00:41:00.877: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2398  69c58a58-1407-43a0-ae8f-18a0c304a91a 24521 0 2023-01-28 00:41:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-28 00:41:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 28 00:41:00.877: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2398  69c58a58-1407-43a0-ae8f-18a0c304a91a 24522 0 2023-01-28 00:41:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-28 00:41:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/28/23 00:41:00.877
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/28/23 00:41:00.9
    STEP: changing the label value of the configmap back 01/28/23 00:41:10.901
    STEP: modifying the configmap a third time 01/28/23 00:41:10.926
    STEP: deleting the configmap 01/28/23 00:41:10.949
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/28/23 00:41:10.965
    Jan 28 00:41:10.966: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2398  69c58a58-1407-43a0-ae8f-18a0c304a91a 24553 0 2023-01-28 00:41:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-28 00:41:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 28 00:41:10.966: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2398  69c58a58-1407-43a0-ae8f-18a0c304a91a 24554 0 2023-01-28 00:41:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-28 00:41:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 28 00:41:10.966: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2398  69c58a58-1407-43a0-ae8f-18a0c304a91a 24555 0 2023-01-28 00:41:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-28 00:41:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 28 00:41:10.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2398" for this suite. 01/28/23 00:41:10.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:41:11.007
Jan 28 00:41:11.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename secrets 01/28/23 00:41:11.009
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:41:11.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:41:11.051
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-aa0ccc5b-1aeb-4bc9-85d5-86259d66ce3d 01/28/23 00:41:11.062
STEP: Creating a pod to test consume secrets 01/28/23 00:41:11.075
Jan 28 00:41:11.096: INFO: Waiting up to 5m0s for pod "pod-secrets-256799e7-c282-461c-943d-0f9932854cb3" in namespace "secrets-4392" to be "Succeeded or Failed"
Jan 28 00:41:11.107: INFO: Pod "pod-secrets-256799e7-c282-461c-943d-0f9932854cb3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.061805ms
Jan 28 00:41:13.121: INFO: Pod "pod-secrets-256799e7-c282-461c-943d-0f9932854cb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025355551s
Jan 28 00:41:15.120: INFO: Pod "pod-secrets-256799e7-c282-461c-943d-0f9932854cb3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023853075s
Jan 28 00:41:17.120: INFO: Pod "pod-secrets-256799e7-c282-461c-943d-0f9932854cb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023705161s
STEP: Saw pod success 01/28/23 00:41:17.12
Jan 28 00:41:17.120: INFO: Pod "pod-secrets-256799e7-c282-461c-943d-0f9932854cb3" satisfied condition "Succeeded or Failed"
Jan 28 00:41:17.131: INFO: Trying to get logs from node 10.9.20.126 pod pod-secrets-256799e7-c282-461c-943d-0f9932854cb3 container secret-volume-test: <nil>
STEP: delete the pod 01/28/23 00:41:17.212
Jan 28 00:41:17.239: INFO: Waiting for pod pod-secrets-256799e7-c282-461c-943d-0f9932854cb3 to disappear
Jan 28 00:41:17.250: INFO: Pod pod-secrets-256799e7-c282-461c-943d-0f9932854cb3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 28 00:41:17.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4392" for this suite. 01/28/23 00:41:17.266
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":66,"skipped":1308,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.278 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:41:11.007
    Jan 28 00:41:11.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename secrets 01/28/23 00:41:11.009
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:41:11.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:41:11.051
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-aa0ccc5b-1aeb-4bc9-85d5-86259d66ce3d 01/28/23 00:41:11.062
    STEP: Creating a pod to test consume secrets 01/28/23 00:41:11.075
    Jan 28 00:41:11.096: INFO: Waiting up to 5m0s for pod "pod-secrets-256799e7-c282-461c-943d-0f9932854cb3" in namespace "secrets-4392" to be "Succeeded or Failed"
    Jan 28 00:41:11.107: INFO: Pod "pod-secrets-256799e7-c282-461c-943d-0f9932854cb3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.061805ms
    Jan 28 00:41:13.121: INFO: Pod "pod-secrets-256799e7-c282-461c-943d-0f9932854cb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025355551s
    Jan 28 00:41:15.120: INFO: Pod "pod-secrets-256799e7-c282-461c-943d-0f9932854cb3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023853075s
    Jan 28 00:41:17.120: INFO: Pod "pod-secrets-256799e7-c282-461c-943d-0f9932854cb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023705161s
    STEP: Saw pod success 01/28/23 00:41:17.12
    Jan 28 00:41:17.120: INFO: Pod "pod-secrets-256799e7-c282-461c-943d-0f9932854cb3" satisfied condition "Succeeded or Failed"
    Jan 28 00:41:17.131: INFO: Trying to get logs from node 10.9.20.126 pod pod-secrets-256799e7-c282-461c-943d-0f9932854cb3 container secret-volume-test: <nil>
    STEP: delete the pod 01/28/23 00:41:17.212
    Jan 28 00:41:17.239: INFO: Waiting for pod pod-secrets-256799e7-c282-461c-943d-0f9932854cb3 to disappear
    Jan 28 00:41:17.250: INFO: Pod pod-secrets-256799e7-c282-461c-943d-0f9932854cb3 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 28 00:41:17.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4392" for this suite. 01/28/23 00:41:17.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:41:17.292
Jan 28 00:41:17.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename subpath 01/28/23 00:41:17.294
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:41:17.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:41:17.337
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/28/23 00:41:17.349
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-bfl9 01/28/23 00:41:17.374
STEP: Creating a pod to test atomic-volume-subpath 01/28/23 00:41:17.374
Jan 28 00:41:17.395: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-bfl9" in namespace "subpath-9142" to be "Succeeded or Failed"
Jan 28 00:41:17.407: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.650079ms
Jan 28 00:41:19.423: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 2.028072887s
Jan 28 00:41:21.420: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 4.025870824s
Jan 28 00:41:23.420: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 6.025790308s
Jan 28 00:41:25.420: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 8.025631867s
Jan 28 00:41:27.418: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 10.023696403s
Jan 28 00:41:29.449: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 12.054274383s
Jan 28 00:41:31.450: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 14.055096436s
Jan 28 00:41:33.419: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 16.024132928s
Jan 28 00:41:35.422: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 18.027148947s
Jan 28 00:41:37.421: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 20.026080067s
Jan 28 00:41:39.420: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 22.024940891s
Jan 28 00:41:41.419: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=false. Elapsed: 24.024237405s
Jan 28 00:41:43.418: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.023759679s
STEP: Saw pod success 01/28/23 00:41:43.418
Jan 28 00:41:43.419: INFO: Pod "pod-subpath-test-downwardapi-bfl9" satisfied condition "Succeeded or Failed"
Jan 28 00:41:43.430: INFO: Trying to get logs from node 10.9.20.72 pod pod-subpath-test-downwardapi-bfl9 container test-container-subpath-downwardapi-bfl9: <nil>
STEP: delete the pod 01/28/23 00:41:43.505
Jan 28 00:41:43.563: INFO: Waiting for pod pod-subpath-test-downwardapi-bfl9 to disappear
Jan 28 00:41:43.572: INFO: Pod pod-subpath-test-downwardapi-bfl9 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-bfl9 01/28/23 00:41:43.573
Jan 28 00:41:43.573: INFO: Deleting pod "pod-subpath-test-downwardapi-bfl9" in namespace "subpath-9142"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 28 00:41:43.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9142" for this suite. 01/28/23 00:41:43.602
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":67,"skipped":1364,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.339 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:41:17.292
    Jan 28 00:41:17.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename subpath 01/28/23 00:41:17.294
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:41:17.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:41:17.337
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/28/23 00:41:17.349
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-bfl9 01/28/23 00:41:17.374
    STEP: Creating a pod to test atomic-volume-subpath 01/28/23 00:41:17.374
    Jan 28 00:41:17.395: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-bfl9" in namespace "subpath-9142" to be "Succeeded or Failed"
    Jan 28 00:41:17.407: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.650079ms
    Jan 28 00:41:19.423: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 2.028072887s
    Jan 28 00:41:21.420: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 4.025870824s
    Jan 28 00:41:23.420: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 6.025790308s
    Jan 28 00:41:25.420: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 8.025631867s
    Jan 28 00:41:27.418: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 10.023696403s
    Jan 28 00:41:29.449: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 12.054274383s
    Jan 28 00:41:31.450: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 14.055096436s
    Jan 28 00:41:33.419: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 16.024132928s
    Jan 28 00:41:35.422: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 18.027148947s
    Jan 28 00:41:37.421: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 20.026080067s
    Jan 28 00:41:39.420: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=true. Elapsed: 22.024940891s
    Jan 28 00:41:41.419: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Running", Reason="", readiness=false. Elapsed: 24.024237405s
    Jan 28 00:41:43.418: INFO: Pod "pod-subpath-test-downwardapi-bfl9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.023759679s
    STEP: Saw pod success 01/28/23 00:41:43.418
    Jan 28 00:41:43.419: INFO: Pod "pod-subpath-test-downwardapi-bfl9" satisfied condition "Succeeded or Failed"
    Jan 28 00:41:43.430: INFO: Trying to get logs from node 10.9.20.72 pod pod-subpath-test-downwardapi-bfl9 container test-container-subpath-downwardapi-bfl9: <nil>
    STEP: delete the pod 01/28/23 00:41:43.505
    Jan 28 00:41:43.563: INFO: Waiting for pod pod-subpath-test-downwardapi-bfl9 to disappear
    Jan 28 00:41:43.572: INFO: Pod pod-subpath-test-downwardapi-bfl9 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-bfl9 01/28/23 00:41:43.573
    Jan 28 00:41:43.573: INFO: Deleting pod "pod-subpath-test-downwardapi-bfl9" in namespace "subpath-9142"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 28 00:41:43.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9142" for this suite. 01/28/23 00:41:43.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:41:43.632
Jan 28 00:41:43.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename configmap 01/28/23 00:41:43.635
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:41:43.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:41:43.68
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-12a7badb-161a-4dfa-8269-a5b1451ae39c 01/28/23 00:41:43.735
STEP: Creating the pod 01/28/23 00:41:43.748
Jan 28 00:41:43.767: INFO: Waiting up to 5m0s for pod "pod-configmaps-89a75226-39be-438b-8a82-47a508605395" in namespace "configmap-4617" to be "running"
Jan 28 00:41:43.781: INFO: Pod "pod-configmaps-89a75226-39be-438b-8a82-47a508605395": Phase="Pending", Reason="", readiness=false. Elapsed: 14.148023ms
Jan 28 00:41:45.793: INFO: Pod "pod-configmaps-89a75226-39be-438b-8a82-47a508605395": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025512379s
Jan 28 00:41:47.794: INFO: Pod "pod-configmaps-89a75226-39be-438b-8a82-47a508605395": Phase="Running", Reason="", readiness=false. Elapsed: 4.027332913s
Jan 28 00:41:47.795: INFO: Pod "pod-configmaps-89a75226-39be-438b-8a82-47a508605395" satisfied condition "running"
STEP: Waiting for pod with text data 01/28/23 00:41:47.795
STEP: Waiting for pod with binary data 01/28/23 00:41:47.876
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 28 00:41:47.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4617" for this suite. 01/28/23 00:41:47.945
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":68,"skipped":1371,"failed":0}
------------------------------
â€¢ [4.333 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:41:43.632
    Jan 28 00:41:43.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename configmap 01/28/23 00:41:43.635
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:41:43.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:41:43.68
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-12a7badb-161a-4dfa-8269-a5b1451ae39c 01/28/23 00:41:43.735
    STEP: Creating the pod 01/28/23 00:41:43.748
    Jan 28 00:41:43.767: INFO: Waiting up to 5m0s for pod "pod-configmaps-89a75226-39be-438b-8a82-47a508605395" in namespace "configmap-4617" to be "running"
    Jan 28 00:41:43.781: INFO: Pod "pod-configmaps-89a75226-39be-438b-8a82-47a508605395": Phase="Pending", Reason="", readiness=false. Elapsed: 14.148023ms
    Jan 28 00:41:45.793: INFO: Pod "pod-configmaps-89a75226-39be-438b-8a82-47a508605395": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025512379s
    Jan 28 00:41:47.794: INFO: Pod "pod-configmaps-89a75226-39be-438b-8a82-47a508605395": Phase="Running", Reason="", readiness=false. Elapsed: 4.027332913s
    Jan 28 00:41:47.795: INFO: Pod "pod-configmaps-89a75226-39be-438b-8a82-47a508605395" satisfied condition "running"
    STEP: Waiting for pod with text data 01/28/23 00:41:47.795
    STEP: Waiting for pod with binary data 01/28/23 00:41:47.876
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 28 00:41:47.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4617" for this suite. 01/28/23 00:41:47.945
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:41:47.969
Jan 28 00:41:47.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename init-container 01/28/23 00:41:47.971
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:41:48.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:41:48.016
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 01/28/23 00:41:48.028
Jan 28 00:41:48.029: INFO: PodSpec: initContainers in spec.initContainers
Jan 28 00:42:34.866: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b89bb930-3779-4c5d-ba14-79cc6661fdaa", GenerateName:"", Namespace:"init-container-3774", SelfLink:"", UID:"96d24d6b-116c-468b-b84b-8194163c645c", ResourceVersion:"24792", Generation:0, CreationTimestamp:time.Date(2023, time.January, 28, 0, 41, 48, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"29357484"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"fb772185786a1b6e7921a022f50ae0450613829a400be8cbcadf67dadddb24b8", "cni.projectcalico.org/podIP":"172.30.185.38/32", "cni.projectcalico.org/podIPs":"172.30.185.38/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 28, 0, 41, 48, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00334c168), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 28, 0, 41, 49, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00334c1b0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 28, 0, 42, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00334c1e0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-mbsb7", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00389e1c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-mbsb7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-mbsb7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-mbsb7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00196e348), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.9.20.72", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0009e4150), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00196e3d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00196e3f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00196e3f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00196e3fc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000e020b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 28, 0, 41, 48, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 28, 0, 41, 48, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 28, 0, 41, 48, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 28, 0, 41, 48, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.9.20.72", PodIP:"172.30.185.38", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.185.38"}}, StartTime:time.Date(2023, time.January, 28, 0, 41, 48, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0009e4230)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0009e42a0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://a949cef74dedca2ba03ae111a877a01a1f9a63eecf5368c1af5e83173635db68", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00389e320), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00389e2e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc00196e48f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 28 00:42:34.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3774" for this suite. 01/28/23 00:42:34.888
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":69,"skipped":1376,"failed":0}
------------------------------
â€¢ [SLOW TEST] [46.934 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:41:47.969
    Jan 28 00:41:47.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename init-container 01/28/23 00:41:47.971
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:41:48.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:41:48.016
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 01/28/23 00:41:48.028
    Jan 28 00:41:48.029: INFO: PodSpec: initContainers in spec.initContainers
    Jan 28 00:42:34.866: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b89bb930-3779-4c5d-ba14-79cc6661fdaa", GenerateName:"", Namespace:"init-container-3774", SelfLink:"", UID:"96d24d6b-116c-468b-b84b-8194163c645c", ResourceVersion:"24792", Generation:0, CreationTimestamp:time.Date(2023, time.January, 28, 0, 41, 48, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"29357484"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"fb772185786a1b6e7921a022f50ae0450613829a400be8cbcadf67dadddb24b8", "cni.projectcalico.org/podIP":"172.30.185.38/32", "cni.projectcalico.org/podIPs":"172.30.185.38/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 28, 0, 41, 48, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00334c168), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 28, 0, 41, 49, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00334c1b0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 28, 0, 42, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00334c1e0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-mbsb7", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00389e1c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-mbsb7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-mbsb7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-mbsb7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00196e348), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.9.20.72", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0009e4150), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00196e3d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00196e3f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00196e3f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00196e3fc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000e020b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 28, 0, 41, 48, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 28, 0, 41, 48, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 28, 0, 41, 48, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 28, 0, 41, 48, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.9.20.72", PodIP:"172.30.185.38", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.185.38"}}, StartTime:time.Date(2023, time.January, 28, 0, 41, 48, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0009e4230)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0009e42a0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://a949cef74dedca2ba03ae111a877a01a1f9a63eecf5368c1af5e83173635db68", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00389e320), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00389e2e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc00196e48f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 28 00:42:34.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-3774" for this suite. 01/28/23 00:42:34.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:42:34.907
Jan 28 00:42:34.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename container-probe 01/28/23 00:42:34.909
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:42:34.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:42:34.949
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-6eb26178-523a-481c-be4e-c917d9b59ef1 in namespace container-probe-5875 01/28/23 00:42:34.96
Jan 28 00:42:34.979: INFO: Waiting up to 5m0s for pod "test-webserver-6eb26178-523a-481c-be4e-c917d9b59ef1" in namespace "container-probe-5875" to be "not pending"
Jan 28 00:42:34.991: INFO: Pod "test-webserver-6eb26178-523a-481c-be4e-c917d9b59ef1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.439412ms
Jan 28 00:42:37.007: INFO: Pod "test-webserver-6eb26178-523a-481c-be4e-c917d9b59ef1": Phase="Running", Reason="", readiness=true. Elapsed: 2.028124007s
Jan 28 00:42:37.007: INFO: Pod "test-webserver-6eb26178-523a-481c-be4e-c917d9b59ef1" satisfied condition "not pending"
Jan 28 00:42:37.007: INFO: Started pod test-webserver-6eb26178-523a-481c-be4e-c917d9b59ef1 in namespace container-probe-5875
STEP: checking the pod's current state and verifying that restartCount is present 01/28/23 00:42:37.007
Jan 28 00:42:37.019: INFO: Initial restart count of pod test-webserver-6eb26178-523a-481c-be4e-c917d9b59ef1 is 0
STEP: deleting the pod 01/28/23 00:46:38.564
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 28 00:46:38.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5875" for this suite. 01/28/23 00:46:38.632
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":70,"skipped":1394,"failed":0}
------------------------------
â€¢ [SLOW TEST] [243.738 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:42:34.907
    Jan 28 00:42:34.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename container-probe 01/28/23 00:42:34.909
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:42:34.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:42:34.949
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-6eb26178-523a-481c-be4e-c917d9b59ef1 in namespace container-probe-5875 01/28/23 00:42:34.96
    Jan 28 00:42:34.979: INFO: Waiting up to 5m0s for pod "test-webserver-6eb26178-523a-481c-be4e-c917d9b59ef1" in namespace "container-probe-5875" to be "not pending"
    Jan 28 00:42:34.991: INFO: Pod "test-webserver-6eb26178-523a-481c-be4e-c917d9b59ef1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.439412ms
    Jan 28 00:42:37.007: INFO: Pod "test-webserver-6eb26178-523a-481c-be4e-c917d9b59ef1": Phase="Running", Reason="", readiness=true. Elapsed: 2.028124007s
    Jan 28 00:42:37.007: INFO: Pod "test-webserver-6eb26178-523a-481c-be4e-c917d9b59ef1" satisfied condition "not pending"
    Jan 28 00:42:37.007: INFO: Started pod test-webserver-6eb26178-523a-481c-be4e-c917d9b59ef1 in namespace container-probe-5875
    STEP: checking the pod's current state and verifying that restartCount is present 01/28/23 00:42:37.007
    Jan 28 00:42:37.019: INFO: Initial restart count of pod test-webserver-6eb26178-523a-481c-be4e-c917d9b59ef1 is 0
    STEP: deleting the pod 01/28/23 00:46:38.564
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 28 00:46:38.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5875" for this suite. 01/28/23 00:46:38.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:46:38.653
Jan 28 00:46:38.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename container-lifecycle-hook 01/28/23 00:46:38.655
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:46:38.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:46:38.73
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/28/23 00:46:38.755
Jan 28 00:46:38.773: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4711" to be "running and ready"
Jan 28 00:46:38.789: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 15.685693ms
Jan 28 00:46:38.789: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:46:40.801: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027203863s
Jan 28 00:46:40.801: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:46:42.802: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.028568901s
Jan 28 00:46:42.802: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 28 00:46:42.802: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 01/28/23 00:46:42.814
Jan 28 00:46:42.829: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4711" to be "running and ready"
Jan 28 00:46:42.839: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.701806ms
Jan 28 00:46:42.839: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:46:44.851: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.021138826s
Jan 28 00:46:44.851: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan 28 00:46:44.851: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/28/23 00:46:44.862
Jan 28 00:46:44.888: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 28 00:46:44.897: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 28 00:46:46.897: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 28 00:46:46.907: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 28 00:46:48.897: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 28 00:46:48.909: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/28/23 00:46:48.909
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 28 00:46:48.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4711" for this suite. 01/28/23 00:46:49.006
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":71,"skipped":1418,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.369 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:46:38.653
    Jan 28 00:46:38.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/28/23 00:46:38.655
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:46:38.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:46:38.73
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/28/23 00:46:38.755
    Jan 28 00:46:38.773: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4711" to be "running and ready"
    Jan 28 00:46:38.789: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 15.685693ms
    Jan 28 00:46:38.789: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:46:40.801: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027203863s
    Jan 28 00:46:40.801: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:46:42.802: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.028568901s
    Jan 28 00:46:42.802: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 28 00:46:42.802: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 01/28/23 00:46:42.814
    Jan 28 00:46:42.829: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4711" to be "running and ready"
    Jan 28 00:46:42.839: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.701806ms
    Jan 28 00:46:42.839: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:46:44.851: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.021138826s
    Jan 28 00:46:44.851: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan 28 00:46:44.851: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/28/23 00:46:44.862
    Jan 28 00:46:44.888: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 28 00:46:44.897: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 28 00:46:46.897: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 28 00:46:46.907: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 28 00:46:48.897: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 28 00:46:48.909: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/28/23 00:46:48.909
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 28 00:46:48.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4711" for this suite. 01/28/23 00:46:49.006
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:46:49.022
Jan 28 00:46:49.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename job 01/28/23 00:46:49.025
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:46:49.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:46:49.073
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 01/28/23 00:46:49.082
STEP: Ensure pods equal to paralellism count is attached to the job 01/28/23 00:46:49.096
STEP: patching /status 01/28/23 00:46:53.113
STEP: updating /status 01/28/23 00:46:53.132
STEP: get /status 01/28/23 00:46:53.157
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 28 00:46:53.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2286" for this suite. 01/28/23 00:46:53.184
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":72,"skipped":1419,"failed":0}
------------------------------
â€¢ [4.181 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:46:49.022
    Jan 28 00:46:49.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename job 01/28/23 00:46:49.025
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:46:49.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:46:49.073
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 01/28/23 00:46:49.082
    STEP: Ensure pods equal to paralellism count is attached to the job 01/28/23 00:46:49.096
    STEP: patching /status 01/28/23 00:46:53.113
    STEP: updating /status 01/28/23 00:46:53.132
    STEP: get /status 01/28/23 00:46:53.157
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 28 00:46:53.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-2286" for this suite. 01/28/23 00:46:53.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:46:53.208
Jan 28 00:46:53.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename subpath 01/28/23 00:46:53.212
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:46:53.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:46:53.272
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/28/23 00:46:53.282
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-58bf 01/28/23 00:46:53.31
STEP: Creating a pod to test atomic-volume-subpath 01/28/23 00:46:53.311
Jan 28 00:46:53.334: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-58bf" in namespace "subpath-6978" to be "Succeeded or Failed"
Jan 28 00:46:53.342: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.031625ms
Jan 28 00:46:55.357: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022653901s
Jan 28 00:46:57.357: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 4.022265384s
Jan 28 00:46:59.356: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 6.0216992s
Jan 28 00:47:01.357: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 8.022645091s
Jan 28 00:47:03.355: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 10.020803123s
Jan 28 00:47:05.356: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 12.021957815s
Jan 28 00:47:07.355: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 14.020320948s
Jan 28 00:47:09.354: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 16.019855945s
Jan 28 00:47:11.357: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 18.022873519s
Jan 28 00:47:13.358: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 20.023212982s
Jan 28 00:47:15.355: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 22.020789835s
Jan 28 00:47:17.354: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=false. Elapsed: 24.019248123s
Jan 28 00:47:19.357: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.022527053s
STEP: Saw pod success 01/28/23 00:47:19.357
Jan 28 00:47:19.357: INFO: Pod "pod-subpath-test-secret-58bf" satisfied condition "Succeeded or Failed"
Jan 28 00:47:19.369: INFO: Trying to get logs from node 10.9.20.126 pod pod-subpath-test-secret-58bf container test-container-subpath-secret-58bf: <nil>
STEP: delete the pod 01/28/23 00:47:19.445
Jan 28 00:47:19.477: INFO: Waiting for pod pod-subpath-test-secret-58bf to disappear
Jan 28 00:47:19.484: INFO: Pod pod-subpath-test-secret-58bf no longer exists
STEP: Deleting pod pod-subpath-test-secret-58bf 01/28/23 00:47:19.484
Jan 28 00:47:19.484: INFO: Deleting pod "pod-subpath-test-secret-58bf" in namespace "subpath-6978"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 28 00:47:19.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6978" for this suite. 01/28/23 00:47:19.507
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":73,"skipped":1432,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.312 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:46:53.208
    Jan 28 00:46:53.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename subpath 01/28/23 00:46:53.212
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:46:53.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:46:53.272
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/28/23 00:46:53.282
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-58bf 01/28/23 00:46:53.31
    STEP: Creating a pod to test atomic-volume-subpath 01/28/23 00:46:53.311
    Jan 28 00:46:53.334: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-58bf" in namespace "subpath-6978" to be "Succeeded or Failed"
    Jan 28 00:46:53.342: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.031625ms
    Jan 28 00:46:55.357: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022653901s
    Jan 28 00:46:57.357: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 4.022265384s
    Jan 28 00:46:59.356: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 6.0216992s
    Jan 28 00:47:01.357: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 8.022645091s
    Jan 28 00:47:03.355: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 10.020803123s
    Jan 28 00:47:05.356: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 12.021957815s
    Jan 28 00:47:07.355: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 14.020320948s
    Jan 28 00:47:09.354: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 16.019855945s
    Jan 28 00:47:11.357: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 18.022873519s
    Jan 28 00:47:13.358: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 20.023212982s
    Jan 28 00:47:15.355: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=true. Elapsed: 22.020789835s
    Jan 28 00:47:17.354: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Running", Reason="", readiness=false. Elapsed: 24.019248123s
    Jan 28 00:47:19.357: INFO: Pod "pod-subpath-test-secret-58bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.022527053s
    STEP: Saw pod success 01/28/23 00:47:19.357
    Jan 28 00:47:19.357: INFO: Pod "pod-subpath-test-secret-58bf" satisfied condition "Succeeded or Failed"
    Jan 28 00:47:19.369: INFO: Trying to get logs from node 10.9.20.126 pod pod-subpath-test-secret-58bf container test-container-subpath-secret-58bf: <nil>
    STEP: delete the pod 01/28/23 00:47:19.445
    Jan 28 00:47:19.477: INFO: Waiting for pod pod-subpath-test-secret-58bf to disappear
    Jan 28 00:47:19.484: INFO: Pod pod-subpath-test-secret-58bf no longer exists
    STEP: Deleting pod pod-subpath-test-secret-58bf 01/28/23 00:47:19.484
    Jan 28 00:47:19.484: INFO: Deleting pod "pod-subpath-test-secret-58bf" in namespace "subpath-6978"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 28 00:47:19.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-6978" for this suite. 01/28/23 00:47:19.507
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:47:19.526
Jan 28 00:47:19.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename endpointslicemirroring 01/28/23 00:47:19.528
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:47:19.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:47:19.577
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/28/23 00:47:19.608
Jan 28 00:47:19.643: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/28/23 00:47:21.658
Jan 28 00:47:21.697: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 01/28/23 00:47:23.711
Jan 28 00:47:23.750: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Jan 28 00:47:25.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-5618" for this suite. 01/28/23 00:47:25.779
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":74,"skipped":1463,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.272 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:47:19.526
    Jan 28 00:47:19.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename endpointslicemirroring 01/28/23 00:47:19.528
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:47:19.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:47:19.577
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/28/23 00:47:19.608
    Jan 28 00:47:19.643: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/28/23 00:47:21.658
    Jan 28 00:47:21.697: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 01/28/23 00:47:23.711
    Jan 28 00:47:23.750: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Jan 28 00:47:25.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-5618" for this suite. 01/28/23 00:47:25.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:47:25.801
Jan 28 00:47:25.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename cronjob 01/28/23 00:47:25.803
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:47:25.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:47:25.854
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/28/23 00:47:25.865
STEP: Ensuring no jobs are scheduled 01/28/23 00:47:25.881
STEP: Ensuring no job exists by listing jobs explicitly 01/28/23 00:52:25.907
STEP: Removing cronjob 01/28/23 00:52:25.917
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 28 00:52:25.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9713" for this suite. 01/28/23 00:52:25.949
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":75,"skipped":1483,"failed":0}
------------------------------
â€¢ [SLOW TEST] [300.167 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:47:25.801
    Jan 28 00:47:25.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename cronjob 01/28/23 00:47:25.803
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:47:25.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:47:25.854
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/28/23 00:47:25.865
    STEP: Ensuring no jobs are scheduled 01/28/23 00:47:25.881
    STEP: Ensuring no job exists by listing jobs explicitly 01/28/23 00:52:25.907
    STEP: Removing cronjob 01/28/23 00:52:25.917
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 28 00:52:25.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9713" for this suite. 01/28/23 00:52:25.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:52:25.97
Jan 28 00:52:25.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename daemonsets 01/28/23 00:52:25.973
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:26.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:26.027
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 01/28/23 00:52:26.105
STEP: Check that daemon pods launch on every node of the cluster. 01/28/23 00:52:26.12
Jan 28 00:52:26.157: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:52:26.157: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 00:52:27.197: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:52:27.203: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 00:52:28.184: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 28 00:52:28.184: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 00:52:29.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 28 00:52:29.187: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/28/23 00:52:29.198
Jan 28 00:52:29.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:52:29.256: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 00:52:30.283: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:52:30.283: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 00:52:31.288: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 28 00:52:31.288: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/28/23 00:52:31.288
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/28/23 00:52:31.307
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9923, will wait for the garbage collector to delete the pods 01/28/23 00:52:31.307
Jan 28 00:52:31.387: INFO: Deleting DaemonSet.extensions daemon-set took: 17.643692ms
Jan 28 00:52:31.588: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.166375ms
Jan 28 00:52:34.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:52:34.200: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 28 00:52:34.210: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25680"},"items":null}

Jan 28 00:52:34.220: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25680"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 28 00:52:34.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9923" for this suite. 01/28/23 00:52:34.285
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":76,"skipped":1492,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.329 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:52:25.97
    Jan 28 00:52:25.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename daemonsets 01/28/23 00:52:25.973
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:26.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:26.027
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 01/28/23 00:52:26.105
    STEP: Check that daemon pods launch on every node of the cluster. 01/28/23 00:52:26.12
    Jan 28 00:52:26.157: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 00:52:26.157: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 00:52:27.197: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 00:52:27.203: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 00:52:28.184: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 28 00:52:28.184: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 00:52:29.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 28 00:52:29.187: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/28/23 00:52:29.198
    Jan 28 00:52:29.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 28 00:52:29.256: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 00:52:30.283: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 28 00:52:30.283: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 00:52:31.288: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 28 00:52:31.288: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/28/23 00:52:31.288
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/28/23 00:52:31.307
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9923, will wait for the garbage collector to delete the pods 01/28/23 00:52:31.307
    Jan 28 00:52:31.387: INFO: Deleting DaemonSet.extensions daemon-set took: 17.643692ms
    Jan 28 00:52:31.588: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.166375ms
    Jan 28 00:52:34.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 00:52:34.200: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 28 00:52:34.210: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25680"},"items":null}

    Jan 28 00:52:34.220: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25680"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 00:52:34.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9923" for this suite. 01/28/23 00:52:34.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:52:34.305
Jan 28 00:52:34.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename pods 01/28/23 00:52:34.307
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:34.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:34.362
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 01/28/23 00:52:34.373
STEP: setting up watch 01/28/23 00:52:34.374
STEP: submitting the pod to kubernetes 01/28/23 00:52:34.485
STEP: verifying the pod is in kubernetes 01/28/23 00:52:34.512
STEP: verifying pod creation was observed 01/28/23 00:52:34.523
Jan 28 00:52:34.524: INFO: Waiting up to 5m0s for pod "pod-submit-remove-6281aeef-8e2d-4f8d-9d2f-2d3976de1322" in namespace "pods-8611" to be "running"
Jan 28 00:52:34.534: INFO: Pod "pod-submit-remove-6281aeef-8e2d-4f8d-9d2f-2d3976de1322": Phase="Pending", Reason="", readiness=false. Elapsed: 10.532585ms
Jan 28 00:52:36.549: INFO: Pod "pod-submit-remove-6281aeef-8e2d-4f8d-9d2f-2d3976de1322": Phase="Running", Reason="", readiness=true. Elapsed: 2.025018241s
Jan 28 00:52:36.549: INFO: Pod "pod-submit-remove-6281aeef-8e2d-4f8d-9d2f-2d3976de1322" satisfied condition "running"
STEP: deleting the pod gracefully 01/28/23 00:52:36.56
STEP: verifying pod deletion was observed 01/28/23 00:52:36.58
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 28 00:52:39.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8611" for this suite. 01/28/23 00:52:39.185
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":77,"skipped":1533,"failed":0}
------------------------------
â€¢ [4.895 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:52:34.305
    Jan 28 00:52:34.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename pods 01/28/23 00:52:34.307
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:34.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:34.362
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 01/28/23 00:52:34.373
    STEP: setting up watch 01/28/23 00:52:34.374
    STEP: submitting the pod to kubernetes 01/28/23 00:52:34.485
    STEP: verifying the pod is in kubernetes 01/28/23 00:52:34.512
    STEP: verifying pod creation was observed 01/28/23 00:52:34.523
    Jan 28 00:52:34.524: INFO: Waiting up to 5m0s for pod "pod-submit-remove-6281aeef-8e2d-4f8d-9d2f-2d3976de1322" in namespace "pods-8611" to be "running"
    Jan 28 00:52:34.534: INFO: Pod "pod-submit-remove-6281aeef-8e2d-4f8d-9d2f-2d3976de1322": Phase="Pending", Reason="", readiness=false. Elapsed: 10.532585ms
    Jan 28 00:52:36.549: INFO: Pod "pod-submit-remove-6281aeef-8e2d-4f8d-9d2f-2d3976de1322": Phase="Running", Reason="", readiness=true. Elapsed: 2.025018241s
    Jan 28 00:52:36.549: INFO: Pod "pod-submit-remove-6281aeef-8e2d-4f8d-9d2f-2d3976de1322" satisfied condition "running"
    STEP: deleting the pod gracefully 01/28/23 00:52:36.56
    STEP: verifying pod deletion was observed 01/28/23 00:52:36.58
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 28 00:52:39.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8611" for this suite. 01/28/23 00:52:39.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:52:39.205
Jan 28 00:52:39.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename services 01/28/23 00:52:39.207
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:39.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:39.284
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 01/28/23 00:52:39.295
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 28 00:52:39.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7451" for this suite. 01/28/23 00:52:39.324
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":78,"skipped":1548,"failed":0}
------------------------------
â€¢ [0.135 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:52:39.205
    Jan 28 00:52:39.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename services 01/28/23 00:52:39.207
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:39.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:39.284
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 01/28/23 00:52:39.295
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 28 00:52:39.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7451" for this suite. 01/28/23 00:52:39.324
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:52:39.346
Jan 28 00:52:39.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename pods 01/28/23 00:52:39.348
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:39.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:39.41
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/28/23 00:52:39.421
STEP: submitting the pod to kubernetes 01/28/23 00:52:39.422
STEP: verifying QOS class is set on the pod 01/28/23 00:52:39.442
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Jan 28 00:52:39.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5376" for this suite. 01/28/23 00:52:39.47
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":79,"skipped":1565,"failed":0}
------------------------------
â€¢ [0.136 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:52:39.346
    Jan 28 00:52:39.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename pods 01/28/23 00:52:39.348
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:39.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:39.41
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/28/23 00:52:39.421
    STEP: submitting the pod to kubernetes 01/28/23 00:52:39.422
    STEP: verifying QOS class is set on the pod 01/28/23 00:52:39.442
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Jan 28 00:52:39.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5376" for this suite. 01/28/23 00:52:39.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:52:39.483
Jan 28 00:52:39.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename var-expansion 01/28/23 00:52:39.485
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:39.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:39.551
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Jan 28 00:52:39.576: INFO: Waiting up to 2m0s for pod "var-expansion-4c5c2410-7668-4055-83fd-cd9a33d852d3" in namespace "var-expansion-1116" to be "container 0 failed with reason CreateContainerConfigError"
Jan 28 00:52:39.582: INFO: Pod "var-expansion-4c5c2410-7668-4055-83fd-cd9a33d852d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.356432ms
Jan 28 00:52:41.596: INFO: Pod "var-expansion-4c5c2410-7668-4055-83fd-cd9a33d852d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019960544s
Jan 28 00:52:43.594: INFO: Pod "var-expansion-4c5c2410-7668-4055-83fd-cd9a33d852d3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018668843s
Jan 28 00:52:43.595: INFO: Pod "var-expansion-4c5c2410-7668-4055-83fd-cd9a33d852d3" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 28 00:52:43.595: INFO: Deleting pod "var-expansion-4c5c2410-7668-4055-83fd-cd9a33d852d3" in namespace "var-expansion-1116"
Jan 28 00:52:43.621: INFO: Wait up to 5m0s for pod "var-expansion-4c5c2410-7668-4055-83fd-cd9a33d852d3" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 28 00:52:45.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1116" for this suite. 01/28/23 00:52:45.66
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":80,"skipped":1571,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.193 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:52:39.483
    Jan 28 00:52:39.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename var-expansion 01/28/23 00:52:39.485
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:39.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:39.551
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Jan 28 00:52:39.576: INFO: Waiting up to 2m0s for pod "var-expansion-4c5c2410-7668-4055-83fd-cd9a33d852d3" in namespace "var-expansion-1116" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 28 00:52:39.582: INFO: Pod "var-expansion-4c5c2410-7668-4055-83fd-cd9a33d852d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.356432ms
    Jan 28 00:52:41.596: INFO: Pod "var-expansion-4c5c2410-7668-4055-83fd-cd9a33d852d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019960544s
    Jan 28 00:52:43.594: INFO: Pod "var-expansion-4c5c2410-7668-4055-83fd-cd9a33d852d3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018668843s
    Jan 28 00:52:43.595: INFO: Pod "var-expansion-4c5c2410-7668-4055-83fd-cd9a33d852d3" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 28 00:52:43.595: INFO: Deleting pod "var-expansion-4c5c2410-7668-4055-83fd-cd9a33d852d3" in namespace "var-expansion-1116"
    Jan 28 00:52:43.621: INFO: Wait up to 5m0s for pod "var-expansion-4c5c2410-7668-4055-83fd-cd9a33d852d3" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 28 00:52:45.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1116" for this suite. 01/28/23 00:52:45.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:52:45.687
Jan 28 00:52:45.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename runtimeclass 01/28/23 00:52:45.69
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:45.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:45.751
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan 28 00:52:45.795: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2582 to be scheduled
Jan 28 00:52:45.811: INFO: 1 pods are not scheduled: [runtimeclass-2582/test-runtimeclass-runtimeclass-2582-preconfigured-handler-x7pw5(d1c10181-9cb4-40fa-b4d6-d2299b55a51e)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 28 00:52:47.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2582" for this suite. 01/28/23 00:52:47.866
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":81,"skipped":1585,"failed":0}
------------------------------
â€¢ [2.195 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:52:45.687
    Jan 28 00:52:45.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename runtimeclass 01/28/23 00:52:45.69
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:45.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:45.751
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan 28 00:52:45.795: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2582 to be scheduled
    Jan 28 00:52:45.811: INFO: 1 pods are not scheduled: [runtimeclass-2582/test-runtimeclass-runtimeclass-2582-preconfigured-handler-x7pw5(d1c10181-9cb4-40fa-b4d6-d2299b55a51e)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 28 00:52:47.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2582" for this suite. 01/28/23 00:52:47.866
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:52:47.885
Jan 28 00:52:47.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename runtimeclass 01/28/23 00:52:47.887
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:47.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:47.939
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-2068-delete-me 01/28/23 00:52:47.965
STEP: Waiting for the RuntimeClass to disappear 01/28/23 00:52:47.989
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 28 00:52:48.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2068" for this suite. 01/28/23 00:52:48.03
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":82,"skipped":1587,"failed":0}
------------------------------
â€¢ [0.159 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:52:47.885
    Jan 28 00:52:47.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename runtimeclass 01/28/23 00:52:47.887
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:47.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:47.939
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-2068-delete-me 01/28/23 00:52:47.965
    STEP: Waiting for the RuntimeClass to disappear 01/28/23 00:52:47.989
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 28 00:52:48.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2068" for this suite. 01/28/23 00:52:48.03
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:52:48.054
Jan 28 00:52:48.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename pods 01/28/23 00:52:48.056
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:48.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:48.102
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 01/28/23 00:52:48.112
Jan 28 00:52:48.127: INFO: Waiting up to 5m0s for pod "pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507" in namespace "pods-8287" to be "running and ready"
Jan 28 00:52:48.135: INFO: Pod "pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507": Phase="Pending", Reason="", readiness=false. Elapsed: 7.657341ms
Jan 28 00:52:48.135: INFO: The phase of Pod pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:52:50.148: INFO: Pod "pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020793592s
Jan 28 00:52:50.148: INFO: The phase of Pod pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:52:52.146: INFO: Pod "pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507": Phase="Running", Reason="", readiness=true. Elapsed: 4.018125117s
Jan 28 00:52:52.146: INFO: The phase of Pod pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507 is Running (Ready = true)
Jan 28 00:52:52.146: INFO: Pod "pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507" satisfied condition "running and ready"
Jan 28 00:52:52.162: INFO: Pod pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507 has hostIP: 10.9.20.126
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 28 00:52:52.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8287" for this suite. 01/28/23 00:52:52.175
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":83,"skipped":1596,"failed":0}
------------------------------
â€¢ [4.142 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:52:48.054
    Jan 28 00:52:48.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename pods 01/28/23 00:52:48.056
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:48.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:48.102
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 01/28/23 00:52:48.112
    Jan 28 00:52:48.127: INFO: Waiting up to 5m0s for pod "pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507" in namespace "pods-8287" to be "running and ready"
    Jan 28 00:52:48.135: INFO: Pod "pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507": Phase="Pending", Reason="", readiness=false. Elapsed: 7.657341ms
    Jan 28 00:52:48.135: INFO: The phase of Pod pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:52:50.148: INFO: Pod "pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020793592s
    Jan 28 00:52:50.148: INFO: The phase of Pod pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:52:52.146: INFO: Pod "pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507": Phase="Running", Reason="", readiness=true. Elapsed: 4.018125117s
    Jan 28 00:52:52.146: INFO: The phase of Pod pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507 is Running (Ready = true)
    Jan 28 00:52:52.146: INFO: Pod "pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507" satisfied condition "running and ready"
    Jan 28 00:52:52.162: INFO: Pod pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507 has hostIP: 10.9.20.126
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 28 00:52:52.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8287" for this suite. 01/28/23 00:52:52.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:52:52.2
Jan 28 00:52:52.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename replicaset 01/28/23 00:52:52.202
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:52.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:52.256
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/28/23 00:52:52.286
STEP: Verify that the required pods have come up. 01/28/23 00:52:52.307
Jan 28 00:52:52.319: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 28 00:52:57.331: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/28/23 00:52:57.331
STEP: Getting /status 01/28/23 00:52:57.332
Jan 28 00:52:57.368: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/28/23 00:52:57.368
Jan 28 00:52:57.401: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/28/23 00:52:57.401
Jan 28 00:52:57.408: INFO: Observed &ReplicaSet event: ADDED
Jan 28 00:52:57.409: INFO: Observed &ReplicaSet event: MODIFIED
Jan 28 00:52:57.410: INFO: Observed &ReplicaSet event: MODIFIED
Jan 28 00:52:57.410: INFO: Observed &ReplicaSet event: MODIFIED
Jan 28 00:52:57.411: INFO: Found replicaset test-rs in namespace replicaset-2451 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 28 00:52:57.411: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/28/23 00:52:57.411
Jan 28 00:52:57.412: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 28 00:52:57.431: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/28/23 00:52:57.431
Jan 28 00:52:57.439: INFO: Observed &ReplicaSet event: ADDED
Jan 28 00:52:57.439: INFO: Observed &ReplicaSet event: MODIFIED
Jan 28 00:52:57.439: INFO: Observed &ReplicaSet event: MODIFIED
Jan 28 00:52:57.440: INFO: Observed &ReplicaSet event: MODIFIED
Jan 28 00:52:57.440: INFO: Observed replicaset test-rs in namespace replicaset-2451 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 28 00:52:57.440: INFO: Observed &ReplicaSet event: MODIFIED
Jan 28 00:52:57.441: INFO: Found replicaset test-rs in namespace replicaset-2451 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 28 00:52:57.441: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 28 00:52:57.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2451" for this suite. 01/28/23 00:52:57.457
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":84,"skipped":1609,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.272 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:52:52.2
    Jan 28 00:52:52.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename replicaset 01/28/23 00:52:52.202
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:52.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:52.256
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/28/23 00:52:52.286
    STEP: Verify that the required pods have come up. 01/28/23 00:52:52.307
    Jan 28 00:52:52.319: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 28 00:52:57.331: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/28/23 00:52:57.331
    STEP: Getting /status 01/28/23 00:52:57.332
    Jan 28 00:52:57.368: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/28/23 00:52:57.368
    Jan 28 00:52:57.401: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/28/23 00:52:57.401
    Jan 28 00:52:57.408: INFO: Observed &ReplicaSet event: ADDED
    Jan 28 00:52:57.409: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 28 00:52:57.410: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 28 00:52:57.410: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 28 00:52:57.411: INFO: Found replicaset test-rs in namespace replicaset-2451 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 28 00:52:57.411: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/28/23 00:52:57.411
    Jan 28 00:52:57.412: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 28 00:52:57.431: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/28/23 00:52:57.431
    Jan 28 00:52:57.439: INFO: Observed &ReplicaSet event: ADDED
    Jan 28 00:52:57.439: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 28 00:52:57.439: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 28 00:52:57.440: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 28 00:52:57.440: INFO: Observed replicaset test-rs in namespace replicaset-2451 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 28 00:52:57.440: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 28 00:52:57.441: INFO: Found replicaset test-rs in namespace replicaset-2451 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan 28 00:52:57.441: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 28 00:52:57.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2451" for this suite. 01/28/23 00:52:57.457
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:52:57.479
Jan 28 00:52:57.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename sched-pred 01/28/23 00:52:57.481
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:57.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:57.538
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 28 00:52:57.548: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 28 00:52:57.574: INFO: Waiting for terminating namespaces to be deleted...
Jan 28 00:52:57.586: INFO: 
Logging pods the apiserver thinks is on node 10.9.20.126 before test
Jan 28 00:52:57.612: INFO: calico-node-xl9f8 from kube-system started at 2023-01-27 22:05:01 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.612: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 00:52:57.612: INFO: calico-typha-677688fdc5-twxlr from kube-system started at 2023-01-27 22:05:32 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.612: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 00:52:57.612: INFO: coredns-6754846f95-9ck4t from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.612: INFO: 	Container coredns ready: true, restart count 0
Jan 28 00:52:57.612: INFO: ibm-keepalived-watcher-b94td from kube-system started at 2023-01-27 22:05:01 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.612: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 00:52:57.612: INFO: ibm-master-proxy-static-10.9.20.126 from kube-system started at 2023-01-27 22:04:49 +0000 UTC (2 container statuses recorded)
Jan 28 00:52:57.612: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 00:52:57.612: INFO: 	Container pause ready: true, restart count 0
Jan 28 00:52:57.612: INFO: ibmcloud-block-storage-driver-lw66t from kube-system started at 2023-01-27 22:05:09 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.612: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 00:52:57.612: INFO: konnectivity-agent-65zgm from kube-system started at 2023-01-27 22:15:24 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.612: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 00:52:57.612: INFO: metrics-server-9cdf87dc6-4ptjv from kube-system started at 2023-01-27 22:51:34 +0000 UTC (3 container statuses recorded)
Jan 28 00:52:57.612: INFO: 	Container config-watcher ready: true, restart count 0
Jan 28 00:52:57.613: INFO: 	Container metrics-server ready: true, restart count 0
Jan 28 00:52:57.613: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 28 00:52:57.613: INFO: public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-vvqqx from kube-system started at 2023-01-27 22:09:22 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.613: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 28 00:52:57.613: INFO: pod-qos-class-3fed2621-90be-4f81-aa14-74fe009601ea from pods-5376 started at 2023-01-28 00:52:39 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.613: INFO: 	Container agnhost ready: false, restart count 0
Jan 28 00:52:57.613: INFO: pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507 from pods-8287 started at 2023-01-28 00:52:48 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.613: INFO: 	Container test ready: true, restart count 0
Jan 28 00:52:57.613: INFO: sonobuoy from sonobuoy started at 2023-01-28 00:13:47 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.613: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 28 00:52:57.613: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-qbzm7 from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
Jan 28 00:52:57.613: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:52:57.613: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 28 00:52:57.613: INFO: 
Logging pods the apiserver thinks is on node 10.9.20.72 before test
Jan 28 00:52:57.638: INFO: ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-2gg2p from ibm-system started at 2023-01-27 22:09:35 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.638: INFO: 	Container ibm-cloud-provider-ip-163-69-70-149 ready: true, restart count 0
Jan 28 00:52:57.638: INFO: calico-node-4bc72 from kube-system started at 2023-01-27 22:04:57 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.638: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 00:52:57.638: INFO: calico-typha-677688fdc5-6nr4v from kube-system started at 2023-01-27 22:05:47 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.638: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 00:52:57.638: INFO: coredns-6754846f95-b686l from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.638: INFO: 	Container coredns ready: true, restart count 0
Jan 28 00:52:57.638: INFO: ibm-keepalived-watcher-gstl5 from kube-system started at 2023-01-27 22:04:57 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.638: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 00:52:57.638: INFO: ibm-master-proxy-static-10.9.20.72 from kube-system started at 2023-01-27 22:04:44 +0000 UTC (2 container statuses recorded)
Jan 28 00:52:57.638: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 00:52:57.638: INFO: 	Container pause ready: true, restart count 0
Jan 28 00:52:57.638: INFO: ibmcloud-block-storage-driver-sktn2 from kube-system started at 2023-01-27 22:05:04 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.638: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 00:52:57.638: INFO: ingress-cluster-healthcheck-655c49644b-dmz5q from kube-system started at 2023-01-27 22:06:59 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.638: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Jan 28 00:52:57.638: INFO: konnectivity-agent-zbbtb from kube-system started at 2023-01-27 22:15:21 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.638: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 00:52:57.638: INFO: metrics-server-9cdf87dc6-n44dc from kube-system started at 2023-01-27 22:51:34 +0000 UTC (3 container statuses recorded)
Jan 28 00:52:57.638: INFO: 	Container config-watcher ready: true, restart count 0
Jan 28 00:52:57.638: INFO: 	Container metrics-server ready: true, restart count 0
Jan 28 00:52:57.639: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 28 00:52:57.639: INFO: public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-ztg7f from kube-system started at 2023-01-27 22:09:22 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.639: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 28 00:52:57.639: INFO: test-rs-hgpts from replicaset-2451 started at 2023-01-28 00:52:52 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.639: INFO: 	Container httpd ready: true, restart count 0
Jan 28 00:52:57.639: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-tm2wf from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
Jan 28 00:52:57.639: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:52:57.639: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 28 00:52:57.639: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-01-27 22:07:40 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.639: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jan 28 00:52:57.639: INFO: 
Logging pods the apiserver thinks is on node 10.9.20.75 before test
Jan 28 00:52:57.664: INFO: ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-9qs6m from ibm-system started at 2023-01-27 22:09:35 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container ibm-cloud-provider-ip-163-69-70-149 ready: true, restart count 0
Jan 28 00:52:57.665: INFO: calico-kube-controllers-5754dfd4dd-2fl6z from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 28 00:52:57.665: INFO: calico-node-6pnwh from kube-system started at 2023-01-27 22:04:45 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 00:52:57.665: INFO: calico-typha-677688fdc5-s2khq from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 00:52:57.665: INFO: coredns-6754846f95-r75xk from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container coredns ready: true, restart count 0
Jan 28 00:52:57.665: INFO: coredns-autoscaler-669cf746f6-b9s85 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container autoscaler ready: true, restart count 0
Jan 28 00:52:57.665: INFO: dashboard-metrics-scraper-c964d5594-76f8h from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 28 00:52:57.665: INFO: ibm-file-plugin-7dd6c48b68-kn5ff from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jan 28 00:52:57.665: INFO: ibm-keepalived-watcher-gj6b6 from kube-system started at 2023-01-27 22:04:45 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 00:52:57.665: INFO: ibm-master-proxy-static-10.9.20.75 from kube-system started at 2023-01-27 22:04:42 +0000 UTC (2 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 00:52:57.665: INFO: 	Container pause ready: true, restart count 0
Jan 28 00:52:57.665: INFO: ibm-storage-watcher-746995c8c9-7mmhc from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jan 28 00:52:57.665: INFO: ibmcloud-block-storage-driver-jgsbr from kube-system started at 2023-01-27 22:04:51 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 00:52:57.665: INFO: ibmcloud-block-storage-plugin-697cd846b-rglpr from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jan 28 00:52:57.665: INFO: konnectivity-agent-95p2n from kube-system started at 2023-01-27 22:15:28 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 00:52:57.665: INFO: kubernetes-dashboard-55c4d56798-7r7j7 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 28 00:52:57.665: INFO: snapshot-controller-c5c6dddff-25l94 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 28 00:52:57.665: INFO: snapshot-controller-c5c6dddff-5xzjc from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 28 00:52:57.665: INFO: snapshot-controller-c5c6dddff-pzr2k from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 28 00:52:57.665: INFO: sonobuoy-e2e-job-70551213e7cc4fe2 from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container e2e ready: true, restart count 0
Jan 28 00:52:57.665: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:52:57.665: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-jcljc from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
Jan 28 00:52:57.665: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:52:57.665: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/28/23 00:52:57.665
Jan 28 00:52:57.687: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7324" to be "running"
Jan 28 00:52:57.698: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.576583ms
Jan 28 00:52:59.718: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0306904s
Jan 28 00:53:01.718: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.031270015s
Jan 28 00:53:01.718: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/28/23 00:53:01.73
STEP: Trying to apply a random label on the found node. 01/28/23 00:53:01.789
STEP: verifying the node has the label kubernetes.io/e2e-de145e80-4a03-4280-b257-de5ba7474459 42 01/28/23 00:53:01.813
STEP: Trying to relaunch the pod, now with labels. 01/28/23 00:53:01.828
Jan 28 00:53:01.840: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7324" to be "not pending"
Jan 28 00:53:01.857: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 17.614821ms
Jan 28 00:53:03.869: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.029573804s
Jan 28 00:53:03.869: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-de145e80-4a03-4280-b257-de5ba7474459 off the node 10.9.20.72 01/28/23 00:53:03.88
STEP: verifying the node doesn't have the label kubernetes.io/e2e-de145e80-4a03-4280-b257-de5ba7474459 01/28/23 00:53:03.911
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 28 00:53:03.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7324" for this suite. 01/28/23 00:53:03.941
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":85,"skipped":1665,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.482 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:52:57.479
    Jan 28 00:52:57.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename sched-pred 01/28/23 00:52:57.481
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:52:57.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:52:57.538
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 28 00:52:57.548: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 28 00:52:57.574: INFO: Waiting for terminating namespaces to be deleted...
    Jan 28 00:52:57.586: INFO: 
    Logging pods the apiserver thinks is on node 10.9.20.126 before test
    Jan 28 00:52:57.612: INFO: calico-node-xl9f8 from kube-system started at 2023-01-27 22:05:01 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.612: INFO: 	Container calico-node ready: true, restart count 0
    Jan 28 00:52:57.612: INFO: calico-typha-677688fdc5-twxlr from kube-system started at 2023-01-27 22:05:32 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.612: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 28 00:52:57.612: INFO: coredns-6754846f95-9ck4t from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.612: INFO: 	Container coredns ready: true, restart count 0
    Jan 28 00:52:57.612: INFO: ibm-keepalived-watcher-b94td from kube-system started at 2023-01-27 22:05:01 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.612: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 28 00:52:57.612: INFO: ibm-master-proxy-static-10.9.20.126 from kube-system started at 2023-01-27 22:04:49 +0000 UTC (2 container statuses recorded)
    Jan 28 00:52:57.612: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 28 00:52:57.612: INFO: 	Container pause ready: true, restart count 0
    Jan 28 00:52:57.612: INFO: ibmcloud-block-storage-driver-lw66t from kube-system started at 2023-01-27 22:05:09 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.612: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 28 00:52:57.612: INFO: konnectivity-agent-65zgm from kube-system started at 2023-01-27 22:15:24 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.612: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 28 00:52:57.612: INFO: metrics-server-9cdf87dc6-4ptjv from kube-system started at 2023-01-27 22:51:34 +0000 UTC (3 container statuses recorded)
    Jan 28 00:52:57.612: INFO: 	Container config-watcher ready: true, restart count 0
    Jan 28 00:52:57.613: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 28 00:52:57.613: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan 28 00:52:57.613: INFO: public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-vvqqx from kube-system started at 2023-01-27 22:09:22 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.613: INFO: 	Container nginx-ingress ready: true, restart count 0
    Jan 28 00:52:57.613: INFO: pod-qos-class-3fed2621-90be-4f81-aa14-74fe009601ea from pods-5376 started at 2023-01-28 00:52:39 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.613: INFO: 	Container agnhost ready: false, restart count 0
    Jan 28 00:52:57.613: INFO: pod-hostip-6e2aea60-4ddb-45a2-8e70-dc42a0c4d507 from pods-8287 started at 2023-01-28 00:52:48 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.613: INFO: 	Container test ready: true, restart count 0
    Jan 28 00:52:57.613: INFO: sonobuoy from sonobuoy started at 2023-01-28 00:13:47 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.613: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 28 00:52:57.613: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-qbzm7 from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
    Jan 28 00:52:57.613: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 28 00:52:57.613: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 28 00:52:57.613: INFO: 
    Logging pods the apiserver thinks is on node 10.9.20.72 before test
    Jan 28 00:52:57.638: INFO: ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-2gg2p from ibm-system started at 2023-01-27 22:09:35 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.638: INFO: 	Container ibm-cloud-provider-ip-163-69-70-149 ready: true, restart count 0
    Jan 28 00:52:57.638: INFO: calico-node-4bc72 from kube-system started at 2023-01-27 22:04:57 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.638: INFO: 	Container calico-node ready: true, restart count 0
    Jan 28 00:52:57.638: INFO: calico-typha-677688fdc5-6nr4v from kube-system started at 2023-01-27 22:05:47 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.638: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 28 00:52:57.638: INFO: coredns-6754846f95-b686l from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.638: INFO: 	Container coredns ready: true, restart count 0
    Jan 28 00:52:57.638: INFO: ibm-keepalived-watcher-gstl5 from kube-system started at 2023-01-27 22:04:57 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.638: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 28 00:52:57.638: INFO: ibm-master-proxy-static-10.9.20.72 from kube-system started at 2023-01-27 22:04:44 +0000 UTC (2 container statuses recorded)
    Jan 28 00:52:57.638: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 28 00:52:57.638: INFO: 	Container pause ready: true, restart count 0
    Jan 28 00:52:57.638: INFO: ibmcloud-block-storage-driver-sktn2 from kube-system started at 2023-01-27 22:05:04 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.638: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 28 00:52:57.638: INFO: ingress-cluster-healthcheck-655c49644b-dmz5q from kube-system started at 2023-01-27 22:06:59 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.638: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
    Jan 28 00:52:57.638: INFO: konnectivity-agent-zbbtb from kube-system started at 2023-01-27 22:15:21 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.638: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 28 00:52:57.638: INFO: metrics-server-9cdf87dc6-n44dc from kube-system started at 2023-01-27 22:51:34 +0000 UTC (3 container statuses recorded)
    Jan 28 00:52:57.638: INFO: 	Container config-watcher ready: true, restart count 0
    Jan 28 00:52:57.638: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 28 00:52:57.639: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan 28 00:52:57.639: INFO: public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-ztg7f from kube-system started at 2023-01-27 22:09:22 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.639: INFO: 	Container nginx-ingress ready: true, restart count 0
    Jan 28 00:52:57.639: INFO: test-rs-hgpts from replicaset-2451 started at 2023-01-28 00:52:52 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.639: INFO: 	Container httpd ready: true, restart count 0
    Jan 28 00:52:57.639: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-tm2wf from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
    Jan 28 00:52:57.639: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 28 00:52:57.639: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 28 00:52:57.639: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-01-27 22:07:40 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.639: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    Jan 28 00:52:57.639: INFO: 
    Logging pods the apiserver thinks is on node 10.9.20.75 before test
    Jan 28 00:52:57.664: INFO: ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-9qs6m from ibm-system started at 2023-01-27 22:09:35 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container ibm-cloud-provider-ip-163-69-70-149 ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: calico-kube-controllers-5754dfd4dd-2fl6z from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: calico-node-6pnwh from kube-system started at 2023-01-27 22:04:45 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container calico-node ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: calico-typha-677688fdc5-s2khq from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: coredns-6754846f95-r75xk from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container coredns ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: coredns-autoscaler-669cf746f6-b9s85 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: dashboard-metrics-scraper-c964d5594-76f8h from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: ibm-file-plugin-7dd6c48b68-kn5ff from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: ibm-keepalived-watcher-gj6b6 from kube-system started at 2023-01-27 22:04:45 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: ibm-master-proxy-static-10.9.20.75 from kube-system started at 2023-01-27 22:04:42 +0000 UTC (2 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: 	Container pause ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: ibm-storage-watcher-746995c8c9-7mmhc from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: ibmcloud-block-storage-driver-jgsbr from kube-system started at 2023-01-27 22:04:51 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: ibmcloud-block-storage-plugin-697cd846b-rglpr from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: konnectivity-agent-95p2n from kube-system started at 2023-01-27 22:15:28 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: kubernetes-dashboard-55c4d56798-7r7j7 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: snapshot-controller-c5c6dddff-25l94 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: snapshot-controller-c5c6dddff-5xzjc from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: snapshot-controller-c5c6dddff-pzr2k from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: sonobuoy-e2e-job-70551213e7cc4fe2 from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container e2e ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-jcljc from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
    Jan 28 00:52:57.665: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 28 00:52:57.665: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/28/23 00:52:57.665
    Jan 28 00:52:57.687: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7324" to be "running"
    Jan 28 00:52:57.698: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.576583ms
    Jan 28 00:52:59.718: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0306904s
    Jan 28 00:53:01.718: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.031270015s
    Jan 28 00:53:01.718: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/28/23 00:53:01.73
    STEP: Trying to apply a random label on the found node. 01/28/23 00:53:01.789
    STEP: verifying the node has the label kubernetes.io/e2e-de145e80-4a03-4280-b257-de5ba7474459 42 01/28/23 00:53:01.813
    STEP: Trying to relaunch the pod, now with labels. 01/28/23 00:53:01.828
    Jan 28 00:53:01.840: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7324" to be "not pending"
    Jan 28 00:53:01.857: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 17.614821ms
    Jan 28 00:53:03.869: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.029573804s
    Jan 28 00:53:03.869: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-de145e80-4a03-4280-b257-de5ba7474459 off the node 10.9.20.72 01/28/23 00:53:03.88
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-de145e80-4a03-4280-b257-de5ba7474459 01/28/23 00:53:03.911
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 00:53:03.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-7324" for this suite. 01/28/23 00:53:03.941
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:53:03.965
Jan 28 00:53:03.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename disruption 01/28/23 00:53:03.967
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:53:04.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:53:04.023
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 01/28/23 00:53:04.048
STEP: Waiting for all pods to be running 01/28/23 00:53:04.115
Jan 28 00:53:04.125: INFO: running pods: 0 < 3
Jan 28 00:53:06.139: INFO: running pods: 1 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 28 00:53:08.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3741" for this suite. 01/28/23 00:53:08.161
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":86,"skipped":1687,"failed":0}
------------------------------
â€¢ [4.213 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:53:03.965
    Jan 28 00:53:03.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename disruption 01/28/23 00:53:03.967
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:53:04.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:53:04.023
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 01/28/23 00:53:04.048
    STEP: Waiting for all pods to be running 01/28/23 00:53:04.115
    Jan 28 00:53:04.125: INFO: running pods: 0 < 3
    Jan 28 00:53:06.139: INFO: running pods: 1 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 28 00:53:08.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-3741" for this suite. 01/28/23 00:53:08.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:53:08.203
Jan 28 00:53:08.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename configmap 01/28/23 00:53:08.204
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:53:08.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:53:08.265
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 28 00:53:08.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7519" for this suite. 01/28/23 00:53:08.418
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":87,"skipped":1785,"failed":0}
------------------------------
â€¢ [0.235 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:53:08.203
    Jan 28 00:53:08.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename configmap 01/28/23 00:53:08.204
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:53:08.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:53:08.265
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 28 00:53:08.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7519" for this suite. 01/28/23 00:53:08.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:53:08.448
Jan 28 00:53:08.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename services 01/28/23 00:53:08.45
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:53:08.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:53:08.497
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 01/28/23 00:53:08.52
STEP: waiting for available Endpoint 01/28/23 00:53:08.537
STEP: listing all Endpoints 01/28/23 00:53:08.541
STEP: updating the Endpoint 01/28/23 00:53:08.555
STEP: fetching the Endpoint 01/28/23 00:53:08.574
STEP: patching the Endpoint 01/28/23 00:53:08.585
STEP: fetching the Endpoint 01/28/23 00:53:08.636
STEP: deleting the Endpoint by Collection 01/28/23 00:53:08.651
STEP: waiting for Endpoint deletion 01/28/23 00:53:08.687
STEP: fetching the Endpoint 01/28/23 00:53:08.693
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 28 00:53:08.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6602" for this suite. 01/28/23 00:53:08.722
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":88,"skipped":1801,"failed":0}
------------------------------
â€¢ [0.299 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:53:08.448
    Jan 28 00:53:08.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename services 01/28/23 00:53:08.45
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:53:08.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:53:08.497
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 01/28/23 00:53:08.52
    STEP: waiting for available Endpoint 01/28/23 00:53:08.537
    STEP: listing all Endpoints 01/28/23 00:53:08.541
    STEP: updating the Endpoint 01/28/23 00:53:08.555
    STEP: fetching the Endpoint 01/28/23 00:53:08.574
    STEP: patching the Endpoint 01/28/23 00:53:08.585
    STEP: fetching the Endpoint 01/28/23 00:53:08.636
    STEP: deleting the Endpoint by Collection 01/28/23 00:53:08.651
    STEP: waiting for Endpoint deletion 01/28/23 00:53:08.687
    STEP: fetching the Endpoint 01/28/23 00:53:08.693
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 28 00:53:08.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6602" for this suite. 01/28/23 00:53:08.722
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:53:08.757
Jan 28 00:53:08.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 00:53:08.759
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:53:08.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:53:08.813
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 01/28/23 00:53:08.822
Jan 28 00:53:08.837: INFO: Waiting up to 5m0s for pod "downwardapi-volume-802ddc86-9c6d-47e1-b43e-8e7d15fdce31" in namespace "projected-2076" to be "Succeeded or Failed"
Jan 28 00:53:08.845: INFO: Pod "downwardapi-volume-802ddc86-9c6d-47e1-b43e-8e7d15fdce31": Phase="Pending", Reason="", readiness=false. Elapsed: 8.065295ms
Jan 28 00:53:10.859: INFO: Pod "downwardapi-volume-802ddc86-9c6d-47e1-b43e-8e7d15fdce31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022408497s
Jan 28 00:53:12.855: INFO: Pod "downwardapi-volume-802ddc86-9c6d-47e1-b43e-8e7d15fdce31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018445178s
STEP: Saw pod success 01/28/23 00:53:12.856
Jan 28 00:53:12.856: INFO: Pod "downwardapi-volume-802ddc86-9c6d-47e1-b43e-8e7d15fdce31" satisfied condition "Succeeded or Failed"
Jan 28 00:53:12.865: INFO: Trying to get logs from node 10.9.20.75 pod downwardapi-volume-802ddc86-9c6d-47e1-b43e-8e7d15fdce31 container client-container: <nil>
STEP: delete the pod 01/28/23 00:53:12.933
Jan 28 00:53:12.961: INFO: Waiting for pod downwardapi-volume-802ddc86-9c6d-47e1-b43e-8e7d15fdce31 to disappear
Jan 28 00:53:12.972: INFO: Pod downwardapi-volume-802ddc86-9c6d-47e1-b43e-8e7d15fdce31 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 28 00:53:12.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2076" for this suite. 01/28/23 00:53:12.988
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":89,"skipped":1801,"failed":0}
------------------------------
â€¢ [4.254 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:53:08.757
    Jan 28 00:53:08.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 00:53:08.759
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:53:08.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:53:08.813
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 01/28/23 00:53:08.822
    Jan 28 00:53:08.837: INFO: Waiting up to 5m0s for pod "downwardapi-volume-802ddc86-9c6d-47e1-b43e-8e7d15fdce31" in namespace "projected-2076" to be "Succeeded or Failed"
    Jan 28 00:53:08.845: INFO: Pod "downwardapi-volume-802ddc86-9c6d-47e1-b43e-8e7d15fdce31": Phase="Pending", Reason="", readiness=false. Elapsed: 8.065295ms
    Jan 28 00:53:10.859: INFO: Pod "downwardapi-volume-802ddc86-9c6d-47e1-b43e-8e7d15fdce31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022408497s
    Jan 28 00:53:12.855: INFO: Pod "downwardapi-volume-802ddc86-9c6d-47e1-b43e-8e7d15fdce31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018445178s
    STEP: Saw pod success 01/28/23 00:53:12.856
    Jan 28 00:53:12.856: INFO: Pod "downwardapi-volume-802ddc86-9c6d-47e1-b43e-8e7d15fdce31" satisfied condition "Succeeded or Failed"
    Jan 28 00:53:12.865: INFO: Trying to get logs from node 10.9.20.75 pod downwardapi-volume-802ddc86-9c6d-47e1-b43e-8e7d15fdce31 container client-container: <nil>
    STEP: delete the pod 01/28/23 00:53:12.933
    Jan 28 00:53:12.961: INFO: Waiting for pod downwardapi-volume-802ddc86-9c6d-47e1-b43e-8e7d15fdce31 to disappear
    Jan 28 00:53:12.972: INFO: Pod downwardapi-volume-802ddc86-9c6d-47e1-b43e-8e7d15fdce31 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 28 00:53:12.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2076" for this suite. 01/28/23 00:53:12.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:53:13.017
Jan 28 00:53:13.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename emptydir 01/28/23 00:53:13.019
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:53:13.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:53:13.071
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 01/28/23 00:53:13.082
Jan 28 00:53:13.105: INFO: Waiting up to 5m0s for pod "pod-1d896939-cd36-4ccd-b7a2-332b60d89ad2" in namespace "emptydir-13" to be "Succeeded or Failed"
Jan 28 00:53:13.118: INFO: Pod "pod-1d896939-cd36-4ccd-b7a2-332b60d89ad2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.796959ms
Jan 28 00:53:15.129: INFO: Pod "pod-1d896939-cd36-4ccd-b7a2-332b60d89ad2": Phase="Running", Reason="", readiness=true. Elapsed: 2.023915626s
Jan 28 00:53:17.133: INFO: Pod "pod-1d896939-cd36-4ccd-b7a2-332b60d89ad2": Phase="Running", Reason="", readiness=false. Elapsed: 4.027475917s
Jan 28 00:53:19.130: INFO: Pod "pod-1d896939-cd36-4ccd-b7a2-332b60d89ad2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02487045s
STEP: Saw pod success 01/28/23 00:53:19.13
Jan 28 00:53:19.130: INFO: Pod "pod-1d896939-cd36-4ccd-b7a2-332b60d89ad2" satisfied condition "Succeeded or Failed"
Jan 28 00:53:19.141: INFO: Trying to get logs from node 10.9.20.72 pod pod-1d896939-cd36-4ccd-b7a2-332b60d89ad2 container test-container: <nil>
STEP: delete the pod 01/28/23 00:53:19.216
Jan 28 00:53:19.238: INFO: Waiting for pod pod-1d896939-cd36-4ccd-b7a2-332b60d89ad2 to disappear
Jan 28 00:53:19.248: INFO: Pod pod-1d896939-cd36-4ccd-b7a2-332b60d89ad2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 28 00:53:19.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-13" for this suite. 01/28/23 00:53:19.264
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":90,"skipped":1822,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.261 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:53:13.017
    Jan 28 00:53:13.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename emptydir 01/28/23 00:53:13.019
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:53:13.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:53:13.071
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/28/23 00:53:13.082
    Jan 28 00:53:13.105: INFO: Waiting up to 5m0s for pod "pod-1d896939-cd36-4ccd-b7a2-332b60d89ad2" in namespace "emptydir-13" to be "Succeeded or Failed"
    Jan 28 00:53:13.118: INFO: Pod "pod-1d896939-cd36-4ccd-b7a2-332b60d89ad2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.796959ms
    Jan 28 00:53:15.129: INFO: Pod "pod-1d896939-cd36-4ccd-b7a2-332b60d89ad2": Phase="Running", Reason="", readiness=true. Elapsed: 2.023915626s
    Jan 28 00:53:17.133: INFO: Pod "pod-1d896939-cd36-4ccd-b7a2-332b60d89ad2": Phase="Running", Reason="", readiness=false. Elapsed: 4.027475917s
    Jan 28 00:53:19.130: INFO: Pod "pod-1d896939-cd36-4ccd-b7a2-332b60d89ad2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02487045s
    STEP: Saw pod success 01/28/23 00:53:19.13
    Jan 28 00:53:19.130: INFO: Pod "pod-1d896939-cd36-4ccd-b7a2-332b60d89ad2" satisfied condition "Succeeded or Failed"
    Jan 28 00:53:19.141: INFO: Trying to get logs from node 10.9.20.72 pod pod-1d896939-cd36-4ccd-b7a2-332b60d89ad2 container test-container: <nil>
    STEP: delete the pod 01/28/23 00:53:19.216
    Jan 28 00:53:19.238: INFO: Waiting for pod pod-1d896939-cd36-4ccd-b7a2-332b60d89ad2 to disappear
    Jan 28 00:53:19.248: INFO: Pod pod-1d896939-cd36-4ccd-b7a2-332b60d89ad2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 28 00:53:19.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-13" for this suite. 01/28/23 00:53:19.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:53:19.289
Jan 28 00:53:19.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename crd-publish-openapi 01/28/23 00:53:19.291
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:53:19.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:53:19.345
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 01/28/23 00:53:19.354
Jan 28 00:53:19.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: mark a version not serverd 01/28/23 00:53:28.196
STEP: check the unserved version gets removed 01/28/23 00:53:28.231
STEP: check the other version is not changed 01/28/23 00:53:31.615
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 00:53:37.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9218" for this suite. 01/28/23 00:53:37.556
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":91,"skipped":1832,"failed":0}
------------------------------
â€¢ [SLOW TEST] [18.285 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:53:19.289
    Jan 28 00:53:19.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename crd-publish-openapi 01/28/23 00:53:19.291
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:53:19.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:53:19.345
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 01/28/23 00:53:19.354
    Jan 28 00:53:19.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: mark a version not serverd 01/28/23 00:53:28.196
    STEP: check the unserved version gets removed 01/28/23 00:53:28.231
    STEP: check the other version is not changed 01/28/23 00:53:31.615
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 00:53:37.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9218" for this suite. 01/28/23 00:53:37.556
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:53:37.575
Jan 28 00:53:37.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename var-expansion 01/28/23 00:53:37.579
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:53:37.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:53:37.615
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 01/28/23 00:53:37.624
Jan 28 00:53:37.645: INFO: Waiting up to 5m0s for pod "var-expansion-09f94b03-2c28-4f69-8b1d-d35d8050e4c2" in namespace "var-expansion-1223" to be "Succeeded or Failed"
Jan 28 00:53:37.657: INFO: Pod "var-expansion-09f94b03-2c28-4f69-8b1d-d35d8050e4c2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.157331ms
Jan 28 00:53:39.668: INFO: Pod "var-expansion-09f94b03-2c28-4f69-8b1d-d35d8050e4c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023191585s
Jan 28 00:53:41.672: INFO: Pod "var-expansion-09f94b03-2c28-4f69-8b1d-d35d8050e4c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02734755s
Jan 28 00:53:43.669: INFO: Pod "var-expansion-09f94b03-2c28-4f69-8b1d-d35d8050e4c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024157566s
STEP: Saw pod success 01/28/23 00:53:43.669
Jan 28 00:53:43.669: INFO: Pod "var-expansion-09f94b03-2c28-4f69-8b1d-d35d8050e4c2" satisfied condition "Succeeded or Failed"
Jan 28 00:53:43.679: INFO: Trying to get logs from node 10.9.20.72 pod var-expansion-09f94b03-2c28-4f69-8b1d-d35d8050e4c2 container dapi-container: <nil>
STEP: delete the pod 01/28/23 00:53:43.746
Jan 28 00:53:43.821: INFO: Waiting for pod var-expansion-09f94b03-2c28-4f69-8b1d-d35d8050e4c2 to disappear
Jan 28 00:53:43.831: INFO: Pod var-expansion-09f94b03-2c28-4f69-8b1d-d35d8050e4c2 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 28 00:53:43.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1223" for this suite. 01/28/23 00:53:43.854
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":92,"skipped":1833,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.294 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:53:37.575
    Jan 28 00:53:37.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename var-expansion 01/28/23 00:53:37.579
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:53:37.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:53:37.615
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 01/28/23 00:53:37.624
    Jan 28 00:53:37.645: INFO: Waiting up to 5m0s for pod "var-expansion-09f94b03-2c28-4f69-8b1d-d35d8050e4c2" in namespace "var-expansion-1223" to be "Succeeded or Failed"
    Jan 28 00:53:37.657: INFO: Pod "var-expansion-09f94b03-2c28-4f69-8b1d-d35d8050e4c2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.157331ms
    Jan 28 00:53:39.668: INFO: Pod "var-expansion-09f94b03-2c28-4f69-8b1d-d35d8050e4c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023191585s
    Jan 28 00:53:41.672: INFO: Pod "var-expansion-09f94b03-2c28-4f69-8b1d-d35d8050e4c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02734755s
    Jan 28 00:53:43.669: INFO: Pod "var-expansion-09f94b03-2c28-4f69-8b1d-d35d8050e4c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024157566s
    STEP: Saw pod success 01/28/23 00:53:43.669
    Jan 28 00:53:43.669: INFO: Pod "var-expansion-09f94b03-2c28-4f69-8b1d-d35d8050e4c2" satisfied condition "Succeeded or Failed"
    Jan 28 00:53:43.679: INFO: Trying to get logs from node 10.9.20.72 pod var-expansion-09f94b03-2c28-4f69-8b1d-d35d8050e4c2 container dapi-container: <nil>
    STEP: delete the pod 01/28/23 00:53:43.746
    Jan 28 00:53:43.821: INFO: Waiting for pod var-expansion-09f94b03-2c28-4f69-8b1d-d35d8050e4c2 to disappear
    Jan 28 00:53:43.831: INFO: Pod var-expansion-09f94b03-2c28-4f69-8b1d-d35d8050e4c2 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 28 00:53:43.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1223" for this suite. 01/28/23 00:53:43.854
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:53:43.871
Jan 28 00:53:43.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename container-probe 01/28/23 00:53:43.875
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:53:43.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:53:43.919
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Jan 28 00:53:43.947: INFO: Waiting up to 5m0s for pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3" in namespace "container-probe-479" to be "running and ready"
Jan 28 00:53:43.960: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.122923ms
Jan 28 00:53:43.960: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:53:45.970: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022729477s
Jan 28 00:53:45.970: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:53:47.971: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=false. Elapsed: 4.023999339s
Jan 28 00:53:47.971: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = false)
Jan 28 00:53:49.985: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=false. Elapsed: 6.03808201s
Jan 28 00:53:49.986: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = false)
Jan 28 00:53:51.972: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=false. Elapsed: 8.02497844s
Jan 28 00:53:51.972: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = false)
Jan 28 00:53:53.989: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=false. Elapsed: 10.042078563s
Jan 28 00:53:53.990: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = false)
Jan 28 00:53:55.971: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=false. Elapsed: 12.023320721s
Jan 28 00:53:55.971: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = false)
Jan 28 00:53:57.974: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=false. Elapsed: 14.026120953s
Jan 28 00:53:57.974: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = false)
Jan 28 00:53:59.971: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=false. Elapsed: 16.023222488s
Jan 28 00:53:59.971: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = false)
Jan 28 00:54:01.987: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=false. Elapsed: 18.040008599s
Jan 28 00:54:01.987: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = false)
Jan 28 00:54:03.978: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=false. Elapsed: 20.030354475s
Jan 28 00:54:03.978: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = false)
Jan 28 00:54:05.977: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=true. Elapsed: 22.029174505s
Jan 28 00:54:05.977: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = true)
Jan 28 00:54:05.977: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3" satisfied condition "running and ready"
Jan 28 00:54:06.015: INFO: Container started at 2023-01-28 00:53:45 +0000 UTC, pod became ready at 2023-01-28 00:54:04 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 28 00:54:06.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-479" for this suite. 01/28/23 00:54:06.035
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":93,"skipped":1841,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.182 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:53:43.871
    Jan 28 00:53:43.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename container-probe 01/28/23 00:53:43.875
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:53:43.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:53:43.919
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Jan 28 00:53:43.947: INFO: Waiting up to 5m0s for pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3" in namespace "container-probe-479" to be "running and ready"
    Jan 28 00:53:43.960: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.122923ms
    Jan 28 00:53:43.960: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:53:45.970: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022729477s
    Jan 28 00:53:45.970: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:53:47.971: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=false. Elapsed: 4.023999339s
    Jan 28 00:53:47.971: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = false)
    Jan 28 00:53:49.985: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=false. Elapsed: 6.03808201s
    Jan 28 00:53:49.986: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = false)
    Jan 28 00:53:51.972: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=false. Elapsed: 8.02497844s
    Jan 28 00:53:51.972: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = false)
    Jan 28 00:53:53.989: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=false. Elapsed: 10.042078563s
    Jan 28 00:53:53.990: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = false)
    Jan 28 00:53:55.971: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=false. Elapsed: 12.023320721s
    Jan 28 00:53:55.971: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = false)
    Jan 28 00:53:57.974: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=false. Elapsed: 14.026120953s
    Jan 28 00:53:57.974: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = false)
    Jan 28 00:53:59.971: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=false. Elapsed: 16.023222488s
    Jan 28 00:53:59.971: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = false)
    Jan 28 00:54:01.987: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=false. Elapsed: 18.040008599s
    Jan 28 00:54:01.987: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = false)
    Jan 28 00:54:03.978: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=false. Elapsed: 20.030354475s
    Jan 28 00:54:03.978: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = false)
    Jan 28 00:54:05.977: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3": Phase="Running", Reason="", readiness=true. Elapsed: 22.029174505s
    Jan 28 00:54:05.977: INFO: The phase of Pod test-webserver-e59ac711-0738-461f-a122-0d453ea389e3 is Running (Ready = true)
    Jan 28 00:54:05.977: INFO: Pod "test-webserver-e59ac711-0738-461f-a122-0d453ea389e3" satisfied condition "running and ready"
    Jan 28 00:54:06.015: INFO: Container started at 2023-01-28 00:53:45 +0000 UTC, pod became ready at 2023-01-28 00:54:04 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 28 00:54:06.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-479" for this suite. 01/28/23 00:54:06.035
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:54:06.053
Jan 28 00:54:06.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename resourcequota 01/28/23 00:54:06.055
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:54:06.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:54:06.1
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 01/28/23 00:54:06.122
STEP: Ensuring ResourceQuota status is calculated 01/28/23 00:54:06.154
STEP: Creating a ResourceQuota with not best effort scope 01/28/23 00:54:08.191
STEP: Ensuring ResourceQuota status is calculated 01/28/23 00:54:08.204
STEP: Creating a best-effort pod 01/28/23 00:54:10.217
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/28/23 00:54:10.25
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/28/23 00:54:12.262
STEP: Deleting the pod 01/28/23 00:54:14.276
STEP: Ensuring resource quota status released the pod usage 01/28/23 00:54:14.298
STEP: Creating a not best-effort pod 01/28/23 00:54:16.31
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/28/23 00:54:16.335
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/28/23 00:54:18.346
STEP: Deleting the pod 01/28/23 00:54:20.358
STEP: Ensuring resource quota status released the pod usage 01/28/23 00:54:20.418
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 28 00:54:22.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2490" for this suite. 01/28/23 00:54:22.469
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":94,"skipped":1844,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.431 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:54:06.053
    Jan 28 00:54:06.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename resourcequota 01/28/23 00:54:06.055
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:54:06.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:54:06.1
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 01/28/23 00:54:06.122
    STEP: Ensuring ResourceQuota status is calculated 01/28/23 00:54:06.154
    STEP: Creating a ResourceQuota with not best effort scope 01/28/23 00:54:08.191
    STEP: Ensuring ResourceQuota status is calculated 01/28/23 00:54:08.204
    STEP: Creating a best-effort pod 01/28/23 00:54:10.217
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/28/23 00:54:10.25
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/28/23 00:54:12.262
    STEP: Deleting the pod 01/28/23 00:54:14.276
    STEP: Ensuring resource quota status released the pod usage 01/28/23 00:54:14.298
    STEP: Creating a not best-effort pod 01/28/23 00:54:16.31
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/28/23 00:54:16.335
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/28/23 00:54:18.346
    STEP: Deleting the pod 01/28/23 00:54:20.358
    STEP: Ensuring resource quota status released the pod usage 01/28/23 00:54:20.418
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 28 00:54:22.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2490" for this suite. 01/28/23 00:54:22.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:54:22.489
Jan 28 00:54:22.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename secrets 01/28/23 00:54:22.491
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:54:22.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:54:22.532
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 01/28/23 00:54:22.541
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/28/23 00:54:22.561
STEP: patching the secret 01/28/23 00:54:22.58
STEP: deleting the secret using a LabelSelector 01/28/23 00:54:22.608
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/28/23 00:54:22.663
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 28 00:54:22.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1817" for this suite. 01/28/23 00:54:22.73
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":95,"skipped":1857,"failed":0}
------------------------------
â€¢ [0.286 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:54:22.489
    Jan 28 00:54:22.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename secrets 01/28/23 00:54:22.491
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:54:22.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:54:22.532
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 01/28/23 00:54:22.541
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/28/23 00:54:22.561
    STEP: patching the secret 01/28/23 00:54:22.58
    STEP: deleting the secret using a LabelSelector 01/28/23 00:54:22.608
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/28/23 00:54:22.663
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 28 00:54:22.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1817" for this suite. 01/28/23 00:54:22.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:54:22.776
Jan 28 00:54:22.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename sched-preemption 01/28/23 00:54:22.779
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:54:22.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:54:22.845
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 28 00:54:22.902: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 28 00:55:23.011: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:55:23.023
Jan 28 00:55:23.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename sched-preemption-path 01/28/23 00:55:23.025
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:55:23.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:55:23.065
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Jan 28 00:55:23.113: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan 28 00:55:23.122: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Jan 28 00:55:23.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9928" for this suite. 01/28/23 00:55:23.189
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 28 00:55:23.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1300" for this suite. 01/28/23 00:55:23.256
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":96,"skipped":1863,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.608 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:54:22.776
    Jan 28 00:54:22.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename sched-preemption 01/28/23 00:54:22.779
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:54:22.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:54:22.845
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 28 00:54:22.902: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 28 00:55:23.011: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:55:23.023
    Jan 28 00:55:23.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename sched-preemption-path 01/28/23 00:55:23.025
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:55:23.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:55:23.065
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Jan 28 00:55:23.113: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan 28 00:55:23.122: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Jan 28 00:55:23.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-9928" for this suite. 01/28/23 00:55:23.189
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 00:55:23.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-1300" for this suite. 01/28/23 00:55:23.256
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:55:23.405
Jan 28 00:55:23.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename daemonsets 01/28/23 00:55:23.406
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:55:23.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:55:23.466
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 01/28/23 00:55:23.539
STEP: Check that daemon pods launch on every node of the cluster. 01/28/23 00:55:23.555
Jan 28 00:55:23.581: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:55:23.581: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 00:55:24.609: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:55:24.609: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 00:55:25.610: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 28 00:55:25.610: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 00:55:26.610: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 28 00:55:26.610: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/28/23 00:55:26.622
Jan 28 00:55:26.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:55:26.681: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 00:55:27.710: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:55:27.710: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 00:55:28.714: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:55:28.714: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 00:55:29.713: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:55:29.713: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 00:55:30.713: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:55:30.713: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 00:55:31.709: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 28 00:55:31.709: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/28/23 00:55:31.72
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4300, will wait for the garbage collector to delete the pods 01/28/23 00:55:31.72
Jan 28 00:55:31.809: INFO: Deleting DaemonSet.extensions daemon-set took: 22.998573ms
Jan 28 00:55:31.909: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.27549ms
Jan 28 00:55:34.021: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:55:34.022: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 28 00:55:34.033: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26571"},"items":null}

Jan 28 00:55:34.042: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26571"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 28 00:55:34.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4300" for this suite. 01/28/23 00:55:34.116
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":97,"skipped":1865,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.731 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:55:23.405
    Jan 28 00:55:23.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename daemonsets 01/28/23 00:55:23.406
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:55:23.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:55:23.466
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 01/28/23 00:55:23.539
    STEP: Check that daemon pods launch on every node of the cluster. 01/28/23 00:55:23.555
    Jan 28 00:55:23.581: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 00:55:23.581: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 00:55:24.609: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 00:55:24.609: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 00:55:25.610: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 28 00:55:25.610: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 00:55:26.610: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 28 00:55:26.610: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/28/23 00:55:26.622
    Jan 28 00:55:26.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 28 00:55:26.681: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 00:55:27.710: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 28 00:55:27.710: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 00:55:28.714: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 28 00:55:28.714: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 00:55:29.713: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 28 00:55:29.713: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 00:55:30.713: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 28 00:55:30.713: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 00:55:31.709: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 28 00:55:31.709: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/28/23 00:55:31.72
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4300, will wait for the garbage collector to delete the pods 01/28/23 00:55:31.72
    Jan 28 00:55:31.809: INFO: Deleting DaemonSet.extensions daemon-set took: 22.998573ms
    Jan 28 00:55:31.909: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.27549ms
    Jan 28 00:55:34.021: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 00:55:34.022: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 28 00:55:34.033: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26571"},"items":null}

    Jan 28 00:55:34.042: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26571"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 00:55:34.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4300" for this suite. 01/28/23 00:55:34.116
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:55:34.141
Jan 28 00:55:34.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 00:55:34.143
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:55:34.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:55:34.182
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 01/28/23 00:55:34.192
Jan 28 00:55:34.213: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f95858ea-b69c-48c3-b83d-308ba2bb817b" in namespace "projected-3982" to be "Succeeded or Failed"
Jan 28 00:55:34.224: INFO: Pod "downwardapi-volume-f95858ea-b69c-48c3-b83d-308ba2bb817b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.82428ms
Jan 28 00:55:36.234: INFO: Pod "downwardapi-volume-f95858ea-b69c-48c3-b83d-308ba2bb817b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021054943s
Jan 28 00:55:38.236: INFO: Pod "downwardapi-volume-f95858ea-b69c-48c3-b83d-308ba2bb817b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02245597s
STEP: Saw pod success 01/28/23 00:55:38.237
Jan 28 00:55:38.237: INFO: Pod "downwardapi-volume-f95858ea-b69c-48c3-b83d-308ba2bb817b" satisfied condition "Succeeded or Failed"
Jan 28 00:55:38.248: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-f95858ea-b69c-48c3-b83d-308ba2bb817b container client-container: <nil>
STEP: delete the pod 01/28/23 00:55:38.329
Jan 28 00:55:38.357: INFO: Waiting for pod downwardapi-volume-f95858ea-b69c-48c3-b83d-308ba2bb817b to disappear
Jan 28 00:55:38.367: INFO: Pod downwardapi-volume-f95858ea-b69c-48c3-b83d-308ba2bb817b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 28 00:55:38.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3982" for this suite. 01/28/23 00:55:38.386
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":98,"skipped":1865,"failed":0}
------------------------------
â€¢ [4.264 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:55:34.141
    Jan 28 00:55:34.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 00:55:34.143
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:55:34.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:55:34.182
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 01/28/23 00:55:34.192
    Jan 28 00:55:34.213: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f95858ea-b69c-48c3-b83d-308ba2bb817b" in namespace "projected-3982" to be "Succeeded or Failed"
    Jan 28 00:55:34.224: INFO: Pod "downwardapi-volume-f95858ea-b69c-48c3-b83d-308ba2bb817b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.82428ms
    Jan 28 00:55:36.234: INFO: Pod "downwardapi-volume-f95858ea-b69c-48c3-b83d-308ba2bb817b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021054943s
    Jan 28 00:55:38.236: INFO: Pod "downwardapi-volume-f95858ea-b69c-48c3-b83d-308ba2bb817b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02245597s
    STEP: Saw pod success 01/28/23 00:55:38.237
    Jan 28 00:55:38.237: INFO: Pod "downwardapi-volume-f95858ea-b69c-48c3-b83d-308ba2bb817b" satisfied condition "Succeeded or Failed"
    Jan 28 00:55:38.248: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-f95858ea-b69c-48c3-b83d-308ba2bb817b container client-container: <nil>
    STEP: delete the pod 01/28/23 00:55:38.329
    Jan 28 00:55:38.357: INFO: Waiting for pod downwardapi-volume-f95858ea-b69c-48c3-b83d-308ba2bb817b to disappear
    Jan 28 00:55:38.367: INFO: Pod downwardapi-volume-f95858ea-b69c-48c3-b83d-308ba2bb817b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 28 00:55:38.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3982" for this suite. 01/28/23 00:55:38.386
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:55:38.405
Jan 28 00:55:38.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename namespaces 01/28/23 00:55:38.408
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:55:38.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:55:38.446
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 01/28/23 00:55:38.456
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:55:38.489
STEP: Creating a service in the namespace 01/28/23 00:55:38.498
STEP: Deleting the namespace 01/28/23 00:55:38.528
STEP: Waiting for the namespace to be removed. 01/28/23 00:55:38.548
STEP: Recreating the namespace 01/28/23 00:55:44.56
STEP: Verifying there is no service in the namespace 01/28/23 00:55:44.59
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 28 00:55:44.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4845" for this suite. 01/28/23 00:55:44.614
STEP: Destroying namespace "nsdeletetest-5826" for this suite. 01/28/23 00:55:44.632
Jan 28 00:55:44.643: INFO: Namespace nsdeletetest-5826 was already deleted
STEP: Destroying namespace "nsdeletetest-4152" for this suite. 01/28/23 00:55:44.643
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":99,"skipped":1868,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.254 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:55:38.405
    Jan 28 00:55:38.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename namespaces 01/28/23 00:55:38.408
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:55:38.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:55:38.446
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 01/28/23 00:55:38.456
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:55:38.489
    STEP: Creating a service in the namespace 01/28/23 00:55:38.498
    STEP: Deleting the namespace 01/28/23 00:55:38.528
    STEP: Waiting for the namespace to be removed. 01/28/23 00:55:38.548
    STEP: Recreating the namespace 01/28/23 00:55:44.56
    STEP: Verifying there is no service in the namespace 01/28/23 00:55:44.59
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 00:55:44.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-4845" for this suite. 01/28/23 00:55:44.614
    STEP: Destroying namespace "nsdeletetest-5826" for this suite. 01/28/23 00:55:44.632
    Jan 28 00:55:44.643: INFO: Namespace nsdeletetest-5826 was already deleted
    STEP: Destroying namespace "nsdeletetest-4152" for this suite. 01/28/23 00:55:44.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:55:44.663
Jan 28 00:55:44.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename secrets 01/28/23 00:55:44.664
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:55:44.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:55:44.702
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-d6598fe2-4d2e-44eb-8b43-c63116ed73b3 01/28/23 00:55:44.713
STEP: Creating a pod to test consume secrets 01/28/23 00:55:44.727
Jan 28 00:55:44.748: INFO: Waiting up to 5m0s for pod "pod-secrets-a59b8ece-6237-473a-a6ce-6a85bc541c4d" in namespace "secrets-8273" to be "Succeeded or Failed"
Jan 28 00:55:44.760: INFO: Pod "pod-secrets-a59b8ece-6237-473a-a6ce-6a85bc541c4d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.458642ms
Jan 28 00:55:46.772: INFO: Pod "pod-secrets-a59b8ece-6237-473a-a6ce-6a85bc541c4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023887633s
Jan 28 00:55:48.773: INFO: Pod "pod-secrets-a59b8ece-6237-473a-a6ce-6a85bc541c4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024404048s
Jan 28 00:55:50.772: INFO: Pod "pod-secrets-a59b8ece-6237-473a-a6ce-6a85bc541c4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023885855s
STEP: Saw pod success 01/28/23 00:55:50.772
Jan 28 00:55:50.773: INFO: Pod "pod-secrets-a59b8ece-6237-473a-a6ce-6a85bc541c4d" satisfied condition "Succeeded or Failed"
Jan 28 00:55:50.783: INFO: Trying to get logs from node 10.9.20.126 pod pod-secrets-a59b8ece-6237-473a-a6ce-6a85bc541c4d container secret-volume-test: <nil>
STEP: delete the pod 01/28/23 00:55:50.818
Jan 28 00:55:50.844: INFO: Waiting for pod pod-secrets-a59b8ece-6237-473a-a6ce-6a85bc541c4d to disappear
Jan 28 00:55:50.855: INFO: Pod pod-secrets-a59b8ece-6237-473a-a6ce-6a85bc541c4d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 28 00:55:50.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8273" for this suite. 01/28/23 00:55:50.87
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":100,"skipped":1881,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.227 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:55:44.663
    Jan 28 00:55:44.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename secrets 01/28/23 00:55:44.664
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:55:44.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:55:44.702
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-d6598fe2-4d2e-44eb-8b43-c63116ed73b3 01/28/23 00:55:44.713
    STEP: Creating a pod to test consume secrets 01/28/23 00:55:44.727
    Jan 28 00:55:44.748: INFO: Waiting up to 5m0s for pod "pod-secrets-a59b8ece-6237-473a-a6ce-6a85bc541c4d" in namespace "secrets-8273" to be "Succeeded or Failed"
    Jan 28 00:55:44.760: INFO: Pod "pod-secrets-a59b8ece-6237-473a-a6ce-6a85bc541c4d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.458642ms
    Jan 28 00:55:46.772: INFO: Pod "pod-secrets-a59b8ece-6237-473a-a6ce-6a85bc541c4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023887633s
    Jan 28 00:55:48.773: INFO: Pod "pod-secrets-a59b8ece-6237-473a-a6ce-6a85bc541c4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024404048s
    Jan 28 00:55:50.772: INFO: Pod "pod-secrets-a59b8ece-6237-473a-a6ce-6a85bc541c4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023885855s
    STEP: Saw pod success 01/28/23 00:55:50.772
    Jan 28 00:55:50.773: INFO: Pod "pod-secrets-a59b8ece-6237-473a-a6ce-6a85bc541c4d" satisfied condition "Succeeded or Failed"
    Jan 28 00:55:50.783: INFO: Trying to get logs from node 10.9.20.126 pod pod-secrets-a59b8ece-6237-473a-a6ce-6a85bc541c4d container secret-volume-test: <nil>
    STEP: delete the pod 01/28/23 00:55:50.818
    Jan 28 00:55:50.844: INFO: Waiting for pod pod-secrets-a59b8ece-6237-473a-a6ce-6a85bc541c4d to disappear
    Jan 28 00:55:50.855: INFO: Pod pod-secrets-a59b8ece-6237-473a-a6ce-6a85bc541c4d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 28 00:55:50.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8273" for this suite. 01/28/23 00:55:50.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:55:50.892
Jan 28 00:55:50.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubectl 01/28/23 00:55:50.895
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:55:50.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:55:50.934
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Jan 28 00:55:50.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-1690 create -f -'
Jan 28 00:55:51.239: INFO: stderr: ""
Jan 28 00:55:51.239: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 28 00:55:51.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-1690 create -f -'
Jan 28 00:55:51.558: INFO: stderr: ""
Jan 28 00:55:51.559: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/28/23 00:55:51.559
Jan 28 00:55:52.571: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 00:55:52.571: INFO: Found 0 / 1
Jan 28 00:55:53.572: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 00:55:53.572: INFO: Found 0 / 1
Jan 28 00:55:54.572: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 00:55:54.573: INFO: Found 1 / 1
Jan 28 00:55:54.573: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 28 00:55:54.584: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 00:55:54.584: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 28 00:55:54.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-1690 describe pod agnhost-primary-jktm9'
Jan 28 00:55:54.735: INFO: stderr: ""
Jan 28 00:55:54.736: INFO: stdout: "Name:             agnhost-primary-jktm9\nNamespace:        kubectl-1690\nPriority:         0\nService Account:  default\nNode:             10.9.20.72/10.9.20.72\nStart Time:       Sat, 28 Jan 2023 00:55:51 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 22a356a735db6ef625a4fb213bd530e531942e12b5a996eed209d633057b6242\n                  cni.projectcalico.org/podIP: 172.30.185.30/32\n                  cni.projectcalico.org/podIPs: 172.30.185.30/32\nStatus:           Running\nIP:               172.30.185.30\nIPs:\n  IP:           172.30.185.30\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://2c1babab161c755564551e5f1f1880525204a4399e9f6e2f00da9db066f293c0\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 28 Jan 2023 00:55:52 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-v5qt2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-v5qt2:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 600s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 600s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-1690/agnhost-primary-jktm9 to 10.9.20.72\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Jan 28 00:55:54.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-1690 describe rc agnhost-primary'
Jan 28 00:55:54.885: INFO: stderr: ""
Jan 28 00:55:54.886: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1690\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-jktm9\n"
Jan 28 00:55:54.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-1690 describe service agnhost-primary'
Jan 28 00:55:55.037: INFO: stderr: ""
Jan 28 00:55:55.037: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1690\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.115.9\nIPs:               172.21.115.9\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.185.30:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 28 00:55:55.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-1690 describe node 10.9.20.126'
Jan 28 00:55:55.281: INFO: stderr: ""
Jan 28 00:55:55.281: INFO: stdout: "Name:               10.9.20.126\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=jp-osa\n                    failure-domain.beta.kubernetes.io/zone=osa22\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=163.69.69.110\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.9.20.126\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_18_64\n                    ibm-cloud.kubernetes.io/region=jp-osa\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cfa4dcfo0uc72glhelp0-kubee2epvg4-default-00000205\n                    ibm-cloud.kubernetes.io/worker-pool-id=cfa4dcfo0uc72glhelp0-629c915\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.25.5_1528\n                    ibm-cloud.kubernetes.io/zone=osa22\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.9.20.126\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    privateVLAN=2981444\n                    publicVLAN=2981450\n                    topology.kubernetes.io/region=jp-osa\n                    topology.kubernetes.io/zone=osa22\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.9.20.126/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.12.192\nCreationTimestamp:  Fri, 27 Jan 2023 22:05:01 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.9.20.126\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 28 Jan 2023 00:55:48 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 27 Jan 2023 22:06:05 +0000   Fri, 27 Jan 2023 22:06:05 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sat, 28 Jan 2023 00:53:47 +0000   Fri, 27 Jan 2023 22:05:01 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 28 Jan 2023 00:53:47 +0000   Fri, 27 Jan 2023 22:05:01 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 28 Jan 2023 00:53:47 +0000   Fri, 27 Jan 2023 22:05:01 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 28 Jan 2023 00:53:47 +0000   Fri, 27 Jan 2023 22:05:32 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.9.20.126\n  ExternalIP:  163.69.69.110\n  Hostname:    10.9.20.126\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102624184Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16212384Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  93927226085\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13440416Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 87ea14a1f62c4fbb9013b50b6ec72ab0\n  System UUID:                C901FAEA-4DEE-796D-CA90-BDDBE1B8EB89\n  Boot ID:                    508e5ca2-9c71-479a-8e86-8cb5d2157a59\n  Kernel Version:             4.15.0-202-generic\n  OS Image:                   Ubuntu 18.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.15\n  Kubelet Version:            v1.25.5+IKS\n  Kube-Proxy Version:         v1.25.5+IKS\nProviderID:                   ibm://fee034388aa6435883a1f720010ab3a2///cfa4dcfo0uc72glhelp0/kube-cfa4dcfo0uc72glhelp0-kubee2epvg4-default-00000205\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-xl9f8                                          250m (6%)     0 (0%)      90Mi (0%)        0 (0%)         170m\n  kube-system                 calico-typha-677688fdc5-twxlr                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         176m\n  kube-system                 coredns-6754846f95-9ck4t                                   100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     159m\n  kube-system                 ibm-keepalived-watcher-b94td                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         170m\n  kube-system                 ibm-master-proxy-static-10.9.20.126                        25m (0%)      300m (7%)   32M (0%)         512M (3%)      169m\n  kube-system                 ibmcloud-block-storage-driver-lw66t                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     170m\n  kube-system                 konnectivity-agent-65zgm                                   10m (0%)      0 (0%)      10Mi (0%)        500Mi (3%)     160m\n  kube-system                 metrics-server-9cdf87dc6-4ptjv                             126m (3%)     266m (6%)   191Mi (1%)       536Mi (4%)     124m\n  kube-system                 public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-vvqqx        20m (0%)      0 (0%)      115Mi (0%)       0 (0%)         166m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-qbzm7    0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                836m (21%)     866m (22%)\n  memory             713234Ki (5%)  2277664Ki (16%)\n  ephemeral-storage  0 (0%)         0 (0%)\n  hugepages-1Gi      0 (0%)         0 (0%)\n  hugepages-2Mi      0 (0%)         0 (0%)\nEvents:              <none>\n"
Jan 28 00:55:55.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-1690 describe namespace kubectl-1690'
Jan 28 00:55:55.443: INFO: stderr: ""
Jan 28 00:55:55.443: INFO: stdout: "Name:         kubectl-1690\nLabels:       e2e-framework=kubectl\n              e2e-run=ba43e6ee-618b-4498-aa9f-cb14cf317c48\n              kubernetes.io/metadata.name=kubectl-1690\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 28 00:55:55.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1690" for this suite. 01/28/23 00:55:55.461
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":101,"skipped":1888,"failed":0}
------------------------------
â€¢ [4.588 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:55:50.892
    Jan 28 00:55:50.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubectl 01/28/23 00:55:50.895
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:55:50.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:55:50.934
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Jan 28 00:55:50.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-1690 create -f -'
    Jan 28 00:55:51.239: INFO: stderr: ""
    Jan 28 00:55:51.239: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan 28 00:55:51.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-1690 create -f -'
    Jan 28 00:55:51.558: INFO: stderr: ""
    Jan 28 00:55:51.559: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/28/23 00:55:51.559
    Jan 28 00:55:52.571: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 28 00:55:52.571: INFO: Found 0 / 1
    Jan 28 00:55:53.572: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 28 00:55:53.572: INFO: Found 0 / 1
    Jan 28 00:55:54.572: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 28 00:55:54.573: INFO: Found 1 / 1
    Jan 28 00:55:54.573: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 28 00:55:54.584: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 28 00:55:54.584: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 28 00:55:54.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-1690 describe pod agnhost-primary-jktm9'
    Jan 28 00:55:54.735: INFO: stderr: ""
    Jan 28 00:55:54.736: INFO: stdout: "Name:             agnhost-primary-jktm9\nNamespace:        kubectl-1690\nPriority:         0\nService Account:  default\nNode:             10.9.20.72/10.9.20.72\nStart Time:       Sat, 28 Jan 2023 00:55:51 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 22a356a735db6ef625a4fb213bd530e531942e12b5a996eed209d633057b6242\n                  cni.projectcalico.org/podIP: 172.30.185.30/32\n                  cni.projectcalico.org/podIPs: 172.30.185.30/32\nStatus:           Running\nIP:               172.30.185.30\nIPs:\n  IP:           172.30.185.30\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://2c1babab161c755564551e5f1f1880525204a4399e9f6e2f00da9db066f293c0\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 28 Jan 2023 00:55:52 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-v5qt2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-v5qt2:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 600s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 600s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-1690/agnhost-primary-jktm9 to 10.9.20.72\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Jan 28 00:55:54.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-1690 describe rc agnhost-primary'
    Jan 28 00:55:54.885: INFO: stderr: ""
    Jan 28 00:55:54.886: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1690\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-jktm9\n"
    Jan 28 00:55:54.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-1690 describe service agnhost-primary'
    Jan 28 00:55:55.037: INFO: stderr: ""
    Jan 28 00:55:55.037: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1690\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.115.9\nIPs:               172.21.115.9\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.185.30:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan 28 00:55:55.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-1690 describe node 10.9.20.126'
    Jan 28 00:55:55.281: INFO: stderr: ""
    Jan 28 00:55:55.281: INFO: stdout: "Name:               10.9.20.126\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=jp-osa\n                    failure-domain.beta.kubernetes.io/zone=osa22\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=163.69.69.110\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.9.20.126\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_18_64\n                    ibm-cloud.kubernetes.io/region=jp-osa\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cfa4dcfo0uc72glhelp0-kubee2epvg4-default-00000205\n                    ibm-cloud.kubernetes.io/worker-pool-id=cfa4dcfo0uc72glhelp0-629c915\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.25.5_1528\n                    ibm-cloud.kubernetes.io/zone=osa22\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.9.20.126\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    privateVLAN=2981444\n                    publicVLAN=2981450\n                    topology.kubernetes.io/region=jp-osa\n                    topology.kubernetes.io/zone=osa22\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.9.20.126/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.12.192\nCreationTimestamp:  Fri, 27 Jan 2023 22:05:01 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.9.20.126\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 28 Jan 2023 00:55:48 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 27 Jan 2023 22:06:05 +0000   Fri, 27 Jan 2023 22:06:05 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sat, 28 Jan 2023 00:53:47 +0000   Fri, 27 Jan 2023 22:05:01 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 28 Jan 2023 00:53:47 +0000   Fri, 27 Jan 2023 22:05:01 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 28 Jan 2023 00:53:47 +0000   Fri, 27 Jan 2023 22:05:01 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 28 Jan 2023 00:53:47 +0000   Fri, 27 Jan 2023 22:05:32 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.9.20.126\n  ExternalIP:  163.69.69.110\n  Hostname:    10.9.20.126\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102624184Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16212384Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  93927226085\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13440416Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 87ea14a1f62c4fbb9013b50b6ec72ab0\n  System UUID:                C901FAEA-4DEE-796D-CA90-BDDBE1B8EB89\n  Boot ID:                    508e5ca2-9c71-479a-8e86-8cb5d2157a59\n  Kernel Version:             4.15.0-202-generic\n  OS Image:                   Ubuntu 18.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.15\n  Kubelet Version:            v1.25.5+IKS\n  Kube-Proxy Version:         v1.25.5+IKS\nProviderID:                   ibm://fee034388aa6435883a1f720010ab3a2///cfa4dcfo0uc72glhelp0/kube-cfa4dcfo0uc72glhelp0-kubee2epvg4-default-00000205\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-xl9f8                                          250m (6%)     0 (0%)      90Mi (0%)        0 (0%)         170m\n  kube-system                 calico-typha-677688fdc5-twxlr                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         176m\n  kube-system                 coredns-6754846f95-9ck4t                                   100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     159m\n  kube-system                 ibm-keepalived-watcher-b94td                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         170m\n  kube-system                 ibm-master-proxy-static-10.9.20.126                        25m (0%)      300m (7%)   32M (0%)         512M (3%)      169m\n  kube-system                 ibmcloud-block-storage-driver-lw66t                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     170m\n  kube-system                 konnectivity-agent-65zgm                                   10m (0%)      0 (0%)      10Mi (0%)        500Mi (3%)     160m\n  kube-system                 metrics-server-9cdf87dc6-4ptjv                             126m (3%)     266m (6%)   191Mi (1%)       536Mi (4%)     124m\n  kube-system                 public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-vvqqx        20m (0%)      0 (0%)      115Mi (0%)       0 (0%)         166m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-qbzm7    0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                836m (21%)     866m (22%)\n  memory             713234Ki (5%)  2277664Ki (16%)\n  ephemeral-storage  0 (0%)         0 (0%)\n  hugepages-1Gi      0 (0%)         0 (0%)\n  hugepages-2Mi      0 (0%)         0 (0%)\nEvents:              <none>\n"
    Jan 28 00:55:55.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-1690 describe namespace kubectl-1690'
    Jan 28 00:55:55.443: INFO: stderr: ""
    Jan 28 00:55:55.443: INFO: stdout: "Name:         kubectl-1690\nLabels:       e2e-framework=kubectl\n              e2e-run=ba43e6ee-618b-4498-aa9f-cb14cf317c48\n              kubernetes.io/metadata.name=kubectl-1690\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 28 00:55:55.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1690" for this suite. 01/28/23 00:55:55.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:55:55.496
Jan 28 00:55:55.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename container-lifecycle-hook 01/28/23 00:55:55.498
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:55:55.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:55:55.551
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/28/23 00:55:55.576
Jan 28 00:55:55.596: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4437" to be "running and ready"
Jan 28 00:55:55.610: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 14.416028ms
Jan 28 00:55:55.611: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:55:57.623: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02661282s
Jan 28 00:55:57.623: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:55:59.623: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.026823964s
Jan 28 00:55:59.623: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 28 00:55:59.623: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 01/28/23 00:55:59.634
Jan 28 00:55:59.647: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4437" to be "running and ready"
Jan 28 00:55:59.662: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 14.311355ms
Jan 28 00:55:59.662: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:56:01.676: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028287857s
Jan 28 00:56:01.676: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:56:03.673: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.025795608s
Jan 28 00:56:03.673: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan 28 00:56:03.673: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/28/23 00:56:03.683
Jan 28 00:56:03.702: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 28 00:56:03.713: INFO: Pod pod-with-prestop-http-hook still exists
Jan 28 00:56:05.714: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 28 00:56:05.725: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/28/23 00:56:05.725
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 28 00:56:05.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4437" for this suite. 01/28/23 00:56:05.807
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":102,"skipped":1918,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.327 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:55:55.496
    Jan 28 00:55:55.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/28/23 00:55:55.498
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:55:55.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:55:55.551
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/28/23 00:55:55.576
    Jan 28 00:55:55.596: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4437" to be "running and ready"
    Jan 28 00:55:55.610: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 14.416028ms
    Jan 28 00:55:55.611: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:55:57.623: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02661282s
    Jan 28 00:55:57.623: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:55:59.623: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.026823964s
    Jan 28 00:55:59.623: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 28 00:55:59.623: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 01/28/23 00:55:59.634
    Jan 28 00:55:59.647: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4437" to be "running and ready"
    Jan 28 00:55:59.662: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 14.311355ms
    Jan 28 00:55:59.662: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:56:01.676: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028287857s
    Jan 28 00:56:01.676: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:56:03.673: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.025795608s
    Jan 28 00:56:03.673: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan 28 00:56:03.673: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/28/23 00:56:03.683
    Jan 28 00:56:03.702: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 28 00:56:03.713: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 28 00:56:05.714: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 28 00:56:05.725: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/28/23 00:56:05.725
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 28 00:56:05.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4437" for this suite. 01/28/23 00:56:05.807
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:56:05.828
Jan 28 00:56:05.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename downward-api 01/28/23 00:56:05.83
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:56:05.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:56:05.872
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 01/28/23 00:56:05.881
Jan 28 00:56:05.904: INFO: Waiting up to 5m0s for pod "downward-api-df36f7f1-de0c-4d5f-a4e8-8a05875ff711" in namespace "downward-api-7534" to be "Succeeded or Failed"
Jan 28 00:56:05.915: INFO: Pod "downward-api-df36f7f1-de0c-4d5f-a4e8-8a05875ff711": Phase="Pending", Reason="", readiness=false. Elapsed: 10.190711ms
Jan 28 00:56:07.927: INFO: Pod "downward-api-df36f7f1-de0c-4d5f-a4e8-8a05875ff711": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022688032s
Jan 28 00:56:09.927: INFO: Pod "downward-api-df36f7f1-de0c-4d5f-a4e8-8a05875ff711": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023090067s
STEP: Saw pod success 01/28/23 00:56:09.928
Jan 28 00:56:09.928: INFO: Pod "downward-api-df36f7f1-de0c-4d5f-a4e8-8a05875ff711" satisfied condition "Succeeded or Failed"
Jan 28 00:56:09.938: INFO: Trying to get logs from node 10.9.20.126 pod downward-api-df36f7f1-de0c-4d5f-a4e8-8a05875ff711 container dapi-container: <nil>
STEP: delete the pod 01/28/23 00:56:09.968
Jan 28 00:56:09.997: INFO: Waiting for pod downward-api-df36f7f1-de0c-4d5f-a4e8-8a05875ff711 to disappear
Jan 28 00:56:10.007: INFO: Pod downward-api-df36f7f1-de0c-4d5f-a4e8-8a05875ff711 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 28 00:56:10.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7534" for this suite. 01/28/23 00:56:10.025
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":103,"skipped":1919,"failed":0}
------------------------------
â€¢ [4.214 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:56:05.828
    Jan 28 00:56:05.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename downward-api 01/28/23 00:56:05.83
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:56:05.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:56:05.872
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 01/28/23 00:56:05.881
    Jan 28 00:56:05.904: INFO: Waiting up to 5m0s for pod "downward-api-df36f7f1-de0c-4d5f-a4e8-8a05875ff711" in namespace "downward-api-7534" to be "Succeeded or Failed"
    Jan 28 00:56:05.915: INFO: Pod "downward-api-df36f7f1-de0c-4d5f-a4e8-8a05875ff711": Phase="Pending", Reason="", readiness=false. Elapsed: 10.190711ms
    Jan 28 00:56:07.927: INFO: Pod "downward-api-df36f7f1-de0c-4d5f-a4e8-8a05875ff711": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022688032s
    Jan 28 00:56:09.927: INFO: Pod "downward-api-df36f7f1-de0c-4d5f-a4e8-8a05875ff711": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023090067s
    STEP: Saw pod success 01/28/23 00:56:09.928
    Jan 28 00:56:09.928: INFO: Pod "downward-api-df36f7f1-de0c-4d5f-a4e8-8a05875ff711" satisfied condition "Succeeded or Failed"
    Jan 28 00:56:09.938: INFO: Trying to get logs from node 10.9.20.126 pod downward-api-df36f7f1-de0c-4d5f-a4e8-8a05875ff711 container dapi-container: <nil>
    STEP: delete the pod 01/28/23 00:56:09.968
    Jan 28 00:56:09.997: INFO: Waiting for pod downward-api-df36f7f1-de0c-4d5f-a4e8-8a05875ff711 to disappear
    Jan 28 00:56:10.007: INFO: Pod downward-api-df36f7f1-de0c-4d5f-a4e8-8a05875ff711 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 28 00:56:10.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7534" for this suite. 01/28/23 00:56:10.025
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:56:10.045
Jan 28 00:56:10.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename configmap 01/28/23 00:56:10.049
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:56:10.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:56:10.087
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-00746e2c-9237-4eda-92ff-7d40e96d841b 01/28/23 00:56:10.098
STEP: Creating a pod to test consume configMaps 01/28/23 00:56:10.109
Jan 28 00:56:10.134: INFO: Waiting up to 5m0s for pod "pod-configmaps-4b72f2ac-138e-42cd-b466-2ac39255cabd" in namespace "configmap-5313" to be "Succeeded or Failed"
Jan 28 00:56:10.144: INFO: Pod "pod-configmaps-4b72f2ac-138e-42cd-b466-2ac39255cabd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.200141ms
Jan 28 00:56:12.156: INFO: Pod "pod-configmaps-4b72f2ac-138e-42cd-b466-2ac39255cabd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022502286s
Jan 28 00:56:14.158: INFO: Pod "pod-configmaps-4b72f2ac-138e-42cd-b466-2ac39255cabd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024796822s
Jan 28 00:56:16.156: INFO: Pod "pod-configmaps-4b72f2ac-138e-42cd-b466-2ac39255cabd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022438057s
STEP: Saw pod success 01/28/23 00:56:16.156
Jan 28 00:56:16.156: INFO: Pod "pod-configmaps-4b72f2ac-138e-42cd-b466-2ac39255cabd" satisfied condition "Succeeded or Failed"
Jan 28 00:56:16.166: INFO: Trying to get logs from node 10.9.20.126 pod pod-configmaps-4b72f2ac-138e-42cd-b466-2ac39255cabd container agnhost-container: <nil>
STEP: delete the pod 01/28/23 00:56:16.193
Jan 28 00:56:16.232: INFO: Waiting for pod pod-configmaps-4b72f2ac-138e-42cd-b466-2ac39255cabd to disappear
Jan 28 00:56:16.242: INFO: Pod pod-configmaps-4b72f2ac-138e-42cd-b466-2ac39255cabd no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 28 00:56:16.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5313" for this suite. 01/28/23 00:56:16.258
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":104,"skipped":1922,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.231 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:56:10.045
    Jan 28 00:56:10.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename configmap 01/28/23 00:56:10.049
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:56:10.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:56:10.087
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-00746e2c-9237-4eda-92ff-7d40e96d841b 01/28/23 00:56:10.098
    STEP: Creating a pod to test consume configMaps 01/28/23 00:56:10.109
    Jan 28 00:56:10.134: INFO: Waiting up to 5m0s for pod "pod-configmaps-4b72f2ac-138e-42cd-b466-2ac39255cabd" in namespace "configmap-5313" to be "Succeeded or Failed"
    Jan 28 00:56:10.144: INFO: Pod "pod-configmaps-4b72f2ac-138e-42cd-b466-2ac39255cabd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.200141ms
    Jan 28 00:56:12.156: INFO: Pod "pod-configmaps-4b72f2ac-138e-42cd-b466-2ac39255cabd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022502286s
    Jan 28 00:56:14.158: INFO: Pod "pod-configmaps-4b72f2ac-138e-42cd-b466-2ac39255cabd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024796822s
    Jan 28 00:56:16.156: INFO: Pod "pod-configmaps-4b72f2ac-138e-42cd-b466-2ac39255cabd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022438057s
    STEP: Saw pod success 01/28/23 00:56:16.156
    Jan 28 00:56:16.156: INFO: Pod "pod-configmaps-4b72f2ac-138e-42cd-b466-2ac39255cabd" satisfied condition "Succeeded or Failed"
    Jan 28 00:56:16.166: INFO: Trying to get logs from node 10.9.20.126 pod pod-configmaps-4b72f2ac-138e-42cd-b466-2ac39255cabd container agnhost-container: <nil>
    STEP: delete the pod 01/28/23 00:56:16.193
    Jan 28 00:56:16.232: INFO: Waiting for pod pod-configmaps-4b72f2ac-138e-42cd-b466-2ac39255cabd to disappear
    Jan 28 00:56:16.242: INFO: Pod pod-configmaps-4b72f2ac-138e-42cd-b466-2ac39255cabd no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 28 00:56:16.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5313" for this suite. 01/28/23 00:56:16.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:56:16.279
Jan 28 00:56:16.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename container-lifecycle-hook 01/28/23 00:56:16.283
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:56:16.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:56:16.339
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/28/23 00:56:16.364
Jan 28 00:56:16.393: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1438" to be "running and ready"
Jan 28 00:56:16.403: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.843375ms
Jan 28 00:56:16.403: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:56:18.414: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.02074843s
Jan 28 00:56:18.414: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 28 00:56:18.414: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 01/28/23 00:56:18.429
Jan 28 00:56:18.442: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-1438" to be "running and ready"
Jan 28 00:56:18.452: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.299337ms
Jan 28 00:56:18.452: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:56:20.463: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02100515s
Jan 28 00:56:20.470: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:56:22.466: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.02449301s
Jan 28 00:56:22.466: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan 28 00:56:22.466: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/28/23 00:56:22.479
STEP: delete the pod with lifecycle hook 01/28/23 00:56:22.557
Jan 28 00:56:22.580: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 28 00:56:22.593: INFO: Pod pod-with-poststart-http-hook still exists
Jan 28 00:56:24.593: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 28 00:56:24.607: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 28 00:56:24.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1438" for this suite. 01/28/23 00:56:24.623
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":105,"skipped":1935,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.362 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:56:16.279
    Jan 28 00:56:16.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/28/23 00:56:16.283
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:56:16.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:56:16.339
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/28/23 00:56:16.364
    Jan 28 00:56:16.393: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1438" to be "running and ready"
    Jan 28 00:56:16.403: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.843375ms
    Jan 28 00:56:16.403: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:56:18.414: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.02074843s
    Jan 28 00:56:18.414: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 28 00:56:18.414: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 01/28/23 00:56:18.429
    Jan 28 00:56:18.442: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-1438" to be "running and ready"
    Jan 28 00:56:18.452: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.299337ms
    Jan 28 00:56:18.452: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:56:20.463: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02100515s
    Jan 28 00:56:20.470: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:56:22.466: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.02449301s
    Jan 28 00:56:22.466: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan 28 00:56:22.466: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/28/23 00:56:22.479
    STEP: delete the pod with lifecycle hook 01/28/23 00:56:22.557
    Jan 28 00:56:22.580: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 28 00:56:22.593: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 28 00:56:24.593: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 28 00:56:24.607: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 28 00:56:24.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-1438" for this suite. 01/28/23 00:56:24.623
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:56:24.649
Jan 28 00:56:24.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename replication-controller 01/28/23 00:56:24.65
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:56:24.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:56:24.689
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0 01/28/23 00:56:24.698
Jan 28 00:56:24.720: INFO: Pod name my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0: Found 0 pods out of 1
Jan 28 00:56:29.736: INFO: Pod name my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0: Found 1 pods out of 1
Jan 28 00:56:29.736: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0" are running
Jan 28 00:56:29.736: INFO: Waiting up to 5m0s for pod "my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0-ffdtp" in namespace "replication-controller-6755" to be "running"
Jan 28 00:56:29.747: INFO: Pod "my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0-ffdtp": Phase="Running", Reason="", readiness=true. Elapsed: 10.997676ms
Jan 28 00:56:29.747: INFO: Pod "my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0-ffdtp" satisfied condition "running"
Jan 28 00:56:29.747: INFO: Pod "my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0-ffdtp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 00:56:24 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 00:56:26 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 00:56:26 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 00:56:24 +0000 UTC Reason: Message:}])
Jan 28 00:56:29.747: INFO: Trying to dial the pod
Jan 28 00:56:34.819: INFO: Controller my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0: Got expected result from replica 1 [my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0-ffdtp]: "my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0-ffdtp", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 28 00:56:34.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6755" for this suite. 01/28/23 00:56:34.835
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":106,"skipped":1987,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.205 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:56:24.649
    Jan 28 00:56:24.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename replication-controller 01/28/23 00:56:24.65
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:56:24.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:56:24.689
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0 01/28/23 00:56:24.698
    Jan 28 00:56:24.720: INFO: Pod name my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0: Found 0 pods out of 1
    Jan 28 00:56:29.736: INFO: Pod name my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0: Found 1 pods out of 1
    Jan 28 00:56:29.736: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0" are running
    Jan 28 00:56:29.736: INFO: Waiting up to 5m0s for pod "my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0-ffdtp" in namespace "replication-controller-6755" to be "running"
    Jan 28 00:56:29.747: INFO: Pod "my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0-ffdtp": Phase="Running", Reason="", readiness=true. Elapsed: 10.997676ms
    Jan 28 00:56:29.747: INFO: Pod "my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0-ffdtp" satisfied condition "running"
    Jan 28 00:56:29.747: INFO: Pod "my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0-ffdtp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 00:56:24 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 00:56:26 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 00:56:26 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 00:56:24 +0000 UTC Reason: Message:}])
    Jan 28 00:56:29.747: INFO: Trying to dial the pod
    Jan 28 00:56:34.819: INFO: Controller my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0: Got expected result from replica 1 [my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0-ffdtp]: "my-hostname-basic-de8dfe31-deca-4aba-8f61-227d7f14dbd0-ffdtp", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 28 00:56:34.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6755" for this suite. 01/28/23 00:56:34.835
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:56:34.856
Jan 28 00:56:34.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename services 01/28/23 00:56:34.858
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:56:34.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:56:34.904
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2119 01/28/23 00:56:34.913
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/28/23 00:56:34.949
STEP: creating service externalsvc in namespace services-2119 01/28/23 00:56:34.949
STEP: creating replication controller externalsvc in namespace services-2119 01/28/23 00:56:34.982
I0128 00:56:35.000149      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2119, replica count: 2
I0128 00:56:38.051754      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/28/23 00:56:38.062
Jan 28 00:56:38.111: INFO: Creating new exec pod
Jan 28 00:56:38.124: INFO: Waiting up to 5m0s for pod "execpodlb8jp" in namespace "services-2119" to be "running"
Jan 28 00:56:38.134: INFO: Pod "execpodlb8jp": Phase="Pending", Reason="", readiness=false. Elapsed: 9.939914ms
Jan 28 00:56:40.150: INFO: Pod "execpodlb8jp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025636211s
Jan 28 00:56:42.149: INFO: Pod "execpodlb8jp": Phase="Running", Reason="", readiness=true. Elapsed: 4.024784698s
Jan 28 00:56:42.149: INFO: Pod "execpodlb8jp" satisfied condition "running"
Jan 28 00:56:42.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-2119 exec execpodlb8jp -- /bin/sh -x -c nslookup clusterip-service.services-2119.svc.cluster.local'
Jan 28 00:56:42.521: INFO: stderr: "+ nslookup clusterip-service.services-2119.svc.cluster.local\n"
Jan 28 00:56:42.521: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-2119.svc.cluster.local\tcanonical name = externalsvc.services-2119.svc.cluster.local.\nName:\texternalsvc.services-2119.svc.cluster.local\nAddress: 172.21.76.206\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2119, will wait for the garbage collector to delete the pods 01/28/23 00:56:42.521
Jan 28 00:56:42.599: INFO: Deleting ReplicationController externalsvc took: 17.249388ms
Jan 28 00:56:42.700: INFO: Terminating ReplicationController externalsvc pods took: 100.836919ms
Jan 28 00:56:45.449: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 28 00:56:45.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2119" for this suite. 01/28/23 00:56:45.508
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":107,"skipped":1987,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.670 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:56:34.856
    Jan 28 00:56:34.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename services 01/28/23 00:56:34.858
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:56:34.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:56:34.904
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2119 01/28/23 00:56:34.913
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/28/23 00:56:34.949
    STEP: creating service externalsvc in namespace services-2119 01/28/23 00:56:34.949
    STEP: creating replication controller externalsvc in namespace services-2119 01/28/23 00:56:34.982
    I0128 00:56:35.000149      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2119, replica count: 2
    I0128 00:56:38.051754      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/28/23 00:56:38.062
    Jan 28 00:56:38.111: INFO: Creating new exec pod
    Jan 28 00:56:38.124: INFO: Waiting up to 5m0s for pod "execpodlb8jp" in namespace "services-2119" to be "running"
    Jan 28 00:56:38.134: INFO: Pod "execpodlb8jp": Phase="Pending", Reason="", readiness=false. Elapsed: 9.939914ms
    Jan 28 00:56:40.150: INFO: Pod "execpodlb8jp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025636211s
    Jan 28 00:56:42.149: INFO: Pod "execpodlb8jp": Phase="Running", Reason="", readiness=true. Elapsed: 4.024784698s
    Jan 28 00:56:42.149: INFO: Pod "execpodlb8jp" satisfied condition "running"
    Jan 28 00:56:42.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-2119 exec execpodlb8jp -- /bin/sh -x -c nslookup clusterip-service.services-2119.svc.cluster.local'
    Jan 28 00:56:42.521: INFO: stderr: "+ nslookup clusterip-service.services-2119.svc.cluster.local\n"
    Jan 28 00:56:42.521: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-2119.svc.cluster.local\tcanonical name = externalsvc.services-2119.svc.cluster.local.\nName:\texternalsvc.services-2119.svc.cluster.local\nAddress: 172.21.76.206\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-2119, will wait for the garbage collector to delete the pods 01/28/23 00:56:42.521
    Jan 28 00:56:42.599: INFO: Deleting ReplicationController externalsvc took: 17.249388ms
    Jan 28 00:56:42.700: INFO: Terminating ReplicationController externalsvc pods took: 100.836919ms
    Jan 28 00:56:45.449: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 28 00:56:45.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2119" for this suite. 01/28/23 00:56:45.508
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:56:45.545
Jan 28 00:56:45.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename configmap 01/28/23 00:56:45.546
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:56:45.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:56:45.587
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 01/28/23 00:56:45.597
STEP: fetching the ConfigMap 01/28/23 00:56:45.608
STEP: patching the ConfigMap 01/28/23 00:56:45.618
STEP: listing all ConfigMaps in all namespaces with a label selector 01/28/23 00:56:45.631
STEP: deleting the ConfigMap by collection with a label selector 01/28/23 00:56:45.643
STEP: listing all ConfigMaps in test namespace 01/28/23 00:56:45.664
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 28 00:56:45.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2612" for this suite. 01/28/23 00:56:45.69
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":108,"skipped":2055,"failed":0}
------------------------------
â€¢ [0.179 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:56:45.545
    Jan 28 00:56:45.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename configmap 01/28/23 00:56:45.546
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:56:45.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:56:45.587
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 01/28/23 00:56:45.597
    STEP: fetching the ConfigMap 01/28/23 00:56:45.608
    STEP: patching the ConfigMap 01/28/23 00:56:45.618
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/28/23 00:56:45.631
    STEP: deleting the ConfigMap by collection with a label selector 01/28/23 00:56:45.643
    STEP: listing all ConfigMaps in test namespace 01/28/23 00:56:45.664
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 28 00:56:45.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2612" for this suite. 01/28/23 00:56:45.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:56:45.73
Jan 28 00:56:45.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename security-context 01/28/23 00:56:45.732
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:56:45.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:56:45.773
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/28/23 00:56:45.784
Jan 28 00:56:45.804: INFO: Waiting up to 5m0s for pod "security-context-8991efff-fa25-420d-b6e5-12594ab962ad" in namespace "security-context-7315" to be "Succeeded or Failed"
Jan 28 00:56:45.816: INFO: Pod "security-context-8991efff-fa25-420d-b6e5-12594ab962ad": Phase="Pending", Reason="", readiness=false. Elapsed: 12.266165ms
Jan 28 00:56:47.829: INFO: Pod "security-context-8991efff-fa25-420d-b6e5-12594ab962ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024704094s
Jan 28 00:56:49.828: INFO: Pod "security-context-8991efff-fa25-420d-b6e5-12594ab962ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023630583s
Jan 28 00:56:51.828: INFO: Pod "security-context-8991efff-fa25-420d-b6e5-12594ab962ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024280131s
STEP: Saw pod success 01/28/23 00:56:51.828
Jan 28 00:56:51.829: INFO: Pod "security-context-8991efff-fa25-420d-b6e5-12594ab962ad" satisfied condition "Succeeded or Failed"
Jan 28 00:56:51.839: INFO: Trying to get logs from node 10.9.20.72 pod security-context-8991efff-fa25-420d-b6e5-12594ab962ad container test-container: <nil>
STEP: delete the pod 01/28/23 00:56:51.868
Jan 28 00:56:51.894: INFO: Waiting for pod security-context-8991efff-fa25-420d-b6e5-12594ab962ad to disappear
Jan 28 00:56:51.904: INFO: Pod security-context-8991efff-fa25-420d-b6e5-12594ab962ad no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 28 00:56:51.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-7315" for this suite. 01/28/23 00:56:51.919
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":109,"skipped":2084,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.207 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:56:45.73
    Jan 28 00:56:45.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename security-context 01/28/23 00:56:45.732
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:56:45.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:56:45.773
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/28/23 00:56:45.784
    Jan 28 00:56:45.804: INFO: Waiting up to 5m0s for pod "security-context-8991efff-fa25-420d-b6e5-12594ab962ad" in namespace "security-context-7315" to be "Succeeded or Failed"
    Jan 28 00:56:45.816: INFO: Pod "security-context-8991efff-fa25-420d-b6e5-12594ab962ad": Phase="Pending", Reason="", readiness=false. Elapsed: 12.266165ms
    Jan 28 00:56:47.829: INFO: Pod "security-context-8991efff-fa25-420d-b6e5-12594ab962ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024704094s
    Jan 28 00:56:49.828: INFO: Pod "security-context-8991efff-fa25-420d-b6e5-12594ab962ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023630583s
    Jan 28 00:56:51.828: INFO: Pod "security-context-8991efff-fa25-420d-b6e5-12594ab962ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024280131s
    STEP: Saw pod success 01/28/23 00:56:51.828
    Jan 28 00:56:51.829: INFO: Pod "security-context-8991efff-fa25-420d-b6e5-12594ab962ad" satisfied condition "Succeeded or Failed"
    Jan 28 00:56:51.839: INFO: Trying to get logs from node 10.9.20.72 pod security-context-8991efff-fa25-420d-b6e5-12594ab962ad container test-container: <nil>
    STEP: delete the pod 01/28/23 00:56:51.868
    Jan 28 00:56:51.894: INFO: Waiting for pod security-context-8991efff-fa25-420d-b6e5-12594ab962ad to disappear
    Jan 28 00:56:51.904: INFO: Pod security-context-8991efff-fa25-420d-b6e5-12594ab962ad no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 28 00:56:51.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-7315" for this suite. 01/28/23 00:56:51.919
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:56:51.941
Jan 28 00:56:51.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubectl 01/28/23 00:56:51.944
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:56:51.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:56:51.99
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 01/28/23 00:56:52.002
Jan 28 00:56:52.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-2422 create -f -'
Jan 28 00:56:52.792: INFO: stderr: ""
Jan 28 00:56:52.792: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/28/23 00:56:52.792
Jan 28 00:56:53.804: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 00:56:53.804: INFO: Found 0 / 1
Jan 28 00:56:54.810: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 00:56:54.810: INFO: Found 0 / 1
Jan 28 00:56:55.827: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 00:56:55.827: INFO: Found 1 / 1
Jan 28 00:56:55.827: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/28/23 00:56:55.827
Jan 28 00:56:55.840: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 00:56:55.840: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 28 00:56:55.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-2422 patch pod agnhost-primary-45tj5 -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 28 00:56:55.970: INFO: stderr: ""
Jan 28 00:56:55.970: INFO: stdout: "pod/agnhost-primary-45tj5 patched\n"
STEP: checking annotations 01/28/23 00:56:55.97
Jan 28 00:56:55.980: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 00:56:55.980: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 28 00:56:55.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2422" for this suite. 01/28/23 00:56:55.995
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":110,"skipped":2087,"failed":0}
------------------------------
â€¢ [4.071 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:56:51.941
    Jan 28 00:56:51.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubectl 01/28/23 00:56:51.944
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:56:51.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:56:51.99
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 01/28/23 00:56:52.002
    Jan 28 00:56:52.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-2422 create -f -'
    Jan 28 00:56:52.792: INFO: stderr: ""
    Jan 28 00:56:52.792: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/28/23 00:56:52.792
    Jan 28 00:56:53.804: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 28 00:56:53.804: INFO: Found 0 / 1
    Jan 28 00:56:54.810: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 28 00:56:54.810: INFO: Found 0 / 1
    Jan 28 00:56:55.827: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 28 00:56:55.827: INFO: Found 1 / 1
    Jan 28 00:56:55.827: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/28/23 00:56:55.827
    Jan 28 00:56:55.840: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 28 00:56:55.840: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 28 00:56:55.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-2422 patch pod agnhost-primary-45tj5 -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan 28 00:56:55.970: INFO: stderr: ""
    Jan 28 00:56:55.970: INFO: stdout: "pod/agnhost-primary-45tj5 patched\n"
    STEP: checking annotations 01/28/23 00:56:55.97
    Jan 28 00:56:55.980: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 28 00:56:55.980: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 28 00:56:55.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2422" for this suite. 01/28/23 00:56:55.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:56:56.014
Jan 28 00:56:56.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename crd-publish-openapi 01/28/23 00:56:56.014
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:56:56.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:56:56.052
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Jan 28 00:56:56.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/28/23 00:56:59.705
Jan 28 00:56:59.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-916 --namespace=crd-publish-openapi-916 create -f -'
Jan 28 00:57:01.028: INFO: stderr: ""
Jan 28 00:57:01.028: INFO: stdout: "e2e-test-crd-publish-openapi-2209-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 28 00:57:01.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-916 --namespace=crd-publish-openapi-916 delete e2e-test-crd-publish-openapi-2209-crds test-cr'
Jan 28 00:57:01.199: INFO: stderr: ""
Jan 28 00:57:01.199: INFO: stdout: "e2e-test-crd-publish-openapi-2209-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 28 00:57:01.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-916 --namespace=crd-publish-openapi-916 apply -f -'
Jan 28 00:57:01.531: INFO: stderr: ""
Jan 28 00:57:01.531: INFO: stdout: "e2e-test-crd-publish-openapi-2209-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 28 00:57:01.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-916 --namespace=crd-publish-openapi-916 delete e2e-test-crd-publish-openapi-2209-crds test-cr'
Jan 28 00:57:01.676: INFO: stderr: ""
Jan 28 00:57:01.676: INFO: stdout: "e2e-test-crd-publish-openapi-2209-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/28/23 00:57:01.676
Jan 28 00:57:01.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-916 explain e2e-test-crd-publish-openapi-2209-crds'
Jan 28 00:57:02.406: INFO: stderr: ""
Jan 28 00:57:02.406: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2209-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 00:57:05.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-916" for this suite. 01/28/23 00:57:05.349
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":111,"skipped":2095,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.354 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:56:56.014
    Jan 28 00:56:56.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename crd-publish-openapi 01/28/23 00:56:56.014
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:56:56.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:56:56.052
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Jan 28 00:56:56.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/28/23 00:56:59.705
    Jan 28 00:56:59.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-916 --namespace=crd-publish-openapi-916 create -f -'
    Jan 28 00:57:01.028: INFO: stderr: ""
    Jan 28 00:57:01.028: INFO: stdout: "e2e-test-crd-publish-openapi-2209-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 28 00:57:01.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-916 --namespace=crd-publish-openapi-916 delete e2e-test-crd-publish-openapi-2209-crds test-cr'
    Jan 28 00:57:01.199: INFO: stderr: ""
    Jan 28 00:57:01.199: INFO: stdout: "e2e-test-crd-publish-openapi-2209-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan 28 00:57:01.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-916 --namespace=crd-publish-openapi-916 apply -f -'
    Jan 28 00:57:01.531: INFO: stderr: ""
    Jan 28 00:57:01.531: INFO: stdout: "e2e-test-crd-publish-openapi-2209-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 28 00:57:01.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-916 --namespace=crd-publish-openapi-916 delete e2e-test-crd-publish-openapi-2209-crds test-cr'
    Jan 28 00:57:01.676: INFO: stderr: ""
    Jan 28 00:57:01.676: INFO: stdout: "e2e-test-crd-publish-openapi-2209-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/28/23 00:57:01.676
    Jan 28 00:57:01.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-916 explain e2e-test-crd-publish-openapi-2209-crds'
    Jan 28 00:57:02.406: INFO: stderr: ""
    Jan 28 00:57:02.406: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2209-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 00:57:05.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-916" for this suite. 01/28/23 00:57:05.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:57:05.37
Jan 28 00:57:05.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename endpointslice 01/28/23 00:57:05.374
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:57:05.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:57:05.415
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 28 00:57:07.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8030" for this suite. 01/28/23 00:57:07.58
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":112,"skipped":2100,"failed":0}
------------------------------
â€¢ [2.228 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:57:05.37
    Jan 28 00:57:05.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename endpointslice 01/28/23 00:57:05.374
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:57:05.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:57:05.415
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 28 00:57:07.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-8030" for this suite. 01/28/23 00:57:07.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:57:07.601
Jan 28 00:57:07.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename secrets 01/28/23 00:57:07.603
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:57:07.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:57:07.646
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-c7865c3a-2554-439c-85b1-0a09d57ade71 01/28/23 00:57:07.654
STEP: Creating a pod to test consume secrets 01/28/23 00:57:07.665
Jan 28 00:57:07.682: INFO: Waiting up to 5m0s for pod "pod-secrets-bee02db0-927a-43c3-84a9-adf127ca020d" in namespace "secrets-9019" to be "Succeeded or Failed"
Jan 28 00:57:07.696: INFO: Pod "pod-secrets-bee02db0-927a-43c3-84a9-adf127ca020d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.434179ms
Jan 28 00:57:09.707: INFO: Pod "pod-secrets-bee02db0-927a-43c3-84a9-adf127ca020d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025068975s
Jan 28 00:57:11.707: INFO: Pod "pod-secrets-bee02db0-927a-43c3-84a9-adf127ca020d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024137992s
STEP: Saw pod success 01/28/23 00:57:11.707
Jan 28 00:57:11.707: INFO: Pod "pod-secrets-bee02db0-927a-43c3-84a9-adf127ca020d" satisfied condition "Succeeded or Failed"
Jan 28 00:57:11.716: INFO: Trying to get logs from node 10.9.20.126 pod pod-secrets-bee02db0-927a-43c3-84a9-adf127ca020d container secret-volume-test: <nil>
STEP: delete the pod 01/28/23 00:57:11.745
Jan 28 00:57:11.777: INFO: Waiting for pod pod-secrets-bee02db0-927a-43c3-84a9-adf127ca020d to disappear
Jan 28 00:57:11.787: INFO: Pod pod-secrets-bee02db0-927a-43c3-84a9-adf127ca020d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 28 00:57:11.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9019" for this suite. 01/28/23 00:57:11.8
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":113,"skipped":2114,"failed":0}
------------------------------
â€¢ [4.216 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:57:07.601
    Jan 28 00:57:07.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename secrets 01/28/23 00:57:07.603
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:57:07.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:57:07.646
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-c7865c3a-2554-439c-85b1-0a09d57ade71 01/28/23 00:57:07.654
    STEP: Creating a pod to test consume secrets 01/28/23 00:57:07.665
    Jan 28 00:57:07.682: INFO: Waiting up to 5m0s for pod "pod-secrets-bee02db0-927a-43c3-84a9-adf127ca020d" in namespace "secrets-9019" to be "Succeeded or Failed"
    Jan 28 00:57:07.696: INFO: Pod "pod-secrets-bee02db0-927a-43c3-84a9-adf127ca020d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.434179ms
    Jan 28 00:57:09.707: INFO: Pod "pod-secrets-bee02db0-927a-43c3-84a9-adf127ca020d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025068975s
    Jan 28 00:57:11.707: INFO: Pod "pod-secrets-bee02db0-927a-43c3-84a9-adf127ca020d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024137992s
    STEP: Saw pod success 01/28/23 00:57:11.707
    Jan 28 00:57:11.707: INFO: Pod "pod-secrets-bee02db0-927a-43c3-84a9-adf127ca020d" satisfied condition "Succeeded or Failed"
    Jan 28 00:57:11.716: INFO: Trying to get logs from node 10.9.20.126 pod pod-secrets-bee02db0-927a-43c3-84a9-adf127ca020d container secret-volume-test: <nil>
    STEP: delete the pod 01/28/23 00:57:11.745
    Jan 28 00:57:11.777: INFO: Waiting for pod pod-secrets-bee02db0-927a-43c3-84a9-adf127ca020d to disappear
    Jan 28 00:57:11.787: INFO: Pod pod-secrets-bee02db0-927a-43c3-84a9-adf127ca020d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 28 00:57:11.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9019" for this suite. 01/28/23 00:57:11.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:57:11.833
Jan 28 00:57:11.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename replicaset 01/28/23 00:57:11.834
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:57:11.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:57:11.872
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/28/23 00:57:11.881
Jan 28 00:57:11.902: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 28 00:57:16.915: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/28/23 00:57:16.915
STEP: getting scale subresource 01/28/23 00:57:16.916
STEP: updating a scale subresource 01/28/23 00:57:16.93
STEP: verifying the replicaset Spec.Replicas was modified 01/28/23 00:57:16.943
STEP: Patch a scale subresource 01/28/23 00:57:16.955
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 28 00:57:17.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2688" for this suite. 01/28/23 00:57:17.053
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":114,"skipped":2164,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.239 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:57:11.833
    Jan 28 00:57:11.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename replicaset 01/28/23 00:57:11.834
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:57:11.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:57:11.872
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/28/23 00:57:11.881
    Jan 28 00:57:11.902: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 28 00:57:16.915: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/28/23 00:57:16.915
    STEP: getting scale subresource 01/28/23 00:57:16.916
    STEP: updating a scale subresource 01/28/23 00:57:16.93
    STEP: verifying the replicaset Spec.Replicas was modified 01/28/23 00:57:16.943
    STEP: Patch a scale subresource 01/28/23 00:57:16.955
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 28 00:57:17.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2688" for this suite. 01/28/23 00:57:17.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:57:17.077
Jan 28 00:57:17.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename dns 01/28/23 00:57:17.08
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:57:17.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:57:17.13
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/28/23 00:57:17.14
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1896.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1896.svc.cluster.local; sleep 1; done
 01/28/23 00:57:17.154
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1896.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1896.svc.cluster.local; sleep 1; done
 01/28/23 00:57:17.155
STEP: creating a pod to probe DNS 01/28/23 00:57:17.155
STEP: submitting the pod to kubernetes 01/28/23 00:57:17.156
Jan 28 00:57:17.180: INFO: Waiting up to 15m0s for pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4" in namespace "dns-1896" to be "running"
Jan 28 00:57:17.207: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 26.586085ms
Jan 28 00:57:19.219: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039174969s
Jan 28 00:57:21.219: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038797908s
Jan 28 00:57:23.220: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039569689s
Jan 28 00:57:25.220: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.039938552s
Jan 28 00:57:27.219: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.038753455s
Jan 28 00:57:29.219: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.038419941s
Jan 28 00:57:31.218: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.037444682s
Jan 28 00:57:33.219: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.039253044s
Jan 28 00:57:35.219: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.03851518s
Jan 28 00:57:37.220: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Running", Reason="", readiness=true. Elapsed: 20.039393763s
Jan 28 00:57:37.220: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4" satisfied condition "running"
STEP: retrieving the pod 01/28/23 00:57:37.22
STEP: looking for the results for each expected name from probers 01/28/23 00:57:37.231
Jan 28 00:57:37.295: INFO: DNS probes using dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4 succeeded

STEP: deleting the pod 01/28/23 00:57:37.295
STEP: changing the externalName to bar.example.com 01/28/23 00:57:37.329
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1896.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1896.svc.cluster.local; sleep 1; done
 01/28/23 00:57:37.353
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1896.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1896.svc.cluster.local; sleep 1; done
 01/28/23 00:57:37.353
STEP: creating a second pod to probe DNS 01/28/23 00:57:37.353
STEP: submitting the pod to kubernetes 01/28/23 00:57:37.354
Jan 28 00:57:37.369: INFO: Waiting up to 15m0s for pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060" in namespace "dns-1896" to be "running"
Jan 28 00:57:37.383: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060": Phase="Pending", Reason="", readiness=false. Elapsed: 13.668068ms
Jan 28 00:57:39.404: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03479015s
Jan 28 00:57:41.396: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027289144s
Jan 28 00:57:43.395: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025771637s
Jan 28 00:57:45.396: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027043469s
Jan 28 00:57:47.394: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02494665s
Jan 28 00:57:49.395: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060": Phase="Pending", Reason="", readiness=false. Elapsed: 12.026095086s
Jan 28 00:57:51.396: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026991607s
Jan 28 00:57:53.395: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060": Phase="Running", Reason="", readiness=true. Elapsed: 16.025960611s
Jan 28 00:57:53.395: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060" satisfied condition "running"
STEP: retrieving the pod 01/28/23 00:57:53.395
STEP: looking for the results for each expected name from probers 01/28/23 00:57:53.405
Jan 28 00:57:53.474: INFO: DNS probes using dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060 succeeded

STEP: deleting the pod 01/28/23 00:57:53.474
STEP: changing the service to type=ClusterIP 01/28/23 00:57:53.498
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1896.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1896.svc.cluster.local; sleep 1; done
 01/28/23 00:57:53.542
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1896.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1896.svc.cluster.local; sleep 1; done
 01/28/23 00:57:53.543
STEP: creating a third pod to probe DNS 01/28/23 00:57:53.544
STEP: submitting the pod to kubernetes 01/28/23 00:57:53.556
Jan 28 00:57:53.579: INFO: Waiting up to 15m0s for pod "dns-test-b67d24e7-1e2c-4049-bc02-2decdb6d03e4" in namespace "dns-1896" to be "running"
Jan 28 00:57:53.592: INFO: Pod "dns-test-b67d24e7-1e2c-4049-bc02-2decdb6d03e4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.705342ms
Jan 28 00:57:55.605: INFO: Pod "dns-test-b67d24e7-1e2c-4049-bc02-2decdb6d03e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025472877s
Jan 28 00:57:57.605: INFO: Pod "dns-test-b67d24e7-1e2c-4049-bc02-2decdb6d03e4": Phase="Running", Reason="", readiness=true. Elapsed: 4.02572237s
Jan 28 00:57:57.605: INFO: Pod "dns-test-b67d24e7-1e2c-4049-bc02-2decdb6d03e4" satisfied condition "running"
STEP: retrieving the pod 01/28/23 00:57:57.605
STEP: looking for the results for each expected name from probers 01/28/23 00:57:57.616
Jan 28 00:57:57.677: INFO: DNS probes using dns-test-b67d24e7-1e2c-4049-bc02-2decdb6d03e4 succeeded

STEP: deleting the pod 01/28/23 00:57:57.677
STEP: deleting the test externalName service 01/28/23 00:57:57.702
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 28 00:57:57.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1896" for this suite. 01/28/23 00:57:57.762
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":115,"skipped":2171,"failed":0}
------------------------------
â€¢ [SLOW TEST] [40.703 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:57:17.077
    Jan 28 00:57:17.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename dns 01/28/23 00:57:17.08
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:57:17.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:57:17.13
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/28/23 00:57:17.14
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1896.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1896.svc.cluster.local; sleep 1; done
     01/28/23 00:57:17.154
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1896.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1896.svc.cluster.local; sleep 1; done
     01/28/23 00:57:17.155
    STEP: creating a pod to probe DNS 01/28/23 00:57:17.155
    STEP: submitting the pod to kubernetes 01/28/23 00:57:17.156
    Jan 28 00:57:17.180: INFO: Waiting up to 15m0s for pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4" in namespace "dns-1896" to be "running"
    Jan 28 00:57:17.207: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 26.586085ms
    Jan 28 00:57:19.219: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039174969s
    Jan 28 00:57:21.219: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038797908s
    Jan 28 00:57:23.220: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039569689s
    Jan 28 00:57:25.220: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.039938552s
    Jan 28 00:57:27.219: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.038753455s
    Jan 28 00:57:29.219: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.038419941s
    Jan 28 00:57:31.218: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.037444682s
    Jan 28 00:57:33.219: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.039253044s
    Jan 28 00:57:35.219: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.03851518s
    Jan 28 00:57:37.220: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4": Phase="Running", Reason="", readiness=true. Elapsed: 20.039393763s
    Jan 28 00:57:37.220: INFO: Pod "dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4" satisfied condition "running"
    STEP: retrieving the pod 01/28/23 00:57:37.22
    STEP: looking for the results for each expected name from probers 01/28/23 00:57:37.231
    Jan 28 00:57:37.295: INFO: DNS probes using dns-test-7f92ab78-97c2-4aab-920b-84cb1612cae4 succeeded

    STEP: deleting the pod 01/28/23 00:57:37.295
    STEP: changing the externalName to bar.example.com 01/28/23 00:57:37.329
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1896.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1896.svc.cluster.local; sleep 1; done
     01/28/23 00:57:37.353
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1896.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1896.svc.cluster.local; sleep 1; done
     01/28/23 00:57:37.353
    STEP: creating a second pod to probe DNS 01/28/23 00:57:37.353
    STEP: submitting the pod to kubernetes 01/28/23 00:57:37.354
    Jan 28 00:57:37.369: INFO: Waiting up to 15m0s for pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060" in namespace "dns-1896" to be "running"
    Jan 28 00:57:37.383: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060": Phase="Pending", Reason="", readiness=false. Elapsed: 13.668068ms
    Jan 28 00:57:39.404: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03479015s
    Jan 28 00:57:41.396: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027289144s
    Jan 28 00:57:43.395: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025771637s
    Jan 28 00:57:45.396: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027043469s
    Jan 28 00:57:47.394: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02494665s
    Jan 28 00:57:49.395: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060": Phase="Pending", Reason="", readiness=false. Elapsed: 12.026095086s
    Jan 28 00:57:51.396: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026991607s
    Jan 28 00:57:53.395: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060": Phase="Running", Reason="", readiness=true. Elapsed: 16.025960611s
    Jan 28 00:57:53.395: INFO: Pod "dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060" satisfied condition "running"
    STEP: retrieving the pod 01/28/23 00:57:53.395
    STEP: looking for the results for each expected name from probers 01/28/23 00:57:53.405
    Jan 28 00:57:53.474: INFO: DNS probes using dns-test-1a1f3136-8bc3-48f7-a5e4-31a5bb1e4060 succeeded

    STEP: deleting the pod 01/28/23 00:57:53.474
    STEP: changing the service to type=ClusterIP 01/28/23 00:57:53.498
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1896.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1896.svc.cluster.local; sleep 1; done
     01/28/23 00:57:53.542
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1896.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1896.svc.cluster.local; sleep 1; done
     01/28/23 00:57:53.543
    STEP: creating a third pod to probe DNS 01/28/23 00:57:53.544
    STEP: submitting the pod to kubernetes 01/28/23 00:57:53.556
    Jan 28 00:57:53.579: INFO: Waiting up to 15m0s for pod "dns-test-b67d24e7-1e2c-4049-bc02-2decdb6d03e4" in namespace "dns-1896" to be "running"
    Jan 28 00:57:53.592: INFO: Pod "dns-test-b67d24e7-1e2c-4049-bc02-2decdb6d03e4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.705342ms
    Jan 28 00:57:55.605: INFO: Pod "dns-test-b67d24e7-1e2c-4049-bc02-2decdb6d03e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025472877s
    Jan 28 00:57:57.605: INFO: Pod "dns-test-b67d24e7-1e2c-4049-bc02-2decdb6d03e4": Phase="Running", Reason="", readiness=true. Elapsed: 4.02572237s
    Jan 28 00:57:57.605: INFO: Pod "dns-test-b67d24e7-1e2c-4049-bc02-2decdb6d03e4" satisfied condition "running"
    STEP: retrieving the pod 01/28/23 00:57:57.605
    STEP: looking for the results for each expected name from probers 01/28/23 00:57:57.616
    Jan 28 00:57:57.677: INFO: DNS probes using dns-test-b67d24e7-1e2c-4049-bc02-2decdb6d03e4 succeeded

    STEP: deleting the pod 01/28/23 00:57:57.677
    STEP: deleting the test externalName service 01/28/23 00:57:57.702
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 28 00:57:57.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1896" for this suite. 01/28/23 00:57:57.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:57:57.783
Jan 28 00:57:57.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 00:57:57.784
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:57:57.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:57:57.821
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-2b3d56d9-3b2a-4828-a559-7bc4c972ba58 01/28/23 00:57:57.844
STEP: Creating the pod 01/28/23 00:57:57.855
Jan 28 00:57:57.875: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a3ee3884-37d7-44fb-ba75-f08cafdf764a" in namespace "projected-5808" to be "running and ready"
Jan 28 00:57:57.887: INFO: Pod "pod-projected-configmaps-a3ee3884-37d7-44fb-ba75-f08cafdf764a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.371187ms
Jan 28 00:57:57.887: INFO: The phase of Pod pod-projected-configmaps-a3ee3884-37d7-44fb-ba75-f08cafdf764a is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:57:59.899: INFO: Pod "pod-projected-configmaps-a3ee3884-37d7-44fb-ba75-f08cafdf764a": Phase="Running", Reason="", readiness=true. Elapsed: 2.023406284s
Jan 28 00:57:59.899: INFO: The phase of Pod pod-projected-configmaps-a3ee3884-37d7-44fb-ba75-f08cafdf764a is Running (Ready = true)
Jan 28 00:57:59.899: INFO: Pod "pod-projected-configmaps-a3ee3884-37d7-44fb-ba75-f08cafdf764a" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-2b3d56d9-3b2a-4828-a559-7bc4c972ba58 01/28/23 00:57:59.933
STEP: waiting to observe update in volume 01/28/23 00:57:59.944
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 28 00:58:02.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5808" for this suite. 01/28/23 00:58:02.016
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":116,"skipped":2193,"failed":0}
------------------------------
â€¢ [4.250 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:57:57.783
    Jan 28 00:57:57.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 00:57:57.784
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:57:57.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:57:57.821
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-2b3d56d9-3b2a-4828-a559-7bc4c972ba58 01/28/23 00:57:57.844
    STEP: Creating the pod 01/28/23 00:57:57.855
    Jan 28 00:57:57.875: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a3ee3884-37d7-44fb-ba75-f08cafdf764a" in namespace "projected-5808" to be "running and ready"
    Jan 28 00:57:57.887: INFO: Pod "pod-projected-configmaps-a3ee3884-37d7-44fb-ba75-f08cafdf764a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.371187ms
    Jan 28 00:57:57.887: INFO: The phase of Pod pod-projected-configmaps-a3ee3884-37d7-44fb-ba75-f08cafdf764a is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:57:59.899: INFO: Pod "pod-projected-configmaps-a3ee3884-37d7-44fb-ba75-f08cafdf764a": Phase="Running", Reason="", readiness=true. Elapsed: 2.023406284s
    Jan 28 00:57:59.899: INFO: The phase of Pod pod-projected-configmaps-a3ee3884-37d7-44fb-ba75-f08cafdf764a is Running (Ready = true)
    Jan 28 00:57:59.899: INFO: Pod "pod-projected-configmaps-a3ee3884-37d7-44fb-ba75-f08cafdf764a" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-2b3d56d9-3b2a-4828-a559-7bc4c972ba58 01/28/23 00:57:59.933
    STEP: waiting to observe update in volume 01/28/23 00:57:59.944
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 28 00:58:02.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5808" for this suite. 01/28/23 00:58:02.016
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:58:02.047
Jan 28 00:58:02.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename namespaces 01/28/23 00:58:02.048
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:58:02.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:58:02.092
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 01/28/23 00:58:02.101
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:58:02.131
STEP: Creating a pod in the namespace 01/28/23 00:58:02.139
STEP: Waiting for the pod to have running status 01/28/23 00:58:02.162
Jan 28 00:58:02.163: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3869" to be "running"
Jan 28 00:58:02.172: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.283628ms
Jan 28 00:58:04.194: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031551406s
Jan 28 00:58:06.183: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.020542549s
Jan 28 00:58:06.183: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/28/23 00:58:06.183
STEP: Waiting for the namespace to be removed. 01/28/23 00:58:06.206
STEP: Recreating the namespace 01/28/23 00:58:18.216
STEP: Verifying there are no pods in the namespace 01/28/23 00:58:18.244
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 28 00:58:18.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4166" for this suite. 01/28/23 00:58:18.267
STEP: Destroying namespace "nsdeletetest-3869" for this suite. 01/28/23 00:58:18.284
Jan 28 00:58:18.294: INFO: Namespace nsdeletetest-3869 was already deleted
STEP: Destroying namespace "nsdeletetest-6364" for this suite. 01/28/23 00:58:18.294
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":117,"skipped":2228,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.269 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:58:02.047
    Jan 28 00:58:02.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename namespaces 01/28/23 00:58:02.048
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:58:02.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:58:02.092
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 01/28/23 00:58:02.101
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:58:02.131
    STEP: Creating a pod in the namespace 01/28/23 00:58:02.139
    STEP: Waiting for the pod to have running status 01/28/23 00:58:02.162
    Jan 28 00:58:02.163: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3869" to be "running"
    Jan 28 00:58:02.172: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.283628ms
    Jan 28 00:58:04.194: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031551406s
    Jan 28 00:58:06.183: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.020542549s
    Jan 28 00:58:06.183: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/28/23 00:58:06.183
    STEP: Waiting for the namespace to be removed. 01/28/23 00:58:06.206
    STEP: Recreating the namespace 01/28/23 00:58:18.216
    STEP: Verifying there are no pods in the namespace 01/28/23 00:58:18.244
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 00:58:18.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-4166" for this suite. 01/28/23 00:58:18.267
    STEP: Destroying namespace "nsdeletetest-3869" for this suite. 01/28/23 00:58:18.284
    Jan 28 00:58:18.294: INFO: Namespace nsdeletetest-3869 was already deleted
    STEP: Destroying namespace "nsdeletetest-6364" for this suite. 01/28/23 00:58:18.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:58:18.318
Jan 28 00:58:18.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename init-container 01/28/23 00:58:18.32
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:58:18.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:58:18.372
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 01/28/23 00:58:18.381
Jan 28 00:58:18.382: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 28 00:58:24.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9145" for this suite. 01/28/23 00:58:24.771
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":118,"skipped":2233,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.476 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:58:18.318
    Jan 28 00:58:18.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename init-container 01/28/23 00:58:18.32
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:58:18.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:58:18.372
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 01/28/23 00:58:18.381
    Jan 28 00:58:18.382: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 28 00:58:24.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9145" for this suite. 01/28/23 00:58:24.771
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:58:24.808
Jan 28 00:58:24.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename emptydir 01/28/23 00:58:24.81
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:58:24.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:58:24.858
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 01/28/23 00:58:24.866
Jan 28 00:58:24.886: INFO: Waiting up to 5m0s for pod "pod-bd2d56ed-fb1b-4ce2-b745-fc61a8b4995d" in namespace "emptydir-2310" to be "Succeeded or Failed"
Jan 28 00:58:24.897: INFO: Pod "pod-bd2d56ed-fb1b-4ce2-b745-fc61a8b4995d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.035917ms
Jan 28 00:58:26.908: INFO: Pod "pod-bd2d56ed-fb1b-4ce2-b745-fc61a8b4995d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021161816s
Jan 28 00:58:28.909: INFO: Pod "pod-bd2d56ed-fb1b-4ce2-b745-fc61a8b4995d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0230542s
STEP: Saw pod success 01/28/23 00:58:28.909
Jan 28 00:58:28.910: INFO: Pod "pod-bd2d56ed-fb1b-4ce2-b745-fc61a8b4995d" satisfied condition "Succeeded or Failed"
Jan 28 00:58:28.921: INFO: Trying to get logs from node 10.9.20.126 pod pod-bd2d56ed-fb1b-4ce2-b745-fc61a8b4995d container test-container: <nil>
STEP: delete the pod 01/28/23 00:58:28.946
Jan 28 00:58:28.977: INFO: Waiting for pod pod-bd2d56ed-fb1b-4ce2-b745-fc61a8b4995d to disappear
Jan 28 00:58:28.989: INFO: Pod pod-bd2d56ed-fb1b-4ce2-b745-fc61a8b4995d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 28 00:58:28.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2310" for this suite. 01/28/23 00:58:29.01
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":119,"skipped":2307,"failed":0}
------------------------------
â€¢ [4.220 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:58:24.808
    Jan 28 00:58:24.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename emptydir 01/28/23 00:58:24.81
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:58:24.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:58:24.858
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/28/23 00:58:24.866
    Jan 28 00:58:24.886: INFO: Waiting up to 5m0s for pod "pod-bd2d56ed-fb1b-4ce2-b745-fc61a8b4995d" in namespace "emptydir-2310" to be "Succeeded or Failed"
    Jan 28 00:58:24.897: INFO: Pod "pod-bd2d56ed-fb1b-4ce2-b745-fc61a8b4995d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.035917ms
    Jan 28 00:58:26.908: INFO: Pod "pod-bd2d56ed-fb1b-4ce2-b745-fc61a8b4995d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021161816s
    Jan 28 00:58:28.909: INFO: Pod "pod-bd2d56ed-fb1b-4ce2-b745-fc61a8b4995d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0230542s
    STEP: Saw pod success 01/28/23 00:58:28.909
    Jan 28 00:58:28.910: INFO: Pod "pod-bd2d56ed-fb1b-4ce2-b745-fc61a8b4995d" satisfied condition "Succeeded or Failed"
    Jan 28 00:58:28.921: INFO: Trying to get logs from node 10.9.20.126 pod pod-bd2d56ed-fb1b-4ce2-b745-fc61a8b4995d container test-container: <nil>
    STEP: delete the pod 01/28/23 00:58:28.946
    Jan 28 00:58:28.977: INFO: Waiting for pod pod-bd2d56ed-fb1b-4ce2-b745-fc61a8b4995d to disappear
    Jan 28 00:58:28.989: INFO: Pod pod-bd2d56ed-fb1b-4ce2-b745-fc61a8b4995d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 28 00:58:28.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2310" for this suite. 01/28/23 00:58:29.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:58:29.032
Jan 28 00:58:29.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename containers 01/28/23 00:58:29.035
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:58:29.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:58:29.074
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Jan 28 00:58:29.103: INFO: Waiting up to 5m0s for pod "client-containers-ef94f41a-4707-4cdb-8583-e1e3529b08a6" in namespace "containers-2171" to be "running"
Jan 28 00:58:29.113: INFO: Pod "client-containers-ef94f41a-4707-4cdb-8583-e1e3529b08a6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.043896ms
Jan 28 00:58:31.124: INFO: Pod "client-containers-ef94f41a-4707-4cdb-8583-e1e3529b08a6": Phase="Running", Reason="", readiness=true. Elapsed: 2.021645764s
Jan 28 00:58:31.124: INFO: Pod "client-containers-ef94f41a-4707-4cdb-8583-e1e3529b08a6" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 28 00:58:31.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2171" for this suite. 01/28/23 00:58:31.17
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":120,"skipped":2342,"failed":0}
------------------------------
â€¢ [2.155 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:58:29.032
    Jan 28 00:58:29.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename containers 01/28/23 00:58:29.035
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:58:29.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:58:29.074
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Jan 28 00:58:29.103: INFO: Waiting up to 5m0s for pod "client-containers-ef94f41a-4707-4cdb-8583-e1e3529b08a6" in namespace "containers-2171" to be "running"
    Jan 28 00:58:29.113: INFO: Pod "client-containers-ef94f41a-4707-4cdb-8583-e1e3529b08a6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.043896ms
    Jan 28 00:58:31.124: INFO: Pod "client-containers-ef94f41a-4707-4cdb-8583-e1e3529b08a6": Phase="Running", Reason="", readiness=true. Elapsed: 2.021645764s
    Jan 28 00:58:31.124: INFO: Pod "client-containers-ef94f41a-4707-4cdb-8583-e1e3529b08a6" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 28 00:58:31.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-2171" for this suite. 01/28/23 00:58:31.17
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:58:31.196
Jan 28 00:58:31.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename replication-controller 01/28/23 00:58:31.199
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:58:31.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:58:31.237
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 01/28/23 00:58:31.246
Jan 28 00:58:31.266: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-4501" to be "running and ready"
Jan 28 00:58:31.277: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 10.742031ms
Jan 28 00:58:31.277: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:58:33.288: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.022088989s
Jan 28 00:58:33.288: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan 28 00:58:33.288: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/28/23 00:58:33.298
STEP: Then the orphan pod is adopted 01/28/23 00:58:33.311
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 28 00:58:34.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4501" for this suite. 01/28/23 00:58:34.346
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":121,"skipped":2342,"failed":0}
------------------------------
â€¢ [3.168 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:58:31.196
    Jan 28 00:58:31.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename replication-controller 01/28/23 00:58:31.199
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:58:31.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:58:31.237
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/28/23 00:58:31.246
    Jan 28 00:58:31.266: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-4501" to be "running and ready"
    Jan 28 00:58:31.277: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 10.742031ms
    Jan 28 00:58:31.277: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 00:58:33.288: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.022088989s
    Jan 28 00:58:33.288: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan 28 00:58:33.288: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/28/23 00:58:33.298
    STEP: Then the orphan pod is adopted 01/28/23 00:58:33.311
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 28 00:58:34.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4501" for this suite. 01/28/23 00:58:34.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 00:58:34.38
Jan 28 00:58:34.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename container-probe 01/28/23 00:58:34.381
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:58:34.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:58:34.422
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2 in namespace container-probe-7174 01/28/23 00:58:34.431
Jan 28 00:58:34.450: INFO: Waiting up to 5m0s for pod "liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2" in namespace "container-probe-7174" to be "not pending"
Jan 28 00:58:34.479: INFO: Pod "liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2": Phase="Pending", Reason="", readiness=false. Elapsed: 29.257374ms
Jan 28 00:58:36.491: INFO: Pod "liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040974537s
Jan 28 00:58:38.492: INFO: Pod "liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2": Phase="Running", Reason="", readiness=true. Elapsed: 4.041423951s
Jan 28 00:58:38.492: INFO: Pod "liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2" satisfied condition "not pending"
Jan 28 00:58:38.492: INFO: Started pod liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2 in namespace container-probe-7174
STEP: checking the pod's current state and verifying that restartCount is present 01/28/23 00:58:38.492
Jan 28 00:58:38.502: INFO: Initial restart count of pod liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2 is 0
Jan 28 00:58:56.612: INFO: Restart count of pod container-probe-7174/liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2 is now 1 (18.110060734s elapsed)
Jan 28 00:59:16.730: INFO: Restart count of pod container-probe-7174/liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2 is now 2 (38.228262349s elapsed)
Jan 28 00:59:36.887: INFO: Restart count of pod container-probe-7174/liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2 is now 3 (58.384438127s elapsed)
Jan 28 00:59:57.003: INFO: Restart count of pod container-probe-7174/liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2 is now 4 (1m18.500722815s elapsed)
Jan 28 01:00:59.387: INFO: Restart count of pod container-probe-7174/liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2 is now 5 (2m20.885266455s elapsed)
STEP: deleting the pod 01/28/23 01:00:59.388
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 28 01:00:59.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7174" for this suite. 01/28/23 01:00:59.447
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":122,"skipped":2378,"failed":0}
------------------------------
â€¢ [SLOW TEST] [145.092 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 00:58:34.38
    Jan 28 00:58:34.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename container-probe 01/28/23 00:58:34.381
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 00:58:34.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 00:58:34.422
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2 in namespace container-probe-7174 01/28/23 00:58:34.431
    Jan 28 00:58:34.450: INFO: Waiting up to 5m0s for pod "liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2" in namespace "container-probe-7174" to be "not pending"
    Jan 28 00:58:34.479: INFO: Pod "liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2": Phase="Pending", Reason="", readiness=false. Elapsed: 29.257374ms
    Jan 28 00:58:36.491: INFO: Pod "liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040974537s
    Jan 28 00:58:38.492: INFO: Pod "liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2": Phase="Running", Reason="", readiness=true. Elapsed: 4.041423951s
    Jan 28 00:58:38.492: INFO: Pod "liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2" satisfied condition "not pending"
    Jan 28 00:58:38.492: INFO: Started pod liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2 in namespace container-probe-7174
    STEP: checking the pod's current state and verifying that restartCount is present 01/28/23 00:58:38.492
    Jan 28 00:58:38.502: INFO: Initial restart count of pod liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2 is 0
    Jan 28 00:58:56.612: INFO: Restart count of pod container-probe-7174/liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2 is now 1 (18.110060734s elapsed)
    Jan 28 00:59:16.730: INFO: Restart count of pod container-probe-7174/liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2 is now 2 (38.228262349s elapsed)
    Jan 28 00:59:36.887: INFO: Restart count of pod container-probe-7174/liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2 is now 3 (58.384438127s elapsed)
    Jan 28 00:59:57.003: INFO: Restart count of pod container-probe-7174/liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2 is now 4 (1m18.500722815s elapsed)
    Jan 28 01:00:59.387: INFO: Restart count of pod container-probe-7174/liveness-e2ce51ae-8c05-4800-9af5-2118fad4b1f2 is now 5 (2m20.885266455s elapsed)
    STEP: deleting the pod 01/28/23 01:00:59.388
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 28 01:00:59.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7174" for this suite. 01/28/23 01:00:59.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:00:59.473
Jan 28 01:00:59.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 01:00:59.476
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:00:59.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:00:59.525
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-9bf1dbbd-6ed1-4b6a-a0cf-de6e762b330e 01/28/23 01:00:59.536
STEP: Creating a pod to test consume secrets 01/28/23 01:00:59.554
Jan 28 01:00:59.574: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d5f0ffed-0d04-4a09-88f8-fe89474ea6d3" in namespace "projected-8675" to be "Succeeded or Failed"
Jan 28 01:00:59.584: INFO: Pod "pod-projected-secrets-d5f0ffed-0d04-4a09-88f8-fe89474ea6d3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.302499ms
Jan 28 01:01:01.595: INFO: Pod "pod-projected-secrets-d5f0ffed-0d04-4a09-88f8-fe89474ea6d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021543068s
Jan 28 01:01:03.595: INFO: Pod "pod-projected-secrets-d5f0ffed-0d04-4a09-88f8-fe89474ea6d3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021478737s
Jan 28 01:01:05.597: INFO: Pod "pod-projected-secrets-d5f0ffed-0d04-4a09-88f8-fe89474ea6d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023157032s
STEP: Saw pod success 01/28/23 01:01:05.597
Jan 28 01:01:05.597: INFO: Pod "pod-projected-secrets-d5f0ffed-0d04-4a09-88f8-fe89474ea6d3" satisfied condition "Succeeded or Failed"
Jan 28 01:01:05.607: INFO: Trying to get logs from node 10.9.20.72 pod pod-projected-secrets-d5f0ffed-0d04-4a09-88f8-fe89474ea6d3 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/28/23 01:01:05.693
Jan 28 01:01:05.724: INFO: Waiting for pod pod-projected-secrets-d5f0ffed-0d04-4a09-88f8-fe89474ea6d3 to disappear
Jan 28 01:01:05.733: INFO: Pod pod-projected-secrets-d5f0ffed-0d04-4a09-88f8-fe89474ea6d3 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 28 01:01:05.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8675" for this suite. 01/28/23 01:01:05.754
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":123,"skipped":2383,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.300 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:00:59.473
    Jan 28 01:00:59.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 01:00:59.476
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:00:59.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:00:59.525
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-9bf1dbbd-6ed1-4b6a-a0cf-de6e762b330e 01/28/23 01:00:59.536
    STEP: Creating a pod to test consume secrets 01/28/23 01:00:59.554
    Jan 28 01:00:59.574: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d5f0ffed-0d04-4a09-88f8-fe89474ea6d3" in namespace "projected-8675" to be "Succeeded or Failed"
    Jan 28 01:00:59.584: INFO: Pod "pod-projected-secrets-d5f0ffed-0d04-4a09-88f8-fe89474ea6d3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.302499ms
    Jan 28 01:01:01.595: INFO: Pod "pod-projected-secrets-d5f0ffed-0d04-4a09-88f8-fe89474ea6d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021543068s
    Jan 28 01:01:03.595: INFO: Pod "pod-projected-secrets-d5f0ffed-0d04-4a09-88f8-fe89474ea6d3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021478737s
    Jan 28 01:01:05.597: INFO: Pod "pod-projected-secrets-d5f0ffed-0d04-4a09-88f8-fe89474ea6d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023157032s
    STEP: Saw pod success 01/28/23 01:01:05.597
    Jan 28 01:01:05.597: INFO: Pod "pod-projected-secrets-d5f0ffed-0d04-4a09-88f8-fe89474ea6d3" satisfied condition "Succeeded or Failed"
    Jan 28 01:01:05.607: INFO: Trying to get logs from node 10.9.20.72 pod pod-projected-secrets-d5f0ffed-0d04-4a09-88f8-fe89474ea6d3 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/28/23 01:01:05.693
    Jan 28 01:01:05.724: INFO: Waiting for pod pod-projected-secrets-d5f0ffed-0d04-4a09-88f8-fe89474ea6d3 to disappear
    Jan 28 01:01:05.733: INFO: Pod pod-projected-secrets-d5f0ffed-0d04-4a09-88f8-fe89474ea6d3 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 28 01:01:05.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8675" for this suite. 01/28/23 01:01:05.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:01:05.78
Jan 28 01:01:05.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename var-expansion 01/28/23 01:01:05.783
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:01:05.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:01:05.825
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Jan 28 01:01:05.851: INFO: Waiting up to 2m0s for pod "var-expansion-9bad8c0b-73ef-4a4f-88f2-3abed338becb" in namespace "var-expansion-4795" to be "container 0 failed with reason CreateContainerConfigError"
Jan 28 01:01:05.861: INFO: Pod "var-expansion-9bad8c0b-73ef-4a4f-88f2-3abed338becb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.413729ms
Jan 28 01:01:07.873: INFO: Pod "var-expansion-9bad8c0b-73ef-4a4f-88f2-3abed338becb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022099795s
Jan 28 01:01:07.873: INFO: Pod "var-expansion-9bad8c0b-73ef-4a4f-88f2-3abed338becb" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 28 01:01:07.873: INFO: Deleting pod "var-expansion-9bad8c0b-73ef-4a4f-88f2-3abed338becb" in namespace "var-expansion-4795"
Jan 28 01:01:07.892: INFO: Wait up to 5m0s for pod "var-expansion-9bad8c0b-73ef-4a4f-88f2-3abed338becb" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 28 01:01:11.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4795" for this suite. 01/28/23 01:01:11.931
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":124,"skipped":2389,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.170 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:01:05.78
    Jan 28 01:01:05.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename var-expansion 01/28/23 01:01:05.783
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:01:05.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:01:05.825
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Jan 28 01:01:05.851: INFO: Waiting up to 2m0s for pod "var-expansion-9bad8c0b-73ef-4a4f-88f2-3abed338becb" in namespace "var-expansion-4795" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 28 01:01:05.861: INFO: Pod "var-expansion-9bad8c0b-73ef-4a4f-88f2-3abed338becb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.413729ms
    Jan 28 01:01:07.873: INFO: Pod "var-expansion-9bad8c0b-73ef-4a4f-88f2-3abed338becb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022099795s
    Jan 28 01:01:07.873: INFO: Pod "var-expansion-9bad8c0b-73ef-4a4f-88f2-3abed338becb" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 28 01:01:07.873: INFO: Deleting pod "var-expansion-9bad8c0b-73ef-4a4f-88f2-3abed338becb" in namespace "var-expansion-4795"
    Jan 28 01:01:07.892: INFO: Wait up to 5m0s for pod "var-expansion-9bad8c0b-73ef-4a4f-88f2-3abed338becb" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 28 01:01:11.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4795" for this suite. 01/28/23 01:01:11.931
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:01:11.958
Jan 28 01:01:11.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename downward-api 01/28/23 01:01:11.961
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:01:11.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:01:12.001
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 01/28/23 01:01:12.011
Jan 28 01:01:12.030: INFO: Waiting up to 5m0s for pod "downward-api-8ebb84d6-f579-4310-9762-c0466a94559e" in namespace "downward-api-6946" to be "Succeeded or Failed"
Jan 28 01:01:12.041: INFO: Pod "downward-api-8ebb84d6-f579-4310-9762-c0466a94559e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.213217ms
Jan 28 01:01:14.052: INFO: Pod "downward-api-8ebb84d6-f579-4310-9762-c0466a94559e": Phase="Running", Reason="", readiness=true. Elapsed: 2.021562359s
Jan 28 01:01:16.052: INFO: Pod "downward-api-8ebb84d6-f579-4310-9762-c0466a94559e": Phase="Running", Reason="", readiness=false. Elapsed: 4.021489048s
Jan 28 01:01:18.051: INFO: Pod "downward-api-8ebb84d6-f579-4310-9762-c0466a94559e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021108474s
STEP: Saw pod success 01/28/23 01:01:18.051
Jan 28 01:01:18.052: INFO: Pod "downward-api-8ebb84d6-f579-4310-9762-c0466a94559e" satisfied condition "Succeeded or Failed"
Jan 28 01:01:18.062: INFO: Trying to get logs from node 10.9.20.72 pod downward-api-8ebb84d6-f579-4310-9762-c0466a94559e container dapi-container: <nil>
STEP: delete the pod 01/28/23 01:01:18.097
Jan 28 01:01:18.133: INFO: Waiting for pod downward-api-8ebb84d6-f579-4310-9762-c0466a94559e to disappear
Jan 28 01:01:18.143: INFO: Pod downward-api-8ebb84d6-f579-4310-9762-c0466a94559e no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 28 01:01:18.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6946" for this suite. 01/28/23 01:01:18.159
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":125,"skipped":2402,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.221 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:01:11.958
    Jan 28 01:01:11.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename downward-api 01/28/23 01:01:11.961
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:01:11.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:01:12.001
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 01/28/23 01:01:12.011
    Jan 28 01:01:12.030: INFO: Waiting up to 5m0s for pod "downward-api-8ebb84d6-f579-4310-9762-c0466a94559e" in namespace "downward-api-6946" to be "Succeeded or Failed"
    Jan 28 01:01:12.041: INFO: Pod "downward-api-8ebb84d6-f579-4310-9762-c0466a94559e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.213217ms
    Jan 28 01:01:14.052: INFO: Pod "downward-api-8ebb84d6-f579-4310-9762-c0466a94559e": Phase="Running", Reason="", readiness=true. Elapsed: 2.021562359s
    Jan 28 01:01:16.052: INFO: Pod "downward-api-8ebb84d6-f579-4310-9762-c0466a94559e": Phase="Running", Reason="", readiness=false. Elapsed: 4.021489048s
    Jan 28 01:01:18.051: INFO: Pod "downward-api-8ebb84d6-f579-4310-9762-c0466a94559e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021108474s
    STEP: Saw pod success 01/28/23 01:01:18.051
    Jan 28 01:01:18.052: INFO: Pod "downward-api-8ebb84d6-f579-4310-9762-c0466a94559e" satisfied condition "Succeeded or Failed"
    Jan 28 01:01:18.062: INFO: Trying to get logs from node 10.9.20.72 pod downward-api-8ebb84d6-f579-4310-9762-c0466a94559e container dapi-container: <nil>
    STEP: delete the pod 01/28/23 01:01:18.097
    Jan 28 01:01:18.133: INFO: Waiting for pod downward-api-8ebb84d6-f579-4310-9762-c0466a94559e to disappear
    Jan 28 01:01:18.143: INFO: Pod downward-api-8ebb84d6-f579-4310-9762-c0466a94559e no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 28 01:01:18.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6946" for this suite. 01/28/23 01:01:18.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:01:18.195
Jan 28 01:01:18.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename dns 01/28/23 01:01:18.197
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:01:18.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:01:18.237
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/28/23 01:01:18.245
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-408.svc.cluster.local;sleep 1; done
 01/28/23 01:01:18.258
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-408.svc.cluster.local;sleep 1; done
 01/28/23 01:01:18.258
STEP: creating a pod to probe DNS 01/28/23 01:01:18.258
STEP: submitting the pod to kubernetes 01/28/23 01:01:18.259
Jan 28 01:01:18.290: INFO: Waiting up to 15m0s for pod "dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a" in namespace "dns-408" to be "running"
Jan 28 01:01:18.303: INFO: Pod "dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.539439ms
Jan 28 01:01:20.315: INFO: Pod "dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024830835s
Jan 28 01:01:22.314: INFO: Pod "dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a": Phase="Running", Reason="", readiness=true. Elapsed: 4.024364875s
Jan 28 01:01:22.314: INFO: Pod "dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a" satisfied condition "running"
STEP: retrieving the pod 01/28/23 01:01:22.314
STEP: looking for the results for each expected name from probers 01/28/23 01:01:22.324
Jan 28 01:01:22.394: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local from pod dns-408/dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a: the server could not find the requested resource (get pods dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a)
Jan 28 01:01:22.425: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-408.svc.cluster.local from pod dns-408/dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a: the server could not find the requested resource (get pods dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a)
Jan 28 01:01:22.441: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local from pod dns-408/dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a: the server could not find the requested resource (get pods dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a)
Jan 28 01:01:22.474: INFO: Unable to read jessie_udp@dns-test-service-2.dns-408.svc.cluster.local from pod dns-408/dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a: the server could not find the requested resource (get pods dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a)
Jan 28 01:01:22.493: INFO: Lookups using dns-408/dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a failed for: [wheezy_tcp@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-408.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local jessie_udp@dns-test-service-2.dns-408.svc.cluster.local]

Jan 28 01:01:27.639: INFO: DNS probes using dns-408/dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a succeeded

STEP: deleting the pod 01/28/23 01:01:27.639
STEP: deleting the test headless service 01/28/23 01:01:27.673
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 28 01:01:27.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-408" for this suite. 01/28/23 01:01:27.722
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":126,"skipped":2421,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.548 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:01:18.195
    Jan 28 01:01:18.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename dns 01/28/23 01:01:18.197
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:01:18.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:01:18.237
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/28/23 01:01:18.245
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-408.svc.cluster.local;sleep 1; done
     01/28/23 01:01:18.258
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-408.svc.cluster.local;sleep 1; done
     01/28/23 01:01:18.258
    STEP: creating a pod to probe DNS 01/28/23 01:01:18.258
    STEP: submitting the pod to kubernetes 01/28/23 01:01:18.259
    Jan 28 01:01:18.290: INFO: Waiting up to 15m0s for pod "dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a" in namespace "dns-408" to be "running"
    Jan 28 01:01:18.303: INFO: Pod "dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.539439ms
    Jan 28 01:01:20.315: INFO: Pod "dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024830835s
    Jan 28 01:01:22.314: INFO: Pod "dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a": Phase="Running", Reason="", readiness=true. Elapsed: 4.024364875s
    Jan 28 01:01:22.314: INFO: Pod "dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a" satisfied condition "running"
    STEP: retrieving the pod 01/28/23 01:01:22.314
    STEP: looking for the results for each expected name from probers 01/28/23 01:01:22.324
    Jan 28 01:01:22.394: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local from pod dns-408/dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a: the server could not find the requested resource (get pods dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a)
    Jan 28 01:01:22.425: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-408.svc.cluster.local from pod dns-408/dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a: the server could not find the requested resource (get pods dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a)
    Jan 28 01:01:22.441: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local from pod dns-408/dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a: the server could not find the requested resource (get pods dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a)
    Jan 28 01:01:22.474: INFO: Unable to read jessie_udp@dns-test-service-2.dns-408.svc.cluster.local from pod dns-408/dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a: the server could not find the requested resource (get pods dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a)
    Jan 28 01:01:22.493: INFO: Lookups using dns-408/dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a failed for: [wheezy_tcp@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-408.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local jessie_udp@dns-test-service-2.dns-408.svc.cluster.local]

    Jan 28 01:01:27.639: INFO: DNS probes using dns-408/dns-test-004c49b9-1c00-4bed-b3d0-d1a52554146a succeeded

    STEP: deleting the pod 01/28/23 01:01:27.639
    STEP: deleting the test headless service 01/28/23 01:01:27.673
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 28 01:01:27.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-408" for this suite. 01/28/23 01:01:27.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:01:27.751
Jan 28 01:01:27.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename pods 01/28/23 01:01:27.753
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:01:27.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:01:27.795
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 01/28/23 01:01:27.805
STEP: submitting the pod to kubernetes 01/28/23 01:01:27.806
Jan 28 01:01:27.826: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4" in namespace "pods-2123" to be "running and ready"
Jan 28 01:01:27.836: INFO: Pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.79163ms
Jan 28 01:01:27.836: INFO: The phase of Pod pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:01:29.846: INFO: Pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4": Phase="Running", Reason="", readiness=true. Elapsed: 2.020024075s
Jan 28 01:01:29.846: INFO: The phase of Pod pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4 is Running (Ready = true)
Jan 28 01:01:29.846: INFO: Pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/28/23 01:01:29.855
STEP: updating the pod 01/28/23 01:01:29.866
Jan 28 01:01:30.392: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4"
Jan 28 01:01:30.392: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4" in namespace "pods-2123" to be "terminated with reason DeadlineExceeded"
Jan 28 01:01:30.403: INFO: Pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4": Phase="Running", Reason="", readiness=true. Elapsed: 10.834725ms
Jan 28 01:01:32.415: INFO: Pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4": Phase="Running", Reason="", readiness=true. Elapsed: 2.022947328s
Jan 28 01:01:34.413: INFO: Pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4": Phase="Running", Reason="", readiness=false. Elapsed: 4.021252121s
Jan 28 01:01:36.415: INFO: Pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.0230352s
Jan 28 01:01:36.415: INFO: Pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 28 01:01:36.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2123" for this suite. 01/28/23 01:01:36.432
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":127,"skipped":2427,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.699 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:01:27.751
    Jan 28 01:01:27.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename pods 01/28/23 01:01:27.753
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:01:27.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:01:27.795
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 01/28/23 01:01:27.805
    STEP: submitting the pod to kubernetes 01/28/23 01:01:27.806
    Jan 28 01:01:27.826: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4" in namespace "pods-2123" to be "running and ready"
    Jan 28 01:01:27.836: INFO: Pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.79163ms
    Jan 28 01:01:27.836: INFO: The phase of Pod pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:01:29.846: INFO: Pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4": Phase="Running", Reason="", readiness=true. Elapsed: 2.020024075s
    Jan 28 01:01:29.846: INFO: The phase of Pod pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4 is Running (Ready = true)
    Jan 28 01:01:29.846: INFO: Pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/28/23 01:01:29.855
    STEP: updating the pod 01/28/23 01:01:29.866
    Jan 28 01:01:30.392: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4"
    Jan 28 01:01:30.392: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4" in namespace "pods-2123" to be "terminated with reason DeadlineExceeded"
    Jan 28 01:01:30.403: INFO: Pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4": Phase="Running", Reason="", readiness=true. Elapsed: 10.834725ms
    Jan 28 01:01:32.415: INFO: Pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4": Phase="Running", Reason="", readiness=true. Elapsed: 2.022947328s
    Jan 28 01:01:34.413: INFO: Pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4": Phase="Running", Reason="", readiness=false. Elapsed: 4.021252121s
    Jan 28 01:01:36.415: INFO: Pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.0230352s
    Jan 28 01:01:36.415: INFO: Pod "pod-update-activedeadlineseconds-1451356d-a26b-44a8-ab3a-08c498d69ec4" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 28 01:01:36.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2123" for this suite. 01/28/23 01:01:36.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:01:36.454
Jan 28 01:01:36.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename downward-api 01/28/23 01:01:36.456
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:01:36.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:01:36.496
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 01/28/23 01:01:36.505
Jan 28 01:01:36.528: INFO: Waiting up to 5m0s for pod "downward-api-47dfb79d-4a33-4f22-ba25-4cfdb6004903" in namespace "downward-api-9578" to be "Succeeded or Failed"
Jan 28 01:01:36.540: INFO: Pod "downward-api-47dfb79d-4a33-4f22-ba25-4cfdb6004903": Phase="Pending", Reason="", readiness=false. Elapsed: 12.552758ms
Jan 28 01:01:38.552: INFO: Pod "downward-api-47dfb79d-4a33-4f22-ba25-4cfdb6004903": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024061559s
Jan 28 01:01:40.552: INFO: Pod "downward-api-47dfb79d-4a33-4f22-ba25-4cfdb6004903": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02449678s
Jan 28 01:01:42.553: INFO: Pod "downward-api-47dfb79d-4a33-4f22-ba25-4cfdb6004903": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024732889s
STEP: Saw pod success 01/28/23 01:01:42.553
Jan 28 01:01:42.553: INFO: Pod "downward-api-47dfb79d-4a33-4f22-ba25-4cfdb6004903" satisfied condition "Succeeded or Failed"
Jan 28 01:01:42.563: INFO: Trying to get logs from node 10.9.20.72 pod downward-api-47dfb79d-4a33-4f22-ba25-4cfdb6004903 container dapi-container: <nil>
STEP: delete the pod 01/28/23 01:01:42.609
Jan 28 01:01:42.634: INFO: Waiting for pod downward-api-47dfb79d-4a33-4f22-ba25-4cfdb6004903 to disappear
Jan 28 01:01:42.644: INFO: Pod downward-api-47dfb79d-4a33-4f22-ba25-4cfdb6004903 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 28 01:01:42.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9578" for this suite. 01/28/23 01:01:42.659
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":128,"skipped":2442,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.223 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:01:36.454
    Jan 28 01:01:36.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename downward-api 01/28/23 01:01:36.456
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:01:36.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:01:36.496
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 01/28/23 01:01:36.505
    Jan 28 01:01:36.528: INFO: Waiting up to 5m0s for pod "downward-api-47dfb79d-4a33-4f22-ba25-4cfdb6004903" in namespace "downward-api-9578" to be "Succeeded or Failed"
    Jan 28 01:01:36.540: INFO: Pod "downward-api-47dfb79d-4a33-4f22-ba25-4cfdb6004903": Phase="Pending", Reason="", readiness=false. Elapsed: 12.552758ms
    Jan 28 01:01:38.552: INFO: Pod "downward-api-47dfb79d-4a33-4f22-ba25-4cfdb6004903": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024061559s
    Jan 28 01:01:40.552: INFO: Pod "downward-api-47dfb79d-4a33-4f22-ba25-4cfdb6004903": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02449678s
    Jan 28 01:01:42.553: INFO: Pod "downward-api-47dfb79d-4a33-4f22-ba25-4cfdb6004903": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024732889s
    STEP: Saw pod success 01/28/23 01:01:42.553
    Jan 28 01:01:42.553: INFO: Pod "downward-api-47dfb79d-4a33-4f22-ba25-4cfdb6004903" satisfied condition "Succeeded or Failed"
    Jan 28 01:01:42.563: INFO: Trying to get logs from node 10.9.20.72 pod downward-api-47dfb79d-4a33-4f22-ba25-4cfdb6004903 container dapi-container: <nil>
    STEP: delete the pod 01/28/23 01:01:42.609
    Jan 28 01:01:42.634: INFO: Waiting for pod downward-api-47dfb79d-4a33-4f22-ba25-4cfdb6004903 to disappear
    Jan 28 01:01:42.644: INFO: Pod downward-api-47dfb79d-4a33-4f22-ba25-4cfdb6004903 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 28 01:01:42.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9578" for this suite. 01/28/23 01:01:42.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:01:42.682
Jan 28 01:01:42.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename services 01/28/23 01:01:42.686
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:01:42.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:01:42.729
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 01/28/23 01:01:42.749
STEP: watching for the Service to be added 01/28/23 01:01:42.784
Jan 28 01:01:42.790: INFO: Found Service test-service-pl9nt in namespace services-3970 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 28 01:01:42.790: INFO: Service test-service-pl9nt created
STEP: Getting /status 01/28/23 01:01:42.79
Jan 28 01:01:42.809: INFO: Service test-service-pl9nt has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/28/23 01:01:42.809
STEP: watching for the Service to be patched 01/28/23 01:01:42.82
Jan 28 01:01:42.824: INFO: observed Service test-service-pl9nt in namespace services-3970 with annotations: map[] & LoadBalancer: {[]}
Jan 28 01:01:42.824: INFO: Found Service test-service-pl9nt in namespace services-3970 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 28 01:01:42.824: INFO: Service test-service-pl9nt has service status patched
STEP: updating the ServiceStatus 01/28/23 01:01:42.824
Jan 28 01:01:42.845: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/28/23 01:01:42.845
Jan 28 01:01:42.849: INFO: Observed Service test-service-pl9nt in namespace services-3970 with annotations: map[] & Conditions: {[]}
Jan 28 01:01:42.849: INFO: Observed event: &Service{ObjectMeta:{test-service-pl9nt  services-3970  33178447-f447-4d49-a265-a4fe87a9c3c8 28260 0 2023-01-28 01:01:42 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-28 01:01:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-28 01:01:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.235.82,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.235.82],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 28 01:01:42.850: INFO: Found Service test-service-pl9nt in namespace services-3970 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 28 01:01:42.850: INFO: Service test-service-pl9nt has service status updated
STEP: patching the service 01/28/23 01:01:42.85
STEP: watching for the Service to be patched 01/28/23 01:01:42.861
Jan 28 01:01:42.866: INFO: observed Service test-service-pl9nt in namespace services-3970 with labels: map[test-service-static:true]
Jan 28 01:01:42.866: INFO: observed Service test-service-pl9nt in namespace services-3970 with labels: map[test-service-static:true]
Jan 28 01:01:42.866: INFO: observed Service test-service-pl9nt in namespace services-3970 with labels: map[test-service-static:true]
Jan 28 01:01:42.866: INFO: Found Service test-service-pl9nt in namespace services-3970 with labels: map[test-service:patched test-service-static:true]
Jan 28 01:01:42.866: INFO: Service test-service-pl9nt patched
STEP: deleting the service 01/28/23 01:01:42.866
STEP: watching for the Service to be deleted 01/28/23 01:01:42.908
Jan 28 01:01:42.912: INFO: Observed event: ADDED
Jan 28 01:01:42.912: INFO: Observed event: MODIFIED
Jan 28 01:01:42.912: INFO: Observed event: MODIFIED
Jan 28 01:01:42.913: INFO: Observed event: MODIFIED
Jan 28 01:01:42.913: INFO: Found Service test-service-pl9nt in namespace services-3970 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 28 01:01:42.913: INFO: Service test-service-pl9nt deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 28 01:01:42.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3970" for this suite. 01/28/23 01:01:42.929
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":129,"skipped":2448,"failed":0}
------------------------------
â€¢ [0.266 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:01:42.682
    Jan 28 01:01:42.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename services 01/28/23 01:01:42.686
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:01:42.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:01:42.729
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 01/28/23 01:01:42.749
    STEP: watching for the Service to be added 01/28/23 01:01:42.784
    Jan 28 01:01:42.790: INFO: Found Service test-service-pl9nt in namespace services-3970 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan 28 01:01:42.790: INFO: Service test-service-pl9nt created
    STEP: Getting /status 01/28/23 01:01:42.79
    Jan 28 01:01:42.809: INFO: Service test-service-pl9nt has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/28/23 01:01:42.809
    STEP: watching for the Service to be patched 01/28/23 01:01:42.82
    Jan 28 01:01:42.824: INFO: observed Service test-service-pl9nt in namespace services-3970 with annotations: map[] & LoadBalancer: {[]}
    Jan 28 01:01:42.824: INFO: Found Service test-service-pl9nt in namespace services-3970 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan 28 01:01:42.824: INFO: Service test-service-pl9nt has service status patched
    STEP: updating the ServiceStatus 01/28/23 01:01:42.824
    Jan 28 01:01:42.845: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/28/23 01:01:42.845
    Jan 28 01:01:42.849: INFO: Observed Service test-service-pl9nt in namespace services-3970 with annotations: map[] & Conditions: {[]}
    Jan 28 01:01:42.849: INFO: Observed event: &Service{ObjectMeta:{test-service-pl9nt  services-3970  33178447-f447-4d49-a265-a4fe87a9c3c8 28260 0 2023-01-28 01:01:42 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-28 01:01:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-28 01:01:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.235.82,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.235.82],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan 28 01:01:42.850: INFO: Found Service test-service-pl9nt in namespace services-3970 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 28 01:01:42.850: INFO: Service test-service-pl9nt has service status updated
    STEP: patching the service 01/28/23 01:01:42.85
    STEP: watching for the Service to be patched 01/28/23 01:01:42.861
    Jan 28 01:01:42.866: INFO: observed Service test-service-pl9nt in namespace services-3970 with labels: map[test-service-static:true]
    Jan 28 01:01:42.866: INFO: observed Service test-service-pl9nt in namespace services-3970 with labels: map[test-service-static:true]
    Jan 28 01:01:42.866: INFO: observed Service test-service-pl9nt in namespace services-3970 with labels: map[test-service-static:true]
    Jan 28 01:01:42.866: INFO: Found Service test-service-pl9nt in namespace services-3970 with labels: map[test-service:patched test-service-static:true]
    Jan 28 01:01:42.866: INFO: Service test-service-pl9nt patched
    STEP: deleting the service 01/28/23 01:01:42.866
    STEP: watching for the Service to be deleted 01/28/23 01:01:42.908
    Jan 28 01:01:42.912: INFO: Observed event: ADDED
    Jan 28 01:01:42.912: INFO: Observed event: MODIFIED
    Jan 28 01:01:42.912: INFO: Observed event: MODIFIED
    Jan 28 01:01:42.913: INFO: Observed event: MODIFIED
    Jan 28 01:01:42.913: INFO: Found Service test-service-pl9nt in namespace services-3970 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan 28 01:01:42.913: INFO: Service test-service-pl9nt deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 28 01:01:42.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3970" for this suite. 01/28/23 01:01:42.929
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:01:42.958
Jan 28 01:01:42.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename configmap 01/28/23 01:01:42.96
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:01:42.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:01:42.997
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-82054974-5bb9-4e1e-b4d5-6d37ff0ae3b3 01/28/23 01:01:43.005
STEP: Creating a pod to test consume configMaps 01/28/23 01:01:43.017
Jan 28 01:01:43.037: INFO: Waiting up to 5m0s for pod "pod-configmaps-fce7a7ab-0186-487c-bcfc-bef17bef86e5" in namespace "configmap-2511" to be "Succeeded or Failed"
Jan 28 01:01:43.049: INFO: Pod "pod-configmaps-fce7a7ab-0186-487c-bcfc-bef17bef86e5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.809989ms
Jan 28 01:01:45.061: INFO: Pod "pod-configmaps-fce7a7ab-0186-487c-bcfc-bef17bef86e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.023723957s
Jan 28 01:01:47.062: INFO: Pod "pod-configmaps-fce7a7ab-0186-487c-bcfc-bef17bef86e5": Phase="Running", Reason="", readiness=false. Elapsed: 4.024451509s
Jan 28 01:01:49.063: INFO: Pod "pod-configmaps-fce7a7ab-0186-487c-bcfc-bef17bef86e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02512633s
STEP: Saw pod success 01/28/23 01:01:49.063
Jan 28 01:01:49.063: INFO: Pod "pod-configmaps-fce7a7ab-0186-487c-bcfc-bef17bef86e5" satisfied condition "Succeeded or Failed"
Jan 28 01:01:49.073: INFO: Trying to get logs from node 10.9.20.72 pod pod-configmaps-fce7a7ab-0186-487c-bcfc-bef17bef86e5 container agnhost-container: <nil>
STEP: delete the pod 01/28/23 01:01:49.1
Jan 28 01:01:49.130: INFO: Waiting for pod pod-configmaps-fce7a7ab-0186-487c-bcfc-bef17bef86e5 to disappear
Jan 28 01:01:49.139: INFO: Pod pod-configmaps-fce7a7ab-0186-487c-bcfc-bef17bef86e5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 28 01:01:49.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2511" for this suite. 01/28/23 01:01:49.155
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":130,"skipped":2474,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.215 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:01:42.958
    Jan 28 01:01:42.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename configmap 01/28/23 01:01:42.96
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:01:42.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:01:42.997
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-82054974-5bb9-4e1e-b4d5-6d37ff0ae3b3 01/28/23 01:01:43.005
    STEP: Creating a pod to test consume configMaps 01/28/23 01:01:43.017
    Jan 28 01:01:43.037: INFO: Waiting up to 5m0s for pod "pod-configmaps-fce7a7ab-0186-487c-bcfc-bef17bef86e5" in namespace "configmap-2511" to be "Succeeded or Failed"
    Jan 28 01:01:43.049: INFO: Pod "pod-configmaps-fce7a7ab-0186-487c-bcfc-bef17bef86e5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.809989ms
    Jan 28 01:01:45.061: INFO: Pod "pod-configmaps-fce7a7ab-0186-487c-bcfc-bef17bef86e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.023723957s
    Jan 28 01:01:47.062: INFO: Pod "pod-configmaps-fce7a7ab-0186-487c-bcfc-bef17bef86e5": Phase="Running", Reason="", readiness=false. Elapsed: 4.024451509s
    Jan 28 01:01:49.063: INFO: Pod "pod-configmaps-fce7a7ab-0186-487c-bcfc-bef17bef86e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02512633s
    STEP: Saw pod success 01/28/23 01:01:49.063
    Jan 28 01:01:49.063: INFO: Pod "pod-configmaps-fce7a7ab-0186-487c-bcfc-bef17bef86e5" satisfied condition "Succeeded or Failed"
    Jan 28 01:01:49.073: INFO: Trying to get logs from node 10.9.20.72 pod pod-configmaps-fce7a7ab-0186-487c-bcfc-bef17bef86e5 container agnhost-container: <nil>
    STEP: delete the pod 01/28/23 01:01:49.1
    Jan 28 01:01:49.130: INFO: Waiting for pod pod-configmaps-fce7a7ab-0186-487c-bcfc-bef17bef86e5 to disappear
    Jan 28 01:01:49.139: INFO: Pod pod-configmaps-fce7a7ab-0186-487c-bcfc-bef17bef86e5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 28 01:01:49.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2511" for this suite. 01/28/23 01:01:49.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:01:49.179
Jan 28 01:01:49.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename daemonsets 01/28/23 01:01:49.181
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:01:49.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:01:49.225
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Jan 28 01:01:49.294: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/28/23 01:01:49.308
Jan 28 01:01:49.318: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:01:49.318: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/28/23 01:01:49.318
Jan 28 01:01:49.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:01:49.383: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
Jan 28 01:01:50.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:01:50.395: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
Jan 28 01:01:51.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:01:51.395: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
Jan 28 01:01:52.393: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:01:52.394: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
Jan 28 01:01:53.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:01:53.394: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
Jan 28 01:01:54.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 28 01:01:54.394: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/28/23 01:01:54.406
Jan 28 01:01:54.456: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 28 01:01:54.456: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 28 01:01:55.466: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:01:55.467: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/28/23 01:01:55.467
Jan 28 01:01:55.520: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:01:55.521: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
Jan 28 01:01:56.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:01:56.532: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
Jan 28 01:01:57.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:01:57.532: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
Jan 28 01:01:58.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:01:58.532: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
Jan 28 01:01:59.534: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 28 01:01:59.534: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/28/23 01:01:59.557
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1070, will wait for the garbage collector to delete the pods 01/28/23 01:01:59.557
Jan 28 01:01:59.637: INFO: Deleting DaemonSet.extensions daemon-set took: 18.417268ms
Jan 28 01:01:59.738: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.542291ms
Jan 28 01:02:02.049: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:02:02.049: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 28 01:02:02.059: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28399"},"items":null}

Jan 28 01:02:02.069: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28399"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 28 01:02:02.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1070" for this suite. 01/28/23 01:02:02.171
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":131,"skipped":2482,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.009 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:01:49.179
    Jan 28 01:01:49.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename daemonsets 01/28/23 01:01:49.181
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:01:49.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:01:49.225
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Jan 28 01:01:49.294: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/28/23 01:01:49.308
    Jan 28 01:01:49.318: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 01:01:49.318: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/28/23 01:01:49.318
    Jan 28 01:01:49.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 01:01:49.383: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
    Jan 28 01:01:50.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 01:01:50.395: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
    Jan 28 01:01:51.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 01:01:51.395: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
    Jan 28 01:01:52.393: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 01:01:52.394: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
    Jan 28 01:01:53.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 01:01:53.394: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
    Jan 28 01:01:54.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 28 01:01:54.394: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/28/23 01:01:54.406
    Jan 28 01:01:54.456: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 28 01:01:54.456: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jan 28 01:01:55.466: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 01:01:55.467: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/28/23 01:01:55.467
    Jan 28 01:01:55.520: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 01:01:55.521: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
    Jan 28 01:01:56.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 01:01:56.532: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
    Jan 28 01:01:57.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 01:01:57.532: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
    Jan 28 01:01:58.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 01:01:58.532: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
    Jan 28 01:01:59.534: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 28 01:01:59.534: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/28/23 01:01:59.557
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1070, will wait for the garbage collector to delete the pods 01/28/23 01:01:59.557
    Jan 28 01:01:59.637: INFO: Deleting DaemonSet.extensions daemon-set took: 18.417268ms
    Jan 28 01:01:59.738: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.542291ms
    Jan 28 01:02:02.049: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 01:02:02.049: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 28 01:02:02.059: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28399"},"items":null}

    Jan 28 01:02:02.069: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28399"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 01:02:02.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1070" for this suite. 01/28/23 01:02:02.171
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:02:02.193
Jan 28 01:02:02.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 01:02:02.196
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:02:02.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:02:02.236
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-da0fd90f-0ab5-4ef5-874f-7dd42f24f6f1 01/28/23 01:02:02.245
STEP: Creating a pod to test consume secrets 01/28/23 01:02:02.255
Jan 28 01:02:02.278: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bf418c93-8f06-413c-b8ea-e759e96606fc" in namespace "projected-1953" to be "Succeeded or Failed"
Jan 28 01:02:02.288: INFO: Pod "pod-projected-secrets-bf418c93-8f06-413c-b8ea-e759e96606fc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.005927ms
Jan 28 01:02:04.300: INFO: Pod "pod-projected-secrets-bf418c93-8f06-413c-b8ea-e759e96606fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.021277016s
Jan 28 01:02:06.300: INFO: Pod "pod-projected-secrets-bf418c93-8f06-413c-b8ea-e759e96606fc": Phase="Running", Reason="", readiness=false. Elapsed: 4.020848016s
Jan 28 01:02:08.297: INFO: Pod "pod-projected-secrets-bf418c93-8f06-413c-b8ea-e759e96606fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018503974s
STEP: Saw pod success 01/28/23 01:02:08.297
Jan 28 01:02:08.297: INFO: Pod "pod-projected-secrets-bf418c93-8f06-413c-b8ea-e759e96606fc" satisfied condition "Succeeded or Failed"
Jan 28 01:02:08.305: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-secrets-bf418c93-8f06-413c-b8ea-e759e96606fc container projected-secret-volume-test: <nil>
STEP: delete the pod 01/28/23 01:02:08.384
Jan 28 01:02:08.415: INFO: Waiting for pod pod-projected-secrets-bf418c93-8f06-413c-b8ea-e759e96606fc to disappear
Jan 28 01:02:08.425: INFO: Pod pod-projected-secrets-bf418c93-8f06-413c-b8ea-e759e96606fc no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 28 01:02:08.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1953" for this suite. 01/28/23 01:02:08.442
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":132,"skipped":2486,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.268 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:02:02.193
    Jan 28 01:02:02.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 01:02:02.196
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:02:02.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:02:02.236
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-da0fd90f-0ab5-4ef5-874f-7dd42f24f6f1 01/28/23 01:02:02.245
    STEP: Creating a pod to test consume secrets 01/28/23 01:02:02.255
    Jan 28 01:02:02.278: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bf418c93-8f06-413c-b8ea-e759e96606fc" in namespace "projected-1953" to be "Succeeded or Failed"
    Jan 28 01:02:02.288: INFO: Pod "pod-projected-secrets-bf418c93-8f06-413c-b8ea-e759e96606fc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.005927ms
    Jan 28 01:02:04.300: INFO: Pod "pod-projected-secrets-bf418c93-8f06-413c-b8ea-e759e96606fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.021277016s
    Jan 28 01:02:06.300: INFO: Pod "pod-projected-secrets-bf418c93-8f06-413c-b8ea-e759e96606fc": Phase="Running", Reason="", readiness=false. Elapsed: 4.020848016s
    Jan 28 01:02:08.297: INFO: Pod "pod-projected-secrets-bf418c93-8f06-413c-b8ea-e759e96606fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018503974s
    STEP: Saw pod success 01/28/23 01:02:08.297
    Jan 28 01:02:08.297: INFO: Pod "pod-projected-secrets-bf418c93-8f06-413c-b8ea-e759e96606fc" satisfied condition "Succeeded or Failed"
    Jan 28 01:02:08.305: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-secrets-bf418c93-8f06-413c-b8ea-e759e96606fc container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/28/23 01:02:08.384
    Jan 28 01:02:08.415: INFO: Waiting for pod pod-projected-secrets-bf418c93-8f06-413c-b8ea-e759e96606fc to disappear
    Jan 28 01:02:08.425: INFO: Pod pod-projected-secrets-bf418c93-8f06-413c-b8ea-e759e96606fc no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 28 01:02:08.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1953" for this suite. 01/28/23 01:02:08.442
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:02:08.463
Jan 28 01:02:08.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename security-context 01/28/23 01:02:08.465
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:02:08.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:02:08.509
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/28/23 01:02:08.519
Jan 28 01:02:08.538: INFO: Waiting up to 5m0s for pod "security-context-af95538b-1789-4948-b381-ec189c0965b4" in namespace "security-context-9189" to be "Succeeded or Failed"
Jan 28 01:02:08.549: INFO: Pod "security-context-af95538b-1789-4948-b381-ec189c0965b4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.57701ms
Jan 28 01:02:10.558: INFO: Pod "security-context-af95538b-1789-4948-b381-ec189c0965b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020173987s
Jan 28 01:02:12.562: INFO: Pod "security-context-af95538b-1789-4948-b381-ec189c0965b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024273138s
Jan 28 01:02:14.560: INFO: Pod "security-context-af95538b-1789-4948-b381-ec189c0965b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021955009s
STEP: Saw pod success 01/28/23 01:02:14.56
Jan 28 01:02:14.561: INFO: Pod "security-context-af95538b-1789-4948-b381-ec189c0965b4" satisfied condition "Succeeded or Failed"
Jan 28 01:02:14.572: INFO: Trying to get logs from node 10.9.20.72 pod security-context-af95538b-1789-4948-b381-ec189c0965b4 container test-container: <nil>
STEP: delete the pod 01/28/23 01:02:14.599
Jan 28 01:02:14.634: INFO: Waiting for pod security-context-af95538b-1789-4948-b381-ec189c0965b4 to disappear
Jan 28 01:02:14.644: INFO: Pod security-context-af95538b-1789-4948-b381-ec189c0965b4 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 28 01:02:14.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-9189" for this suite. 01/28/23 01:02:14.662
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":133,"skipped":2494,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.216 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:02:08.463
    Jan 28 01:02:08.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename security-context 01/28/23 01:02:08.465
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:02:08.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:02:08.509
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/28/23 01:02:08.519
    Jan 28 01:02:08.538: INFO: Waiting up to 5m0s for pod "security-context-af95538b-1789-4948-b381-ec189c0965b4" in namespace "security-context-9189" to be "Succeeded or Failed"
    Jan 28 01:02:08.549: INFO: Pod "security-context-af95538b-1789-4948-b381-ec189c0965b4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.57701ms
    Jan 28 01:02:10.558: INFO: Pod "security-context-af95538b-1789-4948-b381-ec189c0965b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020173987s
    Jan 28 01:02:12.562: INFO: Pod "security-context-af95538b-1789-4948-b381-ec189c0965b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024273138s
    Jan 28 01:02:14.560: INFO: Pod "security-context-af95538b-1789-4948-b381-ec189c0965b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021955009s
    STEP: Saw pod success 01/28/23 01:02:14.56
    Jan 28 01:02:14.561: INFO: Pod "security-context-af95538b-1789-4948-b381-ec189c0965b4" satisfied condition "Succeeded or Failed"
    Jan 28 01:02:14.572: INFO: Trying to get logs from node 10.9.20.72 pod security-context-af95538b-1789-4948-b381-ec189c0965b4 container test-container: <nil>
    STEP: delete the pod 01/28/23 01:02:14.599
    Jan 28 01:02:14.634: INFO: Waiting for pod security-context-af95538b-1789-4948-b381-ec189c0965b4 to disappear
    Jan 28 01:02:14.644: INFO: Pod security-context-af95538b-1789-4948-b381-ec189c0965b4 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 28 01:02:14.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-9189" for this suite. 01/28/23 01:02:14.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:02:14.684
Jan 28 01:02:14.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename downward-api 01/28/23 01:02:14.685
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:02:14.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:02:14.731
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 01/28/23 01:02:14.74
Jan 28 01:02:14.759: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2055ae4f-4064-4c72-9701-1f4a95f8375b" in namespace "downward-api-4899" to be "Succeeded or Failed"
Jan 28 01:02:14.772: INFO: Pod "downwardapi-volume-2055ae4f-4064-4c72-9701-1f4a95f8375b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.128317ms
Jan 28 01:02:16.784: INFO: Pod "downwardapi-volume-2055ae4f-4064-4c72-9701-1f4a95f8375b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024761028s
Jan 28 01:02:18.783: INFO: Pod "downwardapi-volume-2055ae4f-4064-4c72-9701-1f4a95f8375b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023956356s
Jan 28 01:02:20.783: INFO: Pod "downwardapi-volume-2055ae4f-4064-4c72-9701-1f4a95f8375b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023890833s
STEP: Saw pod success 01/28/23 01:02:20.783
Jan 28 01:02:20.784: INFO: Pod "downwardapi-volume-2055ae4f-4064-4c72-9701-1f4a95f8375b" satisfied condition "Succeeded or Failed"
Jan 28 01:02:20.795: INFO: Trying to get logs from node 10.9.20.72 pod downwardapi-volume-2055ae4f-4064-4c72-9701-1f4a95f8375b container client-container: <nil>
STEP: delete the pod 01/28/23 01:02:20.821
Jan 28 01:02:20.849: INFO: Waiting for pod downwardapi-volume-2055ae4f-4064-4c72-9701-1f4a95f8375b to disappear
Jan 28 01:02:20.866: INFO: Pod downwardapi-volume-2055ae4f-4064-4c72-9701-1f4a95f8375b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 28 01:02:20.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4899" for this suite. 01/28/23 01:02:20.883
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":134,"skipped":2502,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.231 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:02:14.684
    Jan 28 01:02:14.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename downward-api 01/28/23 01:02:14.685
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:02:14.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:02:14.731
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 01/28/23 01:02:14.74
    Jan 28 01:02:14.759: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2055ae4f-4064-4c72-9701-1f4a95f8375b" in namespace "downward-api-4899" to be "Succeeded or Failed"
    Jan 28 01:02:14.772: INFO: Pod "downwardapi-volume-2055ae4f-4064-4c72-9701-1f4a95f8375b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.128317ms
    Jan 28 01:02:16.784: INFO: Pod "downwardapi-volume-2055ae4f-4064-4c72-9701-1f4a95f8375b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024761028s
    Jan 28 01:02:18.783: INFO: Pod "downwardapi-volume-2055ae4f-4064-4c72-9701-1f4a95f8375b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023956356s
    Jan 28 01:02:20.783: INFO: Pod "downwardapi-volume-2055ae4f-4064-4c72-9701-1f4a95f8375b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023890833s
    STEP: Saw pod success 01/28/23 01:02:20.783
    Jan 28 01:02:20.784: INFO: Pod "downwardapi-volume-2055ae4f-4064-4c72-9701-1f4a95f8375b" satisfied condition "Succeeded or Failed"
    Jan 28 01:02:20.795: INFO: Trying to get logs from node 10.9.20.72 pod downwardapi-volume-2055ae4f-4064-4c72-9701-1f4a95f8375b container client-container: <nil>
    STEP: delete the pod 01/28/23 01:02:20.821
    Jan 28 01:02:20.849: INFO: Waiting for pod downwardapi-volume-2055ae4f-4064-4c72-9701-1f4a95f8375b to disappear
    Jan 28 01:02:20.866: INFO: Pod downwardapi-volume-2055ae4f-4064-4c72-9701-1f4a95f8375b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 28 01:02:20.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4899" for this suite. 01/28/23 01:02:20.883
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:02:20.916
Jan 28 01:02:20.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename proxy 01/28/23 01:02:20.918
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:02:20.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:02:20.963
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/28/23 01:02:21.026
STEP: creating replication controller proxy-service-twc2j in namespace proxy-2036 01/28/23 01:02:21.026
I0128 01:02:21.042393      22 runners.go:193] Created replication controller with name: proxy-service-twc2j, namespace: proxy-2036, replica count: 1
I0128 01:02:22.093689      22 runners.go:193] proxy-service-twc2j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0128 01:02:23.094829      22 runners.go:193] proxy-service-twc2j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0128 01:02:24.095773      22 runners.go:193] proxy-service-twc2j Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 01:02:24.108: INFO: setup took 3.136754793s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/28/23 01:02:24.108
Jan 28 01:02:24.170: INFO: (0) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 61.436864ms)
Jan 28 01:02:24.170: INFO: (0) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 61.06323ms)
Jan 28 01:02:24.172: INFO: (0) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 62.46072ms)
Jan 28 01:02:24.172: INFO: (0) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 62.424345ms)
Jan 28 01:02:24.178: INFO: (0) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 69.557844ms)
Jan 28 01:02:24.179: INFO: (0) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 69.060753ms)
Jan 28 01:02:24.180: INFO: (0) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 70.75281ms)
Jan 28 01:02:24.182: INFO: (0) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 72.121597ms)
Jan 28 01:02:24.187: INFO: (0) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 77.739158ms)
Jan 28 01:02:24.188: INFO: (0) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 78.457131ms)
Jan 28 01:02:24.189: INFO: (0) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 80.195425ms)
Jan 28 01:02:24.196: INFO: (0) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 87.252068ms)
Jan 28 01:02:24.196: INFO: (0) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 86.599082ms)
Jan 28 01:02:24.196: INFO: (0) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 87.00749ms)
Jan 28 01:02:24.196: INFO: (0) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 87.161421ms)
Jan 28 01:02:24.199: INFO: (0) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 89.842947ms)
Jan 28 01:02:24.221: INFO: (1) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 20.935632ms)
Jan 28 01:02:24.222: INFO: (1) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 22.043595ms)
Jan 28 01:02:24.222: INFO: (1) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 21.068985ms)
Jan 28 01:02:24.222: INFO: (1) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 21.062385ms)
Jan 28 01:02:24.222: INFO: (1) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 21.526418ms)
Jan 28 01:02:24.222: INFO: (1) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 21.766736ms)
Jan 28 01:02:24.224: INFO: (1) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 22.901984ms)
Jan 28 01:02:24.224: INFO: (1) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 24.132024ms)
Jan 28 01:02:24.224: INFO: (1) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 23.356642ms)
Jan 28 01:02:24.224: INFO: (1) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 23.488872ms)
Jan 28 01:02:24.227: INFO: (1) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 25.801938ms)
Jan 28 01:02:24.227: INFO: (1) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 27.123015ms)
Jan 28 01:02:24.228: INFO: (1) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 27.945512ms)
Jan 28 01:02:24.229: INFO: (1) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 28.542975ms)
Jan 28 01:02:24.231: INFO: (1) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 30.777382ms)
Jan 28 01:02:24.231: INFO: (1) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 30.074128ms)
Jan 28 01:02:24.246: INFO: (2) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 15.154151ms)
Jan 28 01:02:24.247: INFO: (2) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 15.314258ms)
Jan 28 01:02:24.248: INFO: (2) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 16.195255ms)
Jan 28 01:02:24.248: INFO: (2) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 16.285424ms)
Jan 28 01:02:24.249: INFO: (2) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 17.183574ms)
Jan 28 01:02:24.250: INFO: (2) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 17.788801ms)
Jan 28 01:02:24.252: INFO: (2) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 19.413816ms)
Jan 28 01:02:24.253: INFO: (2) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 20.855427ms)
Jan 28 01:02:24.253: INFO: (2) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 20.756038ms)
Jan 28 01:02:24.254: INFO: (2) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 21.964257ms)
Jan 28 01:02:24.255: INFO: (2) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 23.65047ms)
Jan 28 01:02:24.259: INFO: (2) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 26.822955ms)
Jan 28 01:02:24.260: INFO: (2) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 28.415742ms)
Jan 28 01:02:24.260: INFO: (2) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 28.173069ms)
Jan 28 01:02:24.260: INFO: (2) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 27.787749ms)
Jan 28 01:02:24.261: INFO: (2) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 29.338847ms)
Jan 28 01:02:24.285: INFO: (3) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 21.793206ms)
Jan 28 01:02:24.286: INFO: (3) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 23.355802ms)
Jan 28 01:02:24.286: INFO: (3) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 22.841135ms)
Jan 28 01:02:24.287: INFO: (3) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 21.925552ms)
Jan 28 01:02:24.287: INFO: (3) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 24.008977ms)
Jan 28 01:02:24.287: INFO: (3) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 23.612697ms)
Jan 28 01:02:24.287: INFO: (3) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 25.130921ms)
Jan 28 01:02:24.287: INFO: (3) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 24.916339ms)
Jan 28 01:02:24.287: INFO: (3) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 23.520785ms)
Jan 28 01:02:24.288: INFO: (3) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 23.647443ms)
Jan 28 01:02:24.292: INFO: (3) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 28.481508ms)
Jan 28 01:02:24.292: INFO: (3) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 29.796975ms)
Jan 28 01:02:24.292: INFO: (3) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 28.124365ms)
Jan 28 01:02:24.293: INFO: (3) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 28.530225ms)
Jan 28 01:02:24.293: INFO: (3) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 29.704047ms)
Jan 28 01:02:24.294: INFO: (3) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 29.804851ms)
Jan 28 01:02:24.313: INFO: (4) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 18.66641ms)
Jan 28 01:02:24.313: INFO: (4) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 18.375297ms)
Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 50.308332ms)
Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 49.758417ms)
Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 50.725106ms)
Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 48.036006ms)
Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 50.664119ms)
Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 50.174187ms)
Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 50.127686ms)
Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 50.144049ms)
Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 49.041228ms)
Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 50.635337ms)
Jan 28 01:02:24.350: INFO: (4) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 54.06452ms)
Jan 28 01:02:24.350: INFO: (4) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 55.460026ms)
Jan 28 01:02:24.351: INFO: (4) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 54.316731ms)
Jan 28 01:02:24.351: INFO: (4) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 54.888189ms)
Jan 28 01:02:24.369: INFO: (5) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 17.298264ms)
Jan 28 01:02:24.373: INFO: (5) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 22.058343ms)
Jan 28 01:02:24.374: INFO: (5) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 22.201905ms)
Jan 28 01:02:24.375: INFO: (5) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 23.37871ms)
Jan 28 01:02:24.376: INFO: (5) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 24.703044ms)
Jan 28 01:02:24.376: INFO: (5) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 24.433885ms)
Jan 28 01:02:24.376: INFO: (5) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 25.203829ms)
Jan 28 01:02:24.376: INFO: (5) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 24.643004ms)
Jan 28 01:02:24.377: INFO: (5) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 24.740875ms)
Jan 28 01:02:24.376: INFO: (5) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 25.527488ms)
Jan 28 01:02:24.379: INFO: (5) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 28.176674ms)
Jan 28 01:02:24.380: INFO: (5) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 27.604149ms)
Jan 28 01:02:24.384: INFO: (5) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 32.467518ms)
Jan 28 01:02:24.384: INFO: (5) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 32.906131ms)
Jan 28 01:02:24.385: INFO: (5) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 32.805592ms)
Jan 28 01:02:24.385: INFO: (5) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 33.464588ms)
Jan 28 01:02:24.401: INFO: (6) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 15.572709ms)
Jan 28 01:02:24.406: INFO: (6) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 19.380786ms)
Jan 28 01:02:24.408: INFO: (6) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 21.417175ms)
Jan 28 01:02:24.409: INFO: (6) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 22.826184ms)
Jan 28 01:02:24.409: INFO: (6) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 21.4081ms)
Jan 28 01:02:24.409: INFO: (6) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 22.064364ms)
Jan 28 01:02:24.409: INFO: (6) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 21.745813ms)
Jan 28 01:02:24.410: INFO: (6) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 21.462686ms)
Jan 28 01:02:24.411: INFO: (6) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 21.865155ms)
Jan 28 01:02:24.411: INFO: (6) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 22.467668ms)
Jan 28 01:02:24.411: INFO: (6) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 24.371826ms)
Jan 28 01:02:24.413: INFO: (6) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 24.176773ms)
Jan 28 01:02:24.413: INFO: (6) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 26.92994ms)
Jan 28 01:02:24.413: INFO: (6) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 24.863169ms)
Jan 28 01:02:24.416: INFO: (6) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 29.769322ms)
Jan 28 01:02:24.417: INFO: (6) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 29.624593ms)
Jan 28 01:02:24.434: INFO: (7) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 16.281488ms)
Jan 28 01:02:24.434: INFO: (7) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 16.359321ms)
Jan 28 01:02:24.435: INFO: (7) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 17.218292ms)
Jan 28 01:02:24.437: INFO: (7) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 18.930185ms)
Jan 28 01:02:24.439: INFO: (7) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 20.920495ms)
Jan 28 01:02:24.439: INFO: (7) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 21.485803ms)
Jan 28 01:02:24.439: INFO: (7) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 21.110885ms)
Jan 28 01:02:24.442: INFO: (7) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 25.184837ms)
Jan 28 01:02:24.442: INFO: (7) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 24.29071ms)
Jan 28 01:02:24.443: INFO: (7) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 26.036094ms)
Jan 28 01:02:24.448: INFO: (7) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 31.101179ms)
Jan 28 01:02:24.448: INFO: (7) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 31.105014ms)
Jan 28 01:02:24.448: INFO: (7) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 31.43638ms)
Jan 28 01:02:24.449: INFO: (7) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 30.205686ms)
Jan 28 01:02:24.449: INFO: (7) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 31.371585ms)
Jan 28 01:02:24.451: INFO: (7) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 33.121259ms)
Jan 28 01:02:24.465: INFO: (8) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 14.182134ms)
Jan 28 01:02:24.467: INFO: (8) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 15.62493ms)
Jan 28 01:02:24.471: INFO: (8) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 18.848501ms)
Jan 28 01:02:24.472: INFO: (8) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 20.313689ms)
Jan 28 01:02:24.472: INFO: (8) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 19.854662ms)
Jan 28 01:02:24.473: INFO: (8) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 21.206133ms)
Jan 28 01:02:24.474: INFO: (8) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 21.3577ms)
Jan 28 01:02:24.474: INFO: (8) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 21.307234ms)
Jan 28 01:02:24.474: INFO: (8) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 21.97169ms)
Jan 28 01:02:24.474: INFO: (8) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 21.773999ms)
Jan 28 01:02:24.477: INFO: (8) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 24.834168ms)
Jan 28 01:02:24.484: INFO: (8) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 32.152358ms)
Jan 28 01:02:24.484: INFO: (8) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 31.941349ms)
Jan 28 01:02:24.484: INFO: (8) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 31.817382ms)
Jan 28 01:02:24.484: INFO: (8) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 32.589591ms)
Jan 28 01:02:24.485: INFO: (8) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 32.469509ms)
Jan 28 01:02:24.500: INFO: (9) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 14.444433ms)
Jan 28 01:02:24.502: INFO: (9) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 16.956775ms)
Jan 28 01:02:24.503: INFO: (9) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 17.634388ms)
Jan 28 01:02:24.504: INFO: (9) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 17.661875ms)
Jan 28 01:02:24.504: INFO: (9) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 17.807815ms)
Jan 28 01:02:24.504: INFO: (9) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 18.285967ms)
Jan 28 01:02:24.504: INFO: (9) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 17.83681ms)
Jan 28 01:02:24.505: INFO: (9) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 19.719115ms)
Jan 28 01:02:24.506: INFO: (9) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 20.9863ms)
Jan 28 01:02:24.507: INFO: (9) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 21.026789ms)
Jan 28 01:02:24.507: INFO: (9) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 21.850587ms)
Jan 28 01:02:24.509: INFO: (9) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 23.710726ms)
Jan 28 01:02:24.509: INFO: (9) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 23.66539ms)
Jan 28 01:02:24.509: INFO: (9) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 23.468031ms)
Jan 28 01:02:24.511: INFO: (9) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 25.165654ms)
Jan 28 01:02:24.512: INFO: (9) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 26.337664ms)
Jan 28 01:02:24.528: INFO: (10) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 15.576143ms)
Jan 28 01:02:24.529: INFO: (10) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 17.228099ms)
Jan 28 01:02:24.530: INFO: (10) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 16.322004ms)
Jan 28 01:02:24.530: INFO: (10) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 17.058127ms)
Jan 28 01:02:24.533: INFO: (10) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 20.326409ms)
Jan 28 01:02:24.533: INFO: (10) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 20.365785ms)
Jan 28 01:02:24.533: INFO: (10) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 20.927922ms)
Jan 28 01:02:24.533: INFO: (10) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 20.778687ms)
Jan 28 01:02:24.533: INFO: (10) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 20.25417ms)
Jan 28 01:02:24.533: INFO: (10) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 20.31233ms)
Jan 28 01:02:24.534: INFO: (10) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 21.529977ms)
Jan 28 01:02:24.536: INFO: (10) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 22.928746ms)
Jan 28 01:02:24.537: INFO: (10) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 24.517012ms)
Jan 28 01:02:24.538: INFO: (10) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 25.546288ms)
Jan 28 01:02:24.547: INFO: (10) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 33.792484ms)
Jan 28 01:02:24.547: INFO: (10) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 34.393148ms)
Jan 28 01:02:24.574: INFO: (11) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 25.297445ms)
Jan 28 01:02:24.574: INFO: (11) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 25.655603ms)
Jan 28 01:02:24.574: INFO: (11) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 25.666163ms)
Jan 28 01:02:24.574: INFO: (11) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 26.052658ms)
Jan 28 01:02:24.574: INFO: (11) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 25.685787ms)
Jan 28 01:02:24.575: INFO: (11) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 26.006433ms)
Jan 28 01:02:24.576: INFO: (11) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 27.653911ms)
Jan 28 01:02:24.576: INFO: (11) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 27.069078ms)
Jan 28 01:02:24.576: INFO: (11) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 27.010469ms)
Jan 28 01:02:24.576: INFO: (11) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 27.725619ms)
Jan 28 01:02:24.575: INFO: (11) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 26.219748ms)
Jan 28 01:02:24.578: INFO: (11) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 29.739204ms)
Jan 28 01:02:24.579: INFO: (11) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 31.19825ms)
Jan 28 01:02:24.580: INFO: (11) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 31.279386ms)
Jan 28 01:02:24.582: INFO: (11) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 33.932505ms)
Jan 28 01:02:24.586: INFO: (11) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 37.801194ms)
Jan 28 01:02:24.602: INFO: (12) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 16.41202ms)
Jan 28 01:02:24.603: INFO: (12) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 16.246292ms)
Jan 28 01:02:24.603: INFO: (12) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 16.18008ms)
Jan 28 01:02:24.603: INFO: (12) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 16.224946ms)
Jan 28 01:02:24.604: INFO: (12) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 17.239406ms)
Jan 28 01:02:24.604: INFO: (12) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 17.748909ms)
Jan 28 01:02:24.604: INFO: (12) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 17.364233ms)
Jan 28 01:02:24.604: INFO: (12) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 17.420646ms)
Jan 28 01:02:24.605: INFO: (12) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 18.204644ms)
Jan 28 01:02:24.606: INFO: (12) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 19.827474ms)
Jan 28 01:02:24.609: INFO: (12) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 22.867942ms)
Jan 28 01:02:24.609: INFO: (12) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 22.88106ms)
Jan 28 01:02:24.610: INFO: (12) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 24.220703ms)
Jan 28 01:02:24.610: INFO: (12) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 24.07229ms)
Jan 28 01:02:24.612: INFO: (12) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 25.20874ms)
Jan 28 01:02:24.612: INFO: (12) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 25.768478ms)
Jan 28 01:02:24.625: INFO: (13) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 12.497163ms)
Jan 28 01:02:24.628: INFO: (13) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 14.808022ms)
Jan 28 01:02:24.630: INFO: (13) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 16.709673ms)
Jan 28 01:02:24.630: INFO: (13) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 16.665679ms)
Jan 28 01:02:24.630: INFO: (13) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 16.765915ms)
Jan 28 01:02:24.632: INFO: (13) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 18.246676ms)
Jan 28 01:02:24.632: INFO: (13) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 18.346981ms)
Jan 28 01:02:24.632: INFO: (13) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 18.315947ms)
Jan 28 01:02:24.633: INFO: (13) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 19.8407ms)
Jan 28 01:02:24.634: INFO: (13) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 19.490334ms)
Jan 28 01:02:24.634: INFO: (13) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 20.545301ms)
Jan 28 01:02:24.635: INFO: (13) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 21.764429ms)
Jan 28 01:02:24.637: INFO: (13) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 24.177486ms)
Jan 28 01:02:24.638: INFO: (13) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 24.726383ms)
Jan 28 01:02:24.638: INFO: (13) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 24.978976ms)
Jan 28 01:02:24.639: INFO: (13) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 25.511392ms)
Jan 28 01:02:24.655: INFO: (14) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 14.577695ms)
Jan 28 01:02:24.657: INFO: (14) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 16.723066ms)
Jan 28 01:02:24.658: INFO: (14) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 16.525184ms)
Jan 28 01:02:24.658: INFO: (14) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 17.467244ms)
Jan 28 01:02:24.658: INFO: (14) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 17.752933ms)
Jan 28 01:02:24.659: INFO: (14) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 17.733727ms)
Jan 28 01:02:24.659: INFO: (14) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 18.352941ms)
Jan 28 01:02:24.659: INFO: (14) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 17.311857ms)
Jan 28 01:02:24.660: INFO: (14) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 18.107849ms)
Jan 28 01:02:24.661: INFO: (14) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 18.882ms)
Jan 28 01:02:24.668: INFO: (14) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 26.598549ms)
Jan 28 01:02:24.670: INFO: (14) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 29.742254ms)
Jan 28 01:02:24.670: INFO: (14) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 28.348581ms)
Jan 28 01:02:24.672: INFO: (14) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 30.432409ms)
Jan 28 01:02:24.672: INFO: (14) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 30.618707ms)
Jan 28 01:02:24.673: INFO: (14) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 31.412041ms)
Jan 28 01:02:24.691: INFO: (15) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 16.93796ms)
Jan 28 01:02:24.692: INFO: (15) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 17.172569ms)
Jan 28 01:02:24.694: INFO: (15) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 19.080883ms)
Jan 28 01:02:24.699: INFO: (15) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 24.298496ms)
Jan 28 01:02:24.700: INFO: (15) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 26.070328ms)
Jan 28 01:02:24.699: INFO: (15) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 25.016224ms)
Jan 28 01:02:24.700: INFO: (15) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 25.867438ms)
Jan 28 01:02:24.700: INFO: (15) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 25.828515ms)
Jan 28 01:02:24.700: INFO: (15) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 25.427999ms)
Jan 28 01:02:24.700: INFO: (15) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 25.447928ms)
Jan 28 01:02:24.702: INFO: (15) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 28.28059ms)
Jan 28 01:02:24.703: INFO: (15) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 28.678803ms)
Jan 28 01:02:24.703: INFO: (15) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 28.639831ms)
Jan 28 01:02:24.703: INFO: (15) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 28.986001ms)
Jan 28 01:02:24.703: INFO: (15) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 27.44957ms)
Jan 28 01:02:24.703: INFO: (15) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 29.181844ms)
Jan 28 01:02:24.718: INFO: (16) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 14.047249ms)
Jan 28 01:02:24.722: INFO: (16) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 17.428777ms)
Jan 28 01:02:24.725: INFO: (16) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 20.460512ms)
Jan 28 01:02:24.725: INFO: (16) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 20.847779ms)
Jan 28 01:02:24.725: INFO: (16) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 21.120809ms)
Jan 28 01:02:24.725: INFO: (16) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 21.25037ms)
Jan 28 01:02:24.726: INFO: (16) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 21.612721ms)
Jan 28 01:02:24.726: INFO: (16) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 21.702957ms)
Jan 28 01:02:24.727: INFO: (16) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 22.109383ms)
Jan 28 01:02:24.728: INFO: (16) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 22.900914ms)
Jan 28 01:02:24.731: INFO: (16) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 25.88981ms)
Jan 28 01:02:24.731: INFO: (16) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 26.838541ms)
Jan 28 01:02:24.732: INFO: (16) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 28.037444ms)
Jan 28 01:02:24.733: INFO: (16) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 28.00405ms)
Jan 28 01:02:24.733: INFO: (16) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 28.978323ms)
Jan 28 01:02:24.733: INFO: (16) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 29.411913ms)
Jan 28 01:02:24.748: INFO: (17) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 13.620136ms)
Jan 28 01:02:24.755: INFO: (17) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 21.045495ms)
Jan 28 01:02:24.756: INFO: (17) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 21.965305ms)
Jan 28 01:02:24.757: INFO: (17) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 23.244332ms)
Jan 28 01:02:24.758: INFO: (17) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 23.680489ms)
Jan 28 01:02:24.758: INFO: (17) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 24.079124ms)
Jan 28 01:02:24.759: INFO: (17) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 24.283811ms)
Jan 28 01:02:24.758: INFO: (17) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 23.825404ms)
Jan 28 01:02:24.761: INFO: (17) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 26.480457ms)
Jan 28 01:02:24.762: INFO: (17) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 26.848579ms)
Jan 28 01:02:24.762: INFO: (17) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 26.673067ms)
Jan 28 01:02:24.765: INFO: (17) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 30.465381ms)
Jan 28 01:02:24.765: INFO: (17) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 30.734188ms)
Jan 28 01:02:24.765: INFO: (17) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 30.761246ms)
Jan 28 01:02:24.766: INFO: (17) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 31.23602ms)
Jan 28 01:02:24.768: INFO: (17) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 33.914188ms)
Jan 28 01:02:24.781: INFO: (18) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 12.928871ms)
Jan 28 01:02:24.784: INFO: (18) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 15.546199ms)
Jan 28 01:02:24.787: INFO: (18) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 18.305326ms)
Jan 28 01:02:24.789: INFO: (18) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 19.798533ms)
Jan 28 01:02:24.789: INFO: (18) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 19.73672ms)
Jan 28 01:02:24.791: INFO: (18) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 21.83004ms)
Jan 28 01:02:24.791: INFO: (18) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 21.679868ms)
Jan 28 01:02:24.792: INFO: (18) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 23.775192ms)
Jan 28 01:02:24.793: INFO: (18) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 23.266241ms)
Jan 28 01:02:24.798: INFO: (18) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 29.278474ms)
Jan 28 01:02:24.798: INFO: (18) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 28.515312ms)
Jan 28 01:02:24.798: INFO: (18) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 28.644974ms)
Jan 28 01:02:24.798: INFO: (18) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 28.897501ms)
Jan 28 01:02:24.808: INFO: (18) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 39.49024ms)
Jan 28 01:02:24.808: INFO: (18) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 39.08504ms)
Jan 28 01:02:24.808: INFO: (18) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 39.675932ms)
Jan 28 01:02:24.828: INFO: (19) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 18.591151ms)
Jan 28 01:02:24.828: INFO: (19) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 18.268008ms)
Jan 28 01:02:24.828: INFO: (19) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 18.958877ms)
Jan 28 01:02:24.829: INFO: (19) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 19.190559ms)
Jan 28 01:02:24.830: INFO: (19) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 19.997362ms)
Jan 28 01:02:24.830: INFO: (19) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 21.734332ms)
Jan 28 01:02:24.831: INFO: (19) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 21.098442ms)
Jan 28 01:02:24.831: INFO: (19) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 21.709391ms)
Jan 28 01:02:24.832: INFO: (19) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 23.383214ms)
Jan 28 01:02:24.832: INFO: (19) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 22.070057ms)
Jan 28 01:02:24.833: INFO: (19) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 24.225506ms)
Jan 28 01:02:24.838: INFO: (19) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 27.642415ms)
Jan 28 01:02:24.839: INFO: (19) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 29.297461ms)
Jan 28 01:02:24.839: INFO: (19) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 29.402258ms)
Jan 28 01:02:24.839: INFO: (19) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 29.317302ms)
Jan 28 01:02:24.839: INFO: (19) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 30.106752ms)
STEP: deleting ReplicationController proxy-service-twc2j in namespace proxy-2036, will wait for the garbage collector to delete the pods 01/28/23 01:02:24.84
Jan 28 01:02:24.917: INFO: Deleting ReplicationController proxy-service-twc2j took: 17.426513ms
Jan 28 01:02:25.018: INFO: Terminating ReplicationController proxy-service-twc2j pods took: 100.982867ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 28 01:02:27.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2036" for this suite. 01/28/23 01:02:27.035
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":135,"skipped":2503,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.138 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:02:20.916
    Jan 28 01:02:20.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename proxy 01/28/23 01:02:20.918
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:02:20.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:02:20.963
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/28/23 01:02:21.026
    STEP: creating replication controller proxy-service-twc2j in namespace proxy-2036 01/28/23 01:02:21.026
    I0128 01:02:21.042393      22 runners.go:193] Created replication controller with name: proxy-service-twc2j, namespace: proxy-2036, replica count: 1
    I0128 01:02:22.093689      22 runners.go:193] proxy-service-twc2j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0128 01:02:23.094829      22 runners.go:193] proxy-service-twc2j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0128 01:02:24.095773      22 runners.go:193] proxy-service-twc2j Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 28 01:02:24.108: INFO: setup took 3.136754793s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/28/23 01:02:24.108
    Jan 28 01:02:24.170: INFO: (0) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 61.436864ms)
    Jan 28 01:02:24.170: INFO: (0) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 61.06323ms)
    Jan 28 01:02:24.172: INFO: (0) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 62.46072ms)
    Jan 28 01:02:24.172: INFO: (0) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 62.424345ms)
    Jan 28 01:02:24.178: INFO: (0) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 69.557844ms)
    Jan 28 01:02:24.179: INFO: (0) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 69.060753ms)
    Jan 28 01:02:24.180: INFO: (0) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 70.75281ms)
    Jan 28 01:02:24.182: INFO: (0) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 72.121597ms)
    Jan 28 01:02:24.187: INFO: (0) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 77.739158ms)
    Jan 28 01:02:24.188: INFO: (0) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 78.457131ms)
    Jan 28 01:02:24.189: INFO: (0) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 80.195425ms)
    Jan 28 01:02:24.196: INFO: (0) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 87.252068ms)
    Jan 28 01:02:24.196: INFO: (0) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 86.599082ms)
    Jan 28 01:02:24.196: INFO: (0) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 87.00749ms)
    Jan 28 01:02:24.196: INFO: (0) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 87.161421ms)
    Jan 28 01:02:24.199: INFO: (0) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 89.842947ms)
    Jan 28 01:02:24.221: INFO: (1) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 20.935632ms)
    Jan 28 01:02:24.222: INFO: (1) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 22.043595ms)
    Jan 28 01:02:24.222: INFO: (1) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 21.068985ms)
    Jan 28 01:02:24.222: INFO: (1) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 21.062385ms)
    Jan 28 01:02:24.222: INFO: (1) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 21.526418ms)
    Jan 28 01:02:24.222: INFO: (1) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 21.766736ms)
    Jan 28 01:02:24.224: INFO: (1) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 22.901984ms)
    Jan 28 01:02:24.224: INFO: (1) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 24.132024ms)
    Jan 28 01:02:24.224: INFO: (1) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 23.356642ms)
    Jan 28 01:02:24.224: INFO: (1) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 23.488872ms)
    Jan 28 01:02:24.227: INFO: (1) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 25.801938ms)
    Jan 28 01:02:24.227: INFO: (1) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 27.123015ms)
    Jan 28 01:02:24.228: INFO: (1) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 27.945512ms)
    Jan 28 01:02:24.229: INFO: (1) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 28.542975ms)
    Jan 28 01:02:24.231: INFO: (1) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 30.777382ms)
    Jan 28 01:02:24.231: INFO: (1) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 30.074128ms)
    Jan 28 01:02:24.246: INFO: (2) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 15.154151ms)
    Jan 28 01:02:24.247: INFO: (2) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 15.314258ms)
    Jan 28 01:02:24.248: INFO: (2) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 16.195255ms)
    Jan 28 01:02:24.248: INFO: (2) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 16.285424ms)
    Jan 28 01:02:24.249: INFO: (2) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 17.183574ms)
    Jan 28 01:02:24.250: INFO: (2) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 17.788801ms)
    Jan 28 01:02:24.252: INFO: (2) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 19.413816ms)
    Jan 28 01:02:24.253: INFO: (2) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 20.855427ms)
    Jan 28 01:02:24.253: INFO: (2) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 20.756038ms)
    Jan 28 01:02:24.254: INFO: (2) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 21.964257ms)
    Jan 28 01:02:24.255: INFO: (2) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 23.65047ms)
    Jan 28 01:02:24.259: INFO: (2) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 26.822955ms)
    Jan 28 01:02:24.260: INFO: (2) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 28.415742ms)
    Jan 28 01:02:24.260: INFO: (2) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 28.173069ms)
    Jan 28 01:02:24.260: INFO: (2) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 27.787749ms)
    Jan 28 01:02:24.261: INFO: (2) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 29.338847ms)
    Jan 28 01:02:24.285: INFO: (3) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 21.793206ms)
    Jan 28 01:02:24.286: INFO: (3) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 23.355802ms)
    Jan 28 01:02:24.286: INFO: (3) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 22.841135ms)
    Jan 28 01:02:24.287: INFO: (3) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 21.925552ms)
    Jan 28 01:02:24.287: INFO: (3) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 24.008977ms)
    Jan 28 01:02:24.287: INFO: (3) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 23.612697ms)
    Jan 28 01:02:24.287: INFO: (3) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 25.130921ms)
    Jan 28 01:02:24.287: INFO: (3) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 24.916339ms)
    Jan 28 01:02:24.287: INFO: (3) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 23.520785ms)
    Jan 28 01:02:24.288: INFO: (3) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 23.647443ms)
    Jan 28 01:02:24.292: INFO: (3) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 28.481508ms)
    Jan 28 01:02:24.292: INFO: (3) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 29.796975ms)
    Jan 28 01:02:24.292: INFO: (3) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 28.124365ms)
    Jan 28 01:02:24.293: INFO: (3) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 28.530225ms)
    Jan 28 01:02:24.293: INFO: (3) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 29.704047ms)
    Jan 28 01:02:24.294: INFO: (3) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 29.804851ms)
    Jan 28 01:02:24.313: INFO: (4) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 18.66641ms)
    Jan 28 01:02:24.313: INFO: (4) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 18.375297ms)
    Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 50.308332ms)
    Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 49.758417ms)
    Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 50.725106ms)
    Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 48.036006ms)
    Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 50.664119ms)
    Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 50.174187ms)
    Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 50.127686ms)
    Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 50.144049ms)
    Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 49.041228ms)
    Jan 28 01:02:24.345: INFO: (4) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 50.635337ms)
    Jan 28 01:02:24.350: INFO: (4) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 54.06452ms)
    Jan 28 01:02:24.350: INFO: (4) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 55.460026ms)
    Jan 28 01:02:24.351: INFO: (4) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 54.316731ms)
    Jan 28 01:02:24.351: INFO: (4) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 54.888189ms)
    Jan 28 01:02:24.369: INFO: (5) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 17.298264ms)
    Jan 28 01:02:24.373: INFO: (5) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 22.058343ms)
    Jan 28 01:02:24.374: INFO: (5) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 22.201905ms)
    Jan 28 01:02:24.375: INFO: (5) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 23.37871ms)
    Jan 28 01:02:24.376: INFO: (5) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 24.703044ms)
    Jan 28 01:02:24.376: INFO: (5) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 24.433885ms)
    Jan 28 01:02:24.376: INFO: (5) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 25.203829ms)
    Jan 28 01:02:24.376: INFO: (5) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 24.643004ms)
    Jan 28 01:02:24.377: INFO: (5) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 24.740875ms)
    Jan 28 01:02:24.376: INFO: (5) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 25.527488ms)
    Jan 28 01:02:24.379: INFO: (5) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 28.176674ms)
    Jan 28 01:02:24.380: INFO: (5) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 27.604149ms)
    Jan 28 01:02:24.384: INFO: (5) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 32.467518ms)
    Jan 28 01:02:24.384: INFO: (5) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 32.906131ms)
    Jan 28 01:02:24.385: INFO: (5) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 32.805592ms)
    Jan 28 01:02:24.385: INFO: (5) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 33.464588ms)
    Jan 28 01:02:24.401: INFO: (6) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 15.572709ms)
    Jan 28 01:02:24.406: INFO: (6) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 19.380786ms)
    Jan 28 01:02:24.408: INFO: (6) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 21.417175ms)
    Jan 28 01:02:24.409: INFO: (6) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 22.826184ms)
    Jan 28 01:02:24.409: INFO: (6) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 21.4081ms)
    Jan 28 01:02:24.409: INFO: (6) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 22.064364ms)
    Jan 28 01:02:24.409: INFO: (6) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 21.745813ms)
    Jan 28 01:02:24.410: INFO: (6) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 21.462686ms)
    Jan 28 01:02:24.411: INFO: (6) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 21.865155ms)
    Jan 28 01:02:24.411: INFO: (6) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 22.467668ms)
    Jan 28 01:02:24.411: INFO: (6) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 24.371826ms)
    Jan 28 01:02:24.413: INFO: (6) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 24.176773ms)
    Jan 28 01:02:24.413: INFO: (6) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 26.92994ms)
    Jan 28 01:02:24.413: INFO: (6) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 24.863169ms)
    Jan 28 01:02:24.416: INFO: (6) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 29.769322ms)
    Jan 28 01:02:24.417: INFO: (6) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 29.624593ms)
    Jan 28 01:02:24.434: INFO: (7) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 16.281488ms)
    Jan 28 01:02:24.434: INFO: (7) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 16.359321ms)
    Jan 28 01:02:24.435: INFO: (7) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 17.218292ms)
    Jan 28 01:02:24.437: INFO: (7) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 18.930185ms)
    Jan 28 01:02:24.439: INFO: (7) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 20.920495ms)
    Jan 28 01:02:24.439: INFO: (7) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 21.485803ms)
    Jan 28 01:02:24.439: INFO: (7) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 21.110885ms)
    Jan 28 01:02:24.442: INFO: (7) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 25.184837ms)
    Jan 28 01:02:24.442: INFO: (7) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 24.29071ms)
    Jan 28 01:02:24.443: INFO: (7) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 26.036094ms)
    Jan 28 01:02:24.448: INFO: (7) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 31.101179ms)
    Jan 28 01:02:24.448: INFO: (7) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 31.105014ms)
    Jan 28 01:02:24.448: INFO: (7) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 31.43638ms)
    Jan 28 01:02:24.449: INFO: (7) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 30.205686ms)
    Jan 28 01:02:24.449: INFO: (7) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 31.371585ms)
    Jan 28 01:02:24.451: INFO: (7) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 33.121259ms)
    Jan 28 01:02:24.465: INFO: (8) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 14.182134ms)
    Jan 28 01:02:24.467: INFO: (8) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 15.62493ms)
    Jan 28 01:02:24.471: INFO: (8) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 18.848501ms)
    Jan 28 01:02:24.472: INFO: (8) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 20.313689ms)
    Jan 28 01:02:24.472: INFO: (8) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 19.854662ms)
    Jan 28 01:02:24.473: INFO: (8) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 21.206133ms)
    Jan 28 01:02:24.474: INFO: (8) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 21.3577ms)
    Jan 28 01:02:24.474: INFO: (8) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 21.307234ms)
    Jan 28 01:02:24.474: INFO: (8) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 21.97169ms)
    Jan 28 01:02:24.474: INFO: (8) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 21.773999ms)
    Jan 28 01:02:24.477: INFO: (8) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 24.834168ms)
    Jan 28 01:02:24.484: INFO: (8) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 32.152358ms)
    Jan 28 01:02:24.484: INFO: (8) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 31.941349ms)
    Jan 28 01:02:24.484: INFO: (8) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 31.817382ms)
    Jan 28 01:02:24.484: INFO: (8) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 32.589591ms)
    Jan 28 01:02:24.485: INFO: (8) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 32.469509ms)
    Jan 28 01:02:24.500: INFO: (9) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 14.444433ms)
    Jan 28 01:02:24.502: INFO: (9) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 16.956775ms)
    Jan 28 01:02:24.503: INFO: (9) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 17.634388ms)
    Jan 28 01:02:24.504: INFO: (9) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 17.661875ms)
    Jan 28 01:02:24.504: INFO: (9) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 17.807815ms)
    Jan 28 01:02:24.504: INFO: (9) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 18.285967ms)
    Jan 28 01:02:24.504: INFO: (9) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 17.83681ms)
    Jan 28 01:02:24.505: INFO: (9) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 19.719115ms)
    Jan 28 01:02:24.506: INFO: (9) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 20.9863ms)
    Jan 28 01:02:24.507: INFO: (9) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 21.026789ms)
    Jan 28 01:02:24.507: INFO: (9) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 21.850587ms)
    Jan 28 01:02:24.509: INFO: (9) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 23.710726ms)
    Jan 28 01:02:24.509: INFO: (9) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 23.66539ms)
    Jan 28 01:02:24.509: INFO: (9) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 23.468031ms)
    Jan 28 01:02:24.511: INFO: (9) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 25.165654ms)
    Jan 28 01:02:24.512: INFO: (9) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 26.337664ms)
    Jan 28 01:02:24.528: INFO: (10) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 15.576143ms)
    Jan 28 01:02:24.529: INFO: (10) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 17.228099ms)
    Jan 28 01:02:24.530: INFO: (10) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 16.322004ms)
    Jan 28 01:02:24.530: INFO: (10) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 17.058127ms)
    Jan 28 01:02:24.533: INFO: (10) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 20.326409ms)
    Jan 28 01:02:24.533: INFO: (10) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 20.365785ms)
    Jan 28 01:02:24.533: INFO: (10) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 20.927922ms)
    Jan 28 01:02:24.533: INFO: (10) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 20.778687ms)
    Jan 28 01:02:24.533: INFO: (10) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 20.25417ms)
    Jan 28 01:02:24.533: INFO: (10) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 20.31233ms)
    Jan 28 01:02:24.534: INFO: (10) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 21.529977ms)
    Jan 28 01:02:24.536: INFO: (10) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 22.928746ms)
    Jan 28 01:02:24.537: INFO: (10) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 24.517012ms)
    Jan 28 01:02:24.538: INFO: (10) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 25.546288ms)
    Jan 28 01:02:24.547: INFO: (10) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 33.792484ms)
    Jan 28 01:02:24.547: INFO: (10) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 34.393148ms)
    Jan 28 01:02:24.574: INFO: (11) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 25.297445ms)
    Jan 28 01:02:24.574: INFO: (11) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 25.655603ms)
    Jan 28 01:02:24.574: INFO: (11) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 25.666163ms)
    Jan 28 01:02:24.574: INFO: (11) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 26.052658ms)
    Jan 28 01:02:24.574: INFO: (11) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 25.685787ms)
    Jan 28 01:02:24.575: INFO: (11) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 26.006433ms)
    Jan 28 01:02:24.576: INFO: (11) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 27.653911ms)
    Jan 28 01:02:24.576: INFO: (11) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 27.069078ms)
    Jan 28 01:02:24.576: INFO: (11) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 27.010469ms)
    Jan 28 01:02:24.576: INFO: (11) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 27.725619ms)
    Jan 28 01:02:24.575: INFO: (11) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 26.219748ms)
    Jan 28 01:02:24.578: INFO: (11) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 29.739204ms)
    Jan 28 01:02:24.579: INFO: (11) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 31.19825ms)
    Jan 28 01:02:24.580: INFO: (11) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 31.279386ms)
    Jan 28 01:02:24.582: INFO: (11) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 33.932505ms)
    Jan 28 01:02:24.586: INFO: (11) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 37.801194ms)
    Jan 28 01:02:24.602: INFO: (12) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 16.41202ms)
    Jan 28 01:02:24.603: INFO: (12) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 16.246292ms)
    Jan 28 01:02:24.603: INFO: (12) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 16.18008ms)
    Jan 28 01:02:24.603: INFO: (12) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 16.224946ms)
    Jan 28 01:02:24.604: INFO: (12) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 17.239406ms)
    Jan 28 01:02:24.604: INFO: (12) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 17.748909ms)
    Jan 28 01:02:24.604: INFO: (12) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 17.364233ms)
    Jan 28 01:02:24.604: INFO: (12) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 17.420646ms)
    Jan 28 01:02:24.605: INFO: (12) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 18.204644ms)
    Jan 28 01:02:24.606: INFO: (12) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 19.827474ms)
    Jan 28 01:02:24.609: INFO: (12) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 22.867942ms)
    Jan 28 01:02:24.609: INFO: (12) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 22.88106ms)
    Jan 28 01:02:24.610: INFO: (12) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 24.220703ms)
    Jan 28 01:02:24.610: INFO: (12) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 24.07229ms)
    Jan 28 01:02:24.612: INFO: (12) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 25.20874ms)
    Jan 28 01:02:24.612: INFO: (12) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 25.768478ms)
    Jan 28 01:02:24.625: INFO: (13) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 12.497163ms)
    Jan 28 01:02:24.628: INFO: (13) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 14.808022ms)
    Jan 28 01:02:24.630: INFO: (13) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 16.709673ms)
    Jan 28 01:02:24.630: INFO: (13) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 16.665679ms)
    Jan 28 01:02:24.630: INFO: (13) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 16.765915ms)
    Jan 28 01:02:24.632: INFO: (13) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 18.246676ms)
    Jan 28 01:02:24.632: INFO: (13) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 18.346981ms)
    Jan 28 01:02:24.632: INFO: (13) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 18.315947ms)
    Jan 28 01:02:24.633: INFO: (13) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 19.8407ms)
    Jan 28 01:02:24.634: INFO: (13) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 19.490334ms)
    Jan 28 01:02:24.634: INFO: (13) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 20.545301ms)
    Jan 28 01:02:24.635: INFO: (13) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 21.764429ms)
    Jan 28 01:02:24.637: INFO: (13) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 24.177486ms)
    Jan 28 01:02:24.638: INFO: (13) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 24.726383ms)
    Jan 28 01:02:24.638: INFO: (13) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 24.978976ms)
    Jan 28 01:02:24.639: INFO: (13) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 25.511392ms)
    Jan 28 01:02:24.655: INFO: (14) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 14.577695ms)
    Jan 28 01:02:24.657: INFO: (14) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 16.723066ms)
    Jan 28 01:02:24.658: INFO: (14) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 16.525184ms)
    Jan 28 01:02:24.658: INFO: (14) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 17.467244ms)
    Jan 28 01:02:24.658: INFO: (14) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 17.752933ms)
    Jan 28 01:02:24.659: INFO: (14) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 17.733727ms)
    Jan 28 01:02:24.659: INFO: (14) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 18.352941ms)
    Jan 28 01:02:24.659: INFO: (14) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 17.311857ms)
    Jan 28 01:02:24.660: INFO: (14) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 18.107849ms)
    Jan 28 01:02:24.661: INFO: (14) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 18.882ms)
    Jan 28 01:02:24.668: INFO: (14) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 26.598549ms)
    Jan 28 01:02:24.670: INFO: (14) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 29.742254ms)
    Jan 28 01:02:24.670: INFO: (14) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 28.348581ms)
    Jan 28 01:02:24.672: INFO: (14) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 30.432409ms)
    Jan 28 01:02:24.672: INFO: (14) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 30.618707ms)
    Jan 28 01:02:24.673: INFO: (14) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 31.412041ms)
    Jan 28 01:02:24.691: INFO: (15) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 16.93796ms)
    Jan 28 01:02:24.692: INFO: (15) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 17.172569ms)
    Jan 28 01:02:24.694: INFO: (15) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 19.080883ms)
    Jan 28 01:02:24.699: INFO: (15) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 24.298496ms)
    Jan 28 01:02:24.700: INFO: (15) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 26.070328ms)
    Jan 28 01:02:24.699: INFO: (15) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 25.016224ms)
    Jan 28 01:02:24.700: INFO: (15) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 25.867438ms)
    Jan 28 01:02:24.700: INFO: (15) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 25.828515ms)
    Jan 28 01:02:24.700: INFO: (15) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 25.427999ms)
    Jan 28 01:02:24.700: INFO: (15) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 25.447928ms)
    Jan 28 01:02:24.702: INFO: (15) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 28.28059ms)
    Jan 28 01:02:24.703: INFO: (15) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 28.678803ms)
    Jan 28 01:02:24.703: INFO: (15) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 28.639831ms)
    Jan 28 01:02:24.703: INFO: (15) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 28.986001ms)
    Jan 28 01:02:24.703: INFO: (15) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 27.44957ms)
    Jan 28 01:02:24.703: INFO: (15) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 29.181844ms)
    Jan 28 01:02:24.718: INFO: (16) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 14.047249ms)
    Jan 28 01:02:24.722: INFO: (16) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 17.428777ms)
    Jan 28 01:02:24.725: INFO: (16) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 20.460512ms)
    Jan 28 01:02:24.725: INFO: (16) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 20.847779ms)
    Jan 28 01:02:24.725: INFO: (16) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 21.120809ms)
    Jan 28 01:02:24.725: INFO: (16) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 21.25037ms)
    Jan 28 01:02:24.726: INFO: (16) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 21.612721ms)
    Jan 28 01:02:24.726: INFO: (16) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 21.702957ms)
    Jan 28 01:02:24.727: INFO: (16) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 22.109383ms)
    Jan 28 01:02:24.728: INFO: (16) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 22.900914ms)
    Jan 28 01:02:24.731: INFO: (16) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 25.88981ms)
    Jan 28 01:02:24.731: INFO: (16) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 26.838541ms)
    Jan 28 01:02:24.732: INFO: (16) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 28.037444ms)
    Jan 28 01:02:24.733: INFO: (16) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 28.00405ms)
    Jan 28 01:02:24.733: INFO: (16) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 28.978323ms)
    Jan 28 01:02:24.733: INFO: (16) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 29.411913ms)
    Jan 28 01:02:24.748: INFO: (17) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 13.620136ms)
    Jan 28 01:02:24.755: INFO: (17) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 21.045495ms)
    Jan 28 01:02:24.756: INFO: (17) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 21.965305ms)
    Jan 28 01:02:24.757: INFO: (17) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 23.244332ms)
    Jan 28 01:02:24.758: INFO: (17) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 23.680489ms)
    Jan 28 01:02:24.758: INFO: (17) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 24.079124ms)
    Jan 28 01:02:24.759: INFO: (17) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 24.283811ms)
    Jan 28 01:02:24.758: INFO: (17) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 23.825404ms)
    Jan 28 01:02:24.761: INFO: (17) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 26.480457ms)
    Jan 28 01:02:24.762: INFO: (17) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 26.848579ms)
    Jan 28 01:02:24.762: INFO: (17) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 26.673067ms)
    Jan 28 01:02:24.765: INFO: (17) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 30.465381ms)
    Jan 28 01:02:24.765: INFO: (17) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 30.734188ms)
    Jan 28 01:02:24.765: INFO: (17) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 30.761246ms)
    Jan 28 01:02:24.766: INFO: (17) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 31.23602ms)
    Jan 28 01:02:24.768: INFO: (17) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 33.914188ms)
    Jan 28 01:02:24.781: INFO: (18) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 12.928871ms)
    Jan 28 01:02:24.784: INFO: (18) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 15.546199ms)
    Jan 28 01:02:24.787: INFO: (18) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 18.305326ms)
    Jan 28 01:02:24.789: INFO: (18) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 19.798533ms)
    Jan 28 01:02:24.789: INFO: (18) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 19.73672ms)
    Jan 28 01:02:24.791: INFO: (18) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 21.83004ms)
    Jan 28 01:02:24.791: INFO: (18) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 21.679868ms)
    Jan 28 01:02:24.792: INFO: (18) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 23.775192ms)
    Jan 28 01:02:24.793: INFO: (18) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 23.266241ms)
    Jan 28 01:02:24.798: INFO: (18) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 29.278474ms)
    Jan 28 01:02:24.798: INFO: (18) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 28.515312ms)
    Jan 28 01:02:24.798: INFO: (18) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 28.644974ms)
    Jan 28 01:02:24.798: INFO: (18) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 28.897501ms)
    Jan 28 01:02:24.808: INFO: (18) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 39.49024ms)
    Jan 28 01:02:24.808: INFO: (18) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 39.08504ms)
    Jan 28 01:02:24.808: INFO: (18) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 39.675932ms)
    Jan 28 01:02:24.828: INFO: (19) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss/proxy/rewriteme">test</a> (200; 18.591151ms)
    Jan 28 01:02:24.828: INFO: (19) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:160/proxy/: foo (200; 18.268008ms)
    Jan 28 01:02:24.828: INFO: (19) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:460/proxy/: tls baz (200; 18.958877ms)
    Jan 28 01:02:24.829: INFO: (19) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:160/proxy/: foo (200; 19.190559ms)
    Jan 28 01:02:24.830: INFO: (19) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:1080/proxy/rewriteme">test<... (200; 19.997362ms)
    Jan 28 01:02:24.830: INFO: (19) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:1080/proxy/rewriteme">... (200; 21.734332ms)
    Jan 28 01:02:24.831: INFO: (19) /api/v1/namespaces/proxy-2036/pods/http:proxy-service-twc2j-8thss:162/proxy/: bar (200; 21.098442ms)
    Jan 28 01:02:24.831: INFO: (19) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:462/proxy/: tls qux (200; 21.709391ms)
    Jan 28 01:02:24.832: INFO: (19) /api/v1/namespaces/proxy-2036/pods/proxy-service-twc2j-8thss:162/proxy/: bar (200; 23.383214ms)
    Jan 28 01:02:24.832: INFO: (19) /api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/: <a href="/api/v1/namespaces/proxy-2036/pods/https:proxy-service-twc2j-8thss:443/proxy/tlsrewritem... (200; 22.070057ms)
    Jan 28 01:02:24.833: INFO: (19) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname1/proxy/: foo (200; 24.225506ms)
    Jan 28 01:02:24.838: INFO: (19) /api/v1/namespaces/proxy-2036/services/proxy-service-twc2j:portname2/proxy/: bar (200; 27.642415ms)
    Jan 28 01:02:24.839: INFO: (19) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname2/proxy/: bar (200; 29.297461ms)
    Jan 28 01:02:24.839: INFO: (19) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname1/proxy/: tls baz (200; 29.402258ms)
    Jan 28 01:02:24.839: INFO: (19) /api/v1/namespaces/proxy-2036/services/http:proxy-service-twc2j:portname1/proxy/: foo (200; 29.317302ms)
    Jan 28 01:02:24.839: INFO: (19) /api/v1/namespaces/proxy-2036/services/https:proxy-service-twc2j:tlsportname2/proxy/: tls qux (200; 30.106752ms)
    STEP: deleting ReplicationController proxy-service-twc2j in namespace proxy-2036, will wait for the garbage collector to delete the pods 01/28/23 01:02:24.84
    Jan 28 01:02:24.917: INFO: Deleting ReplicationController proxy-service-twc2j took: 17.426513ms
    Jan 28 01:02:25.018: INFO: Terminating ReplicationController proxy-service-twc2j pods took: 100.982867ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 28 01:02:27.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-2036" for this suite. 01/28/23 01:02:27.035
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:02:27.058
Jan 28 01:02:27.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename webhook 01/28/23 01:02:27.062
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:02:27.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:02:27.106
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/28/23 01:02:27.145
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:02:27.395
STEP: Deploying the webhook pod 01/28/23 01:02:27.415
STEP: Wait for the deployment to be ready 01/28/23 01:02:27.444
Jan 28 01:02:27.479: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 01:02:29.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 2, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 2, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 2, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 2, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/28/23 01:02:31.542
STEP: Verifying the service has paired with the endpoint 01/28/23 01:02:31.569
Jan 28 01:02:32.570: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 01/28/23 01:02:32.732
STEP: Creating a configMap that does not comply to the validation webhook rules 01/28/23 01:02:32.845
STEP: Deleting the collection of validation webhooks 01/28/23 01:02:32.936
STEP: Creating a configMap that does not comply to the validation webhook rules 01/28/23 01:02:33.072
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:02:33.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4555" for this suite. 01/28/23 01:02:33.126
STEP: Destroying namespace "webhook-4555-markers" for this suite. 01/28/23 01:02:33.145
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":136,"skipped":2519,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.216 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:02:27.058
    Jan 28 01:02:27.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename webhook 01/28/23 01:02:27.062
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:02:27.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:02:27.106
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/28/23 01:02:27.145
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:02:27.395
    STEP: Deploying the webhook pod 01/28/23 01:02:27.415
    STEP: Wait for the deployment to be ready 01/28/23 01:02:27.444
    Jan 28 01:02:27.479: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 28 01:02:29.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 2, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 2, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 2, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 2, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/28/23 01:02:31.542
    STEP: Verifying the service has paired with the endpoint 01/28/23 01:02:31.569
    Jan 28 01:02:32.570: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 01/28/23 01:02:32.732
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/28/23 01:02:32.845
    STEP: Deleting the collection of validation webhooks 01/28/23 01:02:32.936
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/28/23 01:02:33.072
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:02:33.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4555" for this suite. 01/28/23 01:02:33.126
    STEP: Destroying namespace "webhook-4555-markers" for this suite. 01/28/23 01:02:33.145
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:02:33.279
Jan 28 01:02:33.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 01:02:33.281
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:02:33.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:02:33.321
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-97638d43-6c6a-4056-90da-4d5e0ce906bf 01/28/23 01:02:33.35
STEP: Creating secret with name s-test-opt-upd-48a4fabc-0e49-4ecf-906b-2490de204c71 01/28/23 01:02:33.362
STEP: Creating the pod 01/28/23 01:02:33.379
Jan 28 01:02:33.402: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-896b6eb2-8737-48e5-b7ed-174b9171fac6" in namespace "projected-8940" to be "running and ready"
Jan 28 01:02:33.412: INFO: Pod "pod-projected-secrets-896b6eb2-8737-48e5-b7ed-174b9171fac6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.64557ms
Jan 28 01:02:33.412: INFO: The phase of Pod pod-projected-secrets-896b6eb2-8737-48e5-b7ed-174b9171fac6 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:02:35.425: INFO: Pod "pod-projected-secrets-896b6eb2-8737-48e5-b7ed-174b9171fac6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023347872s
Jan 28 01:02:35.426: INFO: The phase of Pod pod-projected-secrets-896b6eb2-8737-48e5-b7ed-174b9171fac6 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:02:37.425: INFO: Pod "pod-projected-secrets-896b6eb2-8737-48e5-b7ed-174b9171fac6": Phase="Running", Reason="", readiness=true. Elapsed: 4.022693167s
Jan 28 01:02:37.425: INFO: The phase of Pod pod-projected-secrets-896b6eb2-8737-48e5-b7ed-174b9171fac6 is Running (Ready = true)
Jan 28 01:02:37.425: INFO: Pod "pod-projected-secrets-896b6eb2-8737-48e5-b7ed-174b9171fac6" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-97638d43-6c6a-4056-90da-4d5e0ce906bf 01/28/23 01:02:37.515
STEP: Updating secret s-test-opt-upd-48a4fabc-0e49-4ecf-906b-2490de204c71 01/28/23 01:02:37.533
STEP: Creating secret with name s-test-opt-create-55e54846-b0ac-452f-aa0d-0c1d845d87ac 01/28/23 01:02:37.545
STEP: waiting to observe update in volume 01/28/23 01:02:37.556
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 28 01:04:09.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8940" for this suite. 01/28/23 01:04:09.121
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":137,"skipped":2525,"failed":0}
------------------------------
â€¢ [SLOW TEST] [95.861 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:02:33.279
    Jan 28 01:02:33.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 01:02:33.281
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:02:33.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:02:33.321
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-97638d43-6c6a-4056-90da-4d5e0ce906bf 01/28/23 01:02:33.35
    STEP: Creating secret with name s-test-opt-upd-48a4fabc-0e49-4ecf-906b-2490de204c71 01/28/23 01:02:33.362
    STEP: Creating the pod 01/28/23 01:02:33.379
    Jan 28 01:02:33.402: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-896b6eb2-8737-48e5-b7ed-174b9171fac6" in namespace "projected-8940" to be "running and ready"
    Jan 28 01:02:33.412: INFO: Pod "pod-projected-secrets-896b6eb2-8737-48e5-b7ed-174b9171fac6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.64557ms
    Jan 28 01:02:33.412: INFO: The phase of Pod pod-projected-secrets-896b6eb2-8737-48e5-b7ed-174b9171fac6 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:02:35.425: INFO: Pod "pod-projected-secrets-896b6eb2-8737-48e5-b7ed-174b9171fac6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023347872s
    Jan 28 01:02:35.426: INFO: The phase of Pod pod-projected-secrets-896b6eb2-8737-48e5-b7ed-174b9171fac6 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:02:37.425: INFO: Pod "pod-projected-secrets-896b6eb2-8737-48e5-b7ed-174b9171fac6": Phase="Running", Reason="", readiness=true. Elapsed: 4.022693167s
    Jan 28 01:02:37.425: INFO: The phase of Pod pod-projected-secrets-896b6eb2-8737-48e5-b7ed-174b9171fac6 is Running (Ready = true)
    Jan 28 01:02:37.425: INFO: Pod "pod-projected-secrets-896b6eb2-8737-48e5-b7ed-174b9171fac6" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-97638d43-6c6a-4056-90da-4d5e0ce906bf 01/28/23 01:02:37.515
    STEP: Updating secret s-test-opt-upd-48a4fabc-0e49-4ecf-906b-2490de204c71 01/28/23 01:02:37.533
    STEP: Creating secret with name s-test-opt-create-55e54846-b0ac-452f-aa0d-0c1d845d87ac 01/28/23 01:02:37.545
    STEP: waiting to observe update in volume 01/28/23 01:02:37.556
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 28 01:04:09.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8940" for this suite. 01/28/23 01:04:09.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:04:09.143
Jan 28 01:04:09.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename ingressclass 01/28/23 01:04:09.146
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:04:09.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:04:09.192
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/28/23 01:04:09.201
STEP: getting /apis/networking.k8s.io 01/28/23 01:04:09.208
STEP: getting /apis/networking.k8s.iov1 01/28/23 01:04:09.215
STEP: creating 01/28/23 01:04:09.218
STEP: getting 01/28/23 01:04:09.262
STEP: listing 01/28/23 01:04:09.273
STEP: watching 01/28/23 01:04:09.284
Jan 28 01:04:09.284: INFO: starting watch
STEP: patching 01/28/23 01:04:09.288
STEP: updating 01/28/23 01:04:09.304
Jan 28 01:04:09.317: INFO: waiting for watch events with expected annotations
Jan 28 01:04:09.317: INFO: saw patched and updated annotations
STEP: deleting 01/28/23 01:04:09.317
STEP: deleting a collection 01/28/23 01:04:09.36
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Jan 28 01:04:09.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-357" for this suite. 01/28/23 01:04:09.427
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":138,"skipped":2539,"failed":0}
------------------------------
â€¢ [0.302 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:04:09.143
    Jan 28 01:04:09.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename ingressclass 01/28/23 01:04:09.146
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:04:09.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:04:09.192
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/28/23 01:04:09.201
    STEP: getting /apis/networking.k8s.io 01/28/23 01:04:09.208
    STEP: getting /apis/networking.k8s.iov1 01/28/23 01:04:09.215
    STEP: creating 01/28/23 01:04:09.218
    STEP: getting 01/28/23 01:04:09.262
    STEP: listing 01/28/23 01:04:09.273
    STEP: watching 01/28/23 01:04:09.284
    Jan 28 01:04:09.284: INFO: starting watch
    STEP: patching 01/28/23 01:04:09.288
    STEP: updating 01/28/23 01:04:09.304
    Jan 28 01:04:09.317: INFO: waiting for watch events with expected annotations
    Jan 28 01:04:09.317: INFO: saw patched and updated annotations
    STEP: deleting 01/28/23 01:04:09.317
    STEP: deleting a collection 01/28/23 01:04:09.36
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Jan 28 01:04:09.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-357" for this suite. 01/28/23 01:04:09.427
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:04:09.446
Jan 28 01:04:09.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename statefulset 01/28/23 01:04:09.448
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:04:09.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:04:09.484
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2969 01/28/23 01:04:09.494
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 01/28/23 01:04:09.508
Jan 28 01:04:09.535: INFO: Found 0 stateful pods, waiting for 3
Jan 28 01:04:19.548: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 01:04:19.548: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 01:04:19.548: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 01:04:19.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-2969 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 01:04:19.906: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 01:04:19.906: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 01:04:19.906: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/28/23 01:04:29.952
Jan 28 01:04:29.987: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/28/23 01:04:29.987
STEP: Updating Pods in reverse ordinal order 01/28/23 01:04:40.033
Jan 28 01:04:40.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-2969 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 28 01:04:40.393: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 28 01:04:40.393: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 28 01:04:40.393: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 28 01:04:50.459: INFO: Waiting for StatefulSet statefulset-2969/ss2 to complete update
STEP: Rolling back to a previous revision 01/28/23 01:05:00.484
Jan 28 01:05:00.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-2969 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 01:05:00.839: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 01:05:00.839: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 01:05:00.839: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 28 01:05:10.917: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/28/23 01:05:20.963
Jan 28 01:05:20.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-2969 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 28 01:05:21.294: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 28 01:05:21.294: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 28 01:05:21.294: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 28 01:05:31.364: INFO: Waiting for StatefulSet statefulset-2969/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 28 01:05:41.387: INFO: Deleting all statefulset in ns statefulset-2969
Jan 28 01:05:41.405: INFO: Scaling statefulset ss2 to 0
Jan 28 01:05:51.466: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 01:05:51.475: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 28 01:05:51.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2969" for this suite. 01/28/23 01:05:51.584
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":139,"skipped":2541,"failed":0}
------------------------------
â€¢ [SLOW TEST] [102.157 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:04:09.446
    Jan 28 01:04:09.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename statefulset 01/28/23 01:04:09.448
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:04:09.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:04:09.484
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2969 01/28/23 01:04:09.494
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 01/28/23 01:04:09.508
    Jan 28 01:04:09.535: INFO: Found 0 stateful pods, waiting for 3
    Jan 28 01:04:19.548: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 28 01:04:19.548: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 28 01:04:19.548: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan 28 01:04:19.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-2969 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 28 01:04:19.906: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 28 01:04:19.906: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 28 01:04:19.906: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/28/23 01:04:29.952
    Jan 28 01:04:29.987: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/28/23 01:04:29.987
    STEP: Updating Pods in reverse ordinal order 01/28/23 01:04:40.033
    Jan 28 01:04:40.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-2969 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 28 01:04:40.393: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 28 01:04:40.393: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 28 01:04:40.393: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 28 01:04:50.459: INFO: Waiting for StatefulSet statefulset-2969/ss2 to complete update
    STEP: Rolling back to a previous revision 01/28/23 01:05:00.484
    Jan 28 01:05:00.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-2969 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 28 01:05:00.839: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 28 01:05:00.839: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 28 01:05:00.839: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 28 01:05:10.917: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/28/23 01:05:20.963
    Jan 28 01:05:20.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-2969 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 28 01:05:21.294: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 28 01:05:21.294: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 28 01:05:21.294: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 28 01:05:31.364: INFO: Waiting for StatefulSet statefulset-2969/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 28 01:05:41.387: INFO: Deleting all statefulset in ns statefulset-2969
    Jan 28 01:05:41.405: INFO: Scaling statefulset ss2 to 0
    Jan 28 01:05:51.466: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 28 01:05:51.475: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 28 01:05:51.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2969" for this suite. 01/28/23 01:05:51.584
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:05:51.604
Jan 28 01:05:51.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename emptydir 01/28/23 01:05:51.607
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:05:51.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:05:51.698
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/28/23 01:05:51.707
Jan 28 01:05:51.756: INFO: Waiting up to 5m0s for pod "pod-8f90942d-1375-4f35-9537-bfb52fd7c47f" in namespace "emptydir-8198" to be "Succeeded or Failed"
Jan 28 01:05:51.768: INFO: Pod "pod-8f90942d-1375-4f35-9537-bfb52fd7c47f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.288635ms
Jan 28 01:05:53.778: INFO: Pod "pod-8f90942d-1375-4f35-9537-bfb52fd7c47f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022495775s
Jan 28 01:05:55.779: INFO: Pod "pod-8f90942d-1375-4f35-9537-bfb52fd7c47f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023250939s
Jan 28 01:05:57.801: INFO: Pod "pod-8f90942d-1375-4f35-9537-bfb52fd7c47f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044660065s
STEP: Saw pod success 01/28/23 01:05:57.801
Jan 28 01:05:57.801: INFO: Pod "pod-8f90942d-1375-4f35-9537-bfb52fd7c47f" satisfied condition "Succeeded or Failed"
Jan 28 01:05:57.818: INFO: Trying to get logs from node 10.9.20.72 pod pod-8f90942d-1375-4f35-9537-bfb52fd7c47f container test-container: <nil>
STEP: delete the pod 01/28/23 01:05:57.932
Jan 28 01:05:57.981: INFO: Waiting for pod pod-8f90942d-1375-4f35-9537-bfb52fd7c47f to disappear
Jan 28 01:05:57.992: INFO: Pod pod-8f90942d-1375-4f35-9537-bfb52fd7c47f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 28 01:05:57.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8198" for this suite. 01/28/23 01:05:58.037
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":140,"skipped":2541,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.490 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:05:51.604
    Jan 28 01:05:51.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename emptydir 01/28/23 01:05:51.607
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:05:51.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:05:51.698
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/28/23 01:05:51.707
    Jan 28 01:05:51.756: INFO: Waiting up to 5m0s for pod "pod-8f90942d-1375-4f35-9537-bfb52fd7c47f" in namespace "emptydir-8198" to be "Succeeded or Failed"
    Jan 28 01:05:51.768: INFO: Pod "pod-8f90942d-1375-4f35-9537-bfb52fd7c47f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.288635ms
    Jan 28 01:05:53.778: INFO: Pod "pod-8f90942d-1375-4f35-9537-bfb52fd7c47f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022495775s
    Jan 28 01:05:55.779: INFO: Pod "pod-8f90942d-1375-4f35-9537-bfb52fd7c47f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023250939s
    Jan 28 01:05:57.801: INFO: Pod "pod-8f90942d-1375-4f35-9537-bfb52fd7c47f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044660065s
    STEP: Saw pod success 01/28/23 01:05:57.801
    Jan 28 01:05:57.801: INFO: Pod "pod-8f90942d-1375-4f35-9537-bfb52fd7c47f" satisfied condition "Succeeded or Failed"
    Jan 28 01:05:57.818: INFO: Trying to get logs from node 10.9.20.72 pod pod-8f90942d-1375-4f35-9537-bfb52fd7c47f container test-container: <nil>
    STEP: delete the pod 01/28/23 01:05:57.932
    Jan 28 01:05:57.981: INFO: Waiting for pod pod-8f90942d-1375-4f35-9537-bfb52fd7c47f to disappear
    Jan 28 01:05:57.992: INFO: Pod pod-8f90942d-1375-4f35-9537-bfb52fd7c47f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 28 01:05:57.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8198" for this suite. 01/28/23 01:05:58.037
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:05:58.095
Jan 28 01:05:58.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename crd-publish-openapi 01/28/23 01:05:58.098
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:05:58.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:05:58.188
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Jan 28 01:05:58.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/28/23 01:06:01.19
Jan 28 01:06:01.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-2448 --namespace=crd-publish-openapi-2448 create -f -'
Jan 28 01:06:02.136: INFO: stderr: ""
Jan 28 01:06:02.136: INFO: stdout: "e2e-test-crd-publish-openapi-1910-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 28 01:06:02.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-2448 --namespace=crd-publish-openapi-2448 delete e2e-test-crd-publish-openapi-1910-crds test-cr'
Jan 28 01:06:02.406: INFO: stderr: ""
Jan 28 01:06:02.406: INFO: stdout: "e2e-test-crd-publish-openapi-1910-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 28 01:06:02.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-2448 --namespace=crd-publish-openapi-2448 apply -f -'
Jan 28 01:06:03.248: INFO: stderr: ""
Jan 28 01:06:03.249: INFO: stdout: "e2e-test-crd-publish-openapi-1910-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 28 01:06:03.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-2448 --namespace=crd-publish-openapi-2448 delete e2e-test-crd-publish-openapi-1910-crds test-cr'
Jan 28 01:06:03.419: INFO: stderr: ""
Jan 28 01:06:03.419: INFO: stdout: "e2e-test-crd-publish-openapi-1910-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/28/23 01:06:03.419
Jan 28 01:06:03.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-2448 explain e2e-test-crd-publish-openapi-1910-crds'
Jan 28 01:06:03.687: INFO: stderr: ""
Jan 28 01:06:03.687: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1910-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:06:06.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2448" for this suite. 01/28/23 01:06:06.606
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":141,"skipped":2542,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.556 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:05:58.095
    Jan 28 01:05:58.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename crd-publish-openapi 01/28/23 01:05:58.098
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:05:58.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:05:58.188
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Jan 28 01:05:58.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/28/23 01:06:01.19
    Jan 28 01:06:01.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-2448 --namespace=crd-publish-openapi-2448 create -f -'
    Jan 28 01:06:02.136: INFO: stderr: ""
    Jan 28 01:06:02.136: INFO: stdout: "e2e-test-crd-publish-openapi-1910-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 28 01:06:02.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-2448 --namespace=crd-publish-openapi-2448 delete e2e-test-crd-publish-openapi-1910-crds test-cr'
    Jan 28 01:06:02.406: INFO: stderr: ""
    Jan 28 01:06:02.406: INFO: stdout: "e2e-test-crd-publish-openapi-1910-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan 28 01:06:02.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-2448 --namespace=crd-publish-openapi-2448 apply -f -'
    Jan 28 01:06:03.248: INFO: stderr: ""
    Jan 28 01:06:03.249: INFO: stdout: "e2e-test-crd-publish-openapi-1910-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 28 01:06:03.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-2448 --namespace=crd-publish-openapi-2448 delete e2e-test-crd-publish-openapi-1910-crds test-cr'
    Jan 28 01:06:03.419: INFO: stderr: ""
    Jan 28 01:06:03.419: INFO: stdout: "e2e-test-crd-publish-openapi-1910-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/28/23 01:06:03.419
    Jan 28 01:06:03.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-2448 explain e2e-test-crd-publish-openapi-1910-crds'
    Jan 28 01:06:03.687: INFO: stderr: ""
    Jan 28 01:06:03.687: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1910-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:06:06.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2448" for this suite. 01/28/23 01:06:06.606
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:06:06.652
Jan 28 01:06:06.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename secrets 01/28/23 01:06:06.654
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:06:06.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:06:06.77
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-9ca44822-0a2e-43d7-8ecd-fea19aa74036 01/28/23 01:06:06.783
STEP: Creating a pod to test consume secrets 01/28/23 01:06:06.797
Jan 28 01:06:06.818: INFO: Waiting up to 5m0s for pod "pod-secrets-4a85cb35-43a0-424a-9e64-dd3ddb58eb55" in namespace "secrets-459" to be "Succeeded or Failed"
Jan 28 01:06:06.830: INFO: Pod "pod-secrets-4a85cb35-43a0-424a-9e64-dd3ddb58eb55": Phase="Pending", Reason="", readiness=false. Elapsed: 11.258277ms
Jan 28 01:06:08.857: INFO: Pod "pod-secrets-4a85cb35-43a0-424a-9e64-dd3ddb58eb55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039031281s
Jan 28 01:06:10.844: INFO: Pod "pod-secrets-4a85cb35-43a0-424a-9e64-dd3ddb58eb55": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02550735s
Jan 28 01:06:12.841: INFO: Pod "pod-secrets-4a85cb35-43a0-424a-9e64-dd3ddb58eb55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023102242s
STEP: Saw pod success 01/28/23 01:06:12.842
Jan 28 01:06:12.842: INFO: Pod "pod-secrets-4a85cb35-43a0-424a-9e64-dd3ddb58eb55" satisfied condition "Succeeded or Failed"
Jan 28 01:06:12.880: INFO: Trying to get logs from node 10.9.20.72 pod pod-secrets-4a85cb35-43a0-424a-9e64-dd3ddb58eb55 container secret-volume-test: <nil>
STEP: delete the pod 01/28/23 01:06:12.953
Jan 28 01:06:13.007: INFO: Waiting for pod pod-secrets-4a85cb35-43a0-424a-9e64-dd3ddb58eb55 to disappear
Jan 28 01:06:13.022: INFO: Pod pod-secrets-4a85cb35-43a0-424a-9e64-dd3ddb58eb55 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 28 01:06:13.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-459" for this suite. 01/28/23 01:06:13.043
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":142,"skipped":2543,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.408 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:06:06.652
    Jan 28 01:06:06.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename secrets 01/28/23 01:06:06.654
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:06:06.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:06:06.77
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-9ca44822-0a2e-43d7-8ecd-fea19aa74036 01/28/23 01:06:06.783
    STEP: Creating a pod to test consume secrets 01/28/23 01:06:06.797
    Jan 28 01:06:06.818: INFO: Waiting up to 5m0s for pod "pod-secrets-4a85cb35-43a0-424a-9e64-dd3ddb58eb55" in namespace "secrets-459" to be "Succeeded or Failed"
    Jan 28 01:06:06.830: INFO: Pod "pod-secrets-4a85cb35-43a0-424a-9e64-dd3ddb58eb55": Phase="Pending", Reason="", readiness=false. Elapsed: 11.258277ms
    Jan 28 01:06:08.857: INFO: Pod "pod-secrets-4a85cb35-43a0-424a-9e64-dd3ddb58eb55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039031281s
    Jan 28 01:06:10.844: INFO: Pod "pod-secrets-4a85cb35-43a0-424a-9e64-dd3ddb58eb55": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02550735s
    Jan 28 01:06:12.841: INFO: Pod "pod-secrets-4a85cb35-43a0-424a-9e64-dd3ddb58eb55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023102242s
    STEP: Saw pod success 01/28/23 01:06:12.842
    Jan 28 01:06:12.842: INFO: Pod "pod-secrets-4a85cb35-43a0-424a-9e64-dd3ddb58eb55" satisfied condition "Succeeded or Failed"
    Jan 28 01:06:12.880: INFO: Trying to get logs from node 10.9.20.72 pod pod-secrets-4a85cb35-43a0-424a-9e64-dd3ddb58eb55 container secret-volume-test: <nil>
    STEP: delete the pod 01/28/23 01:06:12.953
    Jan 28 01:06:13.007: INFO: Waiting for pod pod-secrets-4a85cb35-43a0-424a-9e64-dd3ddb58eb55 to disappear
    Jan 28 01:06:13.022: INFO: Pod pod-secrets-4a85cb35-43a0-424a-9e64-dd3ddb58eb55 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 28 01:06:13.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-459" for this suite. 01/28/23 01:06:13.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:06:13.062
Jan 28 01:06:13.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename services 01/28/23 01:06:13.064
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:06:13.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:06:13.139
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2639 01/28/23 01:06:13.153
STEP: changing the ExternalName service to type=ClusterIP 01/28/23 01:06:13.167
STEP: creating replication controller externalname-service in namespace services-2639 01/28/23 01:06:13.203
I0128 01:06:13.218634      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2639, replica count: 2
I0128 01:06:16.271580      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 01:06:16.271: INFO: Creating new exec pod
Jan 28 01:06:16.292: INFO: Waiting up to 5m0s for pod "execpod26kn6" in namespace "services-2639" to be "running"
Jan 28 01:06:16.304: INFO: Pod "execpod26kn6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.653411ms
Jan 28 01:06:18.346: INFO: Pod "execpod26kn6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053857428s
Jan 28 01:06:20.319: INFO: Pod "execpod26kn6": Phase="Running", Reason="", readiness=true. Elapsed: 4.027162957s
Jan 28 01:06:20.319: INFO: Pod "execpod26kn6" satisfied condition "running"
Jan 28 01:06:21.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-2639 exec execpod26kn6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 28 01:06:21.755: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 28 01:06:21.755: INFO: stdout: ""
Jan 28 01:06:22.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-2639 exec execpod26kn6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 28 01:06:23.092: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 28 01:06:23.092: INFO: stdout: "externalname-service-s6rkq"
Jan 28 01:06:23.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-2639 exec execpod26kn6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.57.91 80'
Jan 28 01:06:23.430: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.57.91 80\nConnection to 172.21.57.91 80 port [tcp/http] succeeded!\n"
Jan 28 01:06:23.430: INFO: stdout: ""
Jan 28 01:06:24.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-2639 exec execpod26kn6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.57.91 80'
Jan 28 01:06:24.805: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.57.91 80\nConnection to 172.21.57.91 80 port [tcp/http] succeeded!\n"
Jan 28 01:06:24.805: INFO: stdout: ""
Jan 28 01:06:25.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-2639 exec execpod26kn6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.57.91 80'
Jan 28 01:06:25.740: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.57.91 80\nConnection to 172.21.57.91 80 port [tcp/http] succeeded!\n"
Jan 28 01:06:25.740: INFO: stdout: "externalname-service-s6rkq"
Jan 28 01:06:25.741: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 28 01:06:25.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2639" for this suite. 01/28/23 01:06:25.83
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":143,"skipped":2556,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.786 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:06:13.062
    Jan 28 01:06:13.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename services 01/28/23 01:06:13.064
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:06:13.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:06:13.139
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-2639 01/28/23 01:06:13.153
    STEP: changing the ExternalName service to type=ClusterIP 01/28/23 01:06:13.167
    STEP: creating replication controller externalname-service in namespace services-2639 01/28/23 01:06:13.203
    I0128 01:06:13.218634      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2639, replica count: 2
    I0128 01:06:16.271580      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 28 01:06:16.271: INFO: Creating new exec pod
    Jan 28 01:06:16.292: INFO: Waiting up to 5m0s for pod "execpod26kn6" in namespace "services-2639" to be "running"
    Jan 28 01:06:16.304: INFO: Pod "execpod26kn6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.653411ms
    Jan 28 01:06:18.346: INFO: Pod "execpod26kn6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053857428s
    Jan 28 01:06:20.319: INFO: Pod "execpod26kn6": Phase="Running", Reason="", readiness=true. Elapsed: 4.027162957s
    Jan 28 01:06:20.319: INFO: Pod "execpod26kn6" satisfied condition "running"
    Jan 28 01:06:21.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-2639 exec execpod26kn6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 28 01:06:21.755: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 28 01:06:21.755: INFO: stdout: ""
    Jan 28 01:06:22.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-2639 exec execpod26kn6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 28 01:06:23.092: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 28 01:06:23.092: INFO: stdout: "externalname-service-s6rkq"
    Jan 28 01:06:23.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-2639 exec execpod26kn6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.57.91 80'
    Jan 28 01:06:23.430: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.57.91 80\nConnection to 172.21.57.91 80 port [tcp/http] succeeded!\n"
    Jan 28 01:06:23.430: INFO: stdout: ""
    Jan 28 01:06:24.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-2639 exec execpod26kn6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.57.91 80'
    Jan 28 01:06:24.805: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.57.91 80\nConnection to 172.21.57.91 80 port [tcp/http] succeeded!\n"
    Jan 28 01:06:24.805: INFO: stdout: ""
    Jan 28 01:06:25.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-2639 exec execpod26kn6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.57.91 80'
    Jan 28 01:06:25.740: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.57.91 80\nConnection to 172.21.57.91 80 port [tcp/http] succeeded!\n"
    Jan 28 01:06:25.740: INFO: stdout: "externalname-service-s6rkq"
    Jan 28 01:06:25.741: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 28 01:06:25.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2639" for this suite. 01/28/23 01:06:25.83
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:06:25.849
Jan 28 01:06:25.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename subpath 01/28/23 01:06:25.851
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:06:25.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:06:25.905
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/28/23 01:06:25.918
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-99ng 01/28/23 01:06:25.947
STEP: Creating a pod to test atomic-volume-subpath 01/28/23 01:06:25.947
Jan 28 01:06:25.998: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-99ng" in namespace "subpath-8565" to be "Succeeded or Failed"
Jan 28 01:06:26.041: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Pending", Reason="", readiness=false. Elapsed: 42.313483ms
Jan 28 01:06:28.053: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05436153s
Jan 28 01:06:30.052: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 4.053473956s
Jan 28 01:06:32.082: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 6.083432136s
Jan 28 01:06:34.059: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 8.060761583s
Jan 28 01:06:36.057: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 10.058028716s
Jan 28 01:06:38.055: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 12.056726629s
Jan 28 01:06:40.055: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 14.056949974s
Jan 28 01:06:42.063: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 16.06427474s
Jan 28 01:06:44.056: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 18.057096603s
Jan 28 01:06:46.054: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 20.055592032s
Jan 28 01:06:48.055: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 22.056552758s
Jan 28 01:06:50.052: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=false. Elapsed: 24.053189676s
Jan 28 01:06:52.057: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.058696055s
STEP: Saw pod success 01/28/23 01:06:52.057
Jan 28 01:06:52.058: INFO: Pod "pod-subpath-test-configmap-99ng" satisfied condition "Succeeded or Failed"
Jan 28 01:06:52.070: INFO: Trying to get logs from node 10.9.20.72 pod pod-subpath-test-configmap-99ng container test-container-subpath-configmap-99ng: <nil>
STEP: delete the pod 01/28/23 01:06:52.097
Jan 28 01:06:52.126: INFO: Waiting for pod pod-subpath-test-configmap-99ng to disappear
Jan 28 01:06:52.138: INFO: Pod pod-subpath-test-configmap-99ng no longer exists
STEP: Deleting pod pod-subpath-test-configmap-99ng 01/28/23 01:06:52.138
Jan 28 01:06:52.138: INFO: Deleting pod "pod-subpath-test-configmap-99ng" in namespace "subpath-8565"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 28 01:06:52.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8565" for this suite. 01/28/23 01:06:52.175
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":144,"skipped":2565,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.340 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:06:25.849
    Jan 28 01:06:25.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename subpath 01/28/23 01:06:25.851
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:06:25.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:06:25.905
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/28/23 01:06:25.918
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-99ng 01/28/23 01:06:25.947
    STEP: Creating a pod to test atomic-volume-subpath 01/28/23 01:06:25.947
    Jan 28 01:06:25.998: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-99ng" in namespace "subpath-8565" to be "Succeeded or Failed"
    Jan 28 01:06:26.041: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Pending", Reason="", readiness=false. Elapsed: 42.313483ms
    Jan 28 01:06:28.053: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05436153s
    Jan 28 01:06:30.052: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 4.053473956s
    Jan 28 01:06:32.082: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 6.083432136s
    Jan 28 01:06:34.059: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 8.060761583s
    Jan 28 01:06:36.057: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 10.058028716s
    Jan 28 01:06:38.055: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 12.056726629s
    Jan 28 01:06:40.055: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 14.056949974s
    Jan 28 01:06:42.063: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 16.06427474s
    Jan 28 01:06:44.056: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 18.057096603s
    Jan 28 01:06:46.054: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 20.055592032s
    Jan 28 01:06:48.055: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=true. Elapsed: 22.056552758s
    Jan 28 01:06:50.052: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Running", Reason="", readiness=false. Elapsed: 24.053189676s
    Jan 28 01:06:52.057: INFO: Pod "pod-subpath-test-configmap-99ng": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.058696055s
    STEP: Saw pod success 01/28/23 01:06:52.057
    Jan 28 01:06:52.058: INFO: Pod "pod-subpath-test-configmap-99ng" satisfied condition "Succeeded or Failed"
    Jan 28 01:06:52.070: INFO: Trying to get logs from node 10.9.20.72 pod pod-subpath-test-configmap-99ng container test-container-subpath-configmap-99ng: <nil>
    STEP: delete the pod 01/28/23 01:06:52.097
    Jan 28 01:06:52.126: INFO: Waiting for pod pod-subpath-test-configmap-99ng to disappear
    Jan 28 01:06:52.138: INFO: Pod pod-subpath-test-configmap-99ng no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-99ng 01/28/23 01:06:52.138
    Jan 28 01:06:52.138: INFO: Deleting pod "pod-subpath-test-configmap-99ng" in namespace "subpath-8565"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 28 01:06:52.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-8565" for this suite. 01/28/23 01:06:52.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:06:52.208
Jan 28 01:06:52.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename resourcequota 01/28/23 01:06:52.21
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:06:52.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:06:52.27
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 01/28/23 01:06:52.293
STEP: Getting a ResourceQuota 01/28/23 01:06:52.311
STEP: Updating a ResourceQuota 01/28/23 01:06:52.325
STEP: Verifying a ResourceQuota was modified 01/28/23 01:06:52.341
STEP: Deleting a ResourceQuota 01/28/23 01:06:52.354
STEP: Verifying the deleted ResourceQuota 01/28/23 01:06:52.374
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 28 01:06:52.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2024" for this suite. 01/28/23 01:06:52.415
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":145,"skipped":2619,"failed":0}
------------------------------
â€¢ [0.224 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:06:52.208
    Jan 28 01:06:52.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename resourcequota 01/28/23 01:06:52.21
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:06:52.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:06:52.27
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 01/28/23 01:06:52.293
    STEP: Getting a ResourceQuota 01/28/23 01:06:52.311
    STEP: Updating a ResourceQuota 01/28/23 01:06:52.325
    STEP: Verifying a ResourceQuota was modified 01/28/23 01:06:52.341
    STEP: Deleting a ResourceQuota 01/28/23 01:06:52.354
    STEP: Verifying the deleted ResourceQuota 01/28/23 01:06:52.374
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 28 01:06:52.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2024" for this suite. 01/28/23 01:06:52.415
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:06:52.433
Jan 28 01:06:52.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename emptydir 01/28/23 01:06:52.436
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:06:52.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:06:52.496
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 01/28/23 01:06:52.508
Jan 28 01:06:52.532: INFO: Waiting up to 5m0s for pod "pod-7fc74990-dd86-4aa9-8f2f-c86dd33b3ca5" in namespace "emptydir-5955" to be "Succeeded or Failed"
Jan 28 01:06:52.545: INFO: Pod "pod-7fc74990-dd86-4aa9-8f2f-c86dd33b3ca5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.929959ms
Jan 28 01:06:54.562: INFO: Pod "pod-7fc74990-dd86-4aa9-8f2f-c86dd33b3ca5": Phase="Running", Reason="", readiness=true. Elapsed: 2.029641905s
Jan 28 01:06:56.560: INFO: Pod "pod-7fc74990-dd86-4aa9-8f2f-c86dd33b3ca5": Phase="Running", Reason="", readiness=false. Elapsed: 4.02742176s
Jan 28 01:06:58.560: INFO: Pod "pod-7fc74990-dd86-4aa9-8f2f-c86dd33b3ca5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027225282s
STEP: Saw pod success 01/28/23 01:06:58.56
Jan 28 01:06:58.560: INFO: Pod "pod-7fc74990-dd86-4aa9-8f2f-c86dd33b3ca5" satisfied condition "Succeeded or Failed"
Jan 28 01:06:58.575: INFO: Trying to get logs from node 10.9.20.126 pod pod-7fc74990-dd86-4aa9-8f2f-c86dd33b3ca5 container test-container: <nil>
STEP: delete the pod 01/28/23 01:06:58.651
Jan 28 01:06:58.679: INFO: Waiting for pod pod-7fc74990-dd86-4aa9-8f2f-c86dd33b3ca5 to disappear
Jan 28 01:06:58.688: INFO: Pod pod-7fc74990-dd86-4aa9-8f2f-c86dd33b3ca5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 28 01:06:58.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5955" for this suite. 01/28/23 01:06:58.711
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":146,"skipped":2621,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.297 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:06:52.433
    Jan 28 01:06:52.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename emptydir 01/28/23 01:06:52.436
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:06:52.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:06:52.496
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/28/23 01:06:52.508
    Jan 28 01:06:52.532: INFO: Waiting up to 5m0s for pod "pod-7fc74990-dd86-4aa9-8f2f-c86dd33b3ca5" in namespace "emptydir-5955" to be "Succeeded or Failed"
    Jan 28 01:06:52.545: INFO: Pod "pod-7fc74990-dd86-4aa9-8f2f-c86dd33b3ca5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.929959ms
    Jan 28 01:06:54.562: INFO: Pod "pod-7fc74990-dd86-4aa9-8f2f-c86dd33b3ca5": Phase="Running", Reason="", readiness=true. Elapsed: 2.029641905s
    Jan 28 01:06:56.560: INFO: Pod "pod-7fc74990-dd86-4aa9-8f2f-c86dd33b3ca5": Phase="Running", Reason="", readiness=false. Elapsed: 4.02742176s
    Jan 28 01:06:58.560: INFO: Pod "pod-7fc74990-dd86-4aa9-8f2f-c86dd33b3ca5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027225282s
    STEP: Saw pod success 01/28/23 01:06:58.56
    Jan 28 01:06:58.560: INFO: Pod "pod-7fc74990-dd86-4aa9-8f2f-c86dd33b3ca5" satisfied condition "Succeeded or Failed"
    Jan 28 01:06:58.575: INFO: Trying to get logs from node 10.9.20.126 pod pod-7fc74990-dd86-4aa9-8f2f-c86dd33b3ca5 container test-container: <nil>
    STEP: delete the pod 01/28/23 01:06:58.651
    Jan 28 01:06:58.679: INFO: Waiting for pod pod-7fc74990-dd86-4aa9-8f2f-c86dd33b3ca5 to disappear
    Jan 28 01:06:58.688: INFO: Pod pod-7fc74990-dd86-4aa9-8f2f-c86dd33b3ca5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 28 01:06:58.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5955" for this suite. 01/28/23 01:06:58.711
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:06:58.735
Jan 28 01:06:58.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename dns 01/28/23 01:06:58.737
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:06:58.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:06:58.79
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/28/23 01:06:58.802
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2811.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2811.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/28/23 01:06:58.816
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2811.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2811.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/28/23 01:06:58.817
STEP: creating a pod to probe DNS 01/28/23 01:06:58.818
STEP: submitting the pod to kubernetes 01/28/23 01:06:58.818
Jan 28 01:06:58.848: INFO: Waiting up to 15m0s for pod "dns-test-68b7dc01-50bb-4114-a4d4-f7824c7b1b04" in namespace "dns-2811" to be "running"
Jan 28 01:06:58.860: INFO: Pod "dns-test-68b7dc01-50bb-4114-a4d4-f7824c7b1b04": Phase="Pending", Reason="", readiness=false. Elapsed: 11.948437ms
Jan 28 01:07:00.873: INFO: Pod "dns-test-68b7dc01-50bb-4114-a4d4-f7824c7b1b04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024655144s
Jan 28 01:07:02.876: INFO: Pod "dns-test-68b7dc01-50bb-4114-a4d4-f7824c7b1b04": Phase="Running", Reason="", readiness=true. Elapsed: 4.027755293s
Jan 28 01:07:02.876: INFO: Pod "dns-test-68b7dc01-50bb-4114-a4d4-f7824c7b1b04" satisfied condition "running"
STEP: retrieving the pod 01/28/23 01:07:02.876
STEP: looking for the results for each expected name from probers 01/28/23 01:07:02.89
Jan 28 01:07:02.986: INFO: DNS probes using dns-2811/dns-test-68b7dc01-50bb-4114-a4d4-f7824c7b1b04 succeeded

STEP: deleting the pod 01/28/23 01:07:02.986
STEP: deleting the test headless service 01/28/23 01:07:03.022
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 28 01:07:03.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2811" for this suite. 01/28/23 01:07:03.101
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":147,"skipped":2623,"failed":0}
------------------------------
â€¢ [4.387 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:06:58.735
    Jan 28 01:06:58.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename dns 01/28/23 01:06:58.737
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:06:58.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:06:58.79
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/28/23 01:06:58.802
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2811.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2811.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/28/23 01:06:58.816
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2811.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2811.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/28/23 01:06:58.817
    STEP: creating a pod to probe DNS 01/28/23 01:06:58.818
    STEP: submitting the pod to kubernetes 01/28/23 01:06:58.818
    Jan 28 01:06:58.848: INFO: Waiting up to 15m0s for pod "dns-test-68b7dc01-50bb-4114-a4d4-f7824c7b1b04" in namespace "dns-2811" to be "running"
    Jan 28 01:06:58.860: INFO: Pod "dns-test-68b7dc01-50bb-4114-a4d4-f7824c7b1b04": Phase="Pending", Reason="", readiness=false. Elapsed: 11.948437ms
    Jan 28 01:07:00.873: INFO: Pod "dns-test-68b7dc01-50bb-4114-a4d4-f7824c7b1b04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024655144s
    Jan 28 01:07:02.876: INFO: Pod "dns-test-68b7dc01-50bb-4114-a4d4-f7824c7b1b04": Phase="Running", Reason="", readiness=true. Elapsed: 4.027755293s
    Jan 28 01:07:02.876: INFO: Pod "dns-test-68b7dc01-50bb-4114-a4d4-f7824c7b1b04" satisfied condition "running"
    STEP: retrieving the pod 01/28/23 01:07:02.876
    STEP: looking for the results for each expected name from probers 01/28/23 01:07:02.89
    Jan 28 01:07:02.986: INFO: DNS probes using dns-2811/dns-test-68b7dc01-50bb-4114-a4d4-f7824c7b1b04 succeeded

    STEP: deleting the pod 01/28/23 01:07:02.986
    STEP: deleting the test headless service 01/28/23 01:07:03.022
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 28 01:07:03.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2811" for this suite. 01/28/23 01:07:03.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:07:03.128
Jan 28 01:07:03.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 01:07:03.131
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:07:03.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:07:03.212
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-bd13e736-7334-4f80-80c0-be93d51ea409 01/28/23 01:07:03.227
STEP: Creating a pod to test consume secrets 01/28/23 01:07:03.241
Jan 28 01:07:03.259: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-278b2e23-7946-461b-bb67-af0a240380ec" in namespace "projected-8871" to be "Succeeded or Failed"
Jan 28 01:07:03.269: INFO: Pod "pod-projected-secrets-278b2e23-7946-461b-bb67-af0a240380ec": Phase="Pending", Reason="", readiness=false. Elapsed: 9.140493ms
Jan 28 01:07:05.281: INFO: Pod "pod-projected-secrets-278b2e23-7946-461b-bb67-af0a240380ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022026761s
Jan 28 01:07:07.279: INFO: Pod "pod-projected-secrets-278b2e23-7946-461b-bb67-af0a240380ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01997451s
STEP: Saw pod success 01/28/23 01:07:07.279
Jan 28 01:07:07.280: INFO: Pod "pod-projected-secrets-278b2e23-7946-461b-bb67-af0a240380ec" satisfied condition "Succeeded or Failed"
Jan 28 01:07:07.289: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-secrets-278b2e23-7946-461b-bb67-af0a240380ec container projected-secret-volume-test: <nil>
STEP: delete the pod 01/28/23 01:07:07.313
Jan 28 01:07:07.332: INFO: Waiting for pod pod-projected-secrets-278b2e23-7946-461b-bb67-af0a240380ec to disappear
Jan 28 01:07:07.340: INFO: Pod pod-projected-secrets-278b2e23-7946-461b-bb67-af0a240380ec no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 28 01:07:07.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8871" for this suite. 01/28/23 01:07:07.361
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":148,"skipped":2644,"failed":0}
------------------------------
â€¢ [4.247 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:07:03.128
    Jan 28 01:07:03.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 01:07:03.131
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:07:03.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:07:03.212
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-bd13e736-7334-4f80-80c0-be93d51ea409 01/28/23 01:07:03.227
    STEP: Creating a pod to test consume secrets 01/28/23 01:07:03.241
    Jan 28 01:07:03.259: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-278b2e23-7946-461b-bb67-af0a240380ec" in namespace "projected-8871" to be "Succeeded or Failed"
    Jan 28 01:07:03.269: INFO: Pod "pod-projected-secrets-278b2e23-7946-461b-bb67-af0a240380ec": Phase="Pending", Reason="", readiness=false. Elapsed: 9.140493ms
    Jan 28 01:07:05.281: INFO: Pod "pod-projected-secrets-278b2e23-7946-461b-bb67-af0a240380ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022026761s
    Jan 28 01:07:07.279: INFO: Pod "pod-projected-secrets-278b2e23-7946-461b-bb67-af0a240380ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01997451s
    STEP: Saw pod success 01/28/23 01:07:07.279
    Jan 28 01:07:07.280: INFO: Pod "pod-projected-secrets-278b2e23-7946-461b-bb67-af0a240380ec" satisfied condition "Succeeded or Failed"
    Jan 28 01:07:07.289: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-secrets-278b2e23-7946-461b-bb67-af0a240380ec container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/28/23 01:07:07.313
    Jan 28 01:07:07.332: INFO: Waiting for pod pod-projected-secrets-278b2e23-7946-461b-bb67-af0a240380ec to disappear
    Jan 28 01:07:07.340: INFO: Pod pod-projected-secrets-278b2e23-7946-461b-bb67-af0a240380ec no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 28 01:07:07.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8871" for this suite. 01/28/23 01:07:07.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:07:07.379
Jan 28 01:07:07.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename dns 01/28/23 01:07:07.382
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:07:07.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:07:07.434
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/28/23 01:07:07.446
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/28/23 01:07:07.446
STEP: creating a pod to probe DNS 01/28/23 01:07:07.447
STEP: submitting the pod to kubernetes 01/28/23 01:07:07.447
Jan 28 01:07:07.468: INFO: Waiting up to 15m0s for pod "dns-test-459bebfe-2ddf-4be2-a686-7f77a4558e17" in namespace "dns-9343" to be "running"
Jan 28 01:07:07.477: INFO: Pod "dns-test-459bebfe-2ddf-4be2-a686-7f77a4558e17": Phase="Pending", Reason="", readiness=false. Elapsed: 8.88831ms
Jan 28 01:07:09.492: INFO: Pod "dns-test-459bebfe-2ddf-4be2-a686-7f77a4558e17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023684094s
Jan 28 01:07:11.492: INFO: Pod "dns-test-459bebfe-2ddf-4be2-a686-7f77a4558e17": Phase="Running", Reason="", readiness=true. Elapsed: 4.023590992s
Jan 28 01:07:11.492: INFO: Pod "dns-test-459bebfe-2ddf-4be2-a686-7f77a4558e17" satisfied condition "running"
STEP: retrieving the pod 01/28/23 01:07:11.492
STEP: looking for the results for each expected name from probers 01/28/23 01:07:11.515
Jan 28 01:07:11.593: INFO: DNS probes using dns-9343/dns-test-459bebfe-2ddf-4be2-a686-7f77a4558e17 succeeded

STEP: deleting the pod 01/28/23 01:07:11.593
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 28 01:07:11.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9343" for this suite. 01/28/23 01:07:11.647
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":149,"skipped":2665,"failed":0}
------------------------------
â€¢ [4.287 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:07:07.379
    Jan 28 01:07:07.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename dns 01/28/23 01:07:07.382
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:07:07.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:07:07.434
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/28/23 01:07:07.446
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/28/23 01:07:07.446
    STEP: creating a pod to probe DNS 01/28/23 01:07:07.447
    STEP: submitting the pod to kubernetes 01/28/23 01:07:07.447
    Jan 28 01:07:07.468: INFO: Waiting up to 15m0s for pod "dns-test-459bebfe-2ddf-4be2-a686-7f77a4558e17" in namespace "dns-9343" to be "running"
    Jan 28 01:07:07.477: INFO: Pod "dns-test-459bebfe-2ddf-4be2-a686-7f77a4558e17": Phase="Pending", Reason="", readiness=false. Elapsed: 8.88831ms
    Jan 28 01:07:09.492: INFO: Pod "dns-test-459bebfe-2ddf-4be2-a686-7f77a4558e17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023684094s
    Jan 28 01:07:11.492: INFO: Pod "dns-test-459bebfe-2ddf-4be2-a686-7f77a4558e17": Phase="Running", Reason="", readiness=true. Elapsed: 4.023590992s
    Jan 28 01:07:11.492: INFO: Pod "dns-test-459bebfe-2ddf-4be2-a686-7f77a4558e17" satisfied condition "running"
    STEP: retrieving the pod 01/28/23 01:07:11.492
    STEP: looking for the results for each expected name from probers 01/28/23 01:07:11.515
    Jan 28 01:07:11.593: INFO: DNS probes using dns-9343/dns-test-459bebfe-2ddf-4be2-a686-7f77a4558e17 succeeded

    STEP: deleting the pod 01/28/23 01:07:11.593
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 28 01:07:11.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9343" for this suite. 01/28/23 01:07:11.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:07:11.671
Jan 28 01:07:11.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename container-probe 01/28/23 01:07:11.674
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:07:11.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:07:11.729
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-28182c5f-487b-456e-9402-9127a1669266 in namespace container-probe-5558 01/28/23 01:07:11.743
Jan 28 01:07:11.767: INFO: Waiting up to 5m0s for pod "busybox-28182c5f-487b-456e-9402-9127a1669266" in namespace "container-probe-5558" to be "not pending"
Jan 28 01:07:11.777: INFO: Pod "busybox-28182c5f-487b-456e-9402-9127a1669266": Phase="Pending", Reason="", readiness=false. Elapsed: 10.667914ms
Jan 28 01:07:13.791: INFO: Pod "busybox-28182c5f-487b-456e-9402-9127a1669266": Phase="Running", Reason="", readiness=true. Elapsed: 2.024695632s
Jan 28 01:07:13.791: INFO: Pod "busybox-28182c5f-487b-456e-9402-9127a1669266" satisfied condition "not pending"
Jan 28 01:07:13.791: INFO: Started pod busybox-28182c5f-487b-456e-9402-9127a1669266 in namespace container-probe-5558
STEP: checking the pod's current state and verifying that restartCount is present 01/28/23 01:07:13.791
Jan 28 01:07:13.801: INFO: Initial restart count of pod busybox-28182c5f-487b-456e-9402-9127a1669266 is 0
Jan 28 01:08:04.171: INFO: Restart count of pod container-probe-5558/busybox-28182c5f-487b-456e-9402-9127a1669266 is now 1 (50.369577734s elapsed)
STEP: deleting the pod 01/28/23 01:08:04.171
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 28 01:08:04.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5558" for this suite. 01/28/23 01:08:04.224
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":150,"skipped":2699,"failed":0}
------------------------------
â€¢ [SLOW TEST] [52.571 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:07:11.671
    Jan 28 01:07:11.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename container-probe 01/28/23 01:07:11.674
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:07:11.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:07:11.729
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-28182c5f-487b-456e-9402-9127a1669266 in namespace container-probe-5558 01/28/23 01:07:11.743
    Jan 28 01:07:11.767: INFO: Waiting up to 5m0s for pod "busybox-28182c5f-487b-456e-9402-9127a1669266" in namespace "container-probe-5558" to be "not pending"
    Jan 28 01:07:11.777: INFO: Pod "busybox-28182c5f-487b-456e-9402-9127a1669266": Phase="Pending", Reason="", readiness=false. Elapsed: 10.667914ms
    Jan 28 01:07:13.791: INFO: Pod "busybox-28182c5f-487b-456e-9402-9127a1669266": Phase="Running", Reason="", readiness=true. Elapsed: 2.024695632s
    Jan 28 01:07:13.791: INFO: Pod "busybox-28182c5f-487b-456e-9402-9127a1669266" satisfied condition "not pending"
    Jan 28 01:07:13.791: INFO: Started pod busybox-28182c5f-487b-456e-9402-9127a1669266 in namespace container-probe-5558
    STEP: checking the pod's current state and verifying that restartCount is present 01/28/23 01:07:13.791
    Jan 28 01:07:13.801: INFO: Initial restart count of pod busybox-28182c5f-487b-456e-9402-9127a1669266 is 0
    Jan 28 01:08:04.171: INFO: Restart count of pod container-probe-5558/busybox-28182c5f-487b-456e-9402-9127a1669266 is now 1 (50.369577734s elapsed)
    STEP: deleting the pod 01/28/23 01:08:04.171
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 28 01:08:04.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5558" for this suite. 01/28/23 01:08:04.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:08:04.248
Jan 28 01:08:04.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename ephemeral-containers-test 01/28/23 01:08:04.251
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:04.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:04.311
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/28/23 01:08:04.325
Jan 28 01:08:04.355: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2328" to be "running and ready"
Jan 28 01:08:04.368: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.192404ms
Jan 28 01:08:04.368: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:08:06.383: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028696051s
Jan 28 01:08:06.384: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:08:08.382: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.027006879s
Jan 28 01:08:08.382: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan 28 01:08:08.382: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/28/23 01:08:08.392
Jan 28 01:08:08.416: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2328" to be "container debugger running"
Jan 28 01:08:08.428: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 11.933445ms
Jan 28 01:08:10.439: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.0225723s
Jan 28 01:08:10.439: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/28/23 01:08:10.439
Jan 28 01:08:10.440: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2328 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:08:10.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:08:10.442: INFO: ExecWithOptions: Clientset creation
Jan 28 01:08:10.442: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-2328/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan 28 01:08:10.637: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 28 01:08:10.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-2328" for this suite. 01/28/23 01:08:10.68
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":151,"skipped":2715,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.455 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:08:04.248
    Jan 28 01:08:04.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/28/23 01:08:04.251
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:04.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:04.311
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/28/23 01:08:04.325
    Jan 28 01:08:04.355: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2328" to be "running and ready"
    Jan 28 01:08:04.368: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.192404ms
    Jan 28 01:08:04.368: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:08:06.383: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028696051s
    Jan 28 01:08:06.384: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:08:08.382: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.027006879s
    Jan 28 01:08:08.382: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan 28 01:08:08.382: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/28/23 01:08:08.392
    Jan 28 01:08:08.416: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2328" to be "container debugger running"
    Jan 28 01:08:08.428: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 11.933445ms
    Jan 28 01:08:10.439: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.0225723s
    Jan 28 01:08:10.439: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/28/23 01:08:10.439
    Jan 28 01:08:10.440: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2328 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:08:10.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:08:10.442: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:08:10.442: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-2328/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan 28 01:08:10.637: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 28 01:08:10.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-2328" for this suite. 01/28/23 01:08:10.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:08:10.712
Jan 28 01:08:10.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename configmap 01/28/23 01:08:10.715
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:10.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:10.784
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-d6cf7c90-5528-495d-8b45-b5d85b257bac 01/28/23 01:08:10.8
STEP: Creating a pod to test consume configMaps 01/28/23 01:08:10.818
Jan 28 01:08:10.841: INFO: Waiting up to 5m0s for pod "pod-configmaps-064dbd72-4cac-470e-a49c-b433e54fd0db" in namespace "configmap-3815" to be "Succeeded or Failed"
Jan 28 01:08:10.853: INFO: Pod "pod-configmaps-064dbd72-4cac-470e-a49c-b433e54fd0db": Phase="Pending", Reason="", readiness=false. Elapsed: 12.056824ms
Jan 28 01:08:12.866: INFO: Pod "pod-configmaps-064dbd72-4cac-470e-a49c-b433e54fd0db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024510327s
Jan 28 01:08:14.868: INFO: Pod "pod-configmaps-064dbd72-4cac-470e-a49c-b433e54fd0db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027271677s
STEP: Saw pod success 01/28/23 01:08:14.869
Jan 28 01:08:14.869: INFO: Pod "pod-configmaps-064dbd72-4cac-470e-a49c-b433e54fd0db" satisfied condition "Succeeded or Failed"
Jan 28 01:08:14.882: INFO: Trying to get logs from node 10.9.20.126 pod pod-configmaps-064dbd72-4cac-470e-a49c-b433e54fd0db container agnhost-container: <nil>
STEP: delete the pod 01/28/23 01:08:14.916
Jan 28 01:08:14.953: INFO: Waiting for pod pod-configmaps-064dbd72-4cac-470e-a49c-b433e54fd0db to disappear
Jan 28 01:08:14.961: INFO: Pod pod-configmaps-064dbd72-4cac-470e-a49c-b433e54fd0db no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 28 01:08:14.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3815" for this suite. 01/28/23 01:08:14.983
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":152,"skipped":2728,"failed":0}
------------------------------
â€¢ [4.287 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:08:10.712
    Jan 28 01:08:10.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename configmap 01/28/23 01:08:10.715
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:10.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:10.784
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-d6cf7c90-5528-495d-8b45-b5d85b257bac 01/28/23 01:08:10.8
    STEP: Creating a pod to test consume configMaps 01/28/23 01:08:10.818
    Jan 28 01:08:10.841: INFO: Waiting up to 5m0s for pod "pod-configmaps-064dbd72-4cac-470e-a49c-b433e54fd0db" in namespace "configmap-3815" to be "Succeeded or Failed"
    Jan 28 01:08:10.853: INFO: Pod "pod-configmaps-064dbd72-4cac-470e-a49c-b433e54fd0db": Phase="Pending", Reason="", readiness=false. Elapsed: 12.056824ms
    Jan 28 01:08:12.866: INFO: Pod "pod-configmaps-064dbd72-4cac-470e-a49c-b433e54fd0db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024510327s
    Jan 28 01:08:14.868: INFO: Pod "pod-configmaps-064dbd72-4cac-470e-a49c-b433e54fd0db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027271677s
    STEP: Saw pod success 01/28/23 01:08:14.869
    Jan 28 01:08:14.869: INFO: Pod "pod-configmaps-064dbd72-4cac-470e-a49c-b433e54fd0db" satisfied condition "Succeeded or Failed"
    Jan 28 01:08:14.882: INFO: Trying to get logs from node 10.9.20.126 pod pod-configmaps-064dbd72-4cac-470e-a49c-b433e54fd0db container agnhost-container: <nil>
    STEP: delete the pod 01/28/23 01:08:14.916
    Jan 28 01:08:14.953: INFO: Waiting for pod pod-configmaps-064dbd72-4cac-470e-a49c-b433e54fd0db to disappear
    Jan 28 01:08:14.961: INFO: Pod pod-configmaps-064dbd72-4cac-470e-a49c-b433e54fd0db no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 28 01:08:14.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3815" for this suite. 01/28/23 01:08:14.983
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:08:14.999
Jan 28 01:08:15.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename gc 01/28/23 01:08:15.003
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:15.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:15.062
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/28/23 01:08:15.076
STEP: Wait for the Deployment to create new ReplicaSet 01/28/23 01:08:15.095
STEP: delete the deployment 01/28/23 01:08:15.11
STEP: wait for all rs to be garbage collected 01/28/23 01:08:15.133
STEP: expected 0 pods, got 2 pods 01/28/23 01:08:15.201
STEP: Gathering metrics 01/28/23 01:08:15.741
W0128 01:08:15.778316      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 28 01:08:15.778: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 28 01:08:15.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5036" for this suite. 01/28/23 01:08:15.8
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":153,"skipped":2728,"failed":0}
------------------------------
â€¢ [0.818 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:08:14.999
    Jan 28 01:08:15.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename gc 01/28/23 01:08:15.003
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:15.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:15.062
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/28/23 01:08:15.076
    STEP: Wait for the Deployment to create new ReplicaSet 01/28/23 01:08:15.095
    STEP: delete the deployment 01/28/23 01:08:15.11
    STEP: wait for all rs to be garbage collected 01/28/23 01:08:15.133
    STEP: expected 0 pods, got 2 pods 01/28/23 01:08:15.201
    STEP: Gathering metrics 01/28/23 01:08:15.741
    W0128 01:08:15.778316      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 28 01:08:15.778: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 28 01:08:15.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5036" for this suite. 01/28/23 01:08:15.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:08:15.828
Jan 28 01:08:15.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 01:08:15.83
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:15.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:15.892
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 01/28/23 01:08:15.905
Jan 28 01:08:15.929: INFO: Waiting up to 5m0s for pod "labelsupdatef7db0149-0d31-4964-aedb-3cbcd25664c3" in namespace "projected-8794" to be "running and ready"
Jan 28 01:08:15.945: INFO: Pod "labelsupdatef7db0149-0d31-4964-aedb-3cbcd25664c3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.623034ms
Jan 28 01:08:15.945: INFO: The phase of Pod labelsupdatef7db0149-0d31-4964-aedb-3cbcd25664c3 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:08:17.963: INFO: Pod "labelsupdatef7db0149-0d31-4964-aedb-3cbcd25664c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034213005s
Jan 28 01:08:17.963: INFO: The phase of Pod labelsupdatef7db0149-0d31-4964-aedb-3cbcd25664c3 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:08:19.960: INFO: Pod "labelsupdatef7db0149-0d31-4964-aedb-3cbcd25664c3": Phase="Running", Reason="", readiness=true. Elapsed: 4.030820006s
Jan 28 01:08:19.960: INFO: The phase of Pod labelsupdatef7db0149-0d31-4964-aedb-3cbcd25664c3 is Running (Ready = true)
Jan 28 01:08:19.960: INFO: Pod "labelsupdatef7db0149-0d31-4964-aedb-3cbcd25664c3" satisfied condition "running and ready"
Jan 28 01:08:20.526: INFO: Successfully updated pod "labelsupdatef7db0149-0d31-4964-aedb-3cbcd25664c3"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 28 01:08:22.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8794" for this suite. 01/28/23 01:08:22.598
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":154,"skipped":2773,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.792 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:08:15.828
    Jan 28 01:08:15.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 01:08:15.83
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:15.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:15.892
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 01/28/23 01:08:15.905
    Jan 28 01:08:15.929: INFO: Waiting up to 5m0s for pod "labelsupdatef7db0149-0d31-4964-aedb-3cbcd25664c3" in namespace "projected-8794" to be "running and ready"
    Jan 28 01:08:15.945: INFO: Pod "labelsupdatef7db0149-0d31-4964-aedb-3cbcd25664c3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.623034ms
    Jan 28 01:08:15.945: INFO: The phase of Pod labelsupdatef7db0149-0d31-4964-aedb-3cbcd25664c3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:08:17.963: INFO: Pod "labelsupdatef7db0149-0d31-4964-aedb-3cbcd25664c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034213005s
    Jan 28 01:08:17.963: INFO: The phase of Pod labelsupdatef7db0149-0d31-4964-aedb-3cbcd25664c3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:08:19.960: INFO: Pod "labelsupdatef7db0149-0d31-4964-aedb-3cbcd25664c3": Phase="Running", Reason="", readiness=true. Elapsed: 4.030820006s
    Jan 28 01:08:19.960: INFO: The phase of Pod labelsupdatef7db0149-0d31-4964-aedb-3cbcd25664c3 is Running (Ready = true)
    Jan 28 01:08:19.960: INFO: Pod "labelsupdatef7db0149-0d31-4964-aedb-3cbcd25664c3" satisfied condition "running and ready"
    Jan 28 01:08:20.526: INFO: Successfully updated pod "labelsupdatef7db0149-0d31-4964-aedb-3cbcd25664c3"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 28 01:08:22.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8794" for this suite. 01/28/23 01:08:22.598
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:08:22.628
Jan 28 01:08:22.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename webhook 01/28/23 01:08:22.63
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:22.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:22.678
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/28/23 01:08:22.727
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:08:23.268
STEP: Deploying the webhook pod 01/28/23 01:08:23.297
STEP: Wait for the deployment to be ready 01/28/23 01:08:23.334
Jan 28 01:08:23.366: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/28/23 01:08:25.413
STEP: Verifying the service has paired with the endpoint 01/28/23 01:08:25.439
Jan 28 01:08:26.440: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/28/23 01:08:26.461
STEP: create a namespace for the webhook 01/28/23 01:08:26.551
STEP: create a configmap should be unconditionally rejected by the webhook 01/28/23 01:08:26.578
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:08:26.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8279" for this suite. 01/28/23 01:08:26.69
STEP: Destroying namespace "webhook-8279-markers" for this suite. 01/28/23 01:08:26.709
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":155,"skipped":2776,"failed":0}
------------------------------
â€¢ [4.252 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:08:22.628
    Jan 28 01:08:22.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename webhook 01/28/23 01:08:22.63
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:22.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:22.678
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/28/23 01:08:22.727
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:08:23.268
    STEP: Deploying the webhook pod 01/28/23 01:08:23.297
    STEP: Wait for the deployment to be ready 01/28/23 01:08:23.334
    Jan 28 01:08:23.366: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/28/23 01:08:25.413
    STEP: Verifying the service has paired with the endpoint 01/28/23 01:08:25.439
    Jan 28 01:08:26.440: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/28/23 01:08:26.461
    STEP: create a namespace for the webhook 01/28/23 01:08:26.551
    STEP: create a configmap should be unconditionally rejected by the webhook 01/28/23 01:08:26.578
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:08:26.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8279" for this suite. 01/28/23 01:08:26.69
    STEP: Destroying namespace "webhook-8279-markers" for this suite. 01/28/23 01:08:26.709
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:08:26.892
Jan 28 01:08:26.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename secrets 01/28/23 01:08:26.895
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:26.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:26.961
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-e2b5fc05-d353-4794-9050-af0e289d2e52 01/28/23 01:08:26.973
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 28 01:08:26.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3064" for this suite. 01/28/23 01:08:26.997
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":156,"skipped":2793,"failed":0}
------------------------------
â€¢ [0.124 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:08:26.892
    Jan 28 01:08:26.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename secrets 01/28/23 01:08:26.895
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:26.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:26.961
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-e2b5fc05-d353-4794-9050-af0e289d2e52 01/28/23 01:08:26.973
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 28 01:08:26.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3064" for this suite. 01/28/23 01:08:26.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:08:27.025
Jan 28 01:08:27.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 01:08:27.027
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:27.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:27.088
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-5cbb5554-b236-438b-88e0-5c10deffced9 01/28/23 01:08:27.104
STEP: Creating a pod to test consume configMaps 01/28/23 01:08:27.12
Jan 28 01:08:27.145: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ba3742f8-96d8-4c8a-bf7a-6b3cf75a4755" in namespace "projected-6726" to be "Succeeded or Failed"
Jan 28 01:08:27.157: INFO: Pod "pod-projected-configmaps-ba3742f8-96d8-4c8a-bf7a-6b3cf75a4755": Phase="Pending", Reason="", readiness=false. Elapsed: 12.058379ms
Jan 28 01:08:29.172: INFO: Pod "pod-projected-configmaps-ba3742f8-96d8-4c8a-bf7a-6b3cf75a4755": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026612491s
Jan 28 01:08:31.169: INFO: Pod "pod-projected-configmaps-ba3742f8-96d8-4c8a-bf7a-6b3cf75a4755": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024021424s
Jan 28 01:08:33.172: INFO: Pod "pod-projected-configmaps-ba3742f8-96d8-4c8a-bf7a-6b3cf75a4755": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02687777s
STEP: Saw pod success 01/28/23 01:08:33.172
Jan 28 01:08:33.173: INFO: Pod "pod-projected-configmaps-ba3742f8-96d8-4c8a-bf7a-6b3cf75a4755" satisfied condition "Succeeded or Failed"
Jan 28 01:08:33.186: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-configmaps-ba3742f8-96d8-4c8a-bf7a-6b3cf75a4755 container agnhost-container: <nil>
STEP: delete the pod 01/28/23 01:08:33.213
Jan 28 01:08:33.237: INFO: Waiting for pod pod-projected-configmaps-ba3742f8-96d8-4c8a-bf7a-6b3cf75a4755 to disappear
Jan 28 01:08:33.250: INFO: Pod pod-projected-configmaps-ba3742f8-96d8-4c8a-bf7a-6b3cf75a4755 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 28 01:08:33.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6726" for this suite. 01/28/23 01:08:33.271
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":157,"skipped":2824,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.266 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:08:27.025
    Jan 28 01:08:27.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 01:08:27.027
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:27.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:27.088
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-5cbb5554-b236-438b-88e0-5c10deffced9 01/28/23 01:08:27.104
    STEP: Creating a pod to test consume configMaps 01/28/23 01:08:27.12
    Jan 28 01:08:27.145: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ba3742f8-96d8-4c8a-bf7a-6b3cf75a4755" in namespace "projected-6726" to be "Succeeded or Failed"
    Jan 28 01:08:27.157: INFO: Pod "pod-projected-configmaps-ba3742f8-96d8-4c8a-bf7a-6b3cf75a4755": Phase="Pending", Reason="", readiness=false. Elapsed: 12.058379ms
    Jan 28 01:08:29.172: INFO: Pod "pod-projected-configmaps-ba3742f8-96d8-4c8a-bf7a-6b3cf75a4755": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026612491s
    Jan 28 01:08:31.169: INFO: Pod "pod-projected-configmaps-ba3742f8-96d8-4c8a-bf7a-6b3cf75a4755": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024021424s
    Jan 28 01:08:33.172: INFO: Pod "pod-projected-configmaps-ba3742f8-96d8-4c8a-bf7a-6b3cf75a4755": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02687777s
    STEP: Saw pod success 01/28/23 01:08:33.172
    Jan 28 01:08:33.173: INFO: Pod "pod-projected-configmaps-ba3742f8-96d8-4c8a-bf7a-6b3cf75a4755" satisfied condition "Succeeded or Failed"
    Jan 28 01:08:33.186: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-configmaps-ba3742f8-96d8-4c8a-bf7a-6b3cf75a4755 container agnhost-container: <nil>
    STEP: delete the pod 01/28/23 01:08:33.213
    Jan 28 01:08:33.237: INFO: Waiting for pod pod-projected-configmaps-ba3742f8-96d8-4c8a-bf7a-6b3cf75a4755 to disappear
    Jan 28 01:08:33.250: INFO: Pod pod-projected-configmaps-ba3742f8-96d8-4c8a-bf7a-6b3cf75a4755 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 28 01:08:33.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6726" for this suite. 01/28/23 01:08:33.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:08:33.298
Jan 28 01:08:33.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename var-expansion 01/28/23 01:08:33.3
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:33.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:33.375
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 01/28/23 01:08:33.391
Jan 28 01:08:33.417: INFO: Waiting up to 5m0s for pod "var-expansion-46d8b061-f58c-47a6-9cd3-d3c8ee20d3b0" in namespace "var-expansion-2034" to be "Succeeded or Failed"
Jan 28 01:08:33.426: INFO: Pod "var-expansion-46d8b061-f58c-47a6-9cd3-d3c8ee20d3b0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.615637ms
Jan 28 01:08:35.441: INFO: Pod "var-expansion-46d8b061-f58c-47a6-9cd3-d3c8ee20d3b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024498908s
Jan 28 01:08:37.441: INFO: Pod "var-expansion-46d8b061-f58c-47a6-9cd3-d3c8ee20d3b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023789522s
Jan 28 01:08:39.443: INFO: Pod "var-expansion-46d8b061-f58c-47a6-9cd3-d3c8ee20d3b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025909637s
STEP: Saw pod success 01/28/23 01:08:39.443
Jan 28 01:08:39.443: INFO: Pod "var-expansion-46d8b061-f58c-47a6-9cd3-d3c8ee20d3b0" satisfied condition "Succeeded or Failed"
Jan 28 01:08:39.459: INFO: Trying to get logs from node 10.9.20.126 pod var-expansion-46d8b061-f58c-47a6-9cd3-d3c8ee20d3b0 container dapi-container: <nil>
STEP: delete the pod 01/28/23 01:08:39.484
Jan 28 01:08:39.510: INFO: Waiting for pod var-expansion-46d8b061-f58c-47a6-9cd3-d3c8ee20d3b0 to disappear
Jan 28 01:08:39.522: INFO: Pod var-expansion-46d8b061-f58c-47a6-9cd3-d3c8ee20d3b0 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 28 01:08:39.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2034" for this suite. 01/28/23 01:08:39.54
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":158,"skipped":2847,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.258 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:08:33.298
    Jan 28 01:08:33.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename var-expansion 01/28/23 01:08:33.3
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:33.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:33.375
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 01/28/23 01:08:33.391
    Jan 28 01:08:33.417: INFO: Waiting up to 5m0s for pod "var-expansion-46d8b061-f58c-47a6-9cd3-d3c8ee20d3b0" in namespace "var-expansion-2034" to be "Succeeded or Failed"
    Jan 28 01:08:33.426: INFO: Pod "var-expansion-46d8b061-f58c-47a6-9cd3-d3c8ee20d3b0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.615637ms
    Jan 28 01:08:35.441: INFO: Pod "var-expansion-46d8b061-f58c-47a6-9cd3-d3c8ee20d3b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024498908s
    Jan 28 01:08:37.441: INFO: Pod "var-expansion-46d8b061-f58c-47a6-9cd3-d3c8ee20d3b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023789522s
    Jan 28 01:08:39.443: INFO: Pod "var-expansion-46d8b061-f58c-47a6-9cd3-d3c8ee20d3b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025909637s
    STEP: Saw pod success 01/28/23 01:08:39.443
    Jan 28 01:08:39.443: INFO: Pod "var-expansion-46d8b061-f58c-47a6-9cd3-d3c8ee20d3b0" satisfied condition "Succeeded or Failed"
    Jan 28 01:08:39.459: INFO: Trying to get logs from node 10.9.20.126 pod var-expansion-46d8b061-f58c-47a6-9cd3-d3c8ee20d3b0 container dapi-container: <nil>
    STEP: delete the pod 01/28/23 01:08:39.484
    Jan 28 01:08:39.510: INFO: Waiting for pod var-expansion-46d8b061-f58c-47a6-9cd3-d3c8ee20d3b0 to disappear
    Jan 28 01:08:39.522: INFO: Pod var-expansion-46d8b061-f58c-47a6-9cd3-d3c8ee20d3b0 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 28 01:08:39.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2034" for this suite. 01/28/23 01:08:39.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:08:39.562
Jan 28 01:08:39.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename deployment 01/28/23 01:08:39.565
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:39.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:39.622
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan 28 01:08:39.633: INFO: Creating simple deployment test-new-deployment
Jan 28 01:08:39.705: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 01/28/23 01:08:41.77
STEP: updating a scale subresource 01/28/23 01:08:41.785
STEP: verifying the deployment Spec.Replicas was modified 01/28/23 01:08:41.806
STEP: Patch a scale subresource 01/28/23 01:08:41.821
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 28 01:08:41.905: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-3316  566057c8-be57-4d34-8a44-6401d3897ddf 30291 3 2023-01-28 01:08:39 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-28 01:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b4cb18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:2,UpdatedReplicas:2,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-28 01:08:41 +0000 UTC,LastTransitionTime:2023-01-28 01:08:39 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-28 01:08:41 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 28 01:08:41.946: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-3316  5b21dc71-f74c-4e2d-9c74-03f34d63705b 30300 3 2023-01-28 01:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 566057c8-be57-4d34-8a44-6401d3897ddf 0xc002b4cf37 0xc002b4cf38}] [] [{kube-controller-manager Update apps/v1 2023-01-28 01:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"566057c8-be57-4d34-8a44-6401d3897ddf\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:08:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b4cfc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:4,FullyLabeledReplicas:4,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 28 01:08:41.961: INFO: Pod "test-new-deployment-845c8977d9-4v6wm" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-4v6wm test-new-deployment-845c8977d9- deployment-3316  5828efec-e4a8-410c-a04c-dabcc58d2f56 30271 0 2023-01-28 01:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1ac72acd2e66fb5d10d9f173ed285c6488bc39479ec59e8317800684e5f40883 cni.projectcalico.org/podIP:172.30.12.213/32 cni.projectcalico.org/podIPs:172.30.12.213/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 5b21dc71-f74c-4e2d-9c74-03f34d63705b 0xc002b4d387 0xc002b4d388}] [] [{kube-controller-manager Update v1 2023-01-28 01:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b21dc71-f74c-4e2d-9c74-03f34d63705b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:08:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:08:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.213\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l6bwm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l6bwm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:172.30.12.213,StartTime:2023-01-28 01:08:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:08:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2f00abb57920536cb2dce5c2750806f46542fe01b6a616317ce15bbf20574eaa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.12.213,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:08:41.962: INFO: Pod "test-new-deployment-845c8977d9-n457c" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-n457c test-new-deployment-845c8977d9- deployment-3316  7ca3102e-662a-4a56-a2f8-214057022455 30286 0 2023-01-28 01:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 5b21dc71-f74c-4e2d-9c74-03f34d63705b 0xc002b4d597 0xc002b4d598}] [] [{kube-controller-manager Update v1 2023-01-28 01:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b21dc71-f74c-4e2d-9c74-03f34d63705b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:08:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kfrmh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kfrmh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:,StartTime:2023-01-28 01:08:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:08:41.963: INFO: Pod "test-new-deployment-845c8977d9-rnrxf" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-rnrxf test-new-deployment-845c8977d9- deployment-3316  24be4bf7-c946-45a2-a201-9b1f2f6aa941 30294 0 2023-01-28 01:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 5b21dc71-f74c-4e2d-9c74-03f34d63705b 0xc002b4d760 0xc002b4d761}] [] [{kube-controller-manager Update v1 2023-01-28 01:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b21dc71-f74c-4e2d-9c74-03f34d63705b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-62tf8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-62tf8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:08:41.964: INFO: Pod "test-new-deployment-845c8977d9-zwzrr" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-zwzrr test-new-deployment-845c8977d9- deployment-3316  430dcb46-fde6-49b5-9545-462a71ef1a8c 30302 0 2023-01-28 01:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 5b21dc71-f74c-4e2d-9c74-03f34d63705b 0xc002b4d8c0 0xc002b4d8c1}] [] [{kube-controller-manager Update v1 2023-01-28 01:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b21dc71-f74c-4e2d-9c74-03f34d63705b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:08:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wf2wb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wf2wb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 01:08:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 28 01:08:41.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3316" for this suite. 01/28/23 01:08:41.984
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":159,"skipped":2858,"failed":0}
------------------------------
â€¢ [2.438 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:08:39.562
    Jan 28 01:08:39.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename deployment 01/28/23 01:08:39.565
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:39.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:39.622
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan 28 01:08:39.633: INFO: Creating simple deployment test-new-deployment
    Jan 28 01:08:39.705: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 01/28/23 01:08:41.77
    STEP: updating a scale subresource 01/28/23 01:08:41.785
    STEP: verifying the deployment Spec.Replicas was modified 01/28/23 01:08:41.806
    STEP: Patch a scale subresource 01/28/23 01:08:41.821
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 28 01:08:41.905: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-3316  566057c8-be57-4d34-8a44-6401d3897ddf 30291 3 2023-01-28 01:08:39 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-28 01:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b4cb18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:2,UpdatedReplicas:2,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-28 01:08:41 +0000 UTC,LastTransitionTime:2023-01-28 01:08:39 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-28 01:08:41 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 28 01:08:41.946: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-3316  5b21dc71-f74c-4e2d-9c74-03f34d63705b 30300 3 2023-01-28 01:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 566057c8-be57-4d34-8a44-6401d3897ddf 0xc002b4cf37 0xc002b4cf38}] [] [{kube-controller-manager Update apps/v1 2023-01-28 01:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"566057c8-be57-4d34-8a44-6401d3897ddf\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:08:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b4cfc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:4,FullyLabeledReplicas:4,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 28 01:08:41.961: INFO: Pod "test-new-deployment-845c8977d9-4v6wm" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-4v6wm test-new-deployment-845c8977d9- deployment-3316  5828efec-e4a8-410c-a04c-dabcc58d2f56 30271 0 2023-01-28 01:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1ac72acd2e66fb5d10d9f173ed285c6488bc39479ec59e8317800684e5f40883 cni.projectcalico.org/podIP:172.30.12.213/32 cni.projectcalico.org/podIPs:172.30.12.213/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 5b21dc71-f74c-4e2d-9c74-03f34d63705b 0xc002b4d387 0xc002b4d388}] [] [{kube-controller-manager Update v1 2023-01-28 01:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b21dc71-f74c-4e2d-9c74-03f34d63705b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:08:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:08:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.213\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l6bwm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l6bwm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:172.30.12.213,StartTime:2023-01-28 01:08:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:08:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2f00abb57920536cb2dce5c2750806f46542fe01b6a616317ce15bbf20574eaa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.12.213,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 01:08:41.962: INFO: Pod "test-new-deployment-845c8977d9-n457c" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-n457c test-new-deployment-845c8977d9- deployment-3316  7ca3102e-662a-4a56-a2f8-214057022455 30286 0 2023-01-28 01:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 5b21dc71-f74c-4e2d-9c74-03f34d63705b 0xc002b4d597 0xc002b4d598}] [] [{kube-controller-manager Update v1 2023-01-28 01:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b21dc71-f74c-4e2d-9c74-03f34d63705b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:08:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kfrmh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kfrmh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:,StartTime:2023-01-28 01:08:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 01:08:41.963: INFO: Pod "test-new-deployment-845c8977d9-rnrxf" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-rnrxf test-new-deployment-845c8977d9- deployment-3316  24be4bf7-c946-45a2-a201-9b1f2f6aa941 30294 0 2023-01-28 01:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 5b21dc71-f74c-4e2d-9c74-03f34d63705b 0xc002b4d760 0xc002b4d761}] [] [{kube-controller-manager Update v1 2023-01-28 01:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b21dc71-f74c-4e2d-9c74-03f34d63705b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-62tf8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-62tf8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 01:08:41.964: INFO: Pod "test-new-deployment-845c8977d9-zwzrr" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-zwzrr test-new-deployment-845c8977d9- deployment-3316  430dcb46-fde6-49b5-9545-462a71ef1a8c 30302 0 2023-01-28 01:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 5b21dc71-f74c-4e2d-9c74-03f34d63705b 0xc002b4d8c0 0xc002b4d8c1}] [] [{kube-controller-manager Update v1 2023-01-28 01:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b21dc71-f74c-4e2d-9c74-03f34d63705b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:08:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wf2wb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wf2wb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:08:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 01:08:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 28 01:08:41.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3316" for this suite. 01/28/23 01:08:41.984
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:08:42.002
Jan 28 01:08:42.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename downward-api 01/28/23 01:08:42.004
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:42.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:42.06
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 01/28/23 01:08:42.074
Jan 28 01:08:42.103: INFO: Waiting up to 5m0s for pod "downwardapi-volume-571b3279-f18d-477f-a9e9-999582e3e279" in namespace "downward-api-7906" to be "Succeeded or Failed"
Jan 28 01:08:42.120: INFO: Pod "downwardapi-volume-571b3279-f18d-477f-a9e9-999582e3e279": Phase="Pending", Reason="", readiness=false. Elapsed: 17.402614ms
Jan 28 01:08:44.131: INFO: Pod "downwardapi-volume-571b3279-f18d-477f-a9e9-999582e3e279": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027787167s
Jan 28 01:08:46.135: INFO: Pod "downwardapi-volume-571b3279-f18d-477f-a9e9-999582e3e279": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031912855s
Jan 28 01:08:48.134: INFO: Pod "downwardapi-volume-571b3279-f18d-477f-a9e9-999582e3e279": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031756327s
STEP: Saw pod success 01/28/23 01:08:48.135
Jan 28 01:08:48.135: INFO: Pod "downwardapi-volume-571b3279-f18d-477f-a9e9-999582e3e279" satisfied condition "Succeeded or Failed"
Jan 28 01:08:48.147: INFO: Trying to get logs from node 10.9.20.72 pod downwardapi-volume-571b3279-f18d-477f-a9e9-999582e3e279 container client-container: <nil>
STEP: delete the pod 01/28/23 01:08:48.17
Jan 28 01:08:48.200: INFO: Waiting for pod downwardapi-volume-571b3279-f18d-477f-a9e9-999582e3e279 to disappear
Jan 28 01:08:48.211: INFO: Pod downwardapi-volume-571b3279-f18d-477f-a9e9-999582e3e279 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 28 01:08:48.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7906" for this suite. 01/28/23 01:08:48.233
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":160,"skipped":2860,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.250 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:08:42.002
    Jan 28 01:08:42.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename downward-api 01/28/23 01:08:42.004
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:42.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:42.06
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 01/28/23 01:08:42.074
    Jan 28 01:08:42.103: INFO: Waiting up to 5m0s for pod "downwardapi-volume-571b3279-f18d-477f-a9e9-999582e3e279" in namespace "downward-api-7906" to be "Succeeded or Failed"
    Jan 28 01:08:42.120: INFO: Pod "downwardapi-volume-571b3279-f18d-477f-a9e9-999582e3e279": Phase="Pending", Reason="", readiness=false. Elapsed: 17.402614ms
    Jan 28 01:08:44.131: INFO: Pod "downwardapi-volume-571b3279-f18d-477f-a9e9-999582e3e279": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027787167s
    Jan 28 01:08:46.135: INFO: Pod "downwardapi-volume-571b3279-f18d-477f-a9e9-999582e3e279": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031912855s
    Jan 28 01:08:48.134: INFO: Pod "downwardapi-volume-571b3279-f18d-477f-a9e9-999582e3e279": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031756327s
    STEP: Saw pod success 01/28/23 01:08:48.135
    Jan 28 01:08:48.135: INFO: Pod "downwardapi-volume-571b3279-f18d-477f-a9e9-999582e3e279" satisfied condition "Succeeded or Failed"
    Jan 28 01:08:48.147: INFO: Trying to get logs from node 10.9.20.72 pod downwardapi-volume-571b3279-f18d-477f-a9e9-999582e3e279 container client-container: <nil>
    STEP: delete the pod 01/28/23 01:08:48.17
    Jan 28 01:08:48.200: INFO: Waiting for pod downwardapi-volume-571b3279-f18d-477f-a9e9-999582e3e279 to disappear
    Jan 28 01:08:48.211: INFO: Pod downwardapi-volume-571b3279-f18d-477f-a9e9-999582e3e279 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 28 01:08:48.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7906" for this suite. 01/28/23 01:08:48.233
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:08:48.253
Jan 28 01:08:48.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename container-probe 01/28/23 01:08:48.257
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:48.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:48.327
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-55cce103-dd11-441a-a8f1-c3f1041a08f8 in namespace container-probe-1671 01/28/23 01:08:48.34
Jan 28 01:08:48.362: INFO: Waiting up to 5m0s for pod "liveness-55cce103-dd11-441a-a8f1-c3f1041a08f8" in namespace "container-probe-1671" to be "not pending"
Jan 28 01:08:48.374: INFO: Pod "liveness-55cce103-dd11-441a-a8f1-c3f1041a08f8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.65966ms
Jan 28 01:08:50.389: INFO: Pod "liveness-55cce103-dd11-441a-a8f1-c3f1041a08f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02622536s
Jan 28 01:08:52.385: INFO: Pod "liveness-55cce103-dd11-441a-a8f1-c3f1041a08f8": Phase="Running", Reason="", readiness=true. Elapsed: 4.022256953s
Jan 28 01:08:52.385: INFO: Pod "liveness-55cce103-dd11-441a-a8f1-c3f1041a08f8" satisfied condition "not pending"
Jan 28 01:08:52.385: INFO: Started pod liveness-55cce103-dd11-441a-a8f1-c3f1041a08f8 in namespace container-probe-1671
STEP: checking the pod's current state and verifying that restartCount is present 01/28/23 01:08:52.385
Jan 28 01:08:52.395: INFO: Initial restart count of pod liveness-55cce103-dd11-441a-a8f1-c3f1041a08f8 is 0
STEP: deleting the pod 01/28/23 01:12:54.191
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 28 01:12:54.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1671" for this suite. 01/28/23 01:12:54.235
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":161,"skipped":2862,"failed":0}
------------------------------
â€¢ [SLOW TEST] [246.007 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:08:48.253
    Jan 28 01:08:48.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename container-probe 01/28/23 01:08:48.257
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:08:48.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:08:48.327
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-55cce103-dd11-441a-a8f1-c3f1041a08f8 in namespace container-probe-1671 01/28/23 01:08:48.34
    Jan 28 01:08:48.362: INFO: Waiting up to 5m0s for pod "liveness-55cce103-dd11-441a-a8f1-c3f1041a08f8" in namespace "container-probe-1671" to be "not pending"
    Jan 28 01:08:48.374: INFO: Pod "liveness-55cce103-dd11-441a-a8f1-c3f1041a08f8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.65966ms
    Jan 28 01:08:50.389: INFO: Pod "liveness-55cce103-dd11-441a-a8f1-c3f1041a08f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02622536s
    Jan 28 01:08:52.385: INFO: Pod "liveness-55cce103-dd11-441a-a8f1-c3f1041a08f8": Phase="Running", Reason="", readiness=true. Elapsed: 4.022256953s
    Jan 28 01:08:52.385: INFO: Pod "liveness-55cce103-dd11-441a-a8f1-c3f1041a08f8" satisfied condition "not pending"
    Jan 28 01:08:52.385: INFO: Started pod liveness-55cce103-dd11-441a-a8f1-c3f1041a08f8 in namespace container-probe-1671
    STEP: checking the pod's current state and verifying that restartCount is present 01/28/23 01:08:52.385
    Jan 28 01:08:52.395: INFO: Initial restart count of pod liveness-55cce103-dd11-441a-a8f1-c3f1041a08f8 is 0
    STEP: deleting the pod 01/28/23 01:12:54.191
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 28 01:12:54.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1671" for this suite. 01/28/23 01:12:54.235
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:12:54.261
Jan 28 01:12:54.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename discovery 01/28/23 01:12:54.263
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:12:54.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:12:54.358
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/28/23 01:12:54.375
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan 28 01:12:54.872: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 28 01:12:54.879: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 28 01:12:54.879: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 28 01:12:54.879: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 28 01:12:54.879: INFO: Checking APIGroup: apps
Jan 28 01:12:54.885: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 28 01:12:54.885: INFO: Versions found [{apps/v1 v1}]
Jan 28 01:12:54.885: INFO: apps/v1 matches apps/v1
Jan 28 01:12:54.885: INFO: Checking APIGroup: events.k8s.io
Jan 28 01:12:54.893: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 28 01:12:54.894: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan 28 01:12:54.894: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 28 01:12:54.894: INFO: Checking APIGroup: authentication.k8s.io
Jan 28 01:12:54.900: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 28 01:12:54.900: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 28 01:12:54.900: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 28 01:12:54.900: INFO: Checking APIGroup: authorization.k8s.io
Jan 28 01:12:54.906: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 28 01:12:54.906: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 28 01:12:54.906: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 28 01:12:54.906: INFO: Checking APIGroup: autoscaling
Jan 28 01:12:54.912: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 28 01:12:54.912: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Jan 28 01:12:54.912: INFO: autoscaling/v2 matches autoscaling/v2
Jan 28 01:12:54.912: INFO: Checking APIGroup: batch
Jan 28 01:12:54.917: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 28 01:12:54.918: INFO: Versions found [{batch/v1 v1}]
Jan 28 01:12:54.918: INFO: batch/v1 matches batch/v1
Jan 28 01:12:54.918: INFO: Checking APIGroup: certificates.k8s.io
Jan 28 01:12:54.924: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 28 01:12:54.924: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 28 01:12:54.925: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 28 01:12:54.925: INFO: Checking APIGroup: networking.k8s.io
Jan 28 01:12:54.931: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 28 01:12:54.931: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 28 01:12:54.931: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 28 01:12:54.931: INFO: Checking APIGroup: policy
Jan 28 01:12:54.938: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 28 01:12:54.939: INFO: Versions found [{policy/v1 v1}]
Jan 28 01:12:54.939: INFO: policy/v1 matches policy/v1
Jan 28 01:12:54.939: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 28 01:12:54.945: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 28 01:12:54.945: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 28 01:12:54.945: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 28 01:12:54.945: INFO: Checking APIGroup: storage.k8s.io
Jan 28 01:12:54.952: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 28 01:12:54.952: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 28 01:12:54.952: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 28 01:12:54.952: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 28 01:12:54.958: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 28 01:12:54.958: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 28 01:12:54.958: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 28 01:12:54.958: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 28 01:12:54.963: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 28 01:12:54.963: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 28 01:12:54.963: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 28 01:12:54.963: INFO: Checking APIGroup: scheduling.k8s.io
Jan 28 01:12:54.968: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 28 01:12:54.968: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 28 01:12:54.968: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 28 01:12:54.968: INFO: Checking APIGroup: coordination.k8s.io
Jan 28 01:12:54.974: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 28 01:12:54.974: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 28 01:12:54.975: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 28 01:12:54.975: INFO: Checking APIGroup: node.k8s.io
Jan 28 01:12:54.982: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 28 01:12:54.982: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan 28 01:12:54.982: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 28 01:12:54.982: INFO: Checking APIGroup: discovery.k8s.io
Jan 28 01:12:54.988: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 28 01:12:54.988: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan 28 01:12:54.988: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 28 01:12:54.988: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 28 01:12:54.995: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jan 28 01:12:54.995: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan 28 01:12:54.995: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jan 28 01:12:54.995: INFO: Checking APIGroup: crd.projectcalico.org
Jan 28 01:12:55.001: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan 28 01:12:55.001: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan 28 01:12:55.001: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jan 28 01:12:55.001: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jan 28 01:12:55.006: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jan 28 01:12:55.006: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Jan 28 01:12:55.007: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jan 28 01:12:55.007: INFO: Checking APIGroup: ibm.com
Jan 28 01:12:55.013: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Jan 28 01:12:55.014: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Jan 28 01:12:55.014: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Jan 28 01:12:55.014: INFO: Checking APIGroup: metrics.k8s.io
Jan 28 01:12:55.020: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jan 28 01:12:55.020: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jan 28 01:12:55.021: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Jan 28 01:12:55.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-9920" for this suite. 01/28/23 01:12:55.038
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":162,"skipped":2862,"failed":0}
------------------------------
â€¢ [0.795 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:12:54.261
    Jan 28 01:12:54.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename discovery 01/28/23 01:12:54.263
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:12:54.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:12:54.358
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/28/23 01:12:54.375
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan 28 01:12:54.872: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan 28 01:12:54.879: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan 28 01:12:54.879: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan 28 01:12:54.879: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan 28 01:12:54.879: INFO: Checking APIGroup: apps
    Jan 28 01:12:54.885: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan 28 01:12:54.885: INFO: Versions found [{apps/v1 v1}]
    Jan 28 01:12:54.885: INFO: apps/v1 matches apps/v1
    Jan 28 01:12:54.885: INFO: Checking APIGroup: events.k8s.io
    Jan 28 01:12:54.893: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan 28 01:12:54.894: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan 28 01:12:54.894: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan 28 01:12:54.894: INFO: Checking APIGroup: authentication.k8s.io
    Jan 28 01:12:54.900: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan 28 01:12:54.900: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan 28 01:12:54.900: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan 28 01:12:54.900: INFO: Checking APIGroup: authorization.k8s.io
    Jan 28 01:12:54.906: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan 28 01:12:54.906: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan 28 01:12:54.906: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan 28 01:12:54.906: INFO: Checking APIGroup: autoscaling
    Jan 28 01:12:54.912: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan 28 01:12:54.912: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Jan 28 01:12:54.912: INFO: autoscaling/v2 matches autoscaling/v2
    Jan 28 01:12:54.912: INFO: Checking APIGroup: batch
    Jan 28 01:12:54.917: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan 28 01:12:54.918: INFO: Versions found [{batch/v1 v1}]
    Jan 28 01:12:54.918: INFO: batch/v1 matches batch/v1
    Jan 28 01:12:54.918: INFO: Checking APIGroup: certificates.k8s.io
    Jan 28 01:12:54.924: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan 28 01:12:54.924: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan 28 01:12:54.925: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan 28 01:12:54.925: INFO: Checking APIGroup: networking.k8s.io
    Jan 28 01:12:54.931: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan 28 01:12:54.931: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan 28 01:12:54.931: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan 28 01:12:54.931: INFO: Checking APIGroup: policy
    Jan 28 01:12:54.938: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan 28 01:12:54.939: INFO: Versions found [{policy/v1 v1}]
    Jan 28 01:12:54.939: INFO: policy/v1 matches policy/v1
    Jan 28 01:12:54.939: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan 28 01:12:54.945: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan 28 01:12:54.945: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan 28 01:12:54.945: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan 28 01:12:54.945: INFO: Checking APIGroup: storage.k8s.io
    Jan 28 01:12:54.952: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan 28 01:12:54.952: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan 28 01:12:54.952: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan 28 01:12:54.952: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan 28 01:12:54.958: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan 28 01:12:54.958: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan 28 01:12:54.958: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan 28 01:12:54.958: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan 28 01:12:54.963: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan 28 01:12:54.963: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan 28 01:12:54.963: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan 28 01:12:54.963: INFO: Checking APIGroup: scheduling.k8s.io
    Jan 28 01:12:54.968: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan 28 01:12:54.968: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan 28 01:12:54.968: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan 28 01:12:54.968: INFO: Checking APIGroup: coordination.k8s.io
    Jan 28 01:12:54.974: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan 28 01:12:54.974: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan 28 01:12:54.975: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan 28 01:12:54.975: INFO: Checking APIGroup: node.k8s.io
    Jan 28 01:12:54.982: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan 28 01:12:54.982: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan 28 01:12:54.982: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan 28 01:12:54.982: INFO: Checking APIGroup: discovery.k8s.io
    Jan 28 01:12:54.988: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan 28 01:12:54.988: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan 28 01:12:54.988: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan 28 01:12:54.988: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan 28 01:12:54.995: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Jan 28 01:12:54.995: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Jan 28 01:12:54.995: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Jan 28 01:12:54.995: INFO: Checking APIGroup: crd.projectcalico.org
    Jan 28 01:12:55.001: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jan 28 01:12:55.001: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jan 28 01:12:55.001: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Jan 28 01:12:55.001: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Jan 28 01:12:55.006: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Jan 28 01:12:55.006: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Jan 28 01:12:55.007: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Jan 28 01:12:55.007: INFO: Checking APIGroup: ibm.com
    Jan 28 01:12:55.013: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
    Jan 28 01:12:55.014: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
    Jan 28 01:12:55.014: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
    Jan 28 01:12:55.014: INFO: Checking APIGroup: metrics.k8s.io
    Jan 28 01:12:55.020: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Jan 28 01:12:55.020: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Jan 28 01:12:55.021: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Jan 28 01:12:55.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-9920" for this suite. 01/28/23 01:12:55.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:12:55.065
Jan 28 01:12:55.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename container-runtime 01/28/23 01:12:55.067
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:12:55.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:12:55.156
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 01/28/23 01:12:55.17
STEP: wait for the container to reach Succeeded 01/28/23 01:12:55.189
STEP: get the container status 01/28/23 01:13:00.287
STEP: the container should be terminated 01/28/23 01:13:00.314
STEP: the termination message should be set 01/28/23 01:13:00.314
Jan 28 01:13:00.315: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/28/23 01:13:00.315
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 28 01:13:00.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1149" for this suite. 01/28/23 01:13:00.365
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":163,"skipped":2883,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.315 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:12:55.065
    Jan 28 01:12:55.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename container-runtime 01/28/23 01:12:55.067
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:12:55.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:12:55.156
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 01/28/23 01:12:55.17
    STEP: wait for the container to reach Succeeded 01/28/23 01:12:55.189
    STEP: get the container status 01/28/23 01:13:00.287
    STEP: the container should be terminated 01/28/23 01:13:00.314
    STEP: the termination message should be set 01/28/23 01:13:00.314
    Jan 28 01:13:00.315: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/28/23 01:13:00.315
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 28 01:13:00.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1149" for this suite. 01/28/23 01:13:00.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:13:00.385
Jan 28 01:13:00.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename emptydir-wrapper 01/28/23 01:13:00.388
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:13:00.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:13:00.504
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/28/23 01:13:00.524
STEP: Creating RC which spawns configmap-volume pods 01/28/23 01:13:01.632
Jan 28 01:13:01.663: INFO: Pod name wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5: Found 0 pods out of 5
Jan 28 01:13:06.686: INFO: Pod name wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/28/23 01:13:06.686
Jan 28 01:13:06.686: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-6mfrb" in namespace "emptydir-wrapper-4050" to be "running"
Jan 28 01:13:06.699: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-6mfrb": Phase="Running", Reason="", readiness=true. Elapsed: 12.258311ms
Jan 28 01:13:06.699: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-6mfrb" satisfied condition "running"
Jan 28 01:13:06.699: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-7sxp6" in namespace "emptydir-wrapper-4050" to be "running"
Jan 28 01:13:06.711: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-7sxp6": Phase="Running", Reason="", readiness=true. Elapsed: 12.606346ms
Jan 28 01:13:06.711: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-7sxp6" satisfied condition "running"
Jan 28 01:13:06.711: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-btrjd" in namespace "emptydir-wrapper-4050" to be "running"
Jan 28 01:13:06.755: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-btrjd": Phase="Running", Reason="", readiness=true. Elapsed: 43.081089ms
Jan 28 01:13:06.755: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-btrjd" satisfied condition "running"
Jan 28 01:13:06.755: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-l8tbq" in namespace "emptydir-wrapper-4050" to be "running"
Jan 28 01:13:06.768: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-l8tbq": Phase="Running", Reason="", readiness=true. Elapsed: 13.214956ms
Jan 28 01:13:06.768: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-l8tbq" satisfied condition "running"
Jan 28 01:13:06.768: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-s9qmm" in namespace "emptydir-wrapper-4050" to be "running"
Jan 28 01:13:06.781: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-s9qmm": Phase="Running", Reason="", readiness=true. Elapsed: 12.790612ms
Jan 28 01:13:06.781: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-s9qmm" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5 in namespace emptydir-wrapper-4050, will wait for the garbage collector to delete the pods 01/28/23 01:13:06.781
Jan 28 01:13:06.892: INFO: Deleting ReplicationController wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5 took: 45.444589ms
Jan 28 01:13:06.994: INFO: Terminating ReplicationController wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5 pods took: 102.337431ms
STEP: Creating RC which spawns configmap-volume pods 01/28/23 01:13:09.714
Jan 28 01:13:09.752: INFO: Pod name wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3: Found 1 pods out of 5
Jan 28 01:13:14.771: INFO: Pod name wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/28/23 01:13:14.771
Jan 28 01:13:14.772: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-29f48" in namespace "emptydir-wrapper-4050" to be "running"
Jan 28 01:13:14.784: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-29f48": Phase="Running", Reason="", readiness=true. Elapsed: 12.241321ms
Jan 28 01:13:14.784: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-29f48" satisfied condition "running"
Jan 28 01:13:14.784: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-5bnlv" in namespace "emptydir-wrapper-4050" to be "running"
Jan 28 01:13:14.796: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-5bnlv": Phase="Running", Reason="", readiness=true. Elapsed: 12.52225ms
Jan 28 01:13:14.797: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-5bnlv" satisfied condition "running"
Jan 28 01:13:14.797: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-ct2lv" in namespace "emptydir-wrapper-4050" to be "running"
Jan 28 01:13:14.809: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-ct2lv": Phase="Running", Reason="", readiness=true. Elapsed: 12.134568ms
Jan 28 01:13:14.809: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-ct2lv" satisfied condition "running"
Jan 28 01:13:14.809: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-gzxdx" in namespace "emptydir-wrapper-4050" to be "running"
Jan 28 01:13:14.820: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-gzxdx": Phase="Running", Reason="", readiness=true. Elapsed: 11.443225ms
Jan 28 01:13:14.820: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-gzxdx" satisfied condition "running"
Jan 28 01:13:14.820: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-vfrcb" in namespace "emptydir-wrapper-4050" to be "running"
Jan 28 01:13:14.832: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-vfrcb": Phase="Running", Reason="", readiness=true. Elapsed: 11.832678ms
Jan 28 01:13:14.832: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-vfrcb" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3 in namespace emptydir-wrapper-4050, will wait for the garbage collector to delete the pods 01/28/23 01:13:14.832
Jan 28 01:13:14.913: INFO: Deleting ReplicationController wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3 took: 18.13956ms
Jan 28 01:13:15.116: INFO: Terminating ReplicationController wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3 pods took: 202.922698ms
STEP: Creating RC which spawns configmap-volume pods 01/28/23 01:13:17.851
Jan 28 01:13:17.911: INFO: Pod name wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4: Found 1 pods out of 5
Jan 28 01:13:22.929: INFO: Pod name wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/28/23 01:13:22.929
Jan 28 01:13:22.930: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-56wrj" in namespace "emptydir-wrapper-4050" to be "running"
Jan 28 01:13:22.942: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-56wrj": Phase="Running", Reason="", readiness=true. Elapsed: 12.161852ms
Jan 28 01:13:22.942: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-56wrj" satisfied condition "running"
Jan 28 01:13:22.942: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-6fgnq" in namespace "emptydir-wrapper-4050" to be "running"
Jan 28 01:13:22.954: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-6fgnq": Phase="Running", Reason="", readiness=true. Elapsed: 11.945333ms
Jan 28 01:13:22.954: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-6fgnq" satisfied condition "running"
Jan 28 01:13:22.954: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-7bqgk" in namespace "emptydir-wrapper-4050" to be "running"
Jan 28 01:13:22.983: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-7bqgk": Phase="Running", Reason="", readiness=true. Elapsed: 29.187259ms
Jan 28 01:13:22.983: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-7bqgk" satisfied condition "running"
Jan 28 01:13:22.983: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-qvqmc" in namespace "emptydir-wrapper-4050" to be "running"
Jan 28 01:13:22.996: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-qvqmc": Phase="Running", Reason="", readiness=true. Elapsed: 12.549693ms
Jan 28 01:13:22.996: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-qvqmc" satisfied condition "running"
Jan 28 01:13:22.996: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-wlzp5" in namespace "emptydir-wrapper-4050" to be "running"
Jan 28 01:13:23.009: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-wlzp5": Phase="Running", Reason="", readiness=true. Elapsed: 12.645626ms
Jan 28 01:13:23.009: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-wlzp5" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4 in namespace emptydir-wrapper-4050, will wait for the garbage collector to delete the pods 01/28/23 01:13:23.009
Jan 28 01:13:23.090: INFO: Deleting ReplicationController wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4 took: 17.579716ms
Jan 28 01:13:23.291: INFO: Terminating ReplicationController wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4 pods took: 200.734763ms
STEP: Cleaning up the configMaps 01/28/23 01:13:25.991
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan 28 01:13:27.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4050" for this suite. 01/28/23 01:13:27.295
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":164,"skipped":2894,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.928 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:13:00.385
    Jan 28 01:13:00.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename emptydir-wrapper 01/28/23 01:13:00.388
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:13:00.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:13:00.504
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/28/23 01:13:00.524
    STEP: Creating RC which spawns configmap-volume pods 01/28/23 01:13:01.632
    Jan 28 01:13:01.663: INFO: Pod name wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5: Found 0 pods out of 5
    Jan 28 01:13:06.686: INFO: Pod name wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/28/23 01:13:06.686
    Jan 28 01:13:06.686: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-6mfrb" in namespace "emptydir-wrapper-4050" to be "running"
    Jan 28 01:13:06.699: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-6mfrb": Phase="Running", Reason="", readiness=true. Elapsed: 12.258311ms
    Jan 28 01:13:06.699: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-6mfrb" satisfied condition "running"
    Jan 28 01:13:06.699: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-7sxp6" in namespace "emptydir-wrapper-4050" to be "running"
    Jan 28 01:13:06.711: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-7sxp6": Phase="Running", Reason="", readiness=true. Elapsed: 12.606346ms
    Jan 28 01:13:06.711: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-7sxp6" satisfied condition "running"
    Jan 28 01:13:06.711: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-btrjd" in namespace "emptydir-wrapper-4050" to be "running"
    Jan 28 01:13:06.755: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-btrjd": Phase="Running", Reason="", readiness=true. Elapsed: 43.081089ms
    Jan 28 01:13:06.755: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-btrjd" satisfied condition "running"
    Jan 28 01:13:06.755: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-l8tbq" in namespace "emptydir-wrapper-4050" to be "running"
    Jan 28 01:13:06.768: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-l8tbq": Phase="Running", Reason="", readiness=true. Elapsed: 13.214956ms
    Jan 28 01:13:06.768: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-l8tbq" satisfied condition "running"
    Jan 28 01:13:06.768: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-s9qmm" in namespace "emptydir-wrapper-4050" to be "running"
    Jan 28 01:13:06.781: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-s9qmm": Phase="Running", Reason="", readiness=true. Elapsed: 12.790612ms
    Jan 28 01:13:06.781: INFO: Pod "wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5-s9qmm" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5 in namespace emptydir-wrapper-4050, will wait for the garbage collector to delete the pods 01/28/23 01:13:06.781
    Jan 28 01:13:06.892: INFO: Deleting ReplicationController wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5 took: 45.444589ms
    Jan 28 01:13:06.994: INFO: Terminating ReplicationController wrapped-volume-race-b6e03ccc-0201-4fbc-8d51-e2f77a1c48e5 pods took: 102.337431ms
    STEP: Creating RC which spawns configmap-volume pods 01/28/23 01:13:09.714
    Jan 28 01:13:09.752: INFO: Pod name wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3: Found 1 pods out of 5
    Jan 28 01:13:14.771: INFO: Pod name wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/28/23 01:13:14.771
    Jan 28 01:13:14.772: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-29f48" in namespace "emptydir-wrapper-4050" to be "running"
    Jan 28 01:13:14.784: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-29f48": Phase="Running", Reason="", readiness=true. Elapsed: 12.241321ms
    Jan 28 01:13:14.784: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-29f48" satisfied condition "running"
    Jan 28 01:13:14.784: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-5bnlv" in namespace "emptydir-wrapper-4050" to be "running"
    Jan 28 01:13:14.796: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-5bnlv": Phase="Running", Reason="", readiness=true. Elapsed: 12.52225ms
    Jan 28 01:13:14.797: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-5bnlv" satisfied condition "running"
    Jan 28 01:13:14.797: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-ct2lv" in namespace "emptydir-wrapper-4050" to be "running"
    Jan 28 01:13:14.809: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-ct2lv": Phase="Running", Reason="", readiness=true. Elapsed: 12.134568ms
    Jan 28 01:13:14.809: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-ct2lv" satisfied condition "running"
    Jan 28 01:13:14.809: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-gzxdx" in namespace "emptydir-wrapper-4050" to be "running"
    Jan 28 01:13:14.820: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-gzxdx": Phase="Running", Reason="", readiness=true. Elapsed: 11.443225ms
    Jan 28 01:13:14.820: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-gzxdx" satisfied condition "running"
    Jan 28 01:13:14.820: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-vfrcb" in namespace "emptydir-wrapper-4050" to be "running"
    Jan 28 01:13:14.832: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-vfrcb": Phase="Running", Reason="", readiness=true. Elapsed: 11.832678ms
    Jan 28 01:13:14.832: INFO: Pod "wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3-vfrcb" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3 in namespace emptydir-wrapper-4050, will wait for the garbage collector to delete the pods 01/28/23 01:13:14.832
    Jan 28 01:13:14.913: INFO: Deleting ReplicationController wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3 took: 18.13956ms
    Jan 28 01:13:15.116: INFO: Terminating ReplicationController wrapped-volume-race-03fb98b9-ddea-428c-b0b4-9bf279f095a3 pods took: 202.922698ms
    STEP: Creating RC which spawns configmap-volume pods 01/28/23 01:13:17.851
    Jan 28 01:13:17.911: INFO: Pod name wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4: Found 1 pods out of 5
    Jan 28 01:13:22.929: INFO: Pod name wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/28/23 01:13:22.929
    Jan 28 01:13:22.930: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-56wrj" in namespace "emptydir-wrapper-4050" to be "running"
    Jan 28 01:13:22.942: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-56wrj": Phase="Running", Reason="", readiness=true. Elapsed: 12.161852ms
    Jan 28 01:13:22.942: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-56wrj" satisfied condition "running"
    Jan 28 01:13:22.942: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-6fgnq" in namespace "emptydir-wrapper-4050" to be "running"
    Jan 28 01:13:22.954: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-6fgnq": Phase="Running", Reason="", readiness=true. Elapsed: 11.945333ms
    Jan 28 01:13:22.954: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-6fgnq" satisfied condition "running"
    Jan 28 01:13:22.954: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-7bqgk" in namespace "emptydir-wrapper-4050" to be "running"
    Jan 28 01:13:22.983: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-7bqgk": Phase="Running", Reason="", readiness=true. Elapsed: 29.187259ms
    Jan 28 01:13:22.983: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-7bqgk" satisfied condition "running"
    Jan 28 01:13:22.983: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-qvqmc" in namespace "emptydir-wrapper-4050" to be "running"
    Jan 28 01:13:22.996: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-qvqmc": Phase="Running", Reason="", readiness=true. Elapsed: 12.549693ms
    Jan 28 01:13:22.996: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-qvqmc" satisfied condition "running"
    Jan 28 01:13:22.996: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-wlzp5" in namespace "emptydir-wrapper-4050" to be "running"
    Jan 28 01:13:23.009: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-wlzp5": Phase="Running", Reason="", readiness=true. Elapsed: 12.645626ms
    Jan 28 01:13:23.009: INFO: Pod "wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4-wlzp5" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4 in namespace emptydir-wrapper-4050, will wait for the garbage collector to delete the pods 01/28/23 01:13:23.009
    Jan 28 01:13:23.090: INFO: Deleting ReplicationController wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4 took: 17.579716ms
    Jan 28 01:13:23.291: INFO: Terminating ReplicationController wrapped-volume-race-dbded687-2fe9-47a8-b024-f19551ce66f4 pods took: 200.734763ms
    STEP: Cleaning up the configMaps 01/28/23 01:13:25.991
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan 28 01:13:27.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-4050" for this suite. 01/28/23 01:13:27.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:13:27.32
Jan 28 01:13:27.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubectl 01/28/23 01:13:27.322
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:13:27.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:13:27.387
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 01/28/23 01:13:27.4
Jan 28 01:13:27.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-388 create -f -'
Jan 28 01:13:27.799: INFO: stderr: ""
Jan 28 01:13:27.799: INFO: stdout: "pod/pause created\n"
Jan 28 01:13:27.799: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 28 01:13:27.799: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-388" to be "running and ready"
Jan 28 01:13:27.810: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 11.674692ms
Jan 28 01:13:27.810: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.9.20.126' to be 'Running' but was 'Pending'
Jan 28 01:13:29.823: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024102066s
Jan 28 01:13:29.823: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.9.20.126' to be 'Running' but was 'Pending'
Jan 28 01:13:31.823: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.024391275s
Jan 28 01:13:31.823: INFO: Pod "pause" satisfied condition "running and ready"
Jan 28 01:13:31.823: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 01/28/23 01:13:31.823
Jan 28 01:13:31.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-388 label pods pause testing-label=testing-label-value'
Jan 28 01:13:31.956: INFO: stderr: ""
Jan 28 01:13:31.956: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/28/23 01:13:31.956
Jan 28 01:13:31.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-388 get pod pause -L testing-label'
Jan 28 01:13:32.068: INFO: stderr: ""
Jan 28 01:13:32.068: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/28/23 01:13:32.068
Jan 28 01:13:32.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-388 label pods pause testing-label-'
Jan 28 01:13:32.239: INFO: stderr: ""
Jan 28 01:13:32.239: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/28/23 01:13:32.239
Jan 28 01:13:32.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-388 get pod pause -L testing-label'
Jan 28 01:13:32.334: INFO: stderr: ""
Jan 28 01:13:32.334: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 01/28/23 01:13:32.334
Jan 28 01:13:32.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-388 delete --grace-period=0 --force -f -'
Jan 28 01:13:32.468: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 28 01:13:32.468: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 28 01:13:32.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-388 get rc,svc -l name=pause --no-headers'
Jan 28 01:13:32.593: INFO: stderr: "No resources found in kubectl-388 namespace.\n"
Jan 28 01:13:32.593: INFO: stdout: ""
Jan 28 01:13:32.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-388 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 28 01:13:32.693: INFO: stderr: ""
Jan 28 01:13:32.693: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 28 01:13:32.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-388" for this suite. 01/28/23 01:13:32.711
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":165,"skipped":2920,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.409 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:13:27.32
    Jan 28 01:13:27.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubectl 01/28/23 01:13:27.322
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:13:27.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:13:27.387
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 01/28/23 01:13:27.4
    Jan 28 01:13:27.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-388 create -f -'
    Jan 28 01:13:27.799: INFO: stderr: ""
    Jan 28 01:13:27.799: INFO: stdout: "pod/pause created\n"
    Jan 28 01:13:27.799: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan 28 01:13:27.799: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-388" to be "running and ready"
    Jan 28 01:13:27.810: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 11.674692ms
    Jan 28 01:13:27.810: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.9.20.126' to be 'Running' but was 'Pending'
    Jan 28 01:13:29.823: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024102066s
    Jan 28 01:13:29.823: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.9.20.126' to be 'Running' but was 'Pending'
    Jan 28 01:13:31.823: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.024391275s
    Jan 28 01:13:31.823: INFO: Pod "pause" satisfied condition "running and ready"
    Jan 28 01:13:31.823: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 01/28/23 01:13:31.823
    Jan 28 01:13:31.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-388 label pods pause testing-label=testing-label-value'
    Jan 28 01:13:31.956: INFO: stderr: ""
    Jan 28 01:13:31.956: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/28/23 01:13:31.956
    Jan 28 01:13:31.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-388 get pod pause -L testing-label'
    Jan 28 01:13:32.068: INFO: stderr: ""
    Jan 28 01:13:32.068: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/28/23 01:13:32.068
    Jan 28 01:13:32.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-388 label pods pause testing-label-'
    Jan 28 01:13:32.239: INFO: stderr: ""
    Jan 28 01:13:32.239: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/28/23 01:13:32.239
    Jan 28 01:13:32.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-388 get pod pause -L testing-label'
    Jan 28 01:13:32.334: INFO: stderr: ""
    Jan 28 01:13:32.334: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 01/28/23 01:13:32.334
    Jan 28 01:13:32.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-388 delete --grace-period=0 --force -f -'
    Jan 28 01:13:32.468: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 28 01:13:32.468: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan 28 01:13:32.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-388 get rc,svc -l name=pause --no-headers'
    Jan 28 01:13:32.593: INFO: stderr: "No resources found in kubectl-388 namespace.\n"
    Jan 28 01:13:32.593: INFO: stdout: ""
    Jan 28 01:13:32.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-388 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 28 01:13:32.693: INFO: stderr: ""
    Jan 28 01:13:32.693: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 28 01:13:32.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-388" for this suite. 01/28/23 01:13:32.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:13:32.73
Jan 28 01:13:32.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubectl 01/28/23 01:13:32.731
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:13:32.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:13:32.779
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 01/28/23 01:13:32.79
Jan 28 01:13:32.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3490 create -f -'
Jan 28 01:13:33.114: INFO: stderr: ""
Jan 28 01:13:33.114: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/28/23 01:13:33.114
Jan 28 01:13:33.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3490 diff -f -'
Jan 28 01:13:33.441: INFO: rc: 1
Jan 28 01:13:33.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3490 delete -f -'
Jan 28 01:13:33.560: INFO: stderr: ""
Jan 28 01:13:33.560: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 28 01:13:33.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3490" for this suite. 01/28/23 01:13:33.578
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":166,"skipped":2946,"failed":0}
------------------------------
â€¢ [0.869 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:13:32.73
    Jan 28 01:13:32.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubectl 01/28/23 01:13:32.731
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:13:32.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:13:32.779
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 01/28/23 01:13:32.79
    Jan 28 01:13:32.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3490 create -f -'
    Jan 28 01:13:33.114: INFO: stderr: ""
    Jan 28 01:13:33.114: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/28/23 01:13:33.114
    Jan 28 01:13:33.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3490 diff -f -'
    Jan 28 01:13:33.441: INFO: rc: 1
    Jan 28 01:13:33.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3490 delete -f -'
    Jan 28 01:13:33.560: INFO: stderr: ""
    Jan 28 01:13:33.560: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 28 01:13:33.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3490" for this suite. 01/28/23 01:13:33.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:13:33.604
Jan 28 01:13:33.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename services 01/28/23 01:13:33.606
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:13:33.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:13:33.673
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-176 01/28/23 01:13:33.685
STEP: creating service affinity-clusterip in namespace services-176 01/28/23 01:13:33.685
STEP: creating replication controller affinity-clusterip in namespace services-176 01/28/23 01:13:33.717
I0128 01:13:33.733138      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-176, replica count: 3
I0128 01:13:36.786328      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 01:13:36.820: INFO: Creating new exec pod
Jan 28 01:13:36.843: INFO: Waiting up to 5m0s for pod "execpod-affinityg6lgq" in namespace "services-176" to be "running"
Jan 28 01:13:36.855: INFO: Pod "execpod-affinityg6lgq": Phase="Pending", Reason="", readiness=false. Elapsed: 11.490515ms
Jan 28 01:13:38.867: INFO: Pod "execpod-affinityg6lgq": Phase="Running", Reason="", readiness=true. Elapsed: 2.02373695s
Jan 28 01:13:38.867: INFO: Pod "execpod-affinityg6lgq" satisfied condition "running"
Jan 28 01:13:39.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-176 exec execpod-affinityg6lgq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan 28 01:13:40.199: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 28 01:13:40.200: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:13:40.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-176 exec execpod-affinityg6lgq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.35.177 80'
Jan 28 01:13:40.549: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.35.177 80\nConnection to 172.21.35.177 80 port [tcp/http] succeeded!\n"
Jan 28 01:13:40.549: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:13:40.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-176 exec execpod-affinityg6lgq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.35.177:80/ ; done'
Jan 28 01:13:41.069: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n"
Jan 28 01:13:41.069: INFO: stdout: "\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js"
Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
Jan 28 01:13:41.069: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-176, will wait for the garbage collector to delete the pods 01/28/23 01:13:41.106
Jan 28 01:13:41.192: INFO: Deleting ReplicationController affinity-clusterip took: 24.401768ms
Jan 28 01:13:41.292: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.374499ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 28 01:13:43.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-176" for this suite. 01/28/23 01:13:43.766
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":167,"skipped":2975,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.181 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:13:33.604
    Jan 28 01:13:33.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename services 01/28/23 01:13:33.606
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:13:33.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:13:33.673
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-176 01/28/23 01:13:33.685
    STEP: creating service affinity-clusterip in namespace services-176 01/28/23 01:13:33.685
    STEP: creating replication controller affinity-clusterip in namespace services-176 01/28/23 01:13:33.717
    I0128 01:13:33.733138      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-176, replica count: 3
    I0128 01:13:36.786328      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 28 01:13:36.820: INFO: Creating new exec pod
    Jan 28 01:13:36.843: INFO: Waiting up to 5m0s for pod "execpod-affinityg6lgq" in namespace "services-176" to be "running"
    Jan 28 01:13:36.855: INFO: Pod "execpod-affinityg6lgq": Phase="Pending", Reason="", readiness=false. Elapsed: 11.490515ms
    Jan 28 01:13:38.867: INFO: Pod "execpod-affinityg6lgq": Phase="Running", Reason="", readiness=true. Elapsed: 2.02373695s
    Jan 28 01:13:38.867: INFO: Pod "execpod-affinityg6lgq" satisfied condition "running"
    Jan 28 01:13:39.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-176 exec execpod-affinityg6lgq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Jan 28 01:13:40.199: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan 28 01:13:40.200: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 01:13:40.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-176 exec execpod-affinityg6lgq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.35.177 80'
    Jan 28 01:13:40.549: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.35.177 80\nConnection to 172.21.35.177 80 port [tcp/http] succeeded!\n"
    Jan 28 01:13:40.549: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 01:13:40.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-176 exec execpod-affinityg6lgq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.35.177:80/ ; done'
    Jan 28 01:13:41.069: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.35.177:80/\n"
    Jan 28 01:13:41.069: INFO: stdout: "\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js\naffinity-clusterip-988js"
    Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
    Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
    Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
    Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
    Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
    Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
    Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
    Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
    Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
    Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
    Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
    Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
    Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
    Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
    Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
    Jan 28 01:13:41.069: INFO: Received response from host: affinity-clusterip-988js
    Jan 28 01:13:41.069: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-176, will wait for the garbage collector to delete the pods 01/28/23 01:13:41.106
    Jan 28 01:13:41.192: INFO: Deleting ReplicationController affinity-clusterip took: 24.401768ms
    Jan 28 01:13:41.292: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.374499ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 28 01:13:43.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-176" for this suite. 01/28/23 01:13:43.766
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:13:43.789
Jan 28 01:13:43.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename emptydir 01/28/23 01:13:43.79
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:13:43.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:13:43.845
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 01/28/23 01:13:43.856
Jan 28 01:13:43.878: INFO: Waiting up to 5m0s for pod "pod-57562921-4f90-4a1d-b06f-f654b1fa3b7f" in namespace "emptydir-1843" to be "Succeeded or Failed"
Jan 28 01:13:43.889: INFO: Pod "pod-57562921-4f90-4a1d-b06f-f654b1fa3b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.333839ms
Jan 28 01:13:45.901: INFO: Pod "pod-57562921-4f90-4a1d-b06f-f654b1fa3b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023209054s
Jan 28 01:13:47.902: INFO: Pod "pod-57562921-4f90-4a1d-b06f-f654b1fa3b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024316056s
Jan 28 01:13:49.914: INFO: Pod "pod-57562921-4f90-4a1d-b06f-f654b1fa3b7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036722604s
STEP: Saw pod success 01/28/23 01:13:49.914
Jan 28 01:13:49.915: INFO: Pod "pod-57562921-4f90-4a1d-b06f-f654b1fa3b7f" satisfied condition "Succeeded or Failed"
Jan 28 01:13:49.925: INFO: Trying to get logs from node 10.9.20.126 pod pod-57562921-4f90-4a1d-b06f-f654b1fa3b7f container test-container: <nil>
STEP: delete the pod 01/28/23 01:13:50.004
Jan 28 01:13:50.070: INFO: Waiting for pod pod-57562921-4f90-4a1d-b06f-f654b1fa3b7f to disappear
Jan 28 01:13:50.081: INFO: Pod pod-57562921-4f90-4a1d-b06f-f654b1fa3b7f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 28 01:13:50.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1843" for this suite. 01/28/23 01:13:50.098
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":168,"skipped":3000,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.325 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:13:43.789
    Jan 28 01:13:43.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename emptydir 01/28/23 01:13:43.79
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:13:43.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:13:43.845
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/28/23 01:13:43.856
    Jan 28 01:13:43.878: INFO: Waiting up to 5m0s for pod "pod-57562921-4f90-4a1d-b06f-f654b1fa3b7f" in namespace "emptydir-1843" to be "Succeeded or Failed"
    Jan 28 01:13:43.889: INFO: Pod "pod-57562921-4f90-4a1d-b06f-f654b1fa3b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.333839ms
    Jan 28 01:13:45.901: INFO: Pod "pod-57562921-4f90-4a1d-b06f-f654b1fa3b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023209054s
    Jan 28 01:13:47.902: INFO: Pod "pod-57562921-4f90-4a1d-b06f-f654b1fa3b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024316056s
    Jan 28 01:13:49.914: INFO: Pod "pod-57562921-4f90-4a1d-b06f-f654b1fa3b7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036722604s
    STEP: Saw pod success 01/28/23 01:13:49.914
    Jan 28 01:13:49.915: INFO: Pod "pod-57562921-4f90-4a1d-b06f-f654b1fa3b7f" satisfied condition "Succeeded or Failed"
    Jan 28 01:13:49.925: INFO: Trying to get logs from node 10.9.20.126 pod pod-57562921-4f90-4a1d-b06f-f654b1fa3b7f container test-container: <nil>
    STEP: delete the pod 01/28/23 01:13:50.004
    Jan 28 01:13:50.070: INFO: Waiting for pod pod-57562921-4f90-4a1d-b06f-f654b1fa3b7f to disappear
    Jan 28 01:13:50.081: INFO: Pod pod-57562921-4f90-4a1d-b06f-f654b1fa3b7f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 28 01:13:50.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1843" for this suite. 01/28/23 01:13:50.098
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:13:50.126
Jan 28 01:13:50.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubelet-test 01/28/23 01:13:50.128
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:13:50.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:13:50.174
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 28 01:13:50.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3268" for this suite. 01/28/23 01:13:50.239
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":169,"skipped":3047,"failed":0}
------------------------------
â€¢ [0.132 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:13:50.126
    Jan 28 01:13:50.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubelet-test 01/28/23 01:13:50.128
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:13:50.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:13:50.174
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 28 01:13:50.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-3268" for this suite. 01/28/23 01:13:50.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:13:50.26
Jan 28 01:13:50.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename webhook 01/28/23 01:13:50.263
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:13:50.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:13:50.316
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/28/23 01:13:50.365
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:13:50.694
STEP: Deploying the webhook pod 01/28/23 01:13:50.726
STEP: Wait for the deployment to be ready 01/28/23 01:13:50.816
Jan 28 01:13:50.841: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 01:13:52.885: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 13, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 13, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 13, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 13, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/28/23 01:13:54.9
STEP: Verifying the service has paired with the endpoint 01/28/23 01:13:54.932
Jan 28 01:13:55.933: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 01/28/23 01:13:56.095
STEP: Creating a configMap that should be mutated 01/28/23 01:13:56.207
STEP: Deleting the collection of validation webhooks 01/28/23 01:13:56.378
STEP: Creating a configMap that should not be mutated 01/28/23 01:13:56.541
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:13:56.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9362" for this suite. 01/28/23 01:13:56.623
STEP: Destroying namespace "webhook-9362-markers" for this suite. 01/28/23 01:13:56.642
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":170,"skipped":3054,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.508 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:13:50.26
    Jan 28 01:13:50.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename webhook 01/28/23 01:13:50.263
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:13:50.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:13:50.316
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/28/23 01:13:50.365
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:13:50.694
    STEP: Deploying the webhook pod 01/28/23 01:13:50.726
    STEP: Wait for the deployment to be ready 01/28/23 01:13:50.816
    Jan 28 01:13:50.841: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 28 01:13:52.885: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 13, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 13, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 13, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 13, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/28/23 01:13:54.9
    STEP: Verifying the service has paired with the endpoint 01/28/23 01:13:54.932
    Jan 28 01:13:55.933: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 01/28/23 01:13:56.095
    STEP: Creating a configMap that should be mutated 01/28/23 01:13:56.207
    STEP: Deleting the collection of validation webhooks 01/28/23 01:13:56.378
    STEP: Creating a configMap that should not be mutated 01/28/23 01:13:56.541
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:13:56.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9362" for this suite. 01/28/23 01:13:56.623
    STEP: Destroying namespace "webhook-9362-markers" for this suite. 01/28/23 01:13:56.642
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:13:56.768
Jan 28 01:13:56.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename services 01/28/23 01:13:56.772
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:13:56.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:13:56.822
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 28 01:13:56.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8711" for this suite. 01/28/23 01:13:56.859
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":171,"skipped":3054,"failed":0}
------------------------------
â€¢ [0.109 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:13:56.768
    Jan 28 01:13:56.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename services 01/28/23 01:13:56.772
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:13:56.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:13:56.822
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 28 01:13:56.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8711" for this suite. 01/28/23 01:13:56.859
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:13:56.882
Jan 28 01:13:56.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename statefulset 01/28/23 01:13:56.884
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:13:56.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:13:56.938
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9262 01/28/23 01:13:56.951
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-9262 01/28/23 01:13:56.999
Jan 28 01:13:57.027: INFO: Found 0 stateful pods, waiting for 1
Jan 28 01:14:07.040: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/28/23 01:14:07.064
STEP: Getting /status 01/28/23 01:14:07.084
Jan 28 01:14:07.096: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/28/23 01:14:07.096
Jan 28 01:14:07.123: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/28/23 01:14:07.123
Jan 28 01:14:07.130: INFO: Observed &StatefulSet event: ADDED
Jan 28 01:14:07.130: INFO: Found Statefulset ss in namespace statefulset-9262 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 28 01:14:07.130: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/28/23 01:14:07.13
Jan 28 01:14:07.131: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 28 01:14:07.149: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/28/23 01:14:07.149
Jan 28 01:14:07.156: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 28 01:14:07.156: INFO: Deleting all statefulset in ns statefulset-9262
Jan 28 01:14:07.168: INFO: Scaling statefulset ss to 0
Jan 28 01:14:17.225: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 01:14:17.236: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 28 01:14:17.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9262" for this suite. 01/28/23 01:14:17.294
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":172,"skipped":3068,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.432 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:13:56.882
    Jan 28 01:13:56.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename statefulset 01/28/23 01:13:56.884
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:13:56.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:13:56.938
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9262 01/28/23 01:13:56.951
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-9262 01/28/23 01:13:56.999
    Jan 28 01:13:57.027: INFO: Found 0 stateful pods, waiting for 1
    Jan 28 01:14:07.040: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/28/23 01:14:07.064
    STEP: Getting /status 01/28/23 01:14:07.084
    Jan 28 01:14:07.096: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/28/23 01:14:07.096
    Jan 28 01:14:07.123: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/28/23 01:14:07.123
    Jan 28 01:14:07.130: INFO: Observed &StatefulSet event: ADDED
    Jan 28 01:14:07.130: INFO: Found Statefulset ss in namespace statefulset-9262 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 28 01:14:07.130: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/28/23 01:14:07.13
    Jan 28 01:14:07.131: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 28 01:14:07.149: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/28/23 01:14:07.149
    Jan 28 01:14:07.156: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 28 01:14:07.156: INFO: Deleting all statefulset in ns statefulset-9262
    Jan 28 01:14:07.168: INFO: Scaling statefulset ss to 0
    Jan 28 01:14:17.225: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 28 01:14:17.236: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 28 01:14:17.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9262" for this suite. 01/28/23 01:14:17.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:14:17.335
Jan 28 01:14:17.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename security-context-test 01/28/23 01:14:17.336
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:14:17.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:14:17.39
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Jan 28 01:14:17.427: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188" in namespace "security-context-test-7585" to be "Succeeded or Failed"
Jan 28 01:14:17.439: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188": Phase="Pending", Reason="", readiness=false. Elapsed: 11.55642ms
Jan 28 01:14:19.453: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025479837s
Jan 28 01:14:21.452: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024777734s
Jan 28 01:14:23.452: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024346977s
Jan 28 01:14:25.452: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024525003s
Jan 28 01:14:27.455: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027763731s
Jan 28 01:14:29.451: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02373885s
Jan 28 01:14:31.452: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188": Phase="Pending", Reason="", readiness=false. Elapsed: 14.024632466s
Jan 28 01:14:33.451: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.023761558s
Jan 28 01:14:33.451: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 28 01:14:33.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7585" for this suite. 01/28/23 01:14:33.54
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":173,"skipped":3128,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.225 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:14:17.335
    Jan 28 01:14:17.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename security-context-test 01/28/23 01:14:17.336
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:14:17.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:14:17.39
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Jan 28 01:14:17.427: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188" in namespace "security-context-test-7585" to be "Succeeded or Failed"
    Jan 28 01:14:17.439: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188": Phase="Pending", Reason="", readiness=false. Elapsed: 11.55642ms
    Jan 28 01:14:19.453: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025479837s
    Jan 28 01:14:21.452: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024777734s
    Jan 28 01:14:23.452: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024346977s
    Jan 28 01:14:25.452: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024525003s
    Jan 28 01:14:27.455: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027763731s
    Jan 28 01:14:29.451: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02373885s
    Jan 28 01:14:31.452: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188": Phase="Pending", Reason="", readiness=false. Elapsed: 14.024632466s
    Jan 28 01:14:33.451: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.023761558s
    Jan 28 01:14:33.451: INFO: Pod "alpine-nnp-false-0afc43f4-5911-4b2c-a56c-4ed0bd133188" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 28 01:14:33.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-7585" for this suite. 01/28/23 01:14:33.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:14:33.566
Jan 28 01:14:33.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename var-expansion 01/28/23 01:14:33.569
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:14:33.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:14:33.649
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 01/28/23 01:14:33.66
Jan 28 01:14:33.684: INFO: Waiting up to 5m0s for pod "var-expansion-40338617-a60f-4add-a1b4-839fdd5c1e39" in namespace "var-expansion-6729" to be "Succeeded or Failed"
Jan 28 01:14:33.696: INFO: Pod "var-expansion-40338617-a60f-4add-a1b4-839fdd5c1e39": Phase="Pending", Reason="", readiness=false. Elapsed: 11.91757ms
Jan 28 01:14:35.707: INFO: Pod "var-expansion-40338617-a60f-4add-a1b4-839fdd5c1e39": Phase="Running", Reason="", readiness=true. Elapsed: 2.023362981s
Jan 28 01:14:37.709: INFO: Pod "var-expansion-40338617-a60f-4add-a1b4-839fdd5c1e39": Phase="Running", Reason="", readiness=false. Elapsed: 4.025522514s
Jan 28 01:14:39.709: INFO: Pod "var-expansion-40338617-a60f-4add-a1b4-839fdd5c1e39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023772446s
STEP: Saw pod success 01/28/23 01:14:39.709
Jan 28 01:14:39.709: INFO: Pod "var-expansion-40338617-a60f-4add-a1b4-839fdd5c1e39" satisfied condition "Succeeded or Failed"
Jan 28 01:14:39.720: INFO: Trying to get logs from node 10.9.20.126 pod var-expansion-40338617-a60f-4add-a1b4-839fdd5c1e39 container dapi-container: <nil>
STEP: delete the pod 01/28/23 01:14:39.756
Jan 28 01:14:39.789: INFO: Waiting for pod var-expansion-40338617-a60f-4add-a1b4-839fdd5c1e39 to disappear
Jan 28 01:14:39.800: INFO: Pod var-expansion-40338617-a60f-4add-a1b4-839fdd5c1e39 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 28 01:14:39.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6729" for this suite. 01/28/23 01:14:39.817
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":174,"skipped":3163,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.269 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:14:33.566
    Jan 28 01:14:33.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename var-expansion 01/28/23 01:14:33.569
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:14:33.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:14:33.649
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 01/28/23 01:14:33.66
    Jan 28 01:14:33.684: INFO: Waiting up to 5m0s for pod "var-expansion-40338617-a60f-4add-a1b4-839fdd5c1e39" in namespace "var-expansion-6729" to be "Succeeded or Failed"
    Jan 28 01:14:33.696: INFO: Pod "var-expansion-40338617-a60f-4add-a1b4-839fdd5c1e39": Phase="Pending", Reason="", readiness=false. Elapsed: 11.91757ms
    Jan 28 01:14:35.707: INFO: Pod "var-expansion-40338617-a60f-4add-a1b4-839fdd5c1e39": Phase="Running", Reason="", readiness=true. Elapsed: 2.023362981s
    Jan 28 01:14:37.709: INFO: Pod "var-expansion-40338617-a60f-4add-a1b4-839fdd5c1e39": Phase="Running", Reason="", readiness=false. Elapsed: 4.025522514s
    Jan 28 01:14:39.709: INFO: Pod "var-expansion-40338617-a60f-4add-a1b4-839fdd5c1e39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023772446s
    STEP: Saw pod success 01/28/23 01:14:39.709
    Jan 28 01:14:39.709: INFO: Pod "var-expansion-40338617-a60f-4add-a1b4-839fdd5c1e39" satisfied condition "Succeeded or Failed"
    Jan 28 01:14:39.720: INFO: Trying to get logs from node 10.9.20.126 pod var-expansion-40338617-a60f-4add-a1b4-839fdd5c1e39 container dapi-container: <nil>
    STEP: delete the pod 01/28/23 01:14:39.756
    Jan 28 01:14:39.789: INFO: Waiting for pod var-expansion-40338617-a60f-4add-a1b4-839fdd5c1e39 to disappear
    Jan 28 01:14:39.800: INFO: Pod var-expansion-40338617-a60f-4add-a1b4-839fdd5c1e39 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 28 01:14:39.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6729" for this suite. 01/28/23 01:14:39.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:14:39.844
Jan 28 01:14:39.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename services 01/28/23 01:14:39.846
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:14:39.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:14:39.897
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6501 01/28/23 01:14:39.908
STEP: changing the ExternalName service to type=NodePort 01/28/23 01:14:39.923
STEP: creating replication controller externalname-service in namespace services-6501 01/28/23 01:14:39.977
I0128 01:14:39.994692      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6501, replica count: 2
I0128 01:14:43.045855      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 01:14:43.046: INFO: Creating new exec pod
Jan 28 01:14:43.074: INFO: Waiting up to 5m0s for pod "execpodmwwq5" in namespace "services-6501" to be "running"
Jan 28 01:14:43.115: INFO: Pod "execpodmwwq5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010049ms
Jan 28 01:14:45.128: INFO: Pod "execpodmwwq5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05384825s
Jan 28 01:14:47.127: INFO: Pod "execpodmwwq5": Phase="Running", Reason="", readiness=true. Elapsed: 4.052742855s
Jan 28 01:14:47.127: INFO: Pod "execpodmwwq5" satisfied condition "running"
Jan 28 01:14:48.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6501 exec execpodmwwq5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 28 01:14:48.449: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 28 01:14:48.449: INFO: stdout: "externalname-service-gfl2x"
Jan 28 01:14:48.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6501 exec execpodmwwq5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.196.240 80'
Jan 28 01:14:48.782: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.196.240 80\nConnection to 172.21.196.240 80 port [tcp/http] succeeded!\n"
Jan 28 01:14:48.782: INFO: stdout: "externalname-service-hr4kp"
Jan 28 01:14:48.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6501 exec execpodmwwq5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.75 31578'
Jan 28 01:14:49.142: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.9.20.75 31578\nConnection to 10.9.20.75 31578 port [tcp/*] succeeded!\n"
Jan 28 01:14:49.142: INFO: stdout: "externalname-service-hr4kp"
Jan 28 01:14:49.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6501 exec execpodmwwq5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.72 31578'
Jan 28 01:14:49.447: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.9.20.72 31578\nConnection to 10.9.20.72 31578 port [tcp/*] succeeded!\n"
Jan 28 01:14:49.447: INFO: stdout: "externalname-service-hr4kp"
Jan 28 01:14:49.447: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 28 01:14:49.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6501" for this suite. 01/28/23 01:14:49.526
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":175,"skipped":3195,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.699 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:14:39.844
    Jan 28 01:14:39.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename services 01/28/23 01:14:39.846
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:14:39.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:14:39.897
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6501 01/28/23 01:14:39.908
    STEP: changing the ExternalName service to type=NodePort 01/28/23 01:14:39.923
    STEP: creating replication controller externalname-service in namespace services-6501 01/28/23 01:14:39.977
    I0128 01:14:39.994692      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6501, replica count: 2
    I0128 01:14:43.045855      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 28 01:14:43.046: INFO: Creating new exec pod
    Jan 28 01:14:43.074: INFO: Waiting up to 5m0s for pod "execpodmwwq5" in namespace "services-6501" to be "running"
    Jan 28 01:14:43.115: INFO: Pod "execpodmwwq5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010049ms
    Jan 28 01:14:45.128: INFO: Pod "execpodmwwq5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05384825s
    Jan 28 01:14:47.127: INFO: Pod "execpodmwwq5": Phase="Running", Reason="", readiness=true. Elapsed: 4.052742855s
    Jan 28 01:14:47.127: INFO: Pod "execpodmwwq5" satisfied condition "running"
    Jan 28 01:14:48.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6501 exec execpodmwwq5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 28 01:14:48.449: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 28 01:14:48.449: INFO: stdout: "externalname-service-gfl2x"
    Jan 28 01:14:48.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6501 exec execpodmwwq5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.196.240 80'
    Jan 28 01:14:48.782: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.196.240 80\nConnection to 172.21.196.240 80 port [tcp/http] succeeded!\n"
    Jan 28 01:14:48.782: INFO: stdout: "externalname-service-hr4kp"
    Jan 28 01:14:48.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6501 exec execpodmwwq5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.75 31578'
    Jan 28 01:14:49.142: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.9.20.75 31578\nConnection to 10.9.20.75 31578 port [tcp/*] succeeded!\n"
    Jan 28 01:14:49.142: INFO: stdout: "externalname-service-hr4kp"
    Jan 28 01:14:49.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6501 exec execpodmwwq5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.72 31578'
    Jan 28 01:14:49.447: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.9.20.72 31578\nConnection to 10.9.20.72 31578 port [tcp/*] succeeded!\n"
    Jan 28 01:14:49.447: INFO: stdout: "externalname-service-hr4kp"
    Jan 28 01:14:49.447: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 28 01:14:49.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6501" for this suite. 01/28/23 01:14:49.526
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:14:49.544
Jan 28 01:14:49.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename gc 01/28/23 01:14:49.547
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:14:49.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:14:49.599
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/28/23 01:14:49.61
STEP: delete the rc 01/28/23 01:14:54.637
STEP: wait for all pods to be garbage collected 01/28/23 01:14:54.656
STEP: Gathering metrics 01/28/23 01:14:59.68
W0128 01:14:59.711890      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 28 01:14:59.712: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 28 01:14:59.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-793" for this suite. 01/28/23 01:14:59.725
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":176,"skipped":3197,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.200 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:14:49.544
    Jan 28 01:14:49.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename gc 01/28/23 01:14:49.547
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:14:49.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:14:49.599
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/28/23 01:14:49.61
    STEP: delete the rc 01/28/23 01:14:54.637
    STEP: wait for all pods to be garbage collected 01/28/23 01:14:54.656
    STEP: Gathering metrics 01/28/23 01:14:59.68
    W0128 01:14:59.711890      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 28 01:14:59.712: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 28 01:14:59.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-793" for this suite. 01/28/23 01:14:59.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:14:59.745
Jan 28 01:14:59.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename custom-resource-definition 01/28/23 01:14:59.746
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:14:59.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:14:59.814
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan 28 01:14:59.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:15:06.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5047" for this suite. 01/28/23 01:15:06.693
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":177,"skipped":3219,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.970 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:14:59.745
    Jan 28 01:14:59.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename custom-resource-definition 01/28/23 01:14:59.746
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:14:59.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:14:59.814
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan 28 01:14:59.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:15:06.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-5047" for this suite. 01/28/23 01:15:06.693
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:15:06.716
Jan 28 01:15:06.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename downward-api 01/28/23 01:15:06.718
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:15:06.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:15:06.81
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 01/28/23 01:15:06.822
Jan 28 01:15:06.845: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2990b8f6-2878-4db3-9310-f4201e037777" in namespace "downward-api-6769" to be "Succeeded or Failed"
Jan 28 01:15:06.859: INFO: Pod "downwardapi-volume-2990b8f6-2878-4db3-9310-f4201e037777": Phase="Pending", Reason="", readiness=false. Elapsed: 13.931567ms
Jan 28 01:15:08.872: INFO: Pod "downwardapi-volume-2990b8f6-2878-4db3-9310-f4201e037777": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026431306s
Jan 28 01:15:10.897: INFO: Pod "downwardapi-volume-2990b8f6-2878-4db3-9310-f4201e037777": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05140206s
Jan 28 01:15:12.873: INFO: Pod "downwardapi-volume-2990b8f6-2878-4db3-9310-f4201e037777": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027622078s
STEP: Saw pod success 01/28/23 01:15:12.873
Jan 28 01:15:12.873: INFO: Pod "downwardapi-volume-2990b8f6-2878-4db3-9310-f4201e037777" satisfied condition "Succeeded or Failed"
Jan 28 01:15:12.884: INFO: Trying to get logs from node 10.9.20.72 pod downwardapi-volume-2990b8f6-2878-4db3-9310-f4201e037777 container client-container: <nil>
STEP: delete the pod 01/28/23 01:15:12.911
Jan 28 01:15:12.947: INFO: Waiting for pod downwardapi-volume-2990b8f6-2878-4db3-9310-f4201e037777 to disappear
Jan 28 01:15:12.960: INFO: Pod downwardapi-volume-2990b8f6-2878-4db3-9310-f4201e037777 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 28 01:15:12.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6769" for this suite. 01/28/23 01:15:12.976
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":178,"skipped":3223,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.283 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:15:06.716
    Jan 28 01:15:06.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename downward-api 01/28/23 01:15:06.718
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:15:06.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:15:06.81
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 01/28/23 01:15:06.822
    Jan 28 01:15:06.845: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2990b8f6-2878-4db3-9310-f4201e037777" in namespace "downward-api-6769" to be "Succeeded or Failed"
    Jan 28 01:15:06.859: INFO: Pod "downwardapi-volume-2990b8f6-2878-4db3-9310-f4201e037777": Phase="Pending", Reason="", readiness=false. Elapsed: 13.931567ms
    Jan 28 01:15:08.872: INFO: Pod "downwardapi-volume-2990b8f6-2878-4db3-9310-f4201e037777": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026431306s
    Jan 28 01:15:10.897: INFO: Pod "downwardapi-volume-2990b8f6-2878-4db3-9310-f4201e037777": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05140206s
    Jan 28 01:15:12.873: INFO: Pod "downwardapi-volume-2990b8f6-2878-4db3-9310-f4201e037777": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027622078s
    STEP: Saw pod success 01/28/23 01:15:12.873
    Jan 28 01:15:12.873: INFO: Pod "downwardapi-volume-2990b8f6-2878-4db3-9310-f4201e037777" satisfied condition "Succeeded or Failed"
    Jan 28 01:15:12.884: INFO: Trying to get logs from node 10.9.20.72 pod downwardapi-volume-2990b8f6-2878-4db3-9310-f4201e037777 container client-container: <nil>
    STEP: delete the pod 01/28/23 01:15:12.911
    Jan 28 01:15:12.947: INFO: Waiting for pod downwardapi-volume-2990b8f6-2878-4db3-9310-f4201e037777 to disappear
    Jan 28 01:15:12.960: INFO: Pod downwardapi-volume-2990b8f6-2878-4db3-9310-f4201e037777 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 28 01:15:12.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6769" for this suite. 01/28/23 01:15:12.976
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:15:13.001
Jan 28 01:15:13.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename secrets 01/28/23 01:15:13.003
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:15:13.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:15:13.057
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-5e5bedb4-f8d5-4e76-ac01-049b8e41f3f5 01/28/23 01:15:13.068
STEP: Creating a pod to test consume secrets 01/28/23 01:15:13.084
Jan 28 01:15:13.107: INFO: Waiting up to 5m0s for pod "pod-secrets-627aa628-f479-45f9-94bd-fb06c2fd7c9e" in namespace "secrets-5753" to be "Succeeded or Failed"
Jan 28 01:15:13.118: INFO: Pod "pod-secrets-627aa628-f479-45f9-94bd-fb06c2fd7c9e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.16343ms
Jan 28 01:15:15.132: INFO: Pod "pod-secrets-627aa628-f479-45f9-94bd-fb06c2fd7c9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024375626s
Jan 28 01:15:17.130: INFO: Pod "pod-secrets-627aa628-f479-45f9-94bd-fb06c2fd7c9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023039776s
Jan 28 01:15:19.132: INFO: Pod "pod-secrets-627aa628-f479-45f9-94bd-fb06c2fd7c9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024621803s
STEP: Saw pod success 01/28/23 01:15:19.132
Jan 28 01:15:19.132: INFO: Pod "pod-secrets-627aa628-f479-45f9-94bd-fb06c2fd7c9e" satisfied condition "Succeeded or Failed"
Jan 28 01:15:19.145: INFO: Trying to get logs from node 10.9.20.126 pod pod-secrets-627aa628-f479-45f9-94bd-fb06c2fd7c9e container secret-env-test: <nil>
STEP: delete the pod 01/28/23 01:15:19.176
Jan 28 01:15:19.240: INFO: Waiting for pod pod-secrets-627aa628-f479-45f9-94bd-fb06c2fd7c9e to disappear
Jan 28 01:15:19.252: INFO: Pod pod-secrets-627aa628-f479-45f9-94bd-fb06c2fd7c9e no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 28 01:15:19.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5753" for this suite. 01/28/23 01:15:19.268
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":179,"skipped":3225,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.284 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:15:13.001
    Jan 28 01:15:13.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename secrets 01/28/23 01:15:13.003
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:15:13.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:15:13.057
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-5e5bedb4-f8d5-4e76-ac01-049b8e41f3f5 01/28/23 01:15:13.068
    STEP: Creating a pod to test consume secrets 01/28/23 01:15:13.084
    Jan 28 01:15:13.107: INFO: Waiting up to 5m0s for pod "pod-secrets-627aa628-f479-45f9-94bd-fb06c2fd7c9e" in namespace "secrets-5753" to be "Succeeded or Failed"
    Jan 28 01:15:13.118: INFO: Pod "pod-secrets-627aa628-f479-45f9-94bd-fb06c2fd7c9e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.16343ms
    Jan 28 01:15:15.132: INFO: Pod "pod-secrets-627aa628-f479-45f9-94bd-fb06c2fd7c9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024375626s
    Jan 28 01:15:17.130: INFO: Pod "pod-secrets-627aa628-f479-45f9-94bd-fb06c2fd7c9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023039776s
    Jan 28 01:15:19.132: INFO: Pod "pod-secrets-627aa628-f479-45f9-94bd-fb06c2fd7c9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024621803s
    STEP: Saw pod success 01/28/23 01:15:19.132
    Jan 28 01:15:19.132: INFO: Pod "pod-secrets-627aa628-f479-45f9-94bd-fb06c2fd7c9e" satisfied condition "Succeeded or Failed"
    Jan 28 01:15:19.145: INFO: Trying to get logs from node 10.9.20.126 pod pod-secrets-627aa628-f479-45f9-94bd-fb06c2fd7c9e container secret-env-test: <nil>
    STEP: delete the pod 01/28/23 01:15:19.176
    Jan 28 01:15:19.240: INFO: Waiting for pod pod-secrets-627aa628-f479-45f9-94bd-fb06c2fd7c9e to disappear
    Jan 28 01:15:19.252: INFO: Pod pod-secrets-627aa628-f479-45f9-94bd-fb06c2fd7c9e no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 28 01:15:19.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5753" for this suite. 01/28/23 01:15:19.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:15:19.302
Jan 28 01:15:19.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename emptydir 01/28/23 01:15:19.304
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:15:19.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:15:19.356
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 01/28/23 01:15:19.368
Jan 28 01:15:19.412: INFO: Waiting up to 5m0s for pod "pod-2b5f2f38-d1de-469e-89e2-5e0860f8f4b4" in namespace "emptydir-5921" to be "Succeeded or Failed"
Jan 28 01:15:19.422: INFO: Pod "pod-2b5f2f38-d1de-469e-89e2-5e0860f8f4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.412719ms
Jan 28 01:15:21.435: INFO: Pod "pod-2b5f2f38-d1de-469e-89e2-5e0860f8f4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023520501s
Jan 28 01:15:23.435: INFO: Pod "pod-2b5f2f38-d1de-469e-89e2-5e0860f8f4b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023266099s
STEP: Saw pod success 01/28/23 01:15:23.435
Jan 28 01:15:23.435: INFO: Pod "pod-2b5f2f38-d1de-469e-89e2-5e0860f8f4b4" satisfied condition "Succeeded or Failed"
Jan 28 01:15:23.463: INFO: Trying to get logs from node 10.9.20.72 pod pod-2b5f2f38-d1de-469e-89e2-5e0860f8f4b4 container test-container: <nil>
STEP: delete the pod 01/28/23 01:15:23.501
Jan 28 01:15:23.558: INFO: Waiting for pod pod-2b5f2f38-d1de-469e-89e2-5e0860f8f4b4 to disappear
Jan 28 01:15:23.569: INFO: Pod pod-2b5f2f38-d1de-469e-89e2-5e0860f8f4b4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 28 01:15:23.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5921" for this suite. 01/28/23 01:15:23.585
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":180,"skipped":3254,"failed":0}
------------------------------
â€¢ [4.300 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:15:19.302
    Jan 28 01:15:19.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename emptydir 01/28/23 01:15:19.304
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:15:19.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:15:19.356
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/28/23 01:15:19.368
    Jan 28 01:15:19.412: INFO: Waiting up to 5m0s for pod "pod-2b5f2f38-d1de-469e-89e2-5e0860f8f4b4" in namespace "emptydir-5921" to be "Succeeded or Failed"
    Jan 28 01:15:19.422: INFO: Pod "pod-2b5f2f38-d1de-469e-89e2-5e0860f8f4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.412719ms
    Jan 28 01:15:21.435: INFO: Pod "pod-2b5f2f38-d1de-469e-89e2-5e0860f8f4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023520501s
    Jan 28 01:15:23.435: INFO: Pod "pod-2b5f2f38-d1de-469e-89e2-5e0860f8f4b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023266099s
    STEP: Saw pod success 01/28/23 01:15:23.435
    Jan 28 01:15:23.435: INFO: Pod "pod-2b5f2f38-d1de-469e-89e2-5e0860f8f4b4" satisfied condition "Succeeded or Failed"
    Jan 28 01:15:23.463: INFO: Trying to get logs from node 10.9.20.72 pod pod-2b5f2f38-d1de-469e-89e2-5e0860f8f4b4 container test-container: <nil>
    STEP: delete the pod 01/28/23 01:15:23.501
    Jan 28 01:15:23.558: INFO: Waiting for pod pod-2b5f2f38-d1de-469e-89e2-5e0860f8f4b4 to disappear
    Jan 28 01:15:23.569: INFO: Pod pod-2b5f2f38-d1de-469e-89e2-5e0860f8f4b4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 28 01:15:23.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5921" for this suite. 01/28/23 01:15:23.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:15:23.609
Jan 28 01:15:23.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename deployment 01/28/23 01:15:23.611
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:15:23.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:15:23.669
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan 28 01:15:23.681: INFO: Creating deployment "test-recreate-deployment"
Jan 28 01:15:23.696: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 28 01:15:23.747: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 28 01:15:25.772: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 28 01:15:25.785: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 15, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 15, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 15, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 15, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d8b6f647f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:15:27.797: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 28 01:15:27.825: INFO: Updating deployment test-recreate-deployment
Jan 28 01:15:27.825: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 28 01:15:27.973: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4546  da97968a-6af8-4825-af2c-5ff5a51cafc7 32230 2 2023-01-28 01:15:23 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-28 01:15:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:15:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00388f3d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-28 01:15:27 +0000 UTC,LastTransitionTime:2023-01-28 01:15:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-28 01:15:27 +0000 UTC,LastTransitionTime:2023-01-28 01:15:23 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 28 01:15:27.986: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-4546  012c7542-c9ef-4348-a925-81ef5f7e6b52 32226 1 2023-01-28 01:15:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment da97968a-6af8-4825-af2c-5ff5a51cafc7 0xc00388f890 0xc00388f891}] [] [{kube-controller-manager Update apps/v1 2023-01-28 01:15:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da97968a-6af8-4825-af2c-5ff5a51cafc7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:15:27 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00388f928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 28 01:15:27.986: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 28 01:15:27.987: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-4546  e7f85f06-5def-42e3-a6b7-67cb0b491241 32218 2 2023-01-28 01:15:23 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment da97968a-6af8-4825-af2c-5ff5a51cafc7 0xc00388f777 0xc00388f778}] [] [{kube-controller-manager Update apps/v1 2023-01-28 01:15:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da97968a-6af8-4825-af2c-5ff5a51cafc7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:15:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00388f828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 28 01:15:27.999: INFO: Pod "test-recreate-deployment-9d58999df-ktngn" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-ktngn test-recreate-deployment-9d58999df- deployment-4546  137e1aca-3b4f-4b8b-aec3-32adaeda8854 32227 0 2023-01-28 01:15:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 012c7542-c9ef-4348-a925-81ef5f7e6b52 0xc004064410 0xc004064411}] [] [{kube-controller-manager Update v1 2023-01-28 01:15:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"012c7542-c9ef-4348-a925-81ef5f7e6b52\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:15:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xwncw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xwncw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:15:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:15:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:15:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:15:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:,StartTime:2023-01-28 01:15:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 28 01:15:27.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4546" for this suite. 01/28/23 01:15:28.014
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":181,"skipped":3282,"failed":0}
------------------------------
â€¢ [4.424 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:15:23.609
    Jan 28 01:15:23.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename deployment 01/28/23 01:15:23.611
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:15:23.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:15:23.669
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan 28 01:15:23.681: INFO: Creating deployment "test-recreate-deployment"
    Jan 28 01:15:23.696: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan 28 01:15:23.747: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jan 28 01:15:25.772: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan 28 01:15:25.785: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 15, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 15, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 15, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 15, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d8b6f647f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:15:27.797: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan 28 01:15:27.825: INFO: Updating deployment test-recreate-deployment
    Jan 28 01:15:27.825: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 28 01:15:27.973: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-4546  da97968a-6af8-4825-af2c-5ff5a51cafc7 32230 2 2023-01-28 01:15:23 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-28 01:15:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:15:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00388f3d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-28 01:15:27 +0000 UTC,LastTransitionTime:2023-01-28 01:15:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-28 01:15:27 +0000 UTC,LastTransitionTime:2023-01-28 01:15:23 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 28 01:15:27.986: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-4546  012c7542-c9ef-4348-a925-81ef5f7e6b52 32226 1 2023-01-28 01:15:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment da97968a-6af8-4825-af2c-5ff5a51cafc7 0xc00388f890 0xc00388f891}] [] [{kube-controller-manager Update apps/v1 2023-01-28 01:15:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da97968a-6af8-4825-af2c-5ff5a51cafc7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:15:27 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00388f928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 28 01:15:27.986: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan 28 01:15:27.987: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-4546  e7f85f06-5def-42e3-a6b7-67cb0b491241 32218 2 2023-01-28 01:15:23 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment da97968a-6af8-4825-af2c-5ff5a51cafc7 0xc00388f777 0xc00388f778}] [] [{kube-controller-manager Update apps/v1 2023-01-28 01:15:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da97968a-6af8-4825-af2c-5ff5a51cafc7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:15:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00388f828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 28 01:15:27.999: INFO: Pod "test-recreate-deployment-9d58999df-ktngn" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-ktngn test-recreate-deployment-9d58999df- deployment-4546  137e1aca-3b4f-4b8b-aec3-32adaeda8854 32227 0 2023-01-28 01:15:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 012c7542-c9ef-4348-a925-81ef5f7e6b52 0xc004064410 0xc004064411}] [] [{kube-controller-manager Update v1 2023-01-28 01:15:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"012c7542-c9ef-4348-a925-81ef5f7e6b52\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:15:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xwncw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xwncw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:15:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:15:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:15:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:15:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:,StartTime:2023-01-28 01:15:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 28 01:15:27.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4546" for this suite. 01/28/23 01:15:28.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:15:28.042
Jan 28 01:15:28.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename crd-publish-openapi 01/28/23 01:15:28.045
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:15:28.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:15:28.109
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/28/23 01:15:28.12
Jan 28 01:15:28.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:15:31.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:15:43.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8390" for this suite. 01/28/23 01:15:43.782
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":182,"skipped":3309,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.759 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:15:28.042
    Jan 28 01:15:28.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename crd-publish-openapi 01/28/23 01:15:28.045
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:15:28.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:15:28.109
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/28/23 01:15:28.12
    Jan 28 01:15:28.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:15:31.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:15:43.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8390" for this suite. 01/28/23 01:15:43.782
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:15:43.803
Jan 28 01:15:43.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubectl 01/28/23 01:15:43.806
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:15:43.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:15:43.858
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 01/28/23 01:15:43.87
Jan 28 01:15:43.871: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-8244 proxy --unix-socket=/tmp/kubectl-proxy-unix1851429885/test'
STEP: retrieving proxy /api/ output 01/28/23 01:15:43.959
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 28 01:15:43.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8244" for this suite. 01/28/23 01:15:43.979
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":183,"skipped":3315,"failed":0}
------------------------------
â€¢ [0.197 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:15:43.803
    Jan 28 01:15:43.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubectl 01/28/23 01:15:43.806
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:15:43.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:15:43.858
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 01/28/23 01:15:43.87
    Jan 28 01:15:43.871: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-8244 proxy --unix-socket=/tmp/kubectl-proxy-unix1851429885/test'
    STEP: retrieving proxy /api/ output 01/28/23 01:15:43.959
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 28 01:15:43.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8244" for this suite. 01/28/23 01:15:43.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:15:44.004
Jan 28 01:15:44.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename statefulset 01/28/23 01:15:44.006
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:15:44.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:15:44.07
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1736 01/28/23 01:15:44.082
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Jan 28 01:15:44.129: INFO: Found 0 stateful pods, waiting for 1
Jan 28 01:15:54.143: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/28/23 01:15:54.169
W0128 01:15:54.186735      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 28 01:15:54.214: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 01:15:54.214: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Jan 28 01:16:04.229: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 01:16:04.229: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/28/23 01:16:04.253
STEP: Delete all of the StatefulSets 01/28/23 01:16:04.267
STEP: Verify that StatefulSets have been deleted 01/28/23 01:16:04.295
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 28 01:16:04.314: INFO: Deleting all statefulset in ns statefulset-1736
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 28 01:16:04.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1736" for this suite. 01/28/23 01:16:04.371
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":184,"skipped":3334,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.384 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:15:44.004
    Jan 28 01:15:44.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename statefulset 01/28/23 01:15:44.006
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:15:44.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:15:44.07
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1736 01/28/23 01:15:44.082
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Jan 28 01:15:44.129: INFO: Found 0 stateful pods, waiting for 1
    Jan 28 01:15:54.143: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/28/23 01:15:54.169
    W0128 01:15:54.186735      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 28 01:15:54.214: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 28 01:15:54.214: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
    Jan 28 01:16:04.229: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 28 01:16:04.229: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/28/23 01:16:04.253
    STEP: Delete all of the StatefulSets 01/28/23 01:16:04.267
    STEP: Verify that StatefulSets have been deleted 01/28/23 01:16:04.295
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 28 01:16:04.314: INFO: Deleting all statefulset in ns statefulset-1736
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 28 01:16:04.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1736" for this suite. 01/28/23 01:16:04.371
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:16:04.392
Jan 28 01:16:04.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubectl 01/28/23 01:16:04.394
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:04.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:04.447
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 01/28/23 01:16:04.459
Jan 28 01:16:04.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-8028 api-versions'
Jan 28 01:16:04.557: INFO: stderr: ""
Jan 28 01:16:04.557: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nibm.com/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 28 01:16:04.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8028" for this suite. 01/28/23 01:16:04.573
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":185,"skipped":3341,"failed":0}
------------------------------
â€¢ [0.202 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:16:04.392
    Jan 28 01:16:04.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubectl 01/28/23 01:16:04.394
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:04.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:04.447
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 01/28/23 01:16:04.459
    Jan 28 01:16:04.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-8028 api-versions'
    Jan 28 01:16:04.557: INFO: stderr: ""
    Jan 28 01:16:04.557: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nibm.com/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 28 01:16:04.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8028" for this suite. 01/28/23 01:16:04.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:16:04.598
Jan 28 01:16:04.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename configmap 01/28/23 01:16:04.6
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:04.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:04.652
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-a5baa6be-4e82-4847-9372-cab46e027db9 01/28/23 01:16:04.664
STEP: Creating a pod to test consume configMaps 01/28/23 01:16:04.678
Jan 28 01:16:04.704: INFO: Waiting up to 5m0s for pod "pod-configmaps-e06d47ea-3b07-4124-97fa-0403c210df61" in namespace "configmap-2147" to be "Succeeded or Failed"
Jan 28 01:16:04.723: INFO: Pod "pod-configmaps-e06d47ea-3b07-4124-97fa-0403c210df61": Phase="Pending", Reason="", readiness=false. Elapsed: 19.134438ms
Jan 28 01:16:06.737: INFO: Pod "pod-configmaps-e06d47ea-3b07-4124-97fa-0403c210df61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033123448s
Jan 28 01:16:08.736: INFO: Pod "pod-configmaps-e06d47ea-3b07-4124-97fa-0403c210df61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032256802s
Jan 28 01:16:10.737: INFO: Pod "pod-configmaps-e06d47ea-3b07-4124-97fa-0403c210df61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032958782s
STEP: Saw pod success 01/28/23 01:16:10.737
Jan 28 01:16:10.738: INFO: Pod "pod-configmaps-e06d47ea-3b07-4124-97fa-0403c210df61" satisfied condition "Succeeded or Failed"
Jan 28 01:16:10.750: INFO: Trying to get logs from node 10.9.20.75 pod pod-configmaps-e06d47ea-3b07-4124-97fa-0403c210df61 container agnhost-container: <nil>
STEP: delete the pod 01/28/23 01:16:10.836
Jan 28 01:16:10.869: INFO: Waiting for pod pod-configmaps-e06d47ea-3b07-4124-97fa-0403c210df61 to disappear
Jan 28 01:16:10.880: INFO: Pod pod-configmaps-e06d47ea-3b07-4124-97fa-0403c210df61 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 28 01:16:10.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2147" for this suite. 01/28/23 01:16:10.895
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":186,"skipped":3351,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.315 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:16:04.598
    Jan 28 01:16:04.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename configmap 01/28/23 01:16:04.6
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:04.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:04.652
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-a5baa6be-4e82-4847-9372-cab46e027db9 01/28/23 01:16:04.664
    STEP: Creating a pod to test consume configMaps 01/28/23 01:16:04.678
    Jan 28 01:16:04.704: INFO: Waiting up to 5m0s for pod "pod-configmaps-e06d47ea-3b07-4124-97fa-0403c210df61" in namespace "configmap-2147" to be "Succeeded or Failed"
    Jan 28 01:16:04.723: INFO: Pod "pod-configmaps-e06d47ea-3b07-4124-97fa-0403c210df61": Phase="Pending", Reason="", readiness=false. Elapsed: 19.134438ms
    Jan 28 01:16:06.737: INFO: Pod "pod-configmaps-e06d47ea-3b07-4124-97fa-0403c210df61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033123448s
    Jan 28 01:16:08.736: INFO: Pod "pod-configmaps-e06d47ea-3b07-4124-97fa-0403c210df61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032256802s
    Jan 28 01:16:10.737: INFO: Pod "pod-configmaps-e06d47ea-3b07-4124-97fa-0403c210df61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032958782s
    STEP: Saw pod success 01/28/23 01:16:10.737
    Jan 28 01:16:10.738: INFO: Pod "pod-configmaps-e06d47ea-3b07-4124-97fa-0403c210df61" satisfied condition "Succeeded or Failed"
    Jan 28 01:16:10.750: INFO: Trying to get logs from node 10.9.20.75 pod pod-configmaps-e06d47ea-3b07-4124-97fa-0403c210df61 container agnhost-container: <nil>
    STEP: delete the pod 01/28/23 01:16:10.836
    Jan 28 01:16:10.869: INFO: Waiting for pod pod-configmaps-e06d47ea-3b07-4124-97fa-0403c210df61 to disappear
    Jan 28 01:16:10.880: INFO: Pod pod-configmaps-e06d47ea-3b07-4124-97fa-0403c210df61 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 28 01:16:10.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2147" for this suite. 01/28/23 01:16:10.895
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:16:10.921
Jan 28 01:16:10.921: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename dns 01/28/23 01:16:10.922
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:10.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:10.973
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9012.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9012.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/28/23 01:16:10.985
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9012.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9012.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/28/23 01:16:10.985
STEP: creating a pod to probe /etc/hosts 01/28/23 01:16:10.986
STEP: submitting the pod to kubernetes 01/28/23 01:16:10.986
Jan 28 01:16:11.009: INFO: Waiting up to 15m0s for pod "dns-test-a669c1c7-3ef1-4bff-a28c-d909ca08e614" in namespace "dns-9012" to be "running"
Jan 28 01:16:11.022: INFO: Pod "dns-test-a669c1c7-3ef1-4bff-a28c-d909ca08e614": Phase="Pending", Reason="", readiness=false. Elapsed: 13.682721ms
Jan 28 01:16:13.036: INFO: Pod "dns-test-a669c1c7-3ef1-4bff-a28c-d909ca08e614": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027331802s
Jan 28 01:16:15.038: INFO: Pod "dns-test-a669c1c7-3ef1-4bff-a28c-d909ca08e614": Phase="Running", Reason="", readiness=true. Elapsed: 4.028850766s
Jan 28 01:16:15.038: INFO: Pod "dns-test-a669c1c7-3ef1-4bff-a28c-d909ca08e614" satisfied condition "running"
STEP: retrieving the pod 01/28/23 01:16:15.038
STEP: looking for the results for each expected name from probers 01/28/23 01:16:15.05
Jan 28 01:16:15.169: INFO: DNS probes using dns-9012/dns-test-a669c1c7-3ef1-4bff-a28c-d909ca08e614 succeeded

STEP: deleting the pod 01/28/23 01:16:15.169
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 28 01:16:15.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9012" for this suite. 01/28/23 01:16:15.215
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":187,"skipped":3401,"failed":0}
------------------------------
â€¢ [4.329 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:16:10.921
    Jan 28 01:16:10.921: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename dns 01/28/23 01:16:10.922
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:10.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:10.973
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9012.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9012.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/28/23 01:16:10.985
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9012.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9012.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/28/23 01:16:10.985
    STEP: creating a pod to probe /etc/hosts 01/28/23 01:16:10.986
    STEP: submitting the pod to kubernetes 01/28/23 01:16:10.986
    Jan 28 01:16:11.009: INFO: Waiting up to 15m0s for pod "dns-test-a669c1c7-3ef1-4bff-a28c-d909ca08e614" in namespace "dns-9012" to be "running"
    Jan 28 01:16:11.022: INFO: Pod "dns-test-a669c1c7-3ef1-4bff-a28c-d909ca08e614": Phase="Pending", Reason="", readiness=false. Elapsed: 13.682721ms
    Jan 28 01:16:13.036: INFO: Pod "dns-test-a669c1c7-3ef1-4bff-a28c-d909ca08e614": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027331802s
    Jan 28 01:16:15.038: INFO: Pod "dns-test-a669c1c7-3ef1-4bff-a28c-d909ca08e614": Phase="Running", Reason="", readiness=true. Elapsed: 4.028850766s
    Jan 28 01:16:15.038: INFO: Pod "dns-test-a669c1c7-3ef1-4bff-a28c-d909ca08e614" satisfied condition "running"
    STEP: retrieving the pod 01/28/23 01:16:15.038
    STEP: looking for the results for each expected name from probers 01/28/23 01:16:15.05
    Jan 28 01:16:15.169: INFO: DNS probes using dns-9012/dns-test-a669c1c7-3ef1-4bff-a28c-d909ca08e614 succeeded

    STEP: deleting the pod 01/28/23 01:16:15.169
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 28 01:16:15.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9012" for this suite. 01/28/23 01:16:15.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:16:15.256
Jan 28 01:16:15.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename replication-controller 01/28/23 01:16:15.258
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:15.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:15.312
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 01/28/23 01:16:15.324
STEP: When the matched label of one of its pods change 01/28/23 01:16:15.339
Jan 28 01:16:15.351: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 28 01:16:20.364: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/28/23 01:16:20.392
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 28 01:16:20.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6895" for this suite. 01/28/23 01:16:20.474
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":188,"skipped":3412,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.238 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:16:15.256
    Jan 28 01:16:15.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename replication-controller 01/28/23 01:16:15.258
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:15.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:15.312
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 01/28/23 01:16:15.324
    STEP: When the matched label of one of its pods change 01/28/23 01:16:15.339
    Jan 28 01:16:15.351: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan 28 01:16:20.364: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/28/23 01:16:20.392
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 28 01:16:20.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6895" for this suite. 01/28/23 01:16:20.474
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:16:20.494
Jan 28 01:16:20.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename webhook 01/28/23 01:16:20.497
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:20.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:20.545
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/28/23 01:16:20.613
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:16:21.096
STEP: Deploying the webhook pod 01/28/23 01:16:21.115
STEP: Wait for the deployment to be ready 01/28/23 01:16:21.146
Jan 28 01:16:21.175: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 01:16:23.213: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 16, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 16, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 16, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 16, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/28/23 01:16:25.229
STEP: Verifying the service has paired with the endpoint 01/28/23 01:16:25.261
Jan 28 01:16:26.262: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 01/28/23 01:16:26.28
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/28/23 01:16:26.392
STEP: Creating a configMap that should not be mutated 01/28/23 01:16:26.409
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/28/23 01:16:26.442
STEP: Creating a configMap that should be mutated 01/28/23 01:16:26.458
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:16:26.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3758" for this suite. 01/28/23 01:16:26.593
STEP: Destroying namespace "webhook-3758-markers" for this suite. 01/28/23 01:16:26.613
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":189,"skipped":3413,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.274 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:16:20.494
    Jan 28 01:16:20.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename webhook 01/28/23 01:16:20.497
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:20.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:20.545
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/28/23 01:16:20.613
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:16:21.096
    STEP: Deploying the webhook pod 01/28/23 01:16:21.115
    STEP: Wait for the deployment to be ready 01/28/23 01:16:21.146
    Jan 28 01:16:21.175: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 28 01:16:23.213: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 16, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 16, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 16, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 16, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/28/23 01:16:25.229
    STEP: Verifying the service has paired with the endpoint 01/28/23 01:16:25.261
    Jan 28 01:16:26.262: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 01/28/23 01:16:26.28
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/28/23 01:16:26.392
    STEP: Creating a configMap that should not be mutated 01/28/23 01:16:26.409
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/28/23 01:16:26.442
    STEP: Creating a configMap that should be mutated 01/28/23 01:16:26.458
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:16:26.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3758" for this suite. 01/28/23 01:16:26.593
    STEP: Destroying namespace "webhook-3758-markers" for this suite. 01/28/23 01:16:26.613
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:16:26.771
Jan 28 01:16:26.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename sysctl 01/28/23 01:16:26.775
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:26.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:26.827
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/28/23 01:16:26.84
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 28 01:16:26.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-2398" for this suite. 01/28/23 01:16:26.87
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":190,"skipped":3416,"failed":0}
------------------------------
â€¢ [0.119 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:16:26.771
    Jan 28 01:16:26.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename sysctl 01/28/23 01:16:26.775
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:26.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:26.827
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/28/23 01:16:26.84
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 28 01:16:26.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-2398" for this suite. 01/28/23 01:16:26.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:16:26.892
Jan 28 01:16:26.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename replication-controller 01/28/23 01:16:26.894
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:26.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:26.942
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Jan 28 01:16:26.953: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/28/23 01:16:26.978
STEP: Checking rc "condition-test" has the desired failure condition set 01/28/23 01:16:26.993
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/28/23 01:16:28.021
Jan 28 01:16:28.046: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/28/23 01:16:28.046
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 28 01:16:29.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4500" for this suite. 01/28/23 01:16:29.089
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":191,"skipped":3430,"failed":0}
------------------------------
â€¢ [2.214 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:16:26.892
    Jan 28 01:16:26.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename replication-controller 01/28/23 01:16:26.894
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:26.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:26.942
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Jan 28 01:16:26.953: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/28/23 01:16:26.978
    STEP: Checking rc "condition-test" has the desired failure condition set 01/28/23 01:16:26.993
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/28/23 01:16:28.021
    Jan 28 01:16:28.046: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/28/23 01:16:28.046
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 28 01:16:29.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4500" for this suite. 01/28/23 01:16:29.089
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:16:29.111
Jan 28 01:16:29.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename crd-publish-openapi 01/28/23 01:16:29.113
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:29.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:29.177
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 01/28/23 01:16:29.206
Jan 28 01:16:29.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: rename a version 01/28/23 01:16:36.786
STEP: check the new version name is served 01/28/23 01:16:36.839
STEP: check the old version name is removed 01/28/23 01:16:40.204
STEP: check the other version is not changed 01/28/23 01:16:41.776
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:16:47.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-840" for this suite. 01/28/23 01:16:47.688
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":192,"skipped":3434,"failed":0}
------------------------------
â€¢ [SLOW TEST] [18.597 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:16:29.111
    Jan 28 01:16:29.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename crd-publish-openapi 01/28/23 01:16:29.113
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:29.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:29.177
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 01/28/23 01:16:29.206
    Jan 28 01:16:29.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: rename a version 01/28/23 01:16:36.786
    STEP: check the new version name is served 01/28/23 01:16:36.839
    STEP: check the old version name is removed 01/28/23 01:16:40.204
    STEP: check the other version is not changed 01/28/23 01:16:41.776
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:16:47.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-840" for this suite. 01/28/23 01:16:47.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:16:47.717
Jan 28 01:16:47.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename resourcequota 01/28/23 01:16:47.718
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:47.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:47.763
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 01/28/23 01:16:47.773
STEP: Creating a ResourceQuota 01/28/23 01:16:52.786
STEP: Ensuring resource quota status is calculated 01/28/23 01:16:52.8
STEP: Creating a Service 01/28/23 01:16:54.813
STEP: Creating a NodePort Service 01/28/23 01:16:54.857
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/28/23 01:16:54.91
STEP: Ensuring resource quota status captures service creation 01/28/23 01:16:54.977
STEP: Deleting Services 01/28/23 01:16:56.997
STEP: Ensuring resource quota status released usage 01/28/23 01:16:57.098
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 28 01:16:59.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9850" for this suite. 01/28/23 01:16:59.129
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":193,"skipped":3447,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.431 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:16:47.717
    Jan 28 01:16:47.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename resourcequota 01/28/23 01:16:47.718
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:47.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:47.763
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 01/28/23 01:16:47.773
    STEP: Creating a ResourceQuota 01/28/23 01:16:52.786
    STEP: Ensuring resource quota status is calculated 01/28/23 01:16:52.8
    STEP: Creating a Service 01/28/23 01:16:54.813
    STEP: Creating a NodePort Service 01/28/23 01:16:54.857
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/28/23 01:16:54.91
    STEP: Ensuring resource quota status captures service creation 01/28/23 01:16:54.977
    STEP: Deleting Services 01/28/23 01:16:56.997
    STEP: Ensuring resource quota status released usage 01/28/23 01:16:57.098
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 28 01:16:59.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9850" for this suite. 01/28/23 01:16:59.129
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:16:59.152
Jan 28 01:16:59.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename hostport 01/28/23 01:16:59.155
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:59.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:59.204
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/28/23 01:16:59.252
Jan 28 01:16:59.274: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-7979" to be "running and ready"
Jan 28 01:16:59.285: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.703753ms
Jan 28 01:16:59.285: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:17:01.298: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023727142s
Jan 28 01:17:01.298: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:17:03.298: INFO: Pod "pod1": Phase="Running", Reason="", readiness=false. Elapsed: 4.023192587s
Jan 28 01:17:03.298: INFO: The phase of Pod pod1 is Running (Ready = false)
Jan 28 01:17:05.298: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 6.023938392s
Jan 28 01:17:05.299: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 28 01:17:05.299: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.9.20.75 on the node which pod1 resides and expect scheduled 01/28/23 01:17:05.299
Jan 28 01:17:05.316: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-7979" to be "running and ready"
Jan 28 01:17:05.327: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.388318ms
Jan 28 01:17:05.327: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:17:07.340: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023673963s
Jan 28 01:17:07.344: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:17:09.339: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.022518408s
Jan 28 01:17:09.339: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 28 01:17:09.339: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.9.20.75 but use UDP protocol on the node which pod2 resides 01/28/23 01:17:09.339
Jan 28 01:17:09.362: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-7979" to be "running and ready"
Jan 28 01:17:09.374: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.000364ms
Jan 28 01:17:09.374: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:17:11.387: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025128087s
Jan 28 01:17:11.387: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:17:13.386: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.024594531s
Jan 28 01:17:13.386: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan 28 01:17:13.386: INFO: Pod "pod3" satisfied condition "running and ready"
Jan 28 01:17:13.400: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-7979" to be "running and ready"
Jan 28 01:17:13.411: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 10.952096ms
Jan 28 01:17:13.411: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:17:15.424: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.023630996s
Jan 28 01:17:15.424: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan 28 01:17:15.424: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/28/23 01:17:15.435
Jan 28 01:17:15.436: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.9.20.75 http://127.0.0.1:54323/hostname] Namespace:hostport-7979 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:17:15.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:17:15.437: INFO: ExecWithOptions: Clientset creation
Jan 28 01:17:15.437: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-7979/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.9.20.75+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.9.20.75, port: 54323 01/28/23 01:17:15.659
Jan 28 01:17:15.659: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.9.20.75:54323/hostname] Namespace:hostport-7979 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:17:15.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:17:15.660: INFO: ExecWithOptions: Clientset creation
Jan 28 01:17:15.660: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-7979/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.9.20.75%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.9.20.75, port: 54323 UDP 01/28/23 01:17:15.852
Jan 28 01:17:15.853: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.9.20.75 54323] Namespace:hostport-7979 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:17:15.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:17:15.854: INFO: ExecWithOptions: Clientset creation
Jan 28 01:17:15.854: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-7979/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.9.20.75+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Jan 28 01:17:21.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-7979" for this suite. 01/28/23 01:17:21.085
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":194,"skipped":3450,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.952 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:16:59.152
    Jan 28 01:16:59.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename hostport 01/28/23 01:16:59.155
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:16:59.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:16:59.204
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/28/23 01:16:59.252
    Jan 28 01:16:59.274: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-7979" to be "running and ready"
    Jan 28 01:16:59.285: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.703753ms
    Jan 28 01:16:59.285: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:17:01.298: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023727142s
    Jan 28 01:17:01.298: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:17:03.298: INFO: Pod "pod1": Phase="Running", Reason="", readiness=false. Elapsed: 4.023192587s
    Jan 28 01:17:03.298: INFO: The phase of Pod pod1 is Running (Ready = false)
    Jan 28 01:17:05.298: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 6.023938392s
    Jan 28 01:17:05.299: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 28 01:17:05.299: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.9.20.75 on the node which pod1 resides and expect scheduled 01/28/23 01:17:05.299
    Jan 28 01:17:05.316: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-7979" to be "running and ready"
    Jan 28 01:17:05.327: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.388318ms
    Jan 28 01:17:05.327: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:17:07.340: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023673963s
    Jan 28 01:17:07.344: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:17:09.339: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.022518408s
    Jan 28 01:17:09.339: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 28 01:17:09.339: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.9.20.75 but use UDP protocol on the node which pod2 resides 01/28/23 01:17:09.339
    Jan 28 01:17:09.362: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-7979" to be "running and ready"
    Jan 28 01:17:09.374: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.000364ms
    Jan 28 01:17:09.374: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:17:11.387: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025128087s
    Jan 28 01:17:11.387: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:17:13.386: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.024594531s
    Jan 28 01:17:13.386: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan 28 01:17:13.386: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan 28 01:17:13.400: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-7979" to be "running and ready"
    Jan 28 01:17:13.411: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 10.952096ms
    Jan 28 01:17:13.411: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:17:15.424: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.023630996s
    Jan 28 01:17:15.424: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan 28 01:17:15.424: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/28/23 01:17:15.435
    Jan 28 01:17:15.436: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.9.20.75 http://127.0.0.1:54323/hostname] Namespace:hostport-7979 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:17:15.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:17:15.437: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:17:15.437: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-7979/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.9.20.75+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.9.20.75, port: 54323 01/28/23 01:17:15.659
    Jan 28 01:17:15.659: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.9.20.75:54323/hostname] Namespace:hostport-7979 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:17:15.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:17:15.660: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:17:15.660: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-7979/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.9.20.75%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.9.20.75, port: 54323 UDP 01/28/23 01:17:15.852
    Jan 28 01:17:15.853: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.9.20.75 54323] Namespace:hostport-7979 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:17:15.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:17:15.854: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:17:15.854: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-7979/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.9.20.75+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Jan 28 01:17:21.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-7979" for this suite. 01/28/23 01:17:21.085
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:17:21.109
Jan 28 01:17:21.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename configmap 01/28/23 01:17:21.112
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:17:21.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:17:21.153
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-4143/configmap-test-10c10c0a-ab67-420b-a833-c0f274ee735e 01/28/23 01:17:21.163
STEP: Creating a pod to test consume configMaps 01/28/23 01:17:21.176
Jan 28 01:17:21.197: INFO: Waiting up to 5m0s for pod "pod-configmaps-3359be1d-e023-41af-a529-ad9aee15d354" in namespace "configmap-4143" to be "Succeeded or Failed"
Jan 28 01:17:21.207: INFO: Pod "pod-configmaps-3359be1d-e023-41af-a529-ad9aee15d354": Phase="Pending", Reason="", readiness=false. Elapsed: 10.280271ms
Jan 28 01:17:23.220: INFO: Pod "pod-configmaps-3359be1d-e023-41af-a529-ad9aee15d354": Phase="Running", Reason="", readiness=true. Elapsed: 2.022625609s
Jan 28 01:17:25.223: INFO: Pod "pod-configmaps-3359be1d-e023-41af-a529-ad9aee15d354": Phase="Running", Reason="", readiness=false. Elapsed: 4.025613279s
Jan 28 01:17:27.221: INFO: Pod "pod-configmaps-3359be1d-e023-41af-a529-ad9aee15d354": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024195335s
STEP: Saw pod success 01/28/23 01:17:27.221
Jan 28 01:17:27.222: INFO: Pod "pod-configmaps-3359be1d-e023-41af-a529-ad9aee15d354" satisfied condition "Succeeded or Failed"
Jan 28 01:17:27.234: INFO: Trying to get logs from node 10.9.20.72 pod pod-configmaps-3359be1d-e023-41af-a529-ad9aee15d354 container env-test: <nil>
STEP: delete the pod 01/28/23 01:17:27.306
Jan 28 01:17:27.333: INFO: Waiting for pod pod-configmaps-3359be1d-e023-41af-a529-ad9aee15d354 to disappear
Jan 28 01:17:27.355: INFO: Pod pod-configmaps-3359be1d-e023-41af-a529-ad9aee15d354 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 28 01:17:27.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4143" for this suite. 01/28/23 01:17:27.372
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":195,"skipped":3487,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.282 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:17:21.109
    Jan 28 01:17:21.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename configmap 01/28/23 01:17:21.112
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:17:21.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:17:21.153
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-4143/configmap-test-10c10c0a-ab67-420b-a833-c0f274ee735e 01/28/23 01:17:21.163
    STEP: Creating a pod to test consume configMaps 01/28/23 01:17:21.176
    Jan 28 01:17:21.197: INFO: Waiting up to 5m0s for pod "pod-configmaps-3359be1d-e023-41af-a529-ad9aee15d354" in namespace "configmap-4143" to be "Succeeded or Failed"
    Jan 28 01:17:21.207: INFO: Pod "pod-configmaps-3359be1d-e023-41af-a529-ad9aee15d354": Phase="Pending", Reason="", readiness=false. Elapsed: 10.280271ms
    Jan 28 01:17:23.220: INFO: Pod "pod-configmaps-3359be1d-e023-41af-a529-ad9aee15d354": Phase="Running", Reason="", readiness=true. Elapsed: 2.022625609s
    Jan 28 01:17:25.223: INFO: Pod "pod-configmaps-3359be1d-e023-41af-a529-ad9aee15d354": Phase="Running", Reason="", readiness=false. Elapsed: 4.025613279s
    Jan 28 01:17:27.221: INFO: Pod "pod-configmaps-3359be1d-e023-41af-a529-ad9aee15d354": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024195335s
    STEP: Saw pod success 01/28/23 01:17:27.221
    Jan 28 01:17:27.222: INFO: Pod "pod-configmaps-3359be1d-e023-41af-a529-ad9aee15d354" satisfied condition "Succeeded or Failed"
    Jan 28 01:17:27.234: INFO: Trying to get logs from node 10.9.20.72 pod pod-configmaps-3359be1d-e023-41af-a529-ad9aee15d354 container env-test: <nil>
    STEP: delete the pod 01/28/23 01:17:27.306
    Jan 28 01:17:27.333: INFO: Waiting for pod pod-configmaps-3359be1d-e023-41af-a529-ad9aee15d354 to disappear
    Jan 28 01:17:27.355: INFO: Pod pod-configmaps-3359be1d-e023-41af-a529-ad9aee15d354 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 28 01:17:27.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4143" for this suite. 01/28/23 01:17:27.372
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:17:27.397
Jan 28 01:17:27.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename deployment 01/28/23 01:17:27.399
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:17:27.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:17:27.441
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/28/23 01:17:27.47
Jan 28 01:17:27.471: INFO: Creating simple deployment test-deployment-zb5h8
Jan 28 01:17:27.537: INFO: deployment "test-deployment-zb5h8" doesn't have the required revision set
STEP: Getting /status 01/28/23 01:17:29.587
Jan 28 01:17:29.602: INFO: Deployment test-deployment-zb5h8 has Conditions: [{Available True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zb5h8-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 01/28/23 01:17:29.602
Jan 28 01:17:29.642: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 17, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 17, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 17, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 17, 27, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-zb5h8-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/28/23 01:17:29.642
Jan 28 01:17:29.650: INFO: Observed &Deployment event: ADDED
Jan 28 01:17:29.650: INFO: Observed Deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zb5h8-777898ffcc"}
Jan 28 01:17:29.651: INFO: Observed &Deployment event: MODIFIED
Jan 28 01:17:29.651: INFO: Observed Deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zb5h8-777898ffcc"}
Jan 28 01:17:29.651: INFO: Observed Deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 28 01:17:29.652: INFO: Observed &Deployment event: MODIFIED
Jan 28 01:17:29.652: INFO: Observed Deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 28 01:17:29.652: INFO: Observed Deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-zb5h8-777898ffcc" is progressing.}
Jan 28 01:17:29.652: INFO: Observed &Deployment event: MODIFIED
Jan 28 01:17:29.652: INFO: Observed Deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 28 01:17:29.652: INFO: Observed Deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zb5h8-777898ffcc" has successfully progressed.}
Jan 28 01:17:29.653: INFO: Observed &Deployment event: MODIFIED
Jan 28 01:17:29.653: INFO: Observed Deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 28 01:17:29.653: INFO: Observed Deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zb5h8-777898ffcc" has successfully progressed.}
Jan 28 01:17:29.653: INFO: Found Deployment test-deployment-zb5h8 in namespace deployment-7634 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 28 01:17:29.653: INFO: Deployment test-deployment-zb5h8 has an updated status
STEP: patching the Statefulset Status 01/28/23 01:17:29.653
Jan 28 01:17:29.654: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 28 01:17:29.670: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/28/23 01:17:29.67
Jan 28 01:17:29.678: INFO: Observed &Deployment event: ADDED
Jan 28 01:17:29.678: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zb5h8-777898ffcc"}
Jan 28 01:17:29.678: INFO: Observed &Deployment event: MODIFIED
Jan 28 01:17:29.679: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zb5h8-777898ffcc"}
Jan 28 01:17:29.679: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 28 01:17:29.680: INFO: Observed &Deployment event: MODIFIED
Jan 28 01:17:29.680: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 28 01:17:29.680: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-zb5h8-777898ffcc" is progressing.}
Jan 28 01:17:29.680: INFO: Observed &Deployment event: MODIFIED
Jan 28 01:17:29.680: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 28 01:17:29.680: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zb5h8-777898ffcc" has successfully progressed.}
Jan 28 01:17:29.681: INFO: Observed &Deployment event: MODIFIED
Jan 28 01:17:29.681: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 28 01:17:29.681: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zb5h8-777898ffcc" has successfully progressed.}
Jan 28 01:17:29.681: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 28 01:17:29.682: INFO: Observed &Deployment event: MODIFIED
Jan 28 01:17:29.682: INFO: Found deployment test-deployment-zb5h8 in namespace deployment-7634 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 28 01:17:29.682: INFO: Deployment test-deployment-zb5h8 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 28 01:17:29.697: INFO: Deployment "test-deployment-zb5h8":
&Deployment{ObjectMeta:{test-deployment-zb5h8  deployment-7634  75d77168-672f-48ff-96f3-425e2c76adac 33026 1 2023-01-28 01:17:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-28 01:17:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-28 01:17:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-28 01:17:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00315d8d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-zb5h8-777898ffcc",LastUpdateTime:2023-01-28 01:17:29 +0000 UTC,LastTransitionTime:2023-01-28 01:17:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 28 01:17:29.708: INFO: New ReplicaSet "test-deployment-zb5h8-777898ffcc" of Deployment "test-deployment-zb5h8":
&ReplicaSet{ObjectMeta:{test-deployment-zb5h8-777898ffcc  deployment-7634  7623ea8e-460a-484b-80bd-f99064cec9b3 33018 1 2023-01-28 01:17:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-zb5h8 75d77168-672f-48ff-96f3-425e2c76adac 0xc00478e290 0xc00478e291}] [] [{kube-controller-manager Update apps/v1 2023-01-28 01:17:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75d77168-672f-48ff-96f3-425e2c76adac\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:17:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00478e368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 28 01:17:29.718: INFO: Pod "test-deployment-zb5h8-777898ffcc-9wlcg" is available:
&Pod{ObjectMeta:{test-deployment-zb5h8-777898ffcc-9wlcg test-deployment-zb5h8-777898ffcc- deployment-7634  3bae2f02-697b-4007-9f0e-cd6881aa1d04 33017 0 2023-01-28 01:17:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:1efc2f3bdedd3c414707e54409b67565d300a8706cc556923beac621ca1f592a cni.projectcalico.org/podIP:172.30.12.226/32 cni.projectcalico.org/podIPs:172.30.12.226/32] [{apps/v1 ReplicaSet test-deployment-zb5h8-777898ffcc 7623ea8e-460a-484b-80bd-f99064cec9b3 0xc005827270 0xc005827271}] [] [{kube-controller-manager Update v1 2023-01-28 01:17:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7623ea8e-460a-484b-80bd-f99064cec9b3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:17:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:17:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qgz8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qgz8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:17:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:17:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:17:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:17:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:172.30.12.226,StartTime:2023-01-28 01:17:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:17:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b6ab79551460de12f314459ddf9aa29f24ece9edb3c3335b6183254d491f42e8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.12.226,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 28 01:17:29.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7634" for this suite. 01/28/23 01:17:29.736
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":196,"skipped":3519,"failed":0}
------------------------------
â€¢ [2.359 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:17:27.397
    Jan 28 01:17:27.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename deployment 01/28/23 01:17:27.399
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:17:27.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:17:27.441
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/28/23 01:17:27.47
    Jan 28 01:17:27.471: INFO: Creating simple deployment test-deployment-zb5h8
    Jan 28 01:17:27.537: INFO: deployment "test-deployment-zb5h8" doesn't have the required revision set
    STEP: Getting /status 01/28/23 01:17:29.587
    Jan 28 01:17:29.602: INFO: Deployment test-deployment-zb5h8 has Conditions: [{Available True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zb5h8-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 01/28/23 01:17:29.602
    Jan 28 01:17:29.642: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 17, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 17, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 17, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 17, 27, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-zb5h8-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/28/23 01:17:29.642
    Jan 28 01:17:29.650: INFO: Observed &Deployment event: ADDED
    Jan 28 01:17:29.650: INFO: Observed Deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zb5h8-777898ffcc"}
    Jan 28 01:17:29.651: INFO: Observed &Deployment event: MODIFIED
    Jan 28 01:17:29.651: INFO: Observed Deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zb5h8-777898ffcc"}
    Jan 28 01:17:29.651: INFO: Observed Deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 28 01:17:29.652: INFO: Observed &Deployment event: MODIFIED
    Jan 28 01:17:29.652: INFO: Observed Deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 28 01:17:29.652: INFO: Observed Deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-zb5h8-777898ffcc" is progressing.}
    Jan 28 01:17:29.652: INFO: Observed &Deployment event: MODIFIED
    Jan 28 01:17:29.652: INFO: Observed Deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 28 01:17:29.652: INFO: Observed Deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zb5h8-777898ffcc" has successfully progressed.}
    Jan 28 01:17:29.653: INFO: Observed &Deployment event: MODIFIED
    Jan 28 01:17:29.653: INFO: Observed Deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 28 01:17:29.653: INFO: Observed Deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zb5h8-777898ffcc" has successfully progressed.}
    Jan 28 01:17:29.653: INFO: Found Deployment test-deployment-zb5h8 in namespace deployment-7634 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 28 01:17:29.653: INFO: Deployment test-deployment-zb5h8 has an updated status
    STEP: patching the Statefulset Status 01/28/23 01:17:29.653
    Jan 28 01:17:29.654: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 28 01:17:29.670: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/28/23 01:17:29.67
    Jan 28 01:17:29.678: INFO: Observed &Deployment event: ADDED
    Jan 28 01:17:29.678: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zb5h8-777898ffcc"}
    Jan 28 01:17:29.678: INFO: Observed &Deployment event: MODIFIED
    Jan 28 01:17:29.679: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zb5h8-777898ffcc"}
    Jan 28 01:17:29.679: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 28 01:17:29.680: INFO: Observed &Deployment event: MODIFIED
    Jan 28 01:17:29.680: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 28 01:17:29.680: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:27 +0000 UTC 2023-01-28 01:17:27 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-zb5h8-777898ffcc" is progressing.}
    Jan 28 01:17:29.680: INFO: Observed &Deployment event: MODIFIED
    Jan 28 01:17:29.680: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 28 01:17:29.680: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zb5h8-777898ffcc" has successfully progressed.}
    Jan 28 01:17:29.681: INFO: Observed &Deployment event: MODIFIED
    Jan 28 01:17:29.681: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 28 01:17:29.681: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:17:29 +0000 UTC 2023-01-28 01:17:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zb5h8-777898ffcc" has successfully progressed.}
    Jan 28 01:17:29.681: INFO: Observed deployment test-deployment-zb5h8 in namespace deployment-7634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 28 01:17:29.682: INFO: Observed &Deployment event: MODIFIED
    Jan 28 01:17:29.682: INFO: Found deployment test-deployment-zb5h8 in namespace deployment-7634 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan 28 01:17:29.682: INFO: Deployment test-deployment-zb5h8 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 28 01:17:29.697: INFO: Deployment "test-deployment-zb5h8":
    &Deployment{ObjectMeta:{test-deployment-zb5h8  deployment-7634  75d77168-672f-48ff-96f3-425e2c76adac 33026 1 2023-01-28 01:17:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-28 01:17:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-28 01:17:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-28 01:17:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00315d8d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-zb5h8-777898ffcc",LastUpdateTime:2023-01-28 01:17:29 +0000 UTC,LastTransitionTime:2023-01-28 01:17:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 28 01:17:29.708: INFO: New ReplicaSet "test-deployment-zb5h8-777898ffcc" of Deployment "test-deployment-zb5h8":
    &ReplicaSet{ObjectMeta:{test-deployment-zb5h8-777898ffcc  deployment-7634  7623ea8e-460a-484b-80bd-f99064cec9b3 33018 1 2023-01-28 01:17:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-zb5h8 75d77168-672f-48ff-96f3-425e2c76adac 0xc00478e290 0xc00478e291}] [] [{kube-controller-manager Update apps/v1 2023-01-28 01:17:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75d77168-672f-48ff-96f3-425e2c76adac\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:17:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00478e368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 28 01:17:29.718: INFO: Pod "test-deployment-zb5h8-777898ffcc-9wlcg" is available:
    &Pod{ObjectMeta:{test-deployment-zb5h8-777898ffcc-9wlcg test-deployment-zb5h8-777898ffcc- deployment-7634  3bae2f02-697b-4007-9f0e-cd6881aa1d04 33017 0 2023-01-28 01:17:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:1efc2f3bdedd3c414707e54409b67565d300a8706cc556923beac621ca1f592a cni.projectcalico.org/podIP:172.30.12.226/32 cni.projectcalico.org/podIPs:172.30.12.226/32] [{apps/v1 ReplicaSet test-deployment-zb5h8-777898ffcc 7623ea8e-460a-484b-80bd-f99064cec9b3 0xc005827270 0xc005827271}] [] [{kube-controller-manager Update v1 2023-01-28 01:17:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7623ea8e-460a-484b-80bd-f99064cec9b3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:17:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:17:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qgz8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qgz8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:17:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:17:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:17:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:17:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:172.30.12.226,StartTime:2023-01-28 01:17:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:17:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b6ab79551460de12f314459ddf9aa29f24ece9edb3c3335b6183254d491f42e8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.12.226,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 28 01:17:29.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7634" for this suite. 01/28/23 01:17:29.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:17:29.757
Jan 28 01:17:29.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename init-container 01/28/23 01:17:29.76
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:17:29.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:17:29.81
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 01/28/23 01:17:29.822
Jan 28 01:17:29.822: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 28 01:17:35.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9719" for this suite. 01/28/23 01:17:35.054
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":197,"skipped":3527,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.314 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:17:29.757
    Jan 28 01:17:29.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename init-container 01/28/23 01:17:29.76
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:17:29.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:17:29.81
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 01/28/23 01:17:29.822
    Jan 28 01:17:29.822: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 28 01:17:35.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9719" for this suite. 01/28/23 01:17:35.054
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:17:35.076
Jan 28 01:17:35.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 01:17:35.078
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:17:35.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:17:35.126
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-c5516697-730e-47e3-b98d-211f0391b25b 01/28/23 01:17:35.138
STEP: Creating a pod to test consume configMaps 01/28/23 01:17:35.152
Jan 28 01:17:35.174: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0a4f2a80-fe14-4492-bbd0-9b3eeb75f9b7" in namespace "projected-2370" to be "Succeeded or Failed"
Jan 28 01:17:35.188: INFO: Pod "pod-projected-configmaps-0a4f2a80-fe14-4492-bbd0-9b3eeb75f9b7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.866347ms
Jan 28 01:17:37.201: INFO: Pod "pod-projected-configmaps-0a4f2a80-fe14-4492-bbd0-9b3eeb75f9b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026384896s
Jan 28 01:17:39.200: INFO: Pod "pod-projected-configmaps-0a4f2a80-fe14-4492-bbd0-9b3eeb75f9b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025907236s
Jan 28 01:17:41.201: INFO: Pod "pod-projected-configmaps-0a4f2a80-fe14-4492-bbd0-9b3eeb75f9b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026689782s
STEP: Saw pod success 01/28/23 01:17:41.201
Jan 28 01:17:41.201: INFO: Pod "pod-projected-configmaps-0a4f2a80-fe14-4492-bbd0-9b3eeb75f9b7" satisfied condition "Succeeded or Failed"
Jan 28 01:17:41.213: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-configmaps-0a4f2a80-fe14-4492-bbd0-9b3eeb75f9b7 container agnhost-container: <nil>
STEP: delete the pod 01/28/23 01:17:41.288
Jan 28 01:17:41.320: INFO: Waiting for pod pod-projected-configmaps-0a4f2a80-fe14-4492-bbd0-9b3eeb75f9b7 to disappear
Jan 28 01:17:41.357: INFO: Pod pod-projected-configmaps-0a4f2a80-fe14-4492-bbd0-9b3eeb75f9b7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 28 01:17:41.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2370" for this suite. 01/28/23 01:17:41.407
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":198,"skipped":3531,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.355 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:17:35.076
    Jan 28 01:17:35.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 01:17:35.078
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:17:35.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:17:35.126
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-c5516697-730e-47e3-b98d-211f0391b25b 01/28/23 01:17:35.138
    STEP: Creating a pod to test consume configMaps 01/28/23 01:17:35.152
    Jan 28 01:17:35.174: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0a4f2a80-fe14-4492-bbd0-9b3eeb75f9b7" in namespace "projected-2370" to be "Succeeded or Failed"
    Jan 28 01:17:35.188: INFO: Pod "pod-projected-configmaps-0a4f2a80-fe14-4492-bbd0-9b3eeb75f9b7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.866347ms
    Jan 28 01:17:37.201: INFO: Pod "pod-projected-configmaps-0a4f2a80-fe14-4492-bbd0-9b3eeb75f9b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026384896s
    Jan 28 01:17:39.200: INFO: Pod "pod-projected-configmaps-0a4f2a80-fe14-4492-bbd0-9b3eeb75f9b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025907236s
    Jan 28 01:17:41.201: INFO: Pod "pod-projected-configmaps-0a4f2a80-fe14-4492-bbd0-9b3eeb75f9b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026689782s
    STEP: Saw pod success 01/28/23 01:17:41.201
    Jan 28 01:17:41.201: INFO: Pod "pod-projected-configmaps-0a4f2a80-fe14-4492-bbd0-9b3eeb75f9b7" satisfied condition "Succeeded or Failed"
    Jan 28 01:17:41.213: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-configmaps-0a4f2a80-fe14-4492-bbd0-9b3eeb75f9b7 container agnhost-container: <nil>
    STEP: delete the pod 01/28/23 01:17:41.288
    Jan 28 01:17:41.320: INFO: Waiting for pod pod-projected-configmaps-0a4f2a80-fe14-4492-bbd0-9b3eeb75f9b7 to disappear
    Jan 28 01:17:41.357: INFO: Pod pod-projected-configmaps-0a4f2a80-fe14-4492-bbd0-9b3eeb75f9b7 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 28 01:17:41.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2370" for this suite. 01/28/23 01:17:41.407
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:17:41.462
Jan 28 01:17:41.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename resourcequota 01/28/23 01:17:41.463
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:17:41.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:17:41.506
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 01/28/23 01:17:41.517
STEP: Creating a ResourceQuota 01/28/23 01:17:46.528
STEP: Ensuring resource quota status is calculated 01/28/23 01:17:46.54
STEP: Creating a Pod that fits quota 01/28/23 01:17:48.554
STEP: Ensuring ResourceQuota status captures the pod usage 01/28/23 01:17:48.59
STEP: Not allowing a pod to be created that exceeds remaining quota 01/28/23 01:17:50.603
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/28/23 01:17:50.61
STEP: Ensuring a pod cannot update its resource requirements 01/28/23 01:17:50.617
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/28/23 01:17:50.631
STEP: Deleting the pod 01/28/23 01:17:52.646
STEP: Ensuring resource quota status released the pod usage 01/28/23 01:17:52.684
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 28 01:17:54.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4625" for this suite. 01/28/23 01:17:54.716
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":199,"skipped":3605,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.273 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:17:41.462
    Jan 28 01:17:41.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename resourcequota 01/28/23 01:17:41.463
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:17:41.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:17:41.506
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 01/28/23 01:17:41.517
    STEP: Creating a ResourceQuota 01/28/23 01:17:46.528
    STEP: Ensuring resource quota status is calculated 01/28/23 01:17:46.54
    STEP: Creating a Pod that fits quota 01/28/23 01:17:48.554
    STEP: Ensuring ResourceQuota status captures the pod usage 01/28/23 01:17:48.59
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/28/23 01:17:50.603
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/28/23 01:17:50.61
    STEP: Ensuring a pod cannot update its resource requirements 01/28/23 01:17:50.617
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/28/23 01:17:50.631
    STEP: Deleting the pod 01/28/23 01:17:52.646
    STEP: Ensuring resource quota status released the pod usage 01/28/23 01:17:52.684
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 28 01:17:54.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4625" for this suite. 01/28/23 01:17:54.716
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:17:54.736
Jan 28 01:17:54.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename secrets 01/28/23 01:17:54.738
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:17:54.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:17:54.783
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-4750/secret-test-ed8db4d2-d703-4081-bd44-8e9538f87e2c 01/28/23 01:17:54.794
STEP: Creating a pod to test consume secrets 01/28/23 01:17:54.807
Jan 28 01:17:54.827: INFO: Waiting up to 5m0s for pod "pod-configmaps-a8597f5f-50d4-495e-b3e4-af683a5c98be" in namespace "secrets-4750" to be "Succeeded or Failed"
Jan 28 01:17:54.838: INFO: Pod "pod-configmaps-a8597f5f-50d4-495e-b3e4-af683a5c98be": Phase="Pending", Reason="", readiness=false. Elapsed: 10.383264ms
Jan 28 01:17:56.851: INFO: Pod "pod-configmaps-a8597f5f-50d4-495e-b3e4-af683a5c98be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023914011s
Jan 28 01:17:58.850: INFO: Pod "pod-configmaps-a8597f5f-50d4-495e-b3e4-af683a5c98be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02244009s
Jan 28 01:18:00.850: INFO: Pod "pod-configmaps-a8597f5f-50d4-495e-b3e4-af683a5c98be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0228452s
STEP: Saw pod success 01/28/23 01:18:00.85
Jan 28 01:18:00.851: INFO: Pod "pod-configmaps-a8597f5f-50d4-495e-b3e4-af683a5c98be" satisfied condition "Succeeded or Failed"
Jan 28 01:18:00.861: INFO: Trying to get logs from node 10.9.20.126 pod pod-configmaps-a8597f5f-50d4-495e-b3e4-af683a5c98be container env-test: <nil>
STEP: delete the pod 01/28/23 01:18:00.891
Jan 28 01:18:00.923: INFO: Waiting for pod pod-configmaps-a8597f5f-50d4-495e-b3e4-af683a5c98be to disappear
Jan 28 01:18:00.934: INFO: Pod pod-configmaps-a8597f5f-50d4-495e-b3e4-af683a5c98be no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 28 01:18:00.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4750" for this suite. 01/28/23 01:18:00.954
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":200,"skipped":3610,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.237 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:17:54.736
    Jan 28 01:17:54.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename secrets 01/28/23 01:17:54.738
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:17:54.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:17:54.783
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-4750/secret-test-ed8db4d2-d703-4081-bd44-8e9538f87e2c 01/28/23 01:17:54.794
    STEP: Creating a pod to test consume secrets 01/28/23 01:17:54.807
    Jan 28 01:17:54.827: INFO: Waiting up to 5m0s for pod "pod-configmaps-a8597f5f-50d4-495e-b3e4-af683a5c98be" in namespace "secrets-4750" to be "Succeeded or Failed"
    Jan 28 01:17:54.838: INFO: Pod "pod-configmaps-a8597f5f-50d4-495e-b3e4-af683a5c98be": Phase="Pending", Reason="", readiness=false. Elapsed: 10.383264ms
    Jan 28 01:17:56.851: INFO: Pod "pod-configmaps-a8597f5f-50d4-495e-b3e4-af683a5c98be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023914011s
    Jan 28 01:17:58.850: INFO: Pod "pod-configmaps-a8597f5f-50d4-495e-b3e4-af683a5c98be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02244009s
    Jan 28 01:18:00.850: INFO: Pod "pod-configmaps-a8597f5f-50d4-495e-b3e4-af683a5c98be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0228452s
    STEP: Saw pod success 01/28/23 01:18:00.85
    Jan 28 01:18:00.851: INFO: Pod "pod-configmaps-a8597f5f-50d4-495e-b3e4-af683a5c98be" satisfied condition "Succeeded or Failed"
    Jan 28 01:18:00.861: INFO: Trying to get logs from node 10.9.20.126 pod pod-configmaps-a8597f5f-50d4-495e-b3e4-af683a5c98be container env-test: <nil>
    STEP: delete the pod 01/28/23 01:18:00.891
    Jan 28 01:18:00.923: INFO: Waiting for pod pod-configmaps-a8597f5f-50d4-495e-b3e4-af683a5c98be to disappear
    Jan 28 01:18:00.934: INFO: Pod pod-configmaps-a8597f5f-50d4-495e-b3e4-af683a5c98be no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 28 01:18:00.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4750" for this suite. 01/28/23 01:18:00.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:18:00.977
Jan 28 01:18:00.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename webhook 01/28/23 01:18:00.98
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:18:01.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:18:01.029
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/28/23 01:18:01.071
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:18:01.707
STEP: Deploying the webhook pod 01/28/23 01:18:01.729
STEP: Wait for the deployment to be ready 01/28/23 01:18:01.761
Jan 28 01:18:01.835: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 1, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:18:03.848: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/28/23 01:18:05.846
STEP: Verifying the service has paired with the endpoint 01/28/23 01:18:05.879
Jan 28 01:18:06.880: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/28/23 01:18:06.89
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/28/23 01:18:06.962
STEP: Creating a dummy validating-webhook-configuration object 01/28/23 01:18:07.025
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/28/23 01:18:07.048
STEP: Creating a dummy mutating-webhook-configuration object 01/28/23 01:18:07.064
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/28/23 01:18:07.087
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:18:07.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1267" for this suite. 01/28/23 01:18:07.154
STEP: Destroying namespace "webhook-1267-markers" for this suite. 01/28/23 01:18:07.17
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":201,"skipped":3615,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.312 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:18:00.977
    Jan 28 01:18:00.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename webhook 01/28/23 01:18:00.98
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:18:01.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:18:01.029
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/28/23 01:18:01.071
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:18:01.707
    STEP: Deploying the webhook pod 01/28/23 01:18:01.729
    STEP: Wait for the deployment to be ready 01/28/23 01:18:01.761
    Jan 28 01:18:01.835: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 1, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:18:03.848: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/28/23 01:18:05.846
    STEP: Verifying the service has paired with the endpoint 01/28/23 01:18:05.879
    Jan 28 01:18:06.880: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/28/23 01:18:06.89
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/28/23 01:18:06.962
    STEP: Creating a dummy validating-webhook-configuration object 01/28/23 01:18:07.025
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/28/23 01:18:07.048
    STEP: Creating a dummy mutating-webhook-configuration object 01/28/23 01:18:07.064
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/28/23 01:18:07.087
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:18:07.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1267" for this suite. 01/28/23 01:18:07.154
    STEP: Destroying namespace "webhook-1267-markers" for this suite. 01/28/23 01:18:07.17
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:18:07.296
Jan 28 01:18:07.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename resourcequota 01/28/23 01:18:07.298
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:18:07.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:18:07.338
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 01/28/23 01:18:24.356
STEP: Creating a ResourceQuota 01/28/23 01:18:29.366
STEP: Ensuring resource quota status is calculated 01/28/23 01:18:29.399
STEP: Creating a ConfigMap 01/28/23 01:18:31.41
STEP: Ensuring resource quota status captures configMap creation 01/28/23 01:18:31.458
STEP: Deleting a ConfigMap 01/28/23 01:18:33.476
STEP: Ensuring resource quota status released usage 01/28/23 01:18:33.503
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 28 01:18:35.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5818" for this suite. 01/28/23 01:18:35.525
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":202,"skipped":3628,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.247 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:18:07.296
    Jan 28 01:18:07.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename resourcequota 01/28/23 01:18:07.298
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:18:07.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:18:07.338
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 01/28/23 01:18:24.356
    STEP: Creating a ResourceQuota 01/28/23 01:18:29.366
    STEP: Ensuring resource quota status is calculated 01/28/23 01:18:29.399
    STEP: Creating a ConfigMap 01/28/23 01:18:31.41
    STEP: Ensuring resource quota status captures configMap creation 01/28/23 01:18:31.458
    STEP: Deleting a ConfigMap 01/28/23 01:18:33.476
    STEP: Ensuring resource quota status released usage 01/28/23 01:18:33.503
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 28 01:18:35.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5818" for this suite. 01/28/23 01:18:35.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:18:35.546
Jan 28 01:18:35.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename lease-test 01/28/23 01:18:35.548
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:18:35.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:18:35.602
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Jan 28 01:18:35.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-8269" for this suite. 01/28/23 01:18:35.81
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":203,"skipped":3642,"failed":0}
------------------------------
â€¢ [0.281 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:18:35.546
    Jan 28 01:18:35.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename lease-test 01/28/23 01:18:35.548
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:18:35.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:18:35.602
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Jan 28 01:18:35.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-8269" for this suite. 01/28/23 01:18:35.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:18:35.828
Jan 28 01:18:35.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename svcaccounts 01/28/23 01:18:35.831
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:18:35.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:18:35.879
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  01/28/23 01:18:35.911
Jan 28 01:18:35.932: INFO: Waiting up to 5m0s for pod "test-pod-4b27687a-6dad-4533-bba6-3859d736dc4d" in namespace "svcaccounts-6970" to be "Succeeded or Failed"
Jan 28 01:18:35.941: INFO: Pod "test-pod-4b27687a-6dad-4533-bba6-3859d736dc4d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.965829ms
Jan 28 01:18:37.951: INFO: Pod "test-pod-4b27687a-6dad-4533-bba6-3859d736dc4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019197329s
Jan 28 01:18:39.951: INFO: Pod "test-pod-4b27687a-6dad-4533-bba6-3859d736dc4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019001678s
Jan 28 01:18:41.954: INFO: Pod "test-pod-4b27687a-6dad-4533-bba6-3859d736dc4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022252537s
STEP: Saw pod success 01/28/23 01:18:41.954
Jan 28 01:18:41.955: INFO: Pod "test-pod-4b27687a-6dad-4533-bba6-3859d736dc4d" satisfied condition "Succeeded or Failed"
Jan 28 01:18:41.965: INFO: Trying to get logs from node 10.9.20.126 pod test-pod-4b27687a-6dad-4533-bba6-3859d736dc4d container agnhost-container: <nil>
STEP: delete the pod 01/28/23 01:18:42.038
Jan 28 01:18:42.063: INFO: Waiting for pod test-pod-4b27687a-6dad-4533-bba6-3859d736dc4d to disappear
Jan 28 01:18:42.072: INFO: Pod test-pod-4b27687a-6dad-4533-bba6-3859d736dc4d no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 28 01:18:42.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6970" for this suite. 01/28/23 01:18:42.087
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":204,"skipped":3647,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.276 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:18:35.828
    Jan 28 01:18:35.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename svcaccounts 01/28/23 01:18:35.831
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:18:35.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:18:35.879
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  01/28/23 01:18:35.911
    Jan 28 01:18:35.932: INFO: Waiting up to 5m0s for pod "test-pod-4b27687a-6dad-4533-bba6-3859d736dc4d" in namespace "svcaccounts-6970" to be "Succeeded or Failed"
    Jan 28 01:18:35.941: INFO: Pod "test-pod-4b27687a-6dad-4533-bba6-3859d736dc4d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.965829ms
    Jan 28 01:18:37.951: INFO: Pod "test-pod-4b27687a-6dad-4533-bba6-3859d736dc4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019197329s
    Jan 28 01:18:39.951: INFO: Pod "test-pod-4b27687a-6dad-4533-bba6-3859d736dc4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019001678s
    Jan 28 01:18:41.954: INFO: Pod "test-pod-4b27687a-6dad-4533-bba6-3859d736dc4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022252537s
    STEP: Saw pod success 01/28/23 01:18:41.954
    Jan 28 01:18:41.955: INFO: Pod "test-pod-4b27687a-6dad-4533-bba6-3859d736dc4d" satisfied condition "Succeeded or Failed"
    Jan 28 01:18:41.965: INFO: Trying to get logs from node 10.9.20.126 pod test-pod-4b27687a-6dad-4533-bba6-3859d736dc4d container agnhost-container: <nil>
    STEP: delete the pod 01/28/23 01:18:42.038
    Jan 28 01:18:42.063: INFO: Waiting for pod test-pod-4b27687a-6dad-4533-bba6-3859d736dc4d to disappear
    Jan 28 01:18:42.072: INFO: Pod test-pod-4b27687a-6dad-4533-bba6-3859d736dc4d no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 28 01:18:42.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6970" for this suite. 01/28/23 01:18:42.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:18:42.111
Jan 28 01:18:42.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename aggregator 01/28/23 01:18:42.114
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:18:42.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:18:42.176
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan 28 01:18:42.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/28/23 01:18:42.187
Jan 28 01:18:42.871: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 28 01:18:45.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:18:47.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:18:49.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:18:51.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:18:53.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:18:55.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:18:57.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:18:59.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:19:01.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:19:03.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:19:05.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:19:07.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:19:09.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:19:11.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:19:13.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:19:15.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:19:17.312: INFO: Waited 259.235233ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/28/23 01:19:17.622
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/28/23 01:19:17.632
STEP: List APIServices 01/28/23 01:19:17.655
Jan 28 01:19:17.708: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Jan 28 01:19:18.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5761" for this suite. 01/28/23 01:19:18.183
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":205,"skipped":3659,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.087 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:18:42.111
    Jan 28 01:18:42.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename aggregator 01/28/23 01:18:42.114
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:18:42.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:18:42.176
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan 28 01:18:42.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/28/23 01:18:42.187
    Jan 28 01:18:42.871: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jan 28 01:18:45.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:18:47.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:18:49.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:18:51.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:18:53.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:18:55.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:18:57.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:18:59.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:19:01.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:19:03.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:19:05.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:19:07.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:19:09.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:19:11.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:19:13.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:19:15.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 18, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:19:17.312: INFO: Waited 259.235233ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/28/23 01:19:17.622
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/28/23 01:19:17.632
    STEP: List APIServices 01/28/23 01:19:17.655
    Jan 28 01:19:17.708: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Jan 28 01:19:18.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-5761" for this suite. 01/28/23 01:19:18.183
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:19:18.2
Jan 28 01:19:18.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubectl 01/28/23 01:19:18.203
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:19:18.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:19:18.311
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 01/28/23 01:19:18.318
Jan 28 01:19:18.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6634 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 28 01:19:18.461: INFO: stderr: ""
Jan 28 01:19:18.461: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 01/28/23 01:19:18.461
Jan 28 01:19:18.462: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 28 01:19:18.462: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6634" to be "running and ready, or succeeded"
Jan 28 01:19:18.473: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.851987ms
Jan 28 01:19:18.473: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.9.20.72' to be 'Running' but was 'Pending'
Jan 28 01:19:20.482: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02015776s
Jan 28 01:19:20.482: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.9.20.72' to be 'Running' but was 'Pending'
Jan 28 01:19:22.483: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.021158887s
Jan 28 01:19:22.483: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 28 01:19:22.483: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/28/23 01:19:22.483
Jan 28 01:19:22.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6634 logs logs-generator logs-generator'
Jan 28 01:19:22.698: INFO: stderr: ""
Jan 28 01:19:22.698: INFO: stdout: "I0128 01:19:20.141570       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/tj29 468\nI0128 01:19:20.342199       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/9tlm 544\nI0128 01:19:20.541803       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/2rp 453\nI0128 01:19:20.742226       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/mxm 427\nI0128 01:19:20.942002       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/cnw 286\nI0128 01:19:21.142569       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/fszl 497\nI0128 01:19:21.342112       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/blsr 444\nI0128 01:19:21.545213       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/qwpb 476\nI0128 01:19:21.742054       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/ld6v 502\nI0128 01:19:21.941925       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/n92 447\nI0128 01:19:22.142363       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/q2w 459\nI0128 01:19:22.341770       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/rw7r 399\nI0128 01:19:22.542295       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/6jw6 306\n"
STEP: limiting log lines 01/28/23 01:19:22.698
Jan 28 01:19:22.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6634 logs logs-generator logs-generator --tail=1'
Jan 28 01:19:22.954: INFO: stderr: ""
Jan 28 01:19:22.954: INFO: stdout: "I0128 01:19:22.942180       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/cc2k 303\n"
Jan 28 01:19:22.954: INFO: got output "I0128 01:19:22.942180       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/cc2k 303\n"
STEP: limiting log bytes 01/28/23 01:19:22.954
Jan 28 01:19:22.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6634 logs logs-generator logs-generator --limit-bytes=1'
Jan 28 01:19:23.181: INFO: stderr: ""
Jan 28 01:19:23.181: INFO: stdout: "I"
Jan 28 01:19:23.181: INFO: got output "I"
STEP: exposing timestamps 01/28/23 01:19:23.181
Jan 28 01:19:23.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6634 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 28 01:19:23.363: INFO: stderr: ""
Jan 28 01:19:23.364: INFO: stdout: "2023-01-28T01:19:23.342268220Z I0128 01:19:23.342030       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/zhn6 368\n"
Jan 28 01:19:23.364: INFO: got output "2023-01-28T01:19:23.342268220Z I0128 01:19:23.342030       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/zhn6 368\n"
STEP: restricting to a time range 01/28/23 01:19:23.364
Jan 28 01:19:25.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6634 logs logs-generator logs-generator --since=1s'
Jan 28 01:19:26.026: INFO: stderr: ""
Jan 28 01:19:26.026: INFO: stdout: "I0128 01:19:25.142154       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/cvqx 250\nI0128 01:19:25.341621       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/4xn 331\nI0128 01:19:25.542186       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/j45 494\nI0128 01:19:25.741615       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/w5g9 412\nI0128 01:19:25.942219       1 logs_generator.go:76] 29 POST /api/v1/namespaces/kube-system/pods/9flv 479\n"
Jan 28 01:19:26.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6634 logs logs-generator logs-generator --since=24h'
Jan 28 01:19:26.190: INFO: stderr: ""
Jan 28 01:19:26.190: INFO: stdout: "I0128 01:19:20.141570       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/tj29 468\nI0128 01:19:20.342199       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/9tlm 544\nI0128 01:19:20.541803       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/2rp 453\nI0128 01:19:20.742226       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/mxm 427\nI0128 01:19:20.942002       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/cnw 286\nI0128 01:19:21.142569       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/fszl 497\nI0128 01:19:21.342112       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/blsr 444\nI0128 01:19:21.545213       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/qwpb 476\nI0128 01:19:21.742054       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/ld6v 502\nI0128 01:19:21.941925       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/n92 447\nI0128 01:19:22.142363       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/q2w 459\nI0128 01:19:22.341770       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/rw7r 399\nI0128 01:19:22.542295       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/6jw6 306\nI0128 01:19:22.741735       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/x75 594\nI0128 01:19:22.942180       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/cc2k 303\nI0128 01:19:23.142707       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/8ldj 589\nI0128 01:19:23.342030       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/zhn6 368\nI0128 01:19:23.542719       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/k788 233\nI0128 01:19:23.742250       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/9bf 470\nI0128 01:19:23.941825       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/bbzh 419\nI0128 01:19:24.142459       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/mp6f 319\nI0128 01:19:24.341779       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/h7bn 460\nI0128 01:19:24.542341       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/7lr 268\nI0128 01:19:24.741809       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/rlp 523\nI0128 01:19:24.941595       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/v6q 306\nI0128 01:19:25.142154       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/cvqx 250\nI0128 01:19:25.341621       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/4xn 331\nI0128 01:19:25.542186       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/j45 494\nI0128 01:19:25.741615       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/w5g9 412\nI0128 01:19:25.942219       1 logs_generator.go:76] 29 POST /api/v1/namespaces/kube-system/pods/9flv 479\nI0128 01:19:26.141738       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/default/pods/m52p 246\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Jan 28 01:19:26.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6634 delete pod logs-generator'
Jan 28 01:19:27.691: INFO: stderr: ""
Jan 28 01:19:27.691: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 28 01:19:27.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6634" for this suite. 01/28/23 01:19:27.704
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":206,"skipped":3662,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.572 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:19:18.2
    Jan 28 01:19:18.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubectl 01/28/23 01:19:18.203
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:19:18.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:19:18.311
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 01/28/23 01:19:18.318
    Jan 28 01:19:18.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6634 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan 28 01:19:18.461: INFO: stderr: ""
    Jan 28 01:19:18.461: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 01/28/23 01:19:18.461
    Jan 28 01:19:18.462: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan 28 01:19:18.462: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6634" to be "running and ready, or succeeded"
    Jan 28 01:19:18.473: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.851987ms
    Jan 28 01:19:18.473: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.9.20.72' to be 'Running' but was 'Pending'
    Jan 28 01:19:20.482: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02015776s
    Jan 28 01:19:20.482: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.9.20.72' to be 'Running' but was 'Pending'
    Jan 28 01:19:22.483: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.021158887s
    Jan 28 01:19:22.483: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan 28 01:19:22.483: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/28/23 01:19:22.483
    Jan 28 01:19:22.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6634 logs logs-generator logs-generator'
    Jan 28 01:19:22.698: INFO: stderr: ""
    Jan 28 01:19:22.698: INFO: stdout: "I0128 01:19:20.141570       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/tj29 468\nI0128 01:19:20.342199       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/9tlm 544\nI0128 01:19:20.541803       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/2rp 453\nI0128 01:19:20.742226       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/mxm 427\nI0128 01:19:20.942002       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/cnw 286\nI0128 01:19:21.142569       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/fszl 497\nI0128 01:19:21.342112       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/blsr 444\nI0128 01:19:21.545213       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/qwpb 476\nI0128 01:19:21.742054       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/ld6v 502\nI0128 01:19:21.941925       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/n92 447\nI0128 01:19:22.142363       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/q2w 459\nI0128 01:19:22.341770       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/rw7r 399\nI0128 01:19:22.542295       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/6jw6 306\n"
    STEP: limiting log lines 01/28/23 01:19:22.698
    Jan 28 01:19:22.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6634 logs logs-generator logs-generator --tail=1'
    Jan 28 01:19:22.954: INFO: stderr: ""
    Jan 28 01:19:22.954: INFO: stdout: "I0128 01:19:22.942180       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/cc2k 303\n"
    Jan 28 01:19:22.954: INFO: got output "I0128 01:19:22.942180       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/cc2k 303\n"
    STEP: limiting log bytes 01/28/23 01:19:22.954
    Jan 28 01:19:22.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6634 logs logs-generator logs-generator --limit-bytes=1'
    Jan 28 01:19:23.181: INFO: stderr: ""
    Jan 28 01:19:23.181: INFO: stdout: "I"
    Jan 28 01:19:23.181: INFO: got output "I"
    STEP: exposing timestamps 01/28/23 01:19:23.181
    Jan 28 01:19:23.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6634 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan 28 01:19:23.363: INFO: stderr: ""
    Jan 28 01:19:23.364: INFO: stdout: "2023-01-28T01:19:23.342268220Z I0128 01:19:23.342030       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/zhn6 368\n"
    Jan 28 01:19:23.364: INFO: got output "2023-01-28T01:19:23.342268220Z I0128 01:19:23.342030       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/zhn6 368\n"
    STEP: restricting to a time range 01/28/23 01:19:23.364
    Jan 28 01:19:25.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6634 logs logs-generator logs-generator --since=1s'
    Jan 28 01:19:26.026: INFO: stderr: ""
    Jan 28 01:19:26.026: INFO: stdout: "I0128 01:19:25.142154       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/cvqx 250\nI0128 01:19:25.341621       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/4xn 331\nI0128 01:19:25.542186       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/j45 494\nI0128 01:19:25.741615       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/w5g9 412\nI0128 01:19:25.942219       1 logs_generator.go:76] 29 POST /api/v1/namespaces/kube-system/pods/9flv 479\n"
    Jan 28 01:19:26.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6634 logs logs-generator logs-generator --since=24h'
    Jan 28 01:19:26.190: INFO: stderr: ""
    Jan 28 01:19:26.190: INFO: stdout: "I0128 01:19:20.141570       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/tj29 468\nI0128 01:19:20.342199       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/9tlm 544\nI0128 01:19:20.541803       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/2rp 453\nI0128 01:19:20.742226       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/mxm 427\nI0128 01:19:20.942002       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/cnw 286\nI0128 01:19:21.142569       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/fszl 497\nI0128 01:19:21.342112       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/blsr 444\nI0128 01:19:21.545213       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/qwpb 476\nI0128 01:19:21.742054       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/ld6v 502\nI0128 01:19:21.941925       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/n92 447\nI0128 01:19:22.142363       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/q2w 459\nI0128 01:19:22.341770       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/rw7r 399\nI0128 01:19:22.542295       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/6jw6 306\nI0128 01:19:22.741735       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/x75 594\nI0128 01:19:22.942180       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/cc2k 303\nI0128 01:19:23.142707       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/8ldj 589\nI0128 01:19:23.342030       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/zhn6 368\nI0128 01:19:23.542719       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/k788 233\nI0128 01:19:23.742250       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/9bf 470\nI0128 01:19:23.941825       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/bbzh 419\nI0128 01:19:24.142459       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/mp6f 319\nI0128 01:19:24.341779       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/h7bn 460\nI0128 01:19:24.542341       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/7lr 268\nI0128 01:19:24.741809       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/rlp 523\nI0128 01:19:24.941595       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/v6q 306\nI0128 01:19:25.142154       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/cvqx 250\nI0128 01:19:25.341621       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/4xn 331\nI0128 01:19:25.542186       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/j45 494\nI0128 01:19:25.741615       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/w5g9 412\nI0128 01:19:25.942219       1 logs_generator.go:76] 29 POST /api/v1/namespaces/kube-system/pods/9flv 479\nI0128 01:19:26.141738       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/default/pods/m52p 246\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Jan 28 01:19:26.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-6634 delete pod logs-generator'
    Jan 28 01:19:27.691: INFO: stderr: ""
    Jan 28 01:19:27.691: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 28 01:19:27.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6634" for this suite. 01/28/23 01:19:27.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:19:27.774
Jan 28 01:19:27.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename crd-publish-openapi 01/28/23 01:19:27.777
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:19:27.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:19:27.823
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Jan 28 01:19:27.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/28/23 01:19:30.874
Jan 28 01:19:30.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 --namespace=crd-publish-openapi-4597 create -f -'
Jan 28 01:19:31.779: INFO: stderr: ""
Jan 28 01:19:31.780: INFO: stdout: "e2e-test-crd-publish-openapi-3259-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 28 01:19:31.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 --namespace=crd-publish-openapi-4597 delete e2e-test-crd-publish-openapi-3259-crds test-foo'
Jan 28 01:19:31.993: INFO: stderr: ""
Jan 28 01:19:31.993: INFO: stdout: "e2e-test-crd-publish-openapi-3259-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 28 01:19:31.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 --namespace=crd-publish-openapi-4597 apply -f -'
Jan 28 01:19:32.740: INFO: stderr: ""
Jan 28 01:19:32.740: INFO: stdout: "e2e-test-crd-publish-openapi-3259-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 28 01:19:32.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 --namespace=crd-publish-openapi-4597 delete e2e-test-crd-publish-openapi-3259-crds test-foo'
Jan 28 01:19:32.882: INFO: stderr: ""
Jan 28 01:19:32.882: INFO: stdout: "e2e-test-crd-publish-openapi-3259-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/28/23 01:19:32.882
Jan 28 01:19:32.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 --namespace=crd-publish-openapi-4597 create -f -'
Jan 28 01:19:33.192: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/28/23 01:19:33.192
Jan 28 01:19:33.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 --namespace=crd-publish-openapi-4597 create -f -'
Jan 28 01:19:33.463: INFO: rc: 1
Jan 28 01:19:33.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 --namespace=crd-publish-openapi-4597 apply -f -'
Jan 28 01:19:34.167: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/28/23 01:19:34.167
Jan 28 01:19:34.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 --namespace=crd-publish-openapi-4597 create -f -'
Jan 28 01:19:34.429: INFO: rc: 1
Jan 28 01:19:34.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 --namespace=crd-publish-openapi-4597 apply -f -'
Jan 28 01:19:34.734: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/28/23 01:19:34.734
Jan 28 01:19:34.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 explain e2e-test-crd-publish-openapi-3259-crds'
Jan 28 01:19:35.042: INFO: stderr: ""
Jan 28 01:19:35.042: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3259-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/28/23 01:19:35.042
Jan 28 01:19:35.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 explain e2e-test-crd-publish-openapi-3259-crds.metadata'
Jan 28 01:19:35.351: INFO: stderr: ""
Jan 28 01:19:35.351: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3259-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 28 01:19:35.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 explain e2e-test-crd-publish-openapi-3259-crds.spec'
Jan 28 01:19:35.622: INFO: stderr: ""
Jan 28 01:19:35.622: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3259-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 28 01:19:35.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 explain e2e-test-crd-publish-openapi-3259-crds.spec.bars'
Jan 28 01:19:35.914: INFO: stderr: ""
Jan 28 01:19:35.914: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3259-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/28/23 01:19:35.914
Jan 28 01:19:35.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 explain e2e-test-crd-publish-openapi-3259-crds.spec.bars2'
Jan 28 01:19:36.180: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:19:40.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4597" for this suite. 01/28/23 01:19:40.081
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":207,"skipped":3684,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.323 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:19:27.774
    Jan 28 01:19:27.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename crd-publish-openapi 01/28/23 01:19:27.777
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:19:27.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:19:27.823
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Jan 28 01:19:27.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/28/23 01:19:30.874
    Jan 28 01:19:30.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 --namespace=crd-publish-openapi-4597 create -f -'
    Jan 28 01:19:31.779: INFO: stderr: ""
    Jan 28 01:19:31.780: INFO: stdout: "e2e-test-crd-publish-openapi-3259-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 28 01:19:31.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 --namespace=crd-publish-openapi-4597 delete e2e-test-crd-publish-openapi-3259-crds test-foo'
    Jan 28 01:19:31.993: INFO: stderr: ""
    Jan 28 01:19:31.993: INFO: stdout: "e2e-test-crd-publish-openapi-3259-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan 28 01:19:31.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 --namespace=crd-publish-openapi-4597 apply -f -'
    Jan 28 01:19:32.740: INFO: stderr: ""
    Jan 28 01:19:32.740: INFO: stdout: "e2e-test-crd-publish-openapi-3259-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 28 01:19:32.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 --namespace=crd-publish-openapi-4597 delete e2e-test-crd-publish-openapi-3259-crds test-foo'
    Jan 28 01:19:32.882: INFO: stderr: ""
    Jan 28 01:19:32.882: INFO: stdout: "e2e-test-crd-publish-openapi-3259-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/28/23 01:19:32.882
    Jan 28 01:19:32.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 --namespace=crd-publish-openapi-4597 create -f -'
    Jan 28 01:19:33.192: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/28/23 01:19:33.192
    Jan 28 01:19:33.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 --namespace=crd-publish-openapi-4597 create -f -'
    Jan 28 01:19:33.463: INFO: rc: 1
    Jan 28 01:19:33.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 --namespace=crd-publish-openapi-4597 apply -f -'
    Jan 28 01:19:34.167: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/28/23 01:19:34.167
    Jan 28 01:19:34.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 --namespace=crd-publish-openapi-4597 create -f -'
    Jan 28 01:19:34.429: INFO: rc: 1
    Jan 28 01:19:34.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 --namespace=crd-publish-openapi-4597 apply -f -'
    Jan 28 01:19:34.734: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/28/23 01:19:34.734
    Jan 28 01:19:34.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 explain e2e-test-crd-publish-openapi-3259-crds'
    Jan 28 01:19:35.042: INFO: stderr: ""
    Jan 28 01:19:35.042: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3259-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/28/23 01:19:35.042
    Jan 28 01:19:35.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 explain e2e-test-crd-publish-openapi-3259-crds.metadata'
    Jan 28 01:19:35.351: INFO: stderr: ""
    Jan 28 01:19:35.351: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3259-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan 28 01:19:35.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 explain e2e-test-crd-publish-openapi-3259-crds.spec'
    Jan 28 01:19:35.622: INFO: stderr: ""
    Jan 28 01:19:35.622: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3259-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan 28 01:19:35.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 explain e2e-test-crd-publish-openapi-3259-crds.spec.bars'
    Jan 28 01:19:35.914: INFO: stderr: ""
    Jan 28 01:19:35.914: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3259-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/28/23 01:19:35.914
    Jan 28 01:19:35.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=crd-publish-openapi-4597 explain e2e-test-crd-publish-openapi-3259-crds.spec.bars2'
    Jan 28 01:19:36.180: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:19:40.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4597" for this suite. 01/28/23 01:19:40.081
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:19:40.1
Jan 28 01:19:40.101: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename pods 01/28/23 01:19:40.103
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:19:40.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:19:40.197
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Jan 28 01:19:40.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: creating the pod 01/28/23 01:19:40.211
STEP: submitting the pod to kubernetes 01/28/23 01:19:40.212
Jan 28 01:19:40.236: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-77f35e92-85e3-4683-9a81-cf1b38587604" in namespace "pods-724" to be "running and ready"
Jan 28 01:19:40.247: INFO: Pod "pod-logs-websocket-77f35e92-85e3-4683-9a81-cf1b38587604": Phase="Pending", Reason="", readiness=false. Elapsed: 10.728577ms
Jan 28 01:19:40.247: INFO: The phase of Pod pod-logs-websocket-77f35e92-85e3-4683-9a81-cf1b38587604 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:19:42.260: INFO: Pod "pod-logs-websocket-77f35e92-85e3-4683-9a81-cf1b38587604": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024161695s
Jan 28 01:19:42.260: INFO: The phase of Pod pod-logs-websocket-77f35e92-85e3-4683-9a81-cf1b38587604 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:19:44.262: INFO: Pod "pod-logs-websocket-77f35e92-85e3-4683-9a81-cf1b38587604": Phase="Running", Reason="", readiness=true. Elapsed: 4.02542093s
Jan 28 01:19:44.262: INFO: The phase of Pod pod-logs-websocket-77f35e92-85e3-4683-9a81-cf1b38587604 is Running (Ready = true)
Jan 28 01:19:44.262: INFO: Pod "pod-logs-websocket-77f35e92-85e3-4683-9a81-cf1b38587604" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 28 01:19:44.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-724" for this suite. 01/28/23 01:19:44.422
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":208,"skipped":3685,"failed":0}
------------------------------
â€¢ [4.344 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:19:40.1
    Jan 28 01:19:40.101: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename pods 01/28/23 01:19:40.103
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:19:40.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:19:40.197
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Jan 28 01:19:40.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: creating the pod 01/28/23 01:19:40.211
    STEP: submitting the pod to kubernetes 01/28/23 01:19:40.212
    Jan 28 01:19:40.236: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-77f35e92-85e3-4683-9a81-cf1b38587604" in namespace "pods-724" to be "running and ready"
    Jan 28 01:19:40.247: INFO: Pod "pod-logs-websocket-77f35e92-85e3-4683-9a81-cf1b38587604": Phase="Pending", Reason="", readiness=false. Elapsed: 10.728577ms
    Jan 28 01:19:40.247: INFO: The phase of Pod pod-logs-websocket-77f35e92-85e3-4683-9a81-cf1b38587604 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:19:42.260: INFO: Pod "pod-logs-websocket-77f35e92-85e3-4683-9a81-cf1b38587604": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024161695s
    Jan 28 01:19:42.260: INFO: The phase of Pod pod-logs-websocket-77f35e92-85e3-4683-9a81-cf1b38587604 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:19:44.262: INFO: Pod "pod-logs-websocket-77f35e92-85e3-4683-9a81-cf1b38587604": Phase="Running", Reason="", readiness=true. Elapsed: 4.02542093s
    Jan 28 01:19:44.262: INFO: The phase of Pod pod-logs-websocket-77f35e92-85e3-4683-9a81-cf1b38587604 is Running (Ready = true)
    Jan 28 01:19:44.262: INFO: Pod "pod-logs-websocket-77f35e92-85e3-4683-9a81-cf1b38587604" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 28 01:19:44.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-724" for this suite. 01/28/23 01:19:44.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:19:44.448
Jan 28 01:19:44.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename container-probe 01/28/23 01:19:44.45
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:19:44.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:19:44.52
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-57980b02-ef12-4cce-905c-fcf122c9ca5f in namespace container-probe-8054 01/28/23 01:19:44.534
Jan 28 01:19:44.606: INFO: Waiting up to 5m0s for pod "liveness-57980b02-ef12-4cce-905c-fcf122c9ca5f" in namespace "container-probe-8054" to be "not pending"
Jan 28 01:19:44.628: INFO: Pod "liveness-57980b02-ef12-4cce-905c-fcf122c9ca5f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.386182ms
Jan 28 01:19:46.643: INFO: Pod "liveness-57980b02-ef12-4cce-905c-fcf122c9ca5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037086144s
Jan 28 01:19:48.648: INFO: Pod "liveness-57980b02-ef12-4cce-905c-fcf122c9ca5f": Phase="Running", Reason="", readiness=true. Elapsed: 4.041831458s
Jan 28 01:19:48.648: INFO: Pod "liveness-57980b02-ef12-4cce-905c-fcf122c9ca5f" satisfied condition "not pending"
Jan 28 01:19:48.648: INFO: Started pod liveness-57980b02-ef12-4cce-905c-fcf122c9ca5f in namespace container-probe-8054
STEP: checking the pod's current state and verifying that restartCount is present 01/28/23 01:19:48.648
Jan 28 01:19:48.662: INFO: Initial restart count of pod liveness-57980b02-ef12-4cce-905c-fcf122c9ca5f is 0
Jan 28 01:20:06.795: INFO: Restart count of pod container-probe-8054/liveness-57980b02-ef12-4cce-905c-fcf122c9ca5f is now 1 (18.13322146s elapsed)
STEP: deleting the pod 01/28/23 01:20:06.795
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 28 01:20:06.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8054" for this suite. 01/28/23 01:20:06.844
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":209,"skipped":3705,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.416 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:19:44.448
    Jan 28 01:19:44.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename container-probe 01/28/23 01:19:44.45
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:19:44.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:19:44.52
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-57980b02-ef12-4cce-905c-fcf122c9ca5f in namespace container-probe-8054 01/28/23 01:19:44.534
    Jan 28 01:19:44.606: INFO: Waiting up to 5m0s for pod "liveness-57980b02-ef12-4cce-905c-fcf122c9ca5f" in namespace "container-probe-8054" to be "not pending"
    Jan 28 01:19:44.628: INFO: Pod "liveness-57980b02-ef12-4cce-905c-fcf122c9ca5f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.386182ms
    Jan 28 01:19:46.643: INFO: Pod "liveness-57980b02-ef12-4cce-905c-fcf122c9ca5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037086144s
    Jan 28 01:19:48.648: INFO: Pod "liveness-57980b02-ef12-4cce-905c-fcf122c9ca5f": Phase="Running", Reason="", readiness=true. Elapsed: 4.041831458s
    Jan 28 01:19:48.648: INFO: Pod "liveness-57980b02-ef12-4cce-905c-fcf122c9ca5f" satisfied condition "not pending"
    Jan 28 01:19:48.648: INFO: Started pod liveness-57980b02-ef12-4cce-905c-fcf122c9ca5f in namespace container-probe-8054
    STEP: checking the pod's current state and verifying that restartCount is present 01/28/23 01:19:48.648
    Jan 28 01:19:48.662: INFO: Initial restart count of pod liveness-57980b02-ef12-4cce-905c-fcf122c9ca5f is 0
    Jan 28 01:20:06.795: INFO: Restart count of pod container-probe-8054/liveness-57980b02-ef12-4cce-905c-fcf122c9ca5f is now 1 (18.13322146s elapsed)
    STEP: deleting the pod 01/28/23 01:20:06.795
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 28 01:20:06.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8054" for this suite. 01/28/23 01:20:06.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:20:06.867
Jan 28 01:20:06.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename sched-pred 01/28/23 01:20:06.868
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:20:06.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:20:06.918
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 28 01:20:06.930: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 28 01:20:06.965: INFO: Waiting for terminating namespaces to be deleted...
Jan 28 01:20:06.981: INFO: 
Logging pods the apiserver thinks is on node 10.9.20.126 before test
Jan 28 01:20:07.008: INFO: calico-node-xl9f8 from kube-system started at 2023-01-27 22:05:01 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.008: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 01:20:07.008: INFO: calico-typha-677688fdc5-twxlr from kube-system started at 2023-01-27 22:05:32 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.008: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 01:20:07.008: INFO: coredns-6754846f95-9ck4t from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.008: INFO: 	Container coredns ready: true, restart count 0
Jan 28 01:20:07.009: INFO: ibm-keepalived-watcher-b94td from kube-system started at 2023-01-27 22:05:01 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.009: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 01:20:07.009: INFO: ibm-master-proxy-static-10.9.20.126 from kube-system started at 2023-01-27 22:04:49 +0000 UTC (2 container statuses recorded)
Jan 28 01:20:07.009: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 01:20:07.009: INFO: 	Container pause ready: true, restart count 0
Jan 28 01:20:07.009: INFO: ibmcloud-block-storage-driver-lw66t from kube-system started at 2023-01-27 22:05:09 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.009: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 01:20:07.010: INFO: konnectivity-agent-65zgm from kube-system started at 2023-01-27 22:15:24 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.010: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 01:20:07.010: INFO: metrics-server-9cdf87dc6-4ptjv from kube-system started at 2023-01-27 22:51:34 +0000 UTC (3 container statuses recorded)
Jan 28 01:20:07.010: INFO: 	Container config-watcher ready: true, restart count 0
Jan 28 01:20:07.010: INFO: 	Container metrics-server ready: true, restart count 0
Jan 28 01:20:07.010: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 28 01:20:07.010: INFO: public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-vvqqx from kube-system started at 2023-01-27 22:09:22 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.010: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 28 01:20:07.010: INFO: sonobuoy from sonobuoy started at 2023-01-28 00:13:47 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.011: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 28 01:20:07.011: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-qbzm7 from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
Jan 28 01:20:07.011: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 01:20:07.011: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 28 01:20:07.011: INFO: 
Logging pods the apiserver thinks is on node 10.9.20.72 before test
Jan 28 01:20:07.041: INFO: ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-2gg2p from ibm-system started at 2023-01-27 22:09:35 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.041: INFO: 	Container ibm-cloud-provider-ip-163-69-70-149 ready: true, restart count 0
Jan 28 01:20:07.041: INFO: calico-node-4bc72 from kube-system started at 2023-01-27 22:04:57 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.041: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 01:20:07.042: INFO: calico-typha-677688fdc5-6nr4v from kube-system started at 2023-01-27 22:05:47 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.042: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 01:20:07.042: INFO: coredns-6754846f95-b686l from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.042: INFO: 	Container coredns ready: true, restart count 0
Jan 28 01:20:07.042: INFO: ibm-keepalived-watcher-gstl5 from kube-system started at 2023-01-27 22:04:57 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.042: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 01:20:07.043: INFO: ibm-master-proxy-static-10.9.20.72 from kube-system started at 2023-01-27 22:04:44 +0000 UTC (2 container statuses recorded)
Jan 28 01:20:07.043: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 01:20:07.043: INFO: 	Container pause ready: true, restart count 0
Jan 28 01:20:07.043: INFO: ibmcloud-block-storage-driver-sktn2 from kube-system started at 2023-01-27 22:05:04 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.043: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 01:20:07.043: INFO: ingress-cluster-healthcheck-655c49644b-dmz5q from kube-system started at 2023-01-27 22:06:59 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.043: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Jan 28 01:20:07.043: INFO: konnectivity-agent-zbbtb from kube-system started at 2023-01-27 22:15:21 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.043: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 01:20:07.044: INFO: metrics-server-9cdf87dc6-n44dc from kube-system started at 2023-01-27 22:51:34 +0000 UTC (3 container statuses recorded)
Jan 28 01:20:07.044: INFO: 	Container config-watcher ready: true, restart count 0
Jan 28 01:20:07.044: INFO: 	Container metrics-server ready: true, restart count 0
Jan 28 01:20:07.044: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 28 01:20:07.044: INFO: public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-ztg7f from kube-system started at 2023-01-27 22:09:22 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.045: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 28 01:20:07.045: INFO: pod-logs-websocket-77f35e92-85e3-4683-9a81-cf1b38587604 from pods-724 started at 2023-01-28 01:19:40 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.045: INFO: 	Container main ready: true, restart count 0
Jan 28 01:20:07.045: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-tm2wf from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
Jan 28 01:20:07.045: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 01:20:07.045: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 28 01:20:07.045: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-01-27 22:07:40 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.045: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jan 28 01:20:07.045: INFO: 
Logging pods the apiserver thinks is on node 10.9.20.75 before test
Jan 28 01:20:07.097: INFO: ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-9qs6m from ibm-system started at 2023-01-27 22:09:35 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.097: INFO: 	Container ibm-cloud-provider-ip-163-69-70-149 ready: true, restart count 0
Jan 28 01:20:07.097: INFO: calico-kube-controllers-5754dfd4dd-2fl6z from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.097: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 28 01:20:07.097: INFO: calico-node-6pnwh from kube-system started at 2023-01-27 22:04:45 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.097: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 01:20:07.097: INFO: calico-typha-677688fdc5-s2khq from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.097: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 01:20:07.098: INFO: coredns-6754846f95-r75xk from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.098: INFO: 	Container coredns ready: true, restart count 0
Jan 28 01:20:07.098: INFO: coredns-autoscaler-669cf746f6-b9s85 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.098: INFO: 	Container autoscaler ready: true, restart count 0
Jan 28 01:20:07.098: INFO: dashboard-metrics-scraper-c964d5594-76f8h from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.098: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 28 01:20:07.099: INFO: ibm-file-plugin-7dd6c48b68-kn5ff from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.099: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jan 28 01:20:07.099: INFO: ibm-keepalived-watcher-gj6b6 from kube-system started at 2023-01-27 22:04:45 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.099: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 01:20:07.099: INFO: ibm-master-proxy-static-10.9.20.75 from kube-system started at 2023-01-27 22:04:42 +0000 UTC (2 container statuses recorded)
Jan 28 01:20:07.099: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 01:20:07.099: INFO: 	Container pause ready: true, restart count 0
Jan 28 01:20:07.099: INFO: ibm-storage-watcher-746995c8c9-7mmhc from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.099: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jan 28 01:20:07.099: INFO: ibmcloud-block-storage-driver-jgsbr from kube-system started at 2023-01-27 22:04:51 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.100: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 01:20:07.100: INFO: ibmcloud-block-storage-plugin-697cd846b-rglpr from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.100: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jan 28 01:20:07.100: INFO: konnectivity-agent-95p2n from kube-system started at 2023-01-27 22:15:28 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.100: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 01:20:07.100: INFO: kubernetes-dashboard-55c4d56798-7r7j7 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.100: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 28 01:20:07.100: INFO: snapshot-controller-c5c6dddff-25l94 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.100: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 28 01:20:07.101: INFO: snapshot-controller-c5c6dddff-5xzjc from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.101: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 28 01:20:07.101: INFO: snapshot-controller-c5c6dddff-pzr2k from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
Jan 28 01:20:07.101: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 28 01:20:07.102: INFO: sonobuoy-e2e-job-70551213e7cc4fe2 from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
Jan 28 01:20:07.102: INFO: 	Container e2e ready: true, restart count 0
Jan 28 01:20:07.102: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 01:20:07.102: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-jcljc from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
Jan 28 01:20:07.102: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 01:20:07.103: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/28/23 01:20:07.103
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173e544e68b131af], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 01/28/23 01:20:07.183
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 28 01:20:08.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5526" for this suite. 01/28/23 01:20:08.207
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":210,"skipped":3724,"failed":0}
------------------------------
â€¢ [1.361 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:20:06.867
    Jan 28 01:20:06.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename sched-pred 01/28/23 01:20:06.868
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:20:06.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:20:06.918
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 28 01:20:06.930: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 28 01:20:06.965: INFO: Waiting for terminating namespaces to be deleted...
    Jan 28 01:20:06.981: INFO: 
    Logging pods the apiserver thinks is on node 10.9.20.126 before test
    Jan 28 01:20:07.008: INFO: calico-node-xl9f8 from kube-system started at 2023-01-27 22:05:01 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.008: INFO: 	Container calico-node ready: true, restart count 0
    Jan 28 01:20:07.008: INFO: calico-typha-677688fdc5-twxlr from kube-system started at 2023-01-27 22:05:32 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.008: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 28 01:20:07.008: INFO: coredns-6754846f95-9ck4t from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.008: INFO: 	Container coredns ready: true, restart count 0
    Jan 28 01:20:07.009: INFO: ibm-keepalived-watcher-b94td from kube-system started at 2023-01-27 22:05:01 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.009: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 28 01:20:07.009: INFO: ibm-master-proxy-static-10.9.20.126 from kube-system started at 2023-01-27 22:04:49 +0000 UTC (2 container statuses recorded)
    Jan 28 01:20:07.009: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 28 01:20:07.009: INFO: 	Container pause ready: true, restart count 0
    Jan 28 01:20:07.009: INFO: ibmcloud-block-storage-driver-lw66t from kube-system started at 2023-01-27 22:05:09 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.009: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 28 01:20:07.010: INFO: konnectivity-agent-65zgm from kube-system started at 2023-01-27 22:15:24 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.010: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 28 01:20:07.010: INFO: metrics-server-9cdf87dc6-4ptjv from kube-system started at 2023-01-27 22:51:34 +0000 UTC (3 container statuses recorded)
    Jan 28 01:20:07.010: INFO: 	Container config-watcher ready: true, restart count 0
    Jan 28 01:20:07.010: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 28 01:20:07.010: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan 28 01:20:07.010: INFO: public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-vvqqx from kube-system started at 2023-01-27 22:09:22 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.010: INFO: 	Container nginx-ingress ready: true, restart count 0
    Jan 28 01:20:07.010: INFO: sonobuoy from sonobuoy started at 2023-01-28 00:13:47 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.011: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 28 01:20:07.011: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-qbzm7 from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
    Jan 28 01:20:07.011: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 28 01:20:07.011: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 28 01:20:07.011: INFO: 
    Logging pods the apiserver thinks is on node 10.9.20.72 before test
    Jan 28 01:20:07.041: INFO: ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-2gg2p from ibm-system started at 2023-01-27 22:09:35 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.041: INFO: 	Container ibm-cloud-provider-ip-163-69-70-149 ready: true, restart count 0
    Jan 28 01:20:07.041: INFO: calico-node-4bc72 from kube-system started at 2023-01-27 22:04:57 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.041: INFO: 	Container calico-node ready: true, restart count 0
    Jan 28 01:20:07.042: INFO: calico-typha-677688fdc5-6nr4v from kube-system started at 2023-01-27 22:05:47 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.042: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 28 01:20:07.042: INFO: coredns-6754846f95-b686l from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.042: INFO: 	Container coredns ready: true, restart count 0
    Jan 28 01:20:07.042: INFO: ibm-keepalived-watcher-gstl5 from kube-system started at 2023-01-27 22:04:57 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.042: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 28 01:20:07.043: INFO: ibm-master-proxy-static-10.9.20.72 from kube-system started at 2023-01-27 22:04:44 +0000 UTC (2 container statuses recorded)
    Jan 28 01:20:07.043: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 28 01:20:07.043: INFO: 	Container pause ready: true, restart count 0
    Jan 28 01:20:07.043: INFO: ibmcloud-block-storage-driver-sktn2 from kube-system started at 2023-01-27 22:05:04 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.043: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 28 01:20:07.043: INFO: ingress-cluster-healthcheck-655c49644b-dmz5q from kube-system started at 2023-01-27 22:06:59 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.043: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
    Jan 28 01:20:07.043: INFO: konnectivity-agent-zbbtb from kube-system started at 2023-01-27 22:15:21 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.043: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 28 01:20:07.044: INFO: metrics-server-9cdf87dc6-n44dc from kube-system started at 2023-01-27 22:51:34 +0000 UTC (3 container statuses recorded)
    Jan 28 01:20:07.044: INFO: 	Container config-watcher ready: true, restart count 0
    Jan 28 01:20:07.044: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 28 01:20:07.044: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan 28 01:20:07.044: INFO: public-crcfa4dcfo0uc72glhelp0-alb1-7bf6b49dc6-ztg7f from kube-system started at 2023-01-27 22:09:22 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.045: INFO: 	Container nginx-ingress ready: true, restart count 0
    Jan 28 01:20:07.045: INFO: pod-logs-websocket-77f35e92-85e3-4683-9a81-cf1b38587604 from pods-724 started at 2023-01-28 01:19:40 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.045: INFO: 	Container main ready: true, restart count 0
    Jan 28 01:20:07.045: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-tm2wf from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
    Jan 28 01:20:07.045: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 28 01:20:07.045: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 28 01:20:07.045: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-01-27 22:07:40 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.045: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    Jan 28 01:20:07.045: INFO: 
    Logging pods the apiserver thinks is on node 10.9.20.75 before test
    Jan 28 01:20:07.097: INFO: ibm-cloud-provider-ip-163-69-70-149-86b9cdcdc-9qs6m from ibm-system started at 2023-01-27 22:09:35 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.097: INFO: 	Container ibm-cloud-provider-ip-163-69-70-149 ready: true, restart count 0
    Jan 28 01:20:07.097: INFO: calico-kube-controllers-5754dfd4dd-2fl6z from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.097: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 28 01:20:07.097: INFO: calico-node-6pnwh from kube-system started at 2023-01-27 22:04:45 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.097: INFO: 	Container calico-node ready: true, restart count 0
    Jan 28 01:20:07.097: INFO: calico-typha-677688fdc5-s2khq from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.097: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 28 01:20:07.098: INFO: coredns-6754846f95-r75xk from kube-system started at 2023-01-27 22:15:58 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.098: INFO: 	Container coredns ready: true, restart count 0
    Jan 28 01:20:07.098: INFO: coredns-autoscaler-669cf746f6-b9s85 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.098: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 28 01:20:07.098: INFO: dashboard-metrics-scraper-c964d5594-76f8h from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.098: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Jan 28 01:20:07.099: INFO: ibm-file-plugin-7dd6c48b68-kn5ff from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.099: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Jan 28 01:20:07.099: INFO: ibm-keepalived-watcher-gj6b6 from kube-system started at 2023-01-27 22:04:45 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.099: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 28 01:20:07.099: INFO: ibm-master-proxy-static-10.9.20.75 from kube-system started at 2023-01-27 22:04:42 +0000 UTC (2 container statuses recorded)
    Jan 28 01:20:07.099: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 28 01:20:07.099: INFO: 	Container pause ready: true, restart count 0
    Jan 28 01:20:07.099: INFO: ibm-storage-watcher-746995c8c9-7mmhc from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.099: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Jan 28 01:20:07.099: INFO: ibmcloud-block-storage-driver-jgsbr from kube-system started at 2023-01-27 22:04:51 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.100: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 28 01:20:07.100: INFO: ibmcloud-block-storage-plugin-697cd846b-rglpr from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.100: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Jan 28 01:20:07.100: INFO: konnectivity-agent-95p2n from kube-system started at 2023-01-27 22:15:28 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.100: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 28 01:20:07.100: INFO: kubernetes-dashboard-55c4d56798-7r7j7 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.100: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Jan 28 01:20:07.100: INFO: snapshot-controller-c5c6dddff-25l94 from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.100: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 28 01:20:07.101: INFO: snapshot-controller-c5c6dddff-5xzjc from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.101: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 28 01:20:07.101: INFO: snapshot-controller-c5c6dddff-pzr2k from kube-system started at 2023-01-27 22:05:05 +0000 UTC (1 container statuses recorded)
    Jan 28 01:20:07.101: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 28 01:20:07.102: INFO: sonobuoy-e2e-job-70551213e7cc4fe2 from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
    Jan 28 01:20:07.102: INFO: 	Container e2e ready: true, restart count 0
    Jan 28 01:20:07.102: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 28 01:20:07.102: INFO: sonobuoy-systemd-logs-daemon-set-b079e97f79f246b5-jcljc from sonobuoy started at 2023-01-28 00:13:53 +0000 UTC (2 container statuses recorded)
    Jan 28 01:20:07.102: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 28 01:20:07.103: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/28/23 01:20:07.103
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.173e544e68b131af], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 01/28/23 01:20:07.183
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 01:20:08.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5526" for this suite. 01/28/23 01:20:08.207
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:20:08.229
Jan 28 01:20:08.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename daemonsets 01/28/23 01:20:08.23
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:20:08.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:20:08.289
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Jan 28 01:20:08.369: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/28/23 01:20:08.385
Jan 28 01:20:08.448: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:20:08.448: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 01:20:09.492: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:20:09.492: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 01:20:10.478: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 28 01:20:10.478: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 01:20:11.482: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 28 01:20:11.482: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 01/28/23 01:20:11.534
STEP: Check that daemon pods images are updated. 01/28/23 01:20:11.568
Jan 28 01:20:11.583: INFO: Wrong image for pod: daemon-set-4npws. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 01:20:11.583: INFO: Wrong image for pod: daemon-set-8nrvd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 01:20:12.617: INFO: Wrong image for pod: daemon-set-4npws. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 01:20:12.617: INFO: Wrong image for pod: daemon-set-8nrvd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 01:20:13.624: INFO: Wrong image for pod: daemon-set-4npws. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 01:20:13.624: INFO: Wrong image for pod: daemon-set-8nrvd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 01:20:14.619: INFO: Wrong image for pod: daemon-set-4npws. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 01:20:14.619: INFO: Wrong image for pod: daemon-set-8nrvd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 01:20:14.619: INFO: Pod daemon-set-wv8bx is not available
Jan 28 01:20:15.624: INFO: Wrong image for pod: daemon-set-4npws. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 01:20:15.624: INFO: Wrong image for pod: daemon-set-8nrvd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 01:20:15.624: INFO: Pod daemon-set-wv8bx is not available
Jan 28 01:20:16.618: INFO: Wrong image for pod: daemon-set-4npws. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 01:20:17.619: INFO: Wrong image for pod: daemon-set-4npws. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 01:20:18.618: INFO: Wrong image for pod: daemon-set-4npws. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 01:20:18.619: INFO: Pod daemon-set-5wqzt is not available
Jan 28 01:20:19.620: INFO: Wrong image for pod: daemon-set-4npws. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 01:20:19.620: INFO: Pod daemon-set-5wqzt is not available
Jan 28 01:20:22.617: INFO: Pod daemon-set-pds2k is not available
STEP: Check that daemon pods are still running on every node of the cluster. 01/28/23 01:20:22.639
Jan 28 01:20:22.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 01:20:22.669: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
Jan 28 01:20:23.717: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 01:20:23.717: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
Jan 28 01:20:24.700: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 28 01:20:24.700: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/28/23 01:20:24.787
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4901, will wait for the garbage collector to delete the pods 01/28/23 01:20:24.787
Jan 28 01:20:24.873: INFO: Deleting DaemonSet.extensions daemon-set took: 21.179949ms
Jan 28 01:20:24.974: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.806246ms
Jan 28 01:20:27.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:20:27.986: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 28 01:20:27.998: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33856"},"items":null}

Jan 28 01:20:28.015: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33857"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 28 01:20:28.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4901" for this suite. 01/28/23 01:20:28.089
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":211,"skipped":3729,"failed":0}
------------------------------
â€¢ [SLOW TEST] [19.877 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:20:08.229
    Jan 28 01:20:08.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename daemonsets 01/28/23 01:20:08.23
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:20:08.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:20:08.289
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Jan 28 01:20:08.369: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/28/23 01:20:08.385
    Jan 28 01:20:08.448: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 01:20:08.448: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 01:20:09.492: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 01:20:09.492: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 01:20:10.478: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 28 01:20:10.478: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 01:20:11.482: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 28 01:20:11.482: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 01/28/23 01:20:11.534
    STEP: Check that daemon pods images are updated. 01/28/23 01:20:11.568
    Jan 28 01:20:11.583: INFO: Wrong image for pod: daemon-set-4npws. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 28 01:20:11.583: INFO: Wrong image for pod: daemon-set-8nrvd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 28 01:20:12.617: INFO: Wrong image for pod: daemon-set-4npws. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 28 01:20:12.617: INFO: Wrong image for pod: daemon-set-8nrvd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 28 01:20:13.624: INFO: Wrong image for pod: daemon-set-4npws. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 28 01:20:13.624: INFO: Wrong image for pod: daemon-set-8nrvd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 28 01:20:14.619: INFO: Wrong image for pod: daemon-set-4npws. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 28 01:20:14.619: INFO: Wrong image for pod: daemon-set-8nrvd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 28 01:20:14.619: INFO: Pod daemon-set-wv8bx is not available
    Jan 28 01:20:15.624: INFO: Wrong image for pod: daemon-set-4npws. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 28 01:20:15.624: INFO: Wrong image for pod: daemon-set-8nrvd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 28 01:20:15.624: INFO: Pod daemon-set-wv8bx is not available
    Jan 28 01:20:16.618: INFO: Wrong image for pod: daemon-set-4npws. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 28 01:20:17.619: INFO: Wrong image for pod: daemon-set-4npws. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 28 01:20:18.618: INFO: Wrong image for pod: daemon-set-4npws. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 28 01:20:18.619: INFO: Pod daemon-set-5wqzt is not available
    Jan 28 01:20:19.620: INFO: Wrong image for pod: daemon-set-4npws. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 28 01:20:19.620: INFO: Pod daemon-set-5wqzt is not available
    Jan 28 01:20:22.617: INFO: Pod daemon-set-pds2k is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 01/28/23 01:20:22.639
    Jan 28 01:20:22.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 28 01:20:22.669: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
    Jan 28 01:20:23.717: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 28 01:20:23.717: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
    Jan 28 01:20:24.700: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 28 01:20:24.700: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/28/23 01:20:24.787
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4901, will wait for the garbage collector to delete the pods 01/28/23 01:20:24.787
    Jan 28 01:20:24.873: INFO: Deleting DaemonSet.extensions daemon-set took: 21.179949ms
    Jan 28 01:20:24.974: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.806246ms
    Jan 28 01:20:27.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 01:20:27.986: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 28 01:20:27.998: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33856"},"items":null}

    Jan 28 01:20:28.015: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33857"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 01:20:28.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4901" for this suite. 01/28/23 01:20:28.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:20:28.113
Jan 28 01:20:28.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename taint-single-pod 01/28/23 01:20:28.114
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:20:28.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:20:28.162
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jan 28 01:20:28.177: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 28 01:21:28.309: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Jan 28 01:21:28.318: INFO: Starting informer...
STEP: Starting pod... 01/28/23 01:21:28.318
Jan 28 01:21:28.557: INFO: Pod is running on 10.9.20.126. Tainting Node
STEP: Trying to apply a taint on the Node 01/28/23 01:21:28.557
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/28/23 01:21:28.587
STEP: Waiting short time to make sure Pod is queued for deletion 01/28/23 01:21:28.629
Jan 28 01:21:28.630: INFO: Pod wasn't evicted. Proceeding
Jan 28 01:21:28.630: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/28/23 01:21:28.737
STEP: Waiting some time to make sure that toleration time passed. 01/28/23 01:21:28.802
Jan 28 01:22:43.804: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Jan 28 01:22:43.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4709" for this suite. 01/28/23 01:22:43.822
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":212,"skipped":3780,"failed":0}
------------------------------
â€¢ [SLOW TEST] [135.727 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:20:28.113
    Jan 28 01:20:28.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename taint-single-pod 01/28/23 01:20:28.114
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:20:28.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:20:28.162
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Jan 28 01:20:28.177: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 28 01:21:28.309: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Jan 28 01:21:28.318: INFO: Starting informer...
    STEP: Starting pod... 01/28/23 01:21:28.318
    Jan 28 01:21:28.557: INFO: Pod is running on 10.9.20.126. Tainting Node
    STEP: Trying to apply a taint on the Node 01/28/23 01:21:28.557
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/28/23 01:21:28.587
    STEP: Waiting short time to make sure Pod is queued for deletion 01/28/23 01:21:28.629
    Jan 28 01:21:28.630: INFO: Pod wasn't evicted. Proceeding
    Jan 28 01:21:28.630: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/28/23 01:21:28.737
    STEP: Waiting some time to make sure that toleration time passed. 01/28/23 01:21:28.802
    Jan 28 01:22:43.804: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 01:22:43.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-4709" for this suite. 01/28/23 01:22:43.822
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:22:43.843
Jan 28 01:22:43.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename statefulset 01/28/23 01:22:43.845
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:22:43.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:22:43.909
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1203 01/28/23 01:22:43.922
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-1203 01/28/23 01:22:43.934
Jan 28 01:22:43.963: INFO: Found 0 stateful pods, waiting for 1
Jan 28 01:22:53.975: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/28/23 01:22:53.999
STEP: updating a scale subresource 01/28/23 01:22:54.01
STEP: verifying the statefulset Spec.Replicas was modified 01/28/23 01:22:54.024
STEP: Patch a scale subresource 01/28/23 01:22:54.033
STEP: verifying the statefulset Spec.Replicas was modified 01/28/23 01:22:54.057
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 28 01:22:54.066: INFO: Deleting all statefulset in ns statefulset-1203
Jan 28 01:22:54.077: INFO: Scaling statefulset ss to 0
Jan 28 01:23:04.126: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 01:23:04.141: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 28 01:23:04.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1203" for this suite. 01/28/23 01:23:04.19
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":213,"skipped":3800,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.367 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:22:43.843
    Jan 28 01:22:43.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename statefulset 01/28/23 01:22:43.845
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:22:43.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:22:43.909
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1203 01/28/23 01:22:43.922
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-1203 01/28/23 01:22:43.934
    Jan 28 01:22:43.963: INFO: Found 0 stateful pods, waiting for 1
    Jan 28 01:22:53.975: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/28/23 01:22:53.999
    STEP: updating a scale subresource 01/28/23 01:22:54.01
    STEP: verifying the statefulset Spec.Replicas was modified 01/28/23 01:22:54.024
    STEP: Patch a scale subresource 01/28/23 01:22:54.033
    STEP: verifying the statefulset Spec.Replicas was modified 01/28/23 01:22:54.057
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 28 01:22:54.066: INFO: Deleting all statefulset in ns statefulset-1203
    Jan 28 01:22:54.077: INFO: Scaling statefulset ss to 0
    Jan 28 01:23:04.126: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 28 01:23:04.141: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 28 01:23:04.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1203" for this suite. 01/28/23 01:23:04.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:23:04.216
Jan 28 01:23:04.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename custom-resource-definition 01/28/23 01:23:04.219
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:23:04.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:23:04.281
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan 28 01:23:04.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:23:07.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3276" for this suite. 01/28/23 01:23:07.602
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":214,"skipped":3827,"failed":0}
------------------------------
â€¢ [3.400 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:23:04.216
    Jan 28 01:23:04.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename custom-resource-definition 01/28/23 01:23:04.219
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:23:04.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:23:04.281
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan 28 01:23:04.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:23:07.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-3276" for this suite. 01/28/23 01:23:07.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:23:07.618
Jan 28 01:23:07.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename downward-api 01/28/23 01:23:07.621
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:23:07.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:23:07.699
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 01/28/23 01:23:07.708
Jan 28 01:23:07.731: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d2dcb0ab-0c96-4332-b96a-29ae674683ad" in namespace "downward-api-5555" to be "Succeeded or Failed"
Jan 28 01:23:07.743: INFO: Pod "downwardapi-volume-d2dcb0ab-0c96-4332-b96a-29ae674683ad": Phase="Pending", Reason="", readiness=false. Elapsed: 11.721425ms
Jan 28 01:23:09.755: INFO: Pod "downwardapi-volume-d2dcb0ab-0c96-4332-b96a-29ae674683ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023478556s
Jan 28 01:23:11.755: INFO: Pod "downwardapi-volume-d2dcb0ab-0c96-4332-b96a-29ae674683ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023207562s
Jan 28 01:23:13.753: INFO: Pod "downwardapi-volume-d2dcb0ab-0c96-4332-b96a-29ae674683ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021644699s
STEP: Saw pod success 01/28/23 01:23:13.753
Jan 28 01:23:13.754: INFO: Pod "downwardapi-volume-d2dcb0ab-0c96-4332-b96a-29ae674683ad" satisfied condition "Succeeded or Failed"
Jan 28 01:23:13.776: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-d2dcb0ab-0c96-4332-b96a-29ae674683ad container client-container: <nil>
STEP: delete the pod 01/28/23 01:23:13.842
Jan 28 01:23:13.864: INFO: Waiting for pod downwardapi-volume-d2dcb0ab-0c96-4332-b96a-29ae674683ad to disappear
Jan 28 01:23:13.872: INFO: Pod downwardapi-volume-d2dcb0ab-0c96-4332-b96a-29ae674683ad no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 28 01:23:13.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5555" for this suite. 01/28/23 01:23:13.887
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":215,"skipped":3839,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.288 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:23:07.618
    Jan 28 01:23:07.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename downward-api 01/28/23 01:23:07.621
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:23:07.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:23:07.699
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 01/28/23 01:23:07.708
    Jan 28 01:23:07.731: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d2dcb0ab-0c96-4332-b96a-29ae674683ad" in namespace "downward-api-5555" to be "Succeeded or Failed"
    Jan 28 01:23:07.743: INFO: Pod "downwardapi-volume-d2dcb0ab-0c96-4332-b96a-29ae674683ad": Phase="Pending", Reason="", readiness=false. Elapsed: 11.721425ms
    Jan 28 01:23:09.755: INFO: Pod "downwardapi-volume-d2dcb0ab-0c96-4332-b96a-29ae674683ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023478556s
    Jan 28 01:23:11.755: INFO: Pod "downwardapi-volume-d2dcb0ab-0c96-4332-b96a-29ae674683ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023207562s
    Jan 28 01:23:13.753: INFO: Pod "downwardapi-volume-d2dcb0ab-0c96-4332-b96a-29ae674683ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021644699s
    STEP: Saw pod success 01/28/23 01:23:13.753
    Jan 28 01:23:13.754: INFO: Pod "downwardapi-volume-d2dcb0ab-0c96-4332-b96a-29ae674683ad" satisfied condition "Succeeded or Failed"
    Jan 28 01:23:13.776: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-d2dcb0ab-0c96-4332-b96a-29ae674683ad container client-container: <nil>
    STEP: delete the pod 01/28/23 01:23:13.842
    Jan 28 01:23:13.864: INFO: Waiting for pod downwardapi-volume-d2dcb0ab-0c96-4332-b96a-29ae674683ad to disappear
    Jan 28 01:23:13.872: INFO: Pod downwardapi-volume-d2dcb0ab-0c96-4332-b96a-29ae674683ad no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 28 01:23:13.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5555" for this suite. 01/28/23 01:23:13.887
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:23:13.909
Jan 28 01:23:13.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename container-runtime 01/28/23 01:23:13.914
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:23:13.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:23:13.973
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/28/23 01:23:14.009
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/28/23 01:23:32.276
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/28/23 01:23:32.29
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/28/23 01:23:32.316
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/28/23 01:23:32.316
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/28/23 01:23:32.354
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/28/23 01:23:36.41
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/28/23 01:23:38.447
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/28/23 01:23:38.47
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/28/23 01:23:38.471
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/28/23 01:23:38.517
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/28/23 01:23:39.542
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/28/23 01:23:43.603
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/28/23 01:23:43.618
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/28/23 01:23:43.619
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 28 01:23:43.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2405" for this suite. 01/28/23 01:23:43.69
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":216,"skipped":3852,"failed":0}
------------------------------
â€¢ [SLOW TEST] [29.797 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:23:13.909
    Jan 28 01:23:13.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename container-runtime 01/28/23 01:23:13.914
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:23:13.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:23:13.973
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/28/23 01:23:14.009
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/28/23 01:23:32.276
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/28/23 01:23:32.29
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/28/23 01:23:32.316
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/28/23 01:23:32.316
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/28/23 01:23:32.354
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/28/23 01:23:36.41
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/28/23 01:23:38.447
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/28/23 01:23:38.47
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/28/23 01:23:38.471
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/28/23 01:23:38.517
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/28/23 01:23:39.542
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/28/23 01:23:43.603
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/28/23 01:23:43.618
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/28/23 01:23:43.619
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 28 01:23:43.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-2405" for this suite. 01/28/23 01:23:43.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:23:43.715
Jan 28 01:23:43.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename secrets 01/28/23 01:23:43.717
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:23:43.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:23:43.765
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-172a2689-84d7-4772-9e12-56c1b40eaccf 01/28/23 01:23:43.828
STEP: Creating a pod to test consume secrets 01/28/23 01:23:43.839
Jan 28 01:23:43.859: INFO: Waiting up to 5m0s for pod "pod-secrets-f247244e-5c44-4008-9711-9c1455e30af7" in namespace "secrets-9058" to be "Succeeded or Failed"
Jan 28 01:23:43.870: INFO: Pod "pod-secrets-f247244e-5c44-4008-9711-9c1455e30af7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.466225ms
Jan 28 01:23:45.885: INFO: Pod "pod-secrets-f247244e-5c44-4008-9711-9c1455e30af7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025429878s
Jan 28 01:23:47.884: INFO: Pod "pod-secrets-f247244e-5c44-4008-9711-9c1455e30af7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02442388s
Jan 28 01:23:49.881: INFO: Pod "pod-secrets-f247244e-5c44-4008-9711-9c1455e30af7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021310969s
STEP: Saw pod success 01/28/23 01:23:49.881
Jan 28 01:23:49.881: INFO: Pod "pod-secrets-f247244e-5c44-4008-9711-9c1455e30af7" satisfied condition "Succeeded or Failed"
Jan 28 01:23:49.890: INFO: Trying to get logs from node 10.9.20.126 pod pod-secrets-f247244e-5c44-4008-9711-9c1455e30af7 container secret-volume-test: <nil>
STEP: delete the pod 01/28/23 01:23:49.909
Jan 28 01:23:49.928: INFO: Waiting for pod pod-secrets-f247244e-5c44-4008-9711-9c1455e30af7 to disappear
Jan 28 01:23:49.936: INFO: Pod pod-secrets-f247244e-5c44-4008-9711-9c1455e30af7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 28 01:23:49.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9058" for this suite. 01/28/23 01:23:49.953
STEP: Destroying namespace "secret-namespace-3149" for this suite. 01/28/23 01:23:49.968
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":217,"skipped":3871,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.269 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:23:43.715
    Jan 28 01:23:43.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename secrets 01/28/23 01:23:43.717
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:23:43.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:23:43.765
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-172a2689-84d7-4772-9e12-56c1b40eaccf 01/28/23 01:23:43.828
    STEP: Creating a pod to test consume secrets 01/28/23 01:23:43.839
    Jan 28 01:23:43.859: INFO: Waiting up to 5m0s for pod "pod-secrets-f247244e-5c44-4008-9711-9c1455e30af7" in namespace "secrets-9058" to be "Succeeded or Failed"
    Jan 28 01:23:43.870: INFO: Pod "pod-secrets-f247244e-5c44-4008-9711-9c1455e30af7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.466225ms
    Jan 28 01:23:45.885: INFO: Pod "pod-secrets-f247244e-5c44-4008-9711-9c1455e30af7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025429878s
    Jan 28 01:23:47.884: INFO: Pod "pod-secrets-f247244e-5c44-4008-9711-9c1455e30af7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02442388s
    Jan 28 01:23:49.881: INFO: Pod "pod-secrets-f247244e-5c44-4008-9711-9c1455e30af7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021310969s
    STEP: Saw pod success 01/28/23 01:23:49.881
    Jan 28 01:23:49.881: INFO: Pod "pod-secrets-f247244e-5c44-4008-9711-9c1455e30af7" satisfied condition "Succeeded or Failed"
    Jan 28 01:23:49.890: INFO: Trying to get logs from node 10.9.20.126 pod pod-secrets-f247244e-5c44-4008-9711-9c1455e30af7 container secret-volume-test: <nil>
    STEP: delete the pod 01/28/23 01:23:49.909
    Jan 28 01:23:49.928: INFO: Waiting for pod pod-secrets-f247244e-5c44-4008-9711-9c1455e30af7 to disappear
    Jan 28 01:23:49.936: INFO: Pod pod-secrets-f247244e-5c44-4008-9711-9c1455e30af7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 28 01:23:49.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9058" for this suite. 01/28/23 01:23:49.953
    STEP: Destroying namespace "secret-namespace-3149" for this suite. 01/28/23 01:23:49.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:23:49.985
Jan 28 01:23:49.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename statefulset 01/28/23 01:23:49.987
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:23:50.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:23:50.046
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-495 01/28/23 01:23:50.056
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 01/28/23 01:23:50.065
STEP: Creating stateful set ss in namespace statefulset-495 01/28/23 01:23:50.079
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-495 01/28/23 01:23:50.097
Jan 28 01:23:50.104: INFO: Found 0 stateful pods, waiting for 1
Jan 28 01:24:00.130: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/28/23 01:24:00.13
Jan 28 01:24:00.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-495 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 01:24:00.506: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 01:24:00.507: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 01:24:00.507: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 28 01:24:00.517: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 28 01:24:10.527: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 28 01:24:10.528: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 01:24:10.570: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999996022s
Jan 28 01:24:11.581: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991666731s
Jan 28 01:24:12.589: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.981249164s
Jan 28 01:24:13.611: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.972284528s
Jan 28 01:24:14.623: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.95028601s
Jan 28 01:24:15.636: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.938067719s
Jan 28 01:24:16.650: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.926079767s
Jan 28 01:24:17.663: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.911757568s
Jan 28 01:24:18.677: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.899003106s
Jan 28 01:24:19.697: INFO: Verifying statefulset ss doesn't scale past 1 for another 885.182234ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-495 01/28/23 01:24:20.697
Jan 28 01:24:20.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-495 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 28 01:24:21.023: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 28 01:24:21.023: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 28 01:24:21.023: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 28 01:24:21.047: INFO: Found 1 stateful pods, waiting for 3
Jan 28 01:24:31.069: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 01:24:31.069: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 01:24:31.069: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/28/23 01:24:31.07
STEP: Scale down will halt with unhealthy stateful pod 01/28/23 01:24:31.07
Jan 28 01:24:31.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-495 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 01:24:31.386: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 01:24:31.387: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 01:24:31.387: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 28 01:24:31.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-495 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 01:24:31.748: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 01:24:31.748: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 01:24:31.748: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 28 01:24:31.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-495 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 01:24:32.133: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 01:24:32.133: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 01:24:32.133: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 28 01:24:32.133: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 01:24:32.145: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jan 28 01:24:42.171: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 28 01:24:42.171: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 28 01:24:42.171: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 28 01:24:42.215: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999996155s
Jan 28 01:24:43.231: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98914312s
Jan 28 01:24:44.246: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973405588s
Jan 28 01:24:45.261: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.958088883s
Jan 28 01:24:46.276: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.943157779s
Jan 28 01:24:47.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.927879967s
Jan 28 01:24:48.309: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.911974942s
Jan 28 01:24:49.322: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.895015473s
Jan 28 01:24:50.335: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.882740949s
Jan 28 01:24:51.374: INFO: Verifying statefulset ss doesn't scale past 3 for another 869.297219ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-495 01/28/23 01:24:52.374
Jan 28 01:24:52.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-495 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 28 01:24:52.775: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 28 01:24:52.775: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 28 01:24:52.775: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 28 01:24:52.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-495 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 28 01:24:53.178: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 28 01:24:53.178: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 28 01:24:53.178: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 28 01:24:53.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-495 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 28 01:24:53.563: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 28 01:24:53.563: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 28 01:24:53.563: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 28 01:24:53.563: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/28/23 01:25:03.642
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 28 01:25:03.642: INFO: Deleting all statefulset in ns statefulset-495
Jan 28 01:25:03.678: INFO: Scaling statefulset ss to 0
Jan 28 01:25:03.766: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 01:25:03.804: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 28 01:25:03.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-495" for this suite. 01/28/23 01:25:03.893
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":218,"skipped":3882,"failed":0}
------------------------------
â€¢ [SLOW TEST] [73.984 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:23:49.985
    Jan 28 01:23:49.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename statefulset 01/28/23 01:23:49.987
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:23:50.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:23:50.046
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-495 01/28/23 01:23:50.056
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/28/23 01:23:50.065
    STEP: Creating stateful set ss in namespace statefulset-495 01/28/23 01:23:50.079
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-495 01/28/23 01:23:50.097
    Jan 28 01:23:50.104: INFO: Found 0 stateful pods, waiting for 1
    Jan 28 01:24:00.130: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/28/23 01:24:00.13
    Jan 28 01:24:00.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-495 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 28 01:24:00.506: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 28 01:24:00.507: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 28 01:24:00.507: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 28 01:24:00.517: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 28 01:24:10.527: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 28 01:24:10.528: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 28 01:24:10.570: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999996022s
    Jan 28 01:24:11.581: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991666731s
    Jan 28 01:24:12.589: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.981249164s
    Jan 28 01:24:13.611: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.972284528s
    Jan 28 01:24:14.623: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.95028601s
    Jan 28 01:24:15.636: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.938067719s
    Jan 28 01:24:16.650: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.926079767s
    Jan 28 01:24:17.663: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.911757568s
    Jan 28 01:24:18.677: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.899003106s
    Jan 28 01:24:19.697: INFO: Verifying statefulset ss doesn't scale past 1 for another 885.182234ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-495 01/28/23 01:24:20.697
    Jan 28 01:24:20.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-495 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 28 01:24:21.023: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 28 01:24:21.023: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 28 01:24:21.023: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 28 01:24:21.047: INFO: Found 1 stateful pods, waiting for 3
    Jan 28 01:24:31.069: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 28 01:24:31.069: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 28 01:24:31.069: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/28/23 01:24:31.07
    STEP: Scale down will halt with unhealthy stateful pod 01/28/23 01:24:31.07
    Jan 28 01:24:31.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-495 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 28 01:24:31.386: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 28 01:24:31.387: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 28 01:24:31.387: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 28 01:24:31.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-495 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 28 01:24:31.748: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 28 01:24:31.748: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 28 01:24:31.748: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 28 01:24:31.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-495 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 28 01:24:32.133: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 28 01:24:32.133: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 28 01:24:32.133: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 28 01:24:32.133: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 28 01:24:32.145: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
    Jan 28 01:24:42.171: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 28 01:24:42.171: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 28 01:24:42.171: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 28 01:24:42.215: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999996155s
    Jan 28 01:24:43.231: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98914312s
    Jan 28 01:24:44.246: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973405588s
    Jan 28 01:24:45.261: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.958088883s
    Jan 28 01:24:46.276: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.943157779s
    Jan 28 01:24:47.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.927879967s
    Jan 28 01:24:48.309: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.911974942s
    Jan 28 01:24:49.322: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.895015473s
    Jan 28 01:24:50.335: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.882740949s
    Jan 28 01:24:51.374: INFO: Verifying statefulset ss doesn't scale past 3 for another 869.297219ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-495 01/28/23 01:24:52.374
    Jan 28 01:24:52.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-495 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 28 01:24:52.775: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 28 01:24:52.775: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 28 01:24:52.775: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 28 01:24:52.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-495 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 28 01:24:53.178: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 28 01:24:53.178: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 28 01:24:53.178: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 28 01:24:53.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=statefulset-495 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 28 01:24:53.563: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 28 01:24:53.563: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 28 01:24:53.563: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 28 01:24:53.563: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/28/23 01:25:03.642
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 28 01:25:03.642: INFO: Deleting all statefulset in ns statefulset-495
    Jan 28 01:25:03.678: INFO: Scaling statefulset ss to 0
    Jan 28 01:25:03.766: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 28 01:25:03.804: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 28 01:25:03.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-495" for this suite. 01/28/23 01:25:03.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:25:03.973
Jan 28 01:25:03.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubectl 01/28/23 01:25:03.975
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:25:04.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:25:04.027
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 01/28/23 01:25:04.042
Jan 28 01:25:04.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-7011 cluster-info'
Jan 28 01:25:04.191: INFO: stderr: ""
Jan 28 01:25:04.191: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 28 01:25:04.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7011" for this suite. 01/28/23 01:25:04.206
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":219,"skipped":3894,"failed":0}
------------------------------
â€¢ [0.273 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:25:03.973
    Jan 28 01:25:03.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubectl 01/28/23 01:25:03.975
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:25:04.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:25:04.027
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 01/28/23 01:25:04.042
    Jan 28 01:25:04.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-7011 cluster-info'
    Jan 28 01:25:04.191: INFO: stderr: ""
    Jan 28 01:25:04.191: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 28 01:25:04.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7011" for this suite. 01/28/23 01:25:04.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:25:04.249
Jan 28 01:25:04.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename ingress 01/28/23 01:25:04.252
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:25:04.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:25:04.335
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/28/23 01:25:04.349
STEP: getting /apis/networking.k8s.io 01/28/23 01:25:04.362
STEP: getting /apis/networking.k8s.iov1 01/28/23 01:25:04.386
STEP: creating 01/28/23 01:25:04.39
STEP: getting 01/28/23 01:25:04.44
STEP: listing 01/28/23 01:25:04.448
STEP: watching 01/28/23 01:25:04.485
Jan 28 01:25:04.485: INFO: starting watch
STEP: cluster-wide listing 01/28/23 01:25:04.488
STEP: cluster-wide watching 01/28/23 01:25:04.496
Jan 28 01:25:04.496: INFO: starting watch
STEP: patching 01/28/23 01:25:04.501
STEP: updating 01/28/23 01:25:04.516
Jan 28 01:25:04.546: INFO: waiting for watch events with expected annotations
Jan 28 01:25:04.546: INFO: saw patched and updated annotations
STEP: patching /status 01/28/23 01:25:04.546
STEP: updating /status 01/28/23 01:25:04.584
STEP: get /status 01/28/23 01:25:04.638
STEP: deleting 01/28/23 01:25:04.645
STEP: deleting a collection 01/28/23 01:25:04.756
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Jan 28 01:25:04.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9565" for this suite. 01/28/23 01:25:04.863
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":220,"skipped":3907,"failed":0}
------------------------------
â€¢ [0.660 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:25:04.249
    Jan 28 01:25:04.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename ingress 01/28/23 01:25:04.252
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:25:04.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:25:04.335
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/28/23 01:25:04.349
    STEP: getting /apis/networking.k8s.io 01/28/23 01:25:04.362
    STEP: getting /apis/networking.k8s.iov1 01/28/23 01:25:04.386
    STEP: creating 01/28/23 01:25:04.39
    STEP: getting 01/28/23 01:25:04.44
    STEP: listing 01/28/23 01:25:04.448
    STEP: watching 01/28/23 01:25:04.485
    Jan 28 01:25:04.485: INFO: starting watch
    STEP: cluster-wide listing 01/28/23 01:25:04.488
    STEP: cluster-wide watching 01/28/23 01:25:04.496
    Jan 28 01:25:04.496: INFO: starting watch
    STEP: patching 01/28/23 01:25:04.501
    STEP: updating 01/28/23 01:25:04.516
    Jan 28 01:25:04.546: INFO: waiting for watch events with expected annotations
    Jan 28 01:25:04.546: INFO: saw patched and updated annotations
    STEP: patching /status 01/28/23 01:25:04.546
    STEP: updating /status 01/28/23 01:25:04.584
    STEP: get /status 01/28/23 01:25:04.638
    STEP: deleting 01/28/23 01:25:04.645
    STEP: deleting a collection 01/28/23 01:25:04.756
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Jan 28 01:25:04.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-9565" for this suite. 01/28/23 01:25:04.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:25:04.917
Jan 28 01:25:04.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename dns 01/28/23 01:25:04.919
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:25:04.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:25:05
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/28/23 01:25:05.008
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9548.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9548.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9548.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9548.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9548.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9548.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9548.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9548.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9548.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 168.39.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.39.168_udp@PTR;check="$$(dig +tcp +noall +answer +search 168.39.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.39.168_tcp@PTR;sleep 1; done
 01/28/23 01:25:05.099
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9548.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9548.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9548.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9548.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9548.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9548.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9548.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9548.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9548.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9548.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 168.39.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.39.168_udp@PTR;check="$$(dig +tcp +noall +answer +search 168.39.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.39.168_tcp@PTR;sleep 1; done
 01/28/23 01:25:05.099
STEP: creating a pod to probe DNS 01/28/23 01:25:05.1
STEP: submitting the pod to kubernetes 01/28/23 01:25:05.101
Jan 28 01:25:05.169: INFO: Waiting up to 15m0s for pod "dns-test-7e345792-2163-46d4-83ef-bbb388c62895" in namespace "dns-9548" to be "running"
Jan 28 01:25:05.182: INFO: Pod "dns-test-7e345792-2163-46d4-83ef-bbb388c62895": Phase="Pending", Reason="", readiness=false. Elapsed: 13.031742ms
Jan 28 01:25:07.219: INFO: Pod "dns-test-7e345792-2163-46d4-83ef-bbb388c62895": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049620757s
Jan 28 01:25:09.217: INFO: Pod "dns-test-7e345792-2163-46d4-83ef-bbb388c62895": Phase="Running", Reason="", readiness=true. Elapsed: 4.047916448s
Jan 28 01:25:09.217: INFO: Pod "dns-test-7e345792-2163-46d4-83ef-bbb388c62895" satisfied condition "running"
STEP: retrieving the pod 01/28/23 01:25:09.217
STEP: looking for the results for each expected name from probers 01/28/23 01:25:09.252
Jan 28 01:25:09.355: INFO: Unable to read wheezy_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:09.388: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:09.400: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:09.635: INFO: Unable to read jessie_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:09.650: INFO: Unable to read jessie_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:09.720: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:09.804: INFO: Lookups using dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895 failed for: [wheezy_udp@dns-test-service.dns-9548.svc.cluster.local wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local jessie_udp@dns-test-service.dns-9548.svc.cluster.local jessie_tcp@dns-test-service.dns-9548.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local]

Jan 28 01:25:14.819: INFO: Unable to read wheezy_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:14.831: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:15.005: INFO: Unable to read jessie_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:15.041: INFO: Unable to read jessie_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:15.066: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:15.148: INFO: Lookups using dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895 failed for: [wheezy_udp@dns-test-service.dns-9548.svc.cluster.local wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local jessie_udp@dns-test-service.dns-9548.svc.cluster.local jessie_tcp@dns-test-service.dns-9548.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local]

Jan 28 01:25:19.822: INFO: Unable to read wheezy_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:19.836: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:19.994: INFO: Unable to read jessie_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:20.005: INFO: Unable to read jessie_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:20.171: INFO: Lookups using dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895 failed for: [wheezy_udp@dns-test-service.dns-9548.svc.cluster.local wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local jessie_udp@dns-test-service.dns-9548.svc.cluster.local jessie_tcp@dns-test-service.dns-9548.svc.cluster.local]

Jan 28 01:25:24.823: INFO: Unable to read wheezy_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:24.865: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:24.990: INFO: Unable to read jessie_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:25.004: INFO: Unable to read jessie_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:25.170: INFO: Lookups using dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895 failed for: [wheezy_udp@dns-test-service.dns-9548.svc.cluster.local wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local jessie_udp@dns-test-service.dns-9548.svc.cluster.local jessie_tcp@dns-test-service.dns-9548.svc.cluster.local]

Jan 28 01:25:29.823: INFO: Unable to read wheezy_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:29.838: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:30.057: INFO: Unable to read jessie_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:30.073: INFO: Unable to read jessie_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:30.162: INFO: Lookups using dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895 failed for: [wheezy_udp@dns-test-service.dns-9548.svc.cluster.local wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local jessie_udp@dns-test-service.dns-9548.svc.cluster.local jessie_tcp@dns-test-service.dns-9548.svc.cluster.local]

Jan 28 01:25:34.822: INFO: Unable to read wheezy_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:34.839: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:34.971: INFO: Unable to read jessie_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:35.002: INFO: Unable to read jessie_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
Jan 28 01:25:35.095: INFO: Lookups using dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895 failed for: [wheezy_udp@dns-test-service.dns-9548.svc.cluster.local wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local jessie_udp@dns-test-service.dns-9548.svc.cluster.local jessie_tcp@dns-test-service.dns-9548.svc.cluster.local]

Jan 28 01:25:40.113: INFO: DNS probes using dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895 succeeded

STEP: deleting the pod 01/28/23 01:25:40.113
STEP: deleting the test service 01/28/23 01:25:40.18
STEP: deleting the test headless service 01/28/23 01:25:40.283
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 28 01:25:40.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9548" for this suite. 01/28/23 01:25:40.386
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":221,"skipped":3944,"failed":0}
------------------------------
â€¢ [SLOW TEST] [35.515 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:25:04.917
    Jan 28 01:25:04.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename dns 01/28/23 01:25:04.919
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:25:04.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:25:05
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/28/23 01:25:05.008
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9548.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9548.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9548.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9548.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9548.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9548.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9548.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9548.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9548.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 168.39.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.39.168_udp@PTR;check="$$(dig +tcp +noall +answer +search 168.39.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.39.168_tcp@PTR;sleep 1; done
     01/28/23 01:25:05.099
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9548.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9548.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9548.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9548.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9548.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9548.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9548.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9548.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9548.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9548.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 168.39.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.39.168_udp@PTR;check="$$(dig +tcp +noall +answer +search 168.39.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.39.168_tcp@PTR;sleep 1; done
     01/28/23 01:25:05.099
    STEP: creating a pod to probe DNS 01/28/23 01:25:05.1
    STEP: submitting the pod to kubernetes 01/28/23 01:25:05.101
    Jan 28 01:25:05.169: INFO: Waiting up to 15m0s for pod "dns-test-7e345792-2163-46d4-83ef-bbb388c62895" in namespace "dns-9548" to be "running"
    Jan 28 01:25:05.182: INFO: Pod "dns-test-7e345792-2163-46d4-83ef-bbb388c62895": Phase="Pending", Reason="", readiness=false. Elapsed: 13.031742ms
    Jan 28 01:25:07.219: INFO: Pod "dns-test-7e345792-2163-46d4-83ef-bbb388c62895": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049620757s
    Jan 28 01:25:09.217: INFO: Pod "dns-test-7e345792-2163-46d4-83ef-bbb388c62895": Phase="Running", Reason="", readiness=true. Elapsed: 4.047916448s
    Jan 28 01:25:09.217: INFO: Pod "dns-test-7e345792-2163-46d4-83ef-bbb388c62895" satisfied condition "running"
    STEP: retrieving the pod 01/28/23 01:25:09.217
    STEP: looking for the results for each expected name from probers 01/28/23 01:25:09.252
    Jan 28 01:25:09.355: INFO: Unable to read wheezy_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:09.388: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:09.400: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:09.635: INFO: Unable to read jessie_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:09.650: INFO: Unable to read jessie_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:09.720: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:09.804: INFO: Lookups using dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895 failed for: [wheezy_udp@dns-test-service.dns-9548.svc.cluster.local wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local jessie_udp@dns-test-service.dns-9548.svc.cluster.local jessie_tcp@dns-test-service.dns-9548.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local]

    Jan 28 01:25:14.819: INFO: Unable to read wheezy_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:14.831: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:15.005: INFO: Unable to read jessie_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:15.041: INFO: Unable to read jessie_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:15.066: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:15.148: INFO: Lookups using dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895 failed for: [wheezy_udp@dns-test-service.dns-9548.svc.cluster.local wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local jessie_udp@dns-test-service.dns-9548.svc.cluster.local jessie_tcp@dns-test-service.dns-9548.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9548.svc.cluster.local]

    Jan 28 01:25:19.822: INFO: Unable to read wheezy_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:19.836: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:19.994: INFO: Unable to read jessie_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:20.005: INFO: Unable to read jessie_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:20.171: INFO: Lookups using dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895 failed for: [wheezy_udp@dns-test-service.dns-9548.svc.cluster.local wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local jessie_udp@dns-test-service.dns-9548.svc.cluster.local jessie_tcp@dns-test-service.dns-9548.svc.cluster.local]

    Jan 28 01:25:24.823: INFO: Unable to read wheezy_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:24.865: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:24.990: INFO: Unable to read jessie_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:25.004: INFO: Unable to read jessie_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:25.170: INFO: Lookups using dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895 failed for: [wheezy_udp@dns-test-service.dns-9548.svc.cluster.local wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local jessie_udp@dns-test-service.dns-9548.svc.cluster.local jessie_tcp@dns-test-service.dns-9548.svc.cluster.local]

    Jan 28 01:25:29.823: INFO: Unable to read wheezy_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:29.838: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:30.057: INFO: Unable to read jessie_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:30.073: INFO: Unable to read jessie_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:30.162: INFO: Lookups using dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895 failed for: [wheezy_udp@dns-test-service.dns-9548.svc.cluster.local wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local jessie_udp@dns-test-service.dns-9548.svc.cluster.local jessie_tcp@dns-test-service.dns-9548.svc.cluster.local]

    Jan 28 01:25:34.822: INFO: Unable to read wheezy_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:34.839: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:34.971: INFO: Unable to read jessie_udp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:35.002: INFO: Unable to read jessie_tcp@dns-test-service.dns-9548.svc.cluster.local from pod dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895: the server could not find the requested resource (get pods dns-test-7e345792-2163-46d4-83ef-bbb388c62895)
    Jan 28 01:25:35.095: INFO: Lookups using dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895 failed for: [wheezy_udp@dns-test-service.dns-9548.svc.cluster.local wheezy_tcp@dns-test-service.dns-9548.svc.cluster.local jessie_udp@dns-test-service.dns-9548.svc.cluster.local jessie_tcp@dns-test-service.dns-9548.svc.cluster.local]

    Jan 28 01:25:40.113: INFO: DNS probes using dns-9548/dns-test-7e345792-2163-46d4-83ef-bbb388c62895 succeeded

    STEP: deleting the pod 01/28/23 01:25:40.113
    STEP: deleting the test service 01/28/23 01:25:40.18
    STEP: deleting the test headless service 01/28/23 01:25:40.283
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 28 01:25:40.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9548" for this suite. 01/28/23 01:25:40.386
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:25:40.435
Jan 28 01:25:40.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename svcaccounts 01/28/23 01:25:40.437
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:25:40.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:25:40.476
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Jan 28 01:25:40.598: INFO: Waiting up to 5m0s for pod "pod-service-account-10589313-a26f-4a14-b544-fe8185e0ba53" in namespace "svcaccounts-8096" to be "running"
Jan 28 01:25:40.608: INFO: Pod "pod-service-account-10589313-a26f-4a14-b544-fe8185e0ba53": Phase="Pending", Reason="", readiness=false. Elapsed: 10.35663ms
Jan 28 01:25:42.621: INFO: Pod "pod-service-account-10589313-a26f-4a14-b544-fe8185e0ba53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023138552s
Jan 28 01:25:44.621: INFO: Pod "pod-service-account-10589313-a26f-4a14-b544-fe8185e0ba53": Phase="Running", Reason="", readiness=true. Elapsed: 4.023028256s
Jan 28 01:25:44.621: INFO: Pod "pod-service-account-10589313-a26f-4a14-b544-fe8185e0ba53" satisfied condition "running"
STEP: reading a file in the container 01/28/23 01:25:44.621
Jan 28 01:25:44.622: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8096 pod-service-account-10589313-a26f-4a14-b544-fe8185e0ba53 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/28/23 01:25:45.036
Jan 28 01:25:45.037: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8096 pod-service-account-10589313-a26f-4a14-b544-fe8185e0ba53 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/28/23 01:25:45.437
Jan 28 01:25:45.437: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8096 pod-service-account-10589313-a26f-4a14-b544-fe8185e0ba53 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 28 01:25:45.923: INFO: Got root ca configmap in namespace "svcaccounts-8096"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 28 01:25:45.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8096" for this suite. 01/28/23 01:25:45.964
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":222,"skipped":3960,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.576 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:25:40.435
    Jan 28 01:25:40.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename svcaccounts 01/28/23 01:25:40.437
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:25:40.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:25:40.476
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Jan 28 01:25:40.598: INFO: Waiting up to 5m0s for pod "pod-service-account-10589313-a26f-4a14-b544-fe8185e0ba53" in namespace "svcaccounts-8096" to be "running"
    Jan 28 01:25:40.608: INFO: Pod "pod-service-account-10589313-a26f-4a14-b544-fe8185e0ba53": Phase="Pending", Reason="", readiness=false. Elapsed: 10.35663ms
    Jan 28 01:25:42.621: INFO: Pod "pod-service-account-10589313-a26f-4a14-b544-fe8185e0ba53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023138552s
    Jan 28 01:25:44.621: INFO: Pod "pod-service-account-10589313-a26f-4a14-b544-fe8185e0ba53": Phase="Running", Reason="", readiness=true. Elapsed: 4.023028256s
    Jan 28 01:25:44.621: INFO: Pod "pod-service-account-10589313-a26f-4a14-b544-fe8185e0ba53" satisfied condition "running"
    STEP: reading a file in the container 01/28/23 01:25:44.621
    Jan 28 01:25:44.622: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8096 pod-service-account-10589313-a26f-4a14-b544-fe8185e0ba53 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/28/23 01:25:45.036
    Jan 28 01:25:45.037: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8096 pod-service-account-10589313-a26f-4a14-b544-fe8185e0ba53 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/28/23 01:25:45.437
    Jan 28 01:25:45.437: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8096 pod-service-account-10589313-a26f-4a14-b544-fe8185e0ba53 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan 28 01:25:45.923: INFO: Got root ca configmap in namespace "svcaccounts-8096"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 28 01:25:45.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-8096" for this suite. 01/28/23 01:25:45.964
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:25:46.012
Jan 28 01:25:46.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename crd-webhook 01/28/23 01:25:46.013
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:25:46.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:25:46.084
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/28/23 01:25:46.096
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/28/23 01:25:46.331
STEP: Deploying the custom resource conversion webhook pod 01/28/23 01:25:46.349
STEP: Wait for the deployment to be ready 01/28/23 01:25:46.377
Jan 28 01:25:46.438: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 28 01:25:48.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 25, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 25, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 25, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 25, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/28/23 01:25:50.492
STEP: Verifying the service has paired with the endpoint 01/28/23 01:25:50.522
Jan 28 01:25:51.523: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan 28 01:25:51.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Creating a v1 custom resource 01/28/23 01:25:54.402
STEP: Create a v2 custom resource 01/28/23 01:25:54.504
STEP: List CRs in v1 01/28/23 01:25:54.618
STEP: List CRs in v2 01/28/23 01:25:54.638
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:25:55.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3363" for this suite. 01/28/23 01:25:55.232
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":223,"skipped":3978,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.483 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:25:46.012
    Jan 28 01:25:46.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename crd-webhook 01/28/23 01:25:46.013
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:25:46.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:25:46.084
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/28/23 01:25:46.096
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/28/23 01:25:46.331
    STEP: Deploying the custom resource conversion webhook pod 01/28/23 01:25:46.349
    STEP: Wait for the deployment to be ready 01/28/23 01:25:46.377
    Jan 28 01:25:46.438: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Jan 28 01:25:48.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 25, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 25, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 25, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 25, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/28/23 01:25:50.492
    STEP: Verifying the service has paired with the endpoint 01/28/23 01:25:50.522
    Jan 28 01:25:51.523: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan 28 01:25:51.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Creating a v1 custom resource 01/28/23 01:25:54.402
    STEP: Create a v2 custom resource 01/28/23 01:25:54.504
    STEP: List CRs in v1 01/28/23 01:25:54.618
    STEP: List CRs in v2 01/28/23 01:25:54.638
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:25:55.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-3363" for this suite. 01/28/23 01:25:55.232
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:25:55.496
Jan 28 01:25:55.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename emptydir 01/28/23 01:25:55.499
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:25:55.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:25:55.547
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/28/23 01:25:55.558
Jan 28 01:25:55.578: INFO: Waiting up to 5m0s for pod "pod-4658eaea-108a-4827-ba0b-86d25e63248a" in namespace "emptydir-4658" to be "Succeeded or Failed"
Jan 28 01:25:55.588: INFO: Pod "pod-4658eaea-108a-4827-ba0b-86d25e63248a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.134683ms
Jan 28 01:25:57.601: INFO: Pod "pod-4658eaea-108a-4827-ba0b-86d25e63248a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023448146s
Jan 28 01:25:59.600: INFO: Pod "pod-4658eaea-108a-4827-ba0b-86d25e63248a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022434083s
Jan 28 01:26:01.616: INFO: Pod "pod-4658eaea-108a-4827-ba0b-86d25e63248a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038733517s
STEP: Saw pod success 01/28/23 01:26:01.617
Jan 28 01:26:01.617: INFO: Pod "pod-4658eaea-108a-4827-ba0b-86d25e63248a" satisfied condition "Succeeded or Failed"
Jan 28 01:26:01.635: INFO: Trying to get logs from node 10.9.20.126 pod pod-4658eaea-108a-4827-ba0b-86d25e63248a container test-container: <nil>
STEP: delete the pod 01/28/23 01:26:01.702
Jan 28 01:26:01.729: INFO: Waiting for pod pod-4658eaea-108a-4827-ba0b-86d25e63248a to disappear
Jan 28 01:26:01.739: INFO: Pod pod-4658eaea-108a-4827-ba0b-86d25e63248a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 28 01:26:01.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4658" for this suite. 01/28/23 01:26:01.757
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":224,"skipped":3989,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.280 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:25:55.496
    Jan 28 01:25:55.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename emptydir 01/28/23 01:25:55.499
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:25:55.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:25:55.547
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/28/23 01:25:55.558
    Jan 28 01:25:55.578: INFO: Waiting up to 5m0s for pod "pod-4658eaea-108a-4827-ba0b-86d25e63248a" in namespace "emptydir-4658" to be "Succeeded or Failed"
    Jan 28 01:25:55.588: INFO: Pod "pod-4658eaea-108a-4827-ba0b-86d25e63248a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.134683ms
    Jan 28 01:25:57.601: INFO: Pod "pod-4658eaea-108a-4827-ba0b-86d25e63248a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023448146s
    Jan 28 01:25:59.600: INFO: Pod "pod-4658eaea-108a-4827-ba0b-86d25e63248a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022434083s
    Jan 28 01:26:01.616: INFO: Pod "pod-4658eaea-108a-4827-ba0b-86d25e63248a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038733517s
    STEP: Saw pod success 01/28/23 01:26:01.617
    Jan 28 01:26:01.617: INFO: Pod "pod-4658eaea-108a-4827-ba0b-86d25e63248a" satisfied condition "Succeeded or Failed"
    Jan 28 01:26:01.635: INFO: Trying to get logs from node 10.9.20.126 pod pod-4658eaea-108a-4827-ba0b-86d25e63248a container test-container: <nil>
    STEP: delete the pod 01/28/23 01:26:01.702
    Jan 28 01:26:01.729: INFO: Waiting for pod pod-4658eaea-108a-4827-ba0b-86d25e63248a to disappear
    Jan 28 01:26:01.739: INFO: Pod pod-4658eaea-108a-4827-ba0b-86d25e63248a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 28 01:26:01.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4658" for this suite. 01/28/23 01:26:01.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:26:01.778
Jan 28 01:26:01.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename crd-webhook 01/28/23 01:26:01.78
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:26:01.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:26:01.827
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/28/23 01:26:01.837
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/28/23 01:26:02.139
STEP: Deploying the custom resource conversion webhook pod 01/28/23 01:26:02.154
STEP: Wait for the deployment to be ready 01/28/23 01:26:02.184
Jan 28 01:26:02.211: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 28 01:26:04.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 26, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 26, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 26, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 26, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/28/23 01:26:06.261
STEP: Verifying the service has paired with the endpoint 01/28/23 01:26:06.292
Jan 28 01:26:07.294: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan 28 01:26:07.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Creating a v1 custom resource 01/28/23 01:26:10.06
STEP: v2 custom resource should be converted 01/28/23 01:26:10.073
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:26:10.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1731" for this suite. 01/28/23 01:26:10.658
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":225,"skipped":4013,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.003 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:26:01.778
    Jan 28 01:26:01.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename crd-webhook 01/28/23 01:26:01.78
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:26:01.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:26:01.827
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/28/23 01:26:01.837
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/28/23 01:26:02.139
    STEP: Deploying the custom resource conversion webhook pod 01/28/23 01:26:02.154
    STEP: Wait for the deployment to be ready 01/28/23 01:26:02.184
    Jan 28 01:26:02.211: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Jan 28 01:26:04.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 26, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 26, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 26, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 26, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/28/23 01:26:06.261
    STEP: Verifying the service has paired with the endpoint 01/28/23 01:26:06.292
    Jan 28 01:26:07.294: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan 28 01:26:07.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Creating a v1 custom resource 01/28/23 01:26:10.06
    STEP: v2 custom resource should be converted 01/28/23 01:26:10.073
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:26:10.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-1731" for this suite. 01/28/23 01:26:10.658
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:26:10.797
Jan 28 01:26:10.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename daemonsets 01/28/23 01:26:10.799
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:26:10.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:26:10.845
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Jan 28 01:26:10.940: INFO: Create a RollingUpdate DaemonSet
Jan 28 01:26:10.958: INFO: Check that daemon pods launch on every node of the cluster
Jan 28 01:26:11.014: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:26:11.014: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 01:26:12.042: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:26:12.042: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 01:26:13.046: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 28 01:26:13.046: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
Jan 28 01:26:14.055: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 28 01:26:14.055: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Jan 28 01:26:14.055: INFO: Update the DaemonSet to trigger a rollout
Jan 28 01:26:14.084: INFO: Updating DaemonSet daemon-set
Jan 28 01:26:16.147: INFO: Roll back the DaemonSet before rollout is complete
Jan 28 01:26:16.173: INFO: Updating DaemonSet daemon-set
Jan 28 01:26:16.173: INFO: Make sure DaemonSet rollback is complete
Jan 28 01:26:16.192: INFO: Wrong image for pod: daemon-set-6ztqn. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jan 28 01:26:16.192: INFO: Pod daemon-set-6ztqn is not available
Jan 28 01:26:25.223: INFO: Pod daemon-set-9bsqc is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/28/23 01:26:25.269
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4064, will wait for the garbage collector to delete the pods 01/28/23 01:26:25.269
Jan 28 01:26:25.354: INFO: Deleting DaemonSet.extensions daemon-set took: 21.509276ms
Jan 28 01:26:25.454: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.248134ms
Jan 28 01:26:27.966: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:26:27.966: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 28 01:26:27.978: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35462"},"items":null}

Jan 28 01:26:27.989: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35462"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 28 01:26:28.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4064" for this suite. 01/28/23 01:26:28.06
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":226,"skipped":4035,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.282 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:26:10.797
    Jan 28 01:26:10.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename daemonsets 01/28/23 01:26:10.799
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:26:10.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:26:10.845
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Jan 28 01:26:10.940: INFO: Create a RollingUpdate DaemonSet
    Jan 28 01:26:10.958: INFO: Check that daemon pods launch on every node of the cluster
    Jan 28 01:26:11.014: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 01:26:11.014: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 01:26:12.042: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 01:26:12.042: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 01:26:13.046: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 28 01:26:13.046: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
    Jan 28 01:26:14.055: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 28 01:26:14.055: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Jan 28 01:26:14.055: INFO: Update the DaemonSet to trigger a rollout
    Jan 28 01:26:14.084: INFO: Updating DaemonSet daemon-set
    Jan 28 01:26:16.147: INFO: Roll back the DaemonSet before rollout is complete
    Jan 28 01:26:16.173: INFO: Updating DaemonSet daemon-set
    Jan 28 01:26:16.173: INFO: Make sure DaemonSet rollback is complete
    Jan 28 01:26:16.192: INFO: Wrong image for pod: daemon-set-6ztqn. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Jan 28 01:26:16.192: INFO: Pod daemon-set-6ztqn is not available
    Jan 28 01:26:25.223: INFO: Pod daemon-set-9bsqc is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/28/23 01:26:25.269
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4064, will wait for the garbage collector to delete the pods 01/28/23 01:26:25.269
    Jan 28 01:26:25.354: INFO: Deleting DaemonSet.extensions daemon-set took: 21.509276ms
    Jan 28 01:26:25.454: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.248134ms
    Jan 28 01:26:27.966: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 28 01:26:27.966: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 28 01:26:27.978: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35462"},"items":null}

    Jan 28 01:26:27.989: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35462"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 01:26:28.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4064" for this suite. 01/28/23 01:26:28.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:26:28.095
Jan 28 01:26:28.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename var-expansion 01/28/23 01:26:28.096
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:26:28.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:26:28.163
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 01/28/23 01:26:28.175
Jan 28 01:26:28.209: INFO: Waiting up to 5m0s for pod "var-expansion-8888c0cb-81a7-46c0-ae5b-b96e7ccaf34f" in namespace "var-expansion-7853" to be "Succeeded or Failed"
Jan 28 01:26:28.220: INFO: Pod "var-expansion-8888c0cb-81a7-46c0-ae5b-b96e7ccaf34f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.936407ms
Jan 28 01:26:30.231: INFO: Pod "var-expansion-8888c0cb-81a7-46c0-ae5b-b96e7ccaf34f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021836542s
Jan 28 01:26:32.233: INFO: Pod "var-expansion-8888c0cb-81a7-46c0-ae5b-b96e7ccaf34f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023703154s
Jan 28 01:26:34.230: INFO: Pod "var-expansion-8888c0cb-81a7-46c0-ae5b-b96e7ccaf34f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021463796s
STEP: Saw pod success 01/28/23 01:26:34.23
Jan 28 01:26:34.231: INFO: Pod "var-expansion-8888c0cb-81a7-46c0-ae5b-b96e7ccaf34f" satisfied condition "Succeeded or Failed"
Jan 28 01:26:34.241: INFO: Trying to get logs from node 10.9.20.126 pod var-expansion-8888c0cb-81a7-46c0-ae5b-b96e7ccaf34f container dapi-container: <nil>
STEP: delete the pod 01/28/23 01:26:34.274
Jan 28 01:26:34.338: INFO: Waiting for pod var-expansion-8888c0cb-81a7-46c0-ae5b-b96e7ccaf34f to disappear
Jan 28 01:26:34.347: INFO: Pod var-expansion-8888c0cb-81a7-46c0-ae5b-b96e7ccaf34f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 28 01:26:34.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7853" for this suite. 01/28/23 01:26:34.369
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":227,"skipped":4075,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.290 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:26:28.095
    Jan 28 01:26:28.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename var-expansion 01/28/23 01:26:28.096
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:26:28.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:26:28.163
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 01/28/23 01:26:28.175
    Jan 28 01:26:28.209: INFO: Waiting up to 5m0s for pod "var-expansion-8888c0cb-81a7-46c0-ae5b-b96e7ccaf34f" in namespace "var-expansion-7853" to be "Succeeded or Failed"
    Jan 28 01:26:28.220: INFO: Pod "var-expansion-8888c0cb-81a7-46c0-ae5b-b96e7ccaf34f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.936407ms
    Jan 28 01:26:30.231: INFO: Pod "var-expansion-8888c0cb-81a7-46c0-ae5b-b96e7ccaf34f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021836542s
    Jan 28 01:26:32.233: INFO: Pod "var-expansion-8888c0cb-81a7-46c0-ae5b-b96e7ccaf34f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023703154s
    Jan 28 01:26:34.230: INFO: Pod "var-expansion-8888c0cb-81a7-46c0-ae5b-b96e7ccaf34f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021463796s
    STEP: Saw pod success 01/28/23 01:26:34.23
    Jan 28 01:26:34.231: INFO: Pod "var-expansion-8888c0cb-81a7-46c0-ae5b-b96e7ccaf34f" satisfied condition "Succeeded or Failed"
    Jan 28 01:26:34.241: INFO: Trying to get logs from node 10.9.20.126 pod var-expansion-8888c0cb-81a7-46c0-ae5b-b96e7ccaf34f container dapi-container: <nil>
    STEP: delete the pod 01/28/23 01:26:34.274
    Jan 28 01:26:34.338: INFO: Waiting for pod var-expansion-8888c0cb-81a7-46c0-ae5b-b96e7ccaf34f to disappear
    Jan 28 01:26:34.347: INFO: Pod var-expansion-8888c0cb-81a7-46c0-ae5b-b96e7ccaf34f no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 28 01:26:34.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7853" for this suite. 01/28/23 01:26:34.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:26:34.42
Jan 28 01:26:34.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename gc 01/28/23 01:26:34.423
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:26:34.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:26:34.464
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/28/23 01:26:34.499
STEP: delete the rc 01/28/23 01:26:39.537
STEP: wait for the rc to be deleted 01/28/23 01:26:39.612
STEP: Gathering metrics 01/28/23 01:26:40.66
W0128 01:26:40.703198      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 28 01:26:40.703: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 28 01:26:40.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2333" for this suite. 01/28/23 01:26:40.721
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":228,"skipped":4132,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.324 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:26:34.42
    Jan 28 01:26:34.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename gc 01/28/23 01:26:34.423
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:26:34.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:26:34.464
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/28/23 01:26:34.499
    STEP: delete the rc 01/28/23 01:26:39.537
    STEP: wait for the rc to be deleted 01/28/23 01:26:39.612
    STEP: Gathering metrics 01/28/23 01:26:40.66
    W0128 01:26:40.703198      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 28 01:26:40.703: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 28 01:26:40.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2333" for this suite. 01/28/23 01:26:40.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:26:40.745
Jan 28 01:26:40.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename downward-api 01/28/23 01:26:40.746
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:26:40.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:26:40.812
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 01/28/23 01:26:40.825
Jan 28 01:26:40.849: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881" in namespace "downward-api-6189" to be "Succeeded or Failed"
Jan 28 01:26:40.894: INFO: Pod "downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881": Phase="Pending", Reason="", readiness=false. Elapsed: 45.266298ms
Jan 28 01:26:42.918: INFO: Pod "downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068830013s
Jan 28 01:26:44.928: INFO: Pod "downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079460444s
Jan 28 01:26:46.910: INFO: Pod "downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881": Phase="Pending", Reason="", readiness=false. Elapsed: 6.060959069s
Jan 28 01:26:48.915: INFO: Pod "downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881": Phase="Pending", Reason="", readiness=false. Elapsed: 8.066199094s
Jan 28 01:26:50.908: INFO: Pod "downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.05969959s
STEP: Saw pod success 01/28/23 01:26:50.909
Jan 28 01:26:50.909: INFO: Pod "downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881" satisfied condition "Succeeded or Failed"
Jan 28 01:26:50.921: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881 container client-container: <nil>
STEP: delete the pod 01/28/23 01:26:50.997
Jan 28 01:26:51.029: INFO: Waiting for pod downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881 to disappear
Jan 28 01:26:51.041: INFO: Pod downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 28 01:26:51.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6189" for this suite. 01/28/23 01:26:51.057
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":229,"skipped":4142,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.332 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:26:40.745
    Jan 28 01:26:40.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename downward-api 01/28/23 01:26:40.746
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:26:40.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:26:40.812
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 01/28/23 01:26:40.825
    Jan 28 01:26:40.849: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881" in namespace "downward-api-6189" to be "Succeeded or Failed"
    Jan 28 01:26:40.894: INFO: Pod "downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881": Phase="Pending", Reason="", readiness=false. Elapsed: 45.266298ms
    Jan 28 01:26:42.918: INFO: Pod "downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068830013s
    Jan 28 01:26:44.928: INFO: Pod "downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079460444s
    Jan 28 01:26:46.910: INFO: Pod "downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881": Phase="Pending", Reason="", readiness=false. Elapsed: 6.060959069s
    Jan 28 01:26:48.915: INFO: Pod "downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881": Phase="Pending", Reason="", readiness=false. Elapsed: 8.066199094s
    Jan 28 01:26:50.908: INFO: Pod "downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.05969959s
    STEP: Saw pod success 01/28/23 01:26:50.909
    Jan 28 01:26:50.909: INFO: Pod "downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881" satisfied condition "Succeeded or Failed"
    Jan 28 01:26:50.921: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881 container client-container: <nil>
    STEP: delete the pod 01/28/23 01:26:50.997
    Jan 28 01:26:51.029: INFO: Waiting for pod downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881 to disappear
    Jan 28 01:26:51.041: INFO: Pod downwardapi-volume-ea8b32ce-05f3-49aa-8b08-fdd8344b0881 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 28 01:26:51.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6189" for this suite. 01/28/23 01:26:51.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:26:51.085
Jan 28 01:26:51.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename svcaccounts 01/28/23 01:26:51.088
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:26:51.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:26:51.139
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Jan 28 01:26:51.182: INFO: created pod
Jan 28 01:26:51.183: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6371" to be "Succeeded or Failed"
Jan 28 01:26:51.198: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 15.502525ms
Jan 28 01:26:53.211: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028354762s
Jan 28 01:26:55.211: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028405836s
Jan 28 01:26:57.214: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031346903s
STEP: Saw pod success 01/28/23 01:26:57.214
Jan 28 01:26:57.215: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 28 01:27:27.216: INFO: polling logs
Jan 28 01:27:27.245: INFO: Pod logs: 
I0128 01:26:52.698618       1 log.go:195] OK: Got token
I0128 01:26:52.698798       1 log.go:195] validating with in-cluster discovery
I0128 01:26:52.699722       1 log.go:195] OK: got issuer https://kubernetes.default.svc
I0128 01:26:52.699919       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-6371:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674869811, NotBefore:1674869211, IssuedAt:1674869211, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6371", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"174ee2e9-c270-46c3-9e17-7966058a8c09"}}}
I0128 01:26:52.721280       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0128 01:26:52.740065       1 log.go:195] OK: Validated signature on JWT
I0128 01:26:52.740260       1 log.go:195] OK: Got valid claims from token!
I0128 01:26:52.740357       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-6371:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674869811, NotBefore:1674869211, IssuedAt:1674869211, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6371", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"174ee2e9-c270-46c3-9e17-7966058a8c09"}}}

Jan 28 01:27:27.245: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 28 01:27:27.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6371" for this suite. 01/28/23 01:27:27.282
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":230,"skipped":4153,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.216 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:26:51.085
    Jan 28 01:26:51.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename svcaccounts 01/28/23 01:26:51.088
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:26:51.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:26:51.139
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Jan 28 01:26:51.182: INFO: created pod
    Jan 28 01:26:51.183: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6371" to be "Succeeded or Failed"
    Jan 28 01:26:51.198: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 15.502525ms
    Jan 28 01:26:53.211: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028354762s
    Jan 28 01:26:55.211: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028405836s
    Jan 28 01:26:57.214: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031346903s
    STEP: Saw pod success 01/28/23 01:26:57.214
    Jan 28 01:26:57.215: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan 28 01:27:27.216: INFO: polling logs
    Jan 28 01:27:27.245: INFO: Pod logs: 
    I0128 01:26:52.698618       1 log.go:195] OK: Got token
    I0128 01:26:52.698798       1 log.go:195] validating with in-cluster discovery
    I0128 01:26:52.699722       1 log.go:195] OK: got issuer https://kubernetes.default.svc
    I0128 01:26:52.699919       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-6371:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674869811, NotBefore:1674869211, IssuedAt:1674869211, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6371", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"174ee2e9-c270-46c3-9e17-7966058a8c09"}}}
    I0128 01:26:52.721280       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
    I0128 01:26:52.740065       1 log.go:195] OK: Validated signature on JWT
    I0128 01:26:52.740260       1 log.go:195] OK: Got valid claims from token!
    I0128 01:26:52.740357       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-6371:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674869811, NotBefore:1674869211, IssuedAt:1674869211, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6371", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"174ee2e9-c270-46c3-9e17-7966058a8c09"}}}

    Jan 28 01:27:27.245: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 28 01:27:27.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6371" for this suite. 01/28/23 01:27:27.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:27:27.304
Jan 28 01:27:27.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubelet-test 01/28/23 01:27:27.306
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:27:27.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:27:27.362
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/28/23 01:27:27.396
Jan 28 01:27:27.396: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases6a6673c1-3839-40e5-8563-577a91081957" in namespace "kubelet-test-4197" to be "completed"
Jan 28 01:27:27.425: INFO: Pod "agnhost-host-aliases6a6673c1-3839-40e5-8563-577a91081957": Phase="Pending", Reason="", readiness=false. Elapsed: 29.324949ms
Jan 28 01:27:29.438: INFO: Pod "agnhost-host-aliases6a6673c1-3839-40e5-8563-577a91081957": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041712512s
Jan 28 01:27:31.438: INFO: Pod "agnhost-host-aliases6a6673c1-3839-40e5-8563-577a91081957": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042325687s
Jan 28 01:27:33.439: INFO: Pod "agnhost-host-aliases6a6673c1-3839-40e5-8563-577a91081957": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043270658s
Jan 28 01:27:33.439: INFO: Pod "agnhost-host-aliases6a6673c1-3839-40e5-8563-577a91081957" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 28 01:27:33.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4197" for this suite. 01/28/23 01:27:33.486
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":231,"skipped":4161,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.203 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:27:27.304
    Jan 28 01:27:27.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubelet-test 01/28/23 01:27:27.306
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:27:27.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:27:27.362
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/28/23 01:27:27.396
    Jan 28 01:27:27.396: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases6a6673c1-3839-40e5-8563-577a91081957" in namespace "kubelet-test-4197" to be "completed"
    Jan 28 01:27:27.425: INFO: Pod "agnhost-host-aliases6a6673c1-3839-40e5-8563-577a91081957": Phase="Pending", Reason="", readiness=false. Elapsed: 29.324949ms
    Jan 28 01:27:29.438: INFO: Pod "agnhost-host-aliases6a6673c1-3839-40e5-8563-577a91081957": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041712512s
    Jan 28 01:27:31.438: INFO: Pod "agnhost-host-aliases6a6673c1-3839-40e5-8563-577a91081957": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042325687s
    Jan 28 01:27:33.439: INFO: Pod "agnhost-host-aliases6a6673c1-3839-40e5-8563-577a91081957": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043270658s
    Jan 28 01:27:33.439: INFO: Pod "agnhost-host-aliases6a6673c1-3839-40e5-8563-577a91081957" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 28 01:27:33.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4197" for this suite. 01/28/23 01:27:33.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:27:33.51
Jan 28 01:27:33.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename emptydir 01/28/23 01:27:33.512
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:27:33.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:27:33.622
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/28/23 01:27:33.634
Jan 28 01:27:33.657: INFO: Waiting up to 5m0s for pod "pod-08c312ba-d378-4a1d-b54e-a6606279f3d4" in namespace "emptydir-1887" to be "Succeeded or Failed"
Jan 28 01:27:33.696: INFO: Pod "pod-08c312ba-d378-4a1d-b54e-a6606279f3d4": Phase="Pending", Reason="", readiness=false. Elapsed: 38.931667ms
Jan 28 01:27:35.711: INFO: Pod "pod-08c312ba-d378-4a1d-b54e-a6606279f3d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054851453s
Jan 28 01:27:37.710: INFO: Pod "pod-08c312ba-d378-4a1d-b54e-a6606279f3d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053276385s
STEP: Saw pod success 01/28/23 01:27:37.71
Jan 28 01:27:37.711: INFO: Pod "pod-08c312ba-d378-4a1d-b54e-a6606279f3d4" satisfied condition "Succeeded or Failed"
Jan 28 01:27:37.723: INFO: Trying to get logs from node 10.9.20.126 pod pod-08c312ba-d378-4a1d-b54e-a6606279f3d4 container test-container: <nil>
STEP: delete the pod 01/28/23 01:27:37.751
Jan 28 01:27:37.781: INFO: Waiting for pod pod-08c312ba-d378-4a1d-b54e-a6606279f3d4 to disappear
Jan 28 01:27:37.792: INFO: Pod pod-08c312ba-d378-4a1d-b54e-a6606279f3d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 28 01:27:37.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1887" for this suite. 01/28/23 01:27:37.809
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":232,"skipped":4188,"failed":0}
------------------------------
â€¢ [4.317 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:27:33.51
    Jan 28 01:27:33.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename emptydir 01/28/23 01:27:33.512
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:27:33.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:27:33.622
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/28/23 01:27:33.634
    Jan 28 01:27:33.657: INFO: Waiting up to 5m0s for pod "pod-08c312ba-d378-4a1d-b54e-a6606279f3d4" in namespace "emptydir-1887" to be "Succeeded or Failed"
    Jan 28 01:27:33.696: INFO: Pod "pod-08c312ba-d378-4a1d-b54e-a6606279f3d4": Phase="Pending", Reason="", readiness=false. Elapsed: 38.931667ms
    Jan 28 01:27:35.711: INFO: Pod "pod-08c312ba-d378-4a1d-b54e-a6606279f3d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054851453s
    Jan 28 01:27:37.710: INFO: Pod "pod-08c312ba-d378-4a1d-b54e-a6606279f3d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053276385s
    STEP: Saw pod success 01/28/23 01:27:37.71
    Jan 28 01:27:37.711: INFO: Pod "pod-08c312ba-d378-4a1d-b54e-a6606279f3d4" satisfied condition "Succeeded or Failed"
    Jan 28 01:27:37.723: INFO: Trying to get logs from node 10.9.20.126 pod pod-08c312ba-d378-4a1d-b54e-a6606279f3d4 container test-container: <nil>
    STEP: delete the pod 01/28/23 01:27:37.751
    Jan 28 01:27:37.781: INFO: Waiting for pod pod-08c312ba-d378-4a1d-b54e-a6606279f3d4 to disappear
    Jan 28 01:27:37.792: INFO: Pod pod-08c312ba-d378-4a1d-b54e-a6606279f3d4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 28 01:27:37.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1887" for this suite. 01/28/23 01:27:37.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:27:37.833
Jan 28 01:27:37.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 01:27:37.842
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:27:37.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:27:37.898
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-e7fa11f5-6f13-4946-a4a6-59f81898bd3d 01/28/23 01:27:37.91
STEP: Creating a pod to test consume configMaps 01/28/23 01:27:37.923
Jan 28 01:27:37.946: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ac9242c6-503e-48c8-b6db-0b81aa6bba56" in namespace "projected-402" to be "Succeeded or Failed"
Jan 28 01:27:37.959: INFO: Pod "pod-projected-configmaps-ac9242c6-503e-48c8-b6db-0b81aa6bba56": Phase="Pending", Reason="", readiness=false. Elapsed: 13.279843ms
Jan 28 01:27:39.991: INFO: Pod "pod-projected-configmaps-ac9242c6-503e-48c8-b6db-0b81aa6bba56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045304154s
Jan 28 01:27:41.973: INFO: Pod "pod-projected-configmaps-ac9242c6-503e-48c8-b6db-0b81aa6bba56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026459724s
STEP: Saw pod success 01/28/23 01:27:41.973
Jan 28 01:27:41.973: INFO: Pod "pod-projected-configmaps-ac9242c6-503e-48c8-b6db-0b81aa6bba56" satisfied condition "Succeeded or Failed"
Jan 28 01:27:41.984: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-configmaps-ac9242c6-503e-48c8-b6db-0b81aa6bba56 container agnhost-container: <nil>
STEP: delete the pod 01/28/23 01:27:42.015
Jan 28 01:27:42.047: INFO: Waiting for pod pod-projected-configmaps-ac9242c6-503e-48c8-b6db-0b81aa6bba56 to disappear
Jan 28 01:27:42.058: INFO: Pod pod-projected-configmaps-ac9242c6-503e-48c8-b6db-0b81aa6bba56 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 28 01:27:42.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-402" for this suite. 01/28/23 01:27:42.073
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":233,"skipped":4208,"failed":0}
------------------------------
â€¢ [4.260 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:27:37.833
    Jan 28 01:27:37.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 01:27:37.842
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:27:37.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:27:37.898
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-e7fa11f5-6f13-4946-a4a6-59f81898bd3d 01/28/23 01:27:37.91
    STEP: Creating a pod to test consume configMaps 01/28/23 01:27:37.923
    Jan 28 01:27:37.946: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ac9242c6-503e-48c8-b6db-0b81aa6bba56" in namespace "projected-402" to be "Succeeded or Failed"
    Jan 28 01:27:37.959: INFO: Pod "pod-projected-configmaps-ac9242c6-503e-48c8-b6db-0b81aa6bba56": Phase="Pending", Reason="", readiness=false. Elapsed: 13.279843ms
    Jan 28 01:27:39.991: INFO: Pod "pod-projected-configmaps-ac9242c6-503e-48c8-b6db-0b81aa6bba56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045304154s
    Jan 28 01:27:41.973: INFO: Pod "pod-projected-configmaps-ac9242c6-503e-48c8-b6db-0b81aa6bba56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026459724s
    STEP: Saw pod success 01/28/23 01:27:41.973
    Jan 28 01:27:41.973: INFO: Pod "pod-projected-configmaps-ac9242c6-503e-48c8-b6db-0b81aa6bba56" satisfied condition "Succeeded or Failed"
    Jan 28 01:27:41.984: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-configmaps-ac9242c6-503e-48c8-b6db-0b81aa6bba56 container agnhost-container: <nil>
    STEP: delete the pod 01/28/23 01:27:42.015
    Jan 28 01:27:42.047: INFO: Waiting for pod pod-projected-configmaps-ac9242c6-503e-48c8-b6db-0b81aa6bba56 to disappear
    Jan 28 01:27:42.058: INFO: Pod pod-projected-configmaps-ac9242c6-503e-48c8-b6db-0b81aa6bba56 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 28 01:27:42.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-402" for this suite. 01/28/23 01:27:42.073
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:27:42.094
Jan 28 01:27:42.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename svc-latency 01/28/23 01:27:42.095
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:27:42.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:27:42.149
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan 28 01:27:42.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7335 01/28/23 01:27:42.158
I0128 01:27:42.172069      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7335, replica count: 1
I0128 01:27:43.223769      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0128 01:27:44.224033      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0128 01:27:45.225255      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 01:27:45.358: INFO: Created: latency-svc-7wwv7
Jan 28 01:27:45.404: INFO: Got endpoints: latency-svc-7wwv7 [78.56677ms]
Jan 28 01:27:45.461: INFO: Created: latency-svc-xkq8h
Jan 28 01:27:45.466: INFO: Created: latency-svc-4bfnl
Jan 28 01:27:45.484: INFO: Created: latency-svc-jcszd
Jan 28 01:27:45.497: INFO: Created: latency-svc-qh6rd
Jan 28 01:27:45.497: INFO: Got endpoints: latency-svc-4bfnl [91.636515ms]
Jan 28 01:27:45.498: INFO: Got endpoints: latency-svc-xkq8h [93.872334ms]
Jan 28 01:27:45.509: INFO: Created: latency-svc-jfbbx
Jan 28 01:27:45.524: INFO: Created: latency-svc-64vbm
Jan 28 01:27:45.527: INFO: Got endpoints: latency-svc-qh6rd [121.925739ms]
Jan 28 01:27:45.528: INFO: Got endpoints: latency-svc-jfbbx [122.81329ms]
Jan 28 01:27:45.527: INFO: Got endpoints: latency-svc-jcszd [122.159384ms]
Jan 28 01:27:45.533: INFO: Got endpoints: latency-svc-64vbm [127.320546ms]
Jan 28 01:27:45.534: INFO: Created: latency-svc-tmnmb
Jan 28 01:27:45.568: INFO: Created: latency-svc-98wf9
Jan 28 01:27:45.572: INFO: Got endpoints: latency-svc-tmnmb [167.417299ms]
Jan 28 01:27:45.574: INFO: Created: latency-svc-kwpgq
Jan 28 01:27:45.575: INFO: Got endpoints: latency-svc-98wf9 [169.529512ms]
Jan 28 01:27:45.582: INFO: Got endpoints: latency-svc-kwpgq [177.296538ms]
Jan 28 01:27:45.594: INFO: Created: latency-svc-hgv2d
Jan 28 01:27:45.608: INFO: Got endpoints: latency-svc-hgv2d [202.140244ms]
Jan 28 01:27:45.611: INFO: Created: latency-svc-dttpv
Jan 28 01:27:45.622: INFO: Created: latency-svc-5hnwr
Jan 28 01:27:45.622: INFO: Got endpoints: latency-svc-dttpv [217.207165ms]
Jan 28 01:27:45.637: INFO: Created: latency-svc-jj5lr
Jan 28 01:27:45.646: INFO: Got endpoints: latency-svc-jj5lr [241.727623ms]
Jan 28 01:27:45.646: INFO: Got endpoints: latency-svc-5hnwr [240.938129ms]
Jan 28 01:27:45.649: INFO: Created: latency-svc-vlshb
Jan 28 01:27:45.655: INFO: Got endpoints: latency-svc-vlshb [249.773704ms]
Jan 28 01:27:45.661: INFO: Created: latency-svc-w8mc6
Jan 28 01:27:45.670: INFO: Got endpoints: latency-svc-w8mc6 [264.96839ms]
Jan 28 01:27:45.674: INFO: Created: latency-svc-c8b8q
Jan 28 01:27:45.682: INFO: Got endpoints: latency-svc-c8b8q [184.54354ms]
Jan 28 01:27:45.718: INFO: Created: latency-svc-hnwrm
Jan 28 01:27:45.728: INFO: Got endpoints: latency-svc-hnwrm [229.167778ms]
Jan 28 01:27:45.740: INFO: Created: latency-svc-ggdlq
Jan 28 01:27:45.742: INFO: Got endpoints: latency-svc-ggdlq [213.809745ms]
Jan 28 01:27:45.749: INFO: Created: latency-svc-z4zvm
Jan 28 01:27:45.761: INFO: Got endpoints: latency-svc-z4zvm [233.594271ms]
Jan 28 01:27:45.762: INFO: Created: latency-svc-zbqr2
Jan 28 01:27:45.772: INFO: Got endpoints: latency-svc-zbqr2 [243.046691ms]
Jan 28 01:27:45.779: INFO: Created: latency-svc-mbjsw
Jan 28 01:27:45.788: INFO: Got endpoints: latency-svc-mbjsw [254.859938ms]
Jan 28 01:27:45.792: INFO: Created: latency-svc-jg99t
Jan 28 01:27:45.806: INFO: Created: latency-svc-vmbnt
Jan 28 01:27:45.808: INFO: Got endpoints: latency-svc-jg99t [236.027679ms]
Jan 28 01:27:45.813: INFO: Got endpoints: latency-svc-vmbnt [238.31117ms]
Jan 28 01:27:45.818: INFO: Created: latency-svc-7sld7
Jan 28 01:27:45.830: INFO: Got endpoints: latency-svc-7sld7 [247.060737ms]
Jan 28 01:27:45.834: INFO: Created: latency-svc-pgks2
Jan 28 01:27:45.863: INFO: Got endpoints: latency-svc-pgks2 [255.275165ms]
Jan 28 01:27:45.869: INFO: Created: latency-svc-slcrx
Jan 28 01:27:45.876: INFO: Got endpoints: latency-svc-slcrx [253.594486ms]
Jan 28 01:27:45.882: INFO: Created: latency-svc-kjzmb
Jan 28 01:27:45.893: INFO: Got endpoints: latency-svc-kjzmb [246.139837ms]
Jan 28 01:27:45.898: INFO: Created: latency-svc-4k68l
Jan 28 01:27:45.906: INFO: Got endpoints: latency-svc-4k68l [259.155594ms]
Jan 28 01:27:45.912: INFO: Created: latency-svc-w9dgj
Jan 28 01:27:45.921: INFO: Got endpoints: latency-svc-w9dgj [265.919087ms]
Jan 28 01:27:45.925: INFO: Created: latency-svc-ws8zp
Jan 28 01:27:45.937: INFO: Got endpoints: latency-svc-ws8zp [267.018573ms]
Jan 28 01:27:45.943: INFO: Created: latency-svc-2m5pj
Jan 28 01:27:45.947: INFO: Got endpoints: latency-svc-2m5pj [265.272716ms]
Jan 28 01:27:45.953: INFO: Created: latency-svc-9cwrk
Jan 28 01:27:45.963: INFO: Got endpoints: latency-svc-9cwrk [234.650489ms]
Jan 28 01:27:45.968: INFO: Created: latency-svc-rd5wj
Jan 28 01:27:45.975: INFO: Got endpoints: latency-svc-rd5wj [233.346498ms]
Jan 28 01:27:45.984: INFO: Created: latency-svc-cf454
Jan 28 01:27:45.998: INFO: Got endpoints: latency-svc-cf454 [236.561483ms]
Jan 28 01:27:46.004: INFO: Created: latency-svc-bx652
Jan 28 01:27:46.011: INFO: Got endpoints: latency-svc-bx652 [239.777768ms]
Jan 28 01:27:46.014: INFO: Created: latency-svc-xcdj8
Jan 28 01:27:46.025: INFO: Got endpoints: latency-svc-xcdj8 [236.696206ms]
Jan 28 01:27:46.031: INFO: Created: latency-svc-j89k7
Jan 28 01:27:46.038: INFO: Got endpoints: latency-svc-j89k7 [229.720255ms]
Jan 28 01:27:46.044: INFO: Created: latency-svc-b6m6n
Jan 28 01:27:46.052: INFO: Got endpoints: latency-svc-b6m6n [238.976053ms]
Jan 28 01:27:46.059: INFO: Created: latency-svc-ln8rj
Jan 28 01:27:46.066: INFO: Got endpoints: latency-svc-ln8rj [236.363091ms]
Jan 28 01:27:46.069: INFO: Created: latency-svc-gqzc9
Jan 28 01:27:46.083: INFO: Created: latency-svc-59c5v
Jan 28 01:27:46.089: INFO: Got endpoints: latency-svc-gqzc9 [225.815825ms]
Jan 28 01:27:46.092: INFO: Got endpoints: latency-svc-59c5v [215.314326ms]
Jan 28 01:27:46.105: INFO: Created: latency-svc-jbv4d
Jan 28 01:27:46.114: INFO: Got endpoints: latency-svc-jbv4d [220.48354ms]
Jan 28 01:27:46.119: INFO: Created: latency-svc-cd5xw
Jan 28 01:27:46.147: INFO: Got endpoints: latency-svc-cd5xw [240.975428ms]
Jan 28 01:27:46.153: INFO: Created: latency-svc-m5wdq
Jan 28 01:27:46.160: INFO: Got endpoints: latency-svc-m5wdq [238.947162ms]
Jan 28 01:27:46.169: INFO: Created: latency-svc-9z62k
Jan 28 01:27:46.180: INFO: Created: latency-svc-4p2kx
Jan 28 01:27:46.182: INFO: Got endpoints: latency-svc-9z62k [244.194837ms]
Jan 28 01:27:46.189: INFO: Got endpoints: latency-svc-4p2kx [241.517529ms]
Jan 28 01:27:46.193: INFO: Created: latency-svc-q6qqx
Jan 28 01:27:46.204: INFO: Got endpoints: latency-svc-q6qqx [241.42941ms]
Jan 28 01:27:46.206: INFO: Created: latency-svc-687h8
Jan 28 01:27:46.214: INFO: Got endpoints: latency-svc-687h8 [239.23013ms]
Jan 28 01:27:46.247: INFO: Created: latency-svc-xpfxs
Jan 28 01:27:46.251: INFO: Got endpoints: latency-svc-xpfxs [252.839786ms]
Jan 28 01:27:46.260: INFO: Created: latency-svc-vnj5d
Jan 28 01:27:46.269: INFO: Got endpoints: latency-svc-vnj5d [257.388225ms]
Jan 28 01:27:46.276: INFO: Created: latency-svc-5k449
Jan 28 01:27:46.284: INFO: Got endpoints: latency-svc-5k449 [259.046854ms]
Jan 28 01:27:46.290: INFO: Created: latency-svc-brfb4
Jan 28 01:27:46.309: INFO: Created: latency-svc-mcp67
Jan 28 01:27:46.311: INFO: Got endpoints: latency-svc-mcp67 [258.666753ms]
Jan 28 01:27:46.312: INFO: Got endpoints: latency-svc-brfb4 [272.328144ms]
Jan 28 01:27:46.318: INFO: Created: latency-svc-gkjcl
Jan 28 01:27:46.326: INFO: Got endpoints: latency-svc-gkjcl [259.686256ms]
Jan 28 01:27:46.337: INFO: Created: latency-svc-gt46c
Jan 28 01:27:46.340: INFO: Got endpoints: latency-svc-gt46c [250.579043ms]
Jan 28 01:27:46.344: INFO: Created: latency-svc-nbwjv
Jan 28 01:27:46.351: INFO: Got endpoints: latency-svc-nbwjv [259.568243ms]
Jan 28 01:27:46.358: INFO: Created: latency-svc-n7zq8
Jan 28 01:27:46.369: INFO: Got endpoints: latency-svc-n7zq8 [255.022995ms]
Jan 28 01:27:46.392: INFO: Created: latency-svc-4tml5
Jan 28 01:27:46.404: INFO: Got endpoints: latency-svc-4tml5 [256.836225ms]
Jan 28 01:27:46.413: INFO: Created: latency-svc-pmkj6
Jan 28 01:27:46.420: INFO: Got endpoints: latency-svc-pmkj6 [260.167525ms]
Jan 28 01:27:46.428: INFO: Created: latency-svc-5fcvz
Jan 28 01:27:46.440: INFO: Got endpoints: latency-svc-5fcvz [257.888144ms]
Jan 28 01:27:46.443: INFO: Created: latency-svc-rr9dw
Jan 28 01:27:46.458: INFO: Created: latency-svc-nmrp9
Jan 28 01:27:46.471: INFO: Created: latency-svc-q4x6x
Jan 28 01:27:46.474: INFO: Got endpoints: latency-svc-nmrp9 [269.661858ms]
Jan 28 01:27:46.474: INFO: Got endpoints: latency-svc-rr9dw [285.307496ms]
Jan 28 01:27:46.485: INFO: Created: latency-svc-pcz7t
Jan 28 01:27:46.500: INFO: Created: latency-svc-rk6bz
Jan 28 01:27:46.507: INFO: Got endpoints: latency-svc-q4x6x [292.412484ms]
Jan 28 01:27:46.508: INFO: Got endpoints: latency-svc-pcz7t [257.311173ms]
Jan 28 01:27:46.510: INFO: Got endpoints: latency-svc-rk6bz [240.880431ms]
Jan 28 01:27:46.514: INFO: Created: latency-svc-jhs26
Jan 28 01:27:46.526: INFO: Created: latency-svc-qmv6v
Jan 28 01:27:46.541: INFO: Created: latency-svc-jgdwh
Jan 28 01:27:46.543: INFO: Got endpoints: latency-svc-qmv6v [231.35424ms]
Jan 28 01:27:46.544: INFO: Got endpoints: latency-svc-jhs26 [260.177515ms]
Jan 28 01:27:46.549: INFO: Got endpoints: latency-svc-jgdwh [237.833793ms]
Jan 28 01:27:46.554: INFO: Created: latency-svc-vmjg8
Jan 28 01:27:46.563: INFO: Got endpoints: latency-svc-vmjg8 [237.002984ms]
Jan 28 01:27:46.570: INFO: Created: latency-svc-kzhrv
Jan 28 01:27:46.577: INFO: Got endpoints: latency-svc-kzhrv [237.937493ms]
Jan 28 01:27:46.585: INFO: Created: latency-svc-wn7rh
Jan 28 01:27:46.593: INFO: Got endpoints: latency-svc-wn7rh [241.801966ms]
Jan 28 01:27:46.600: INFO: Created: latency-svc-c829t
Jan 28 01:27:46.609: INFO: Got endpoints: latency-svc-c829t [240.281082ms]
Jan 28 01:27:46.614: INFO: Created: latency-svc-dd86n
Jan 28 01:27:46.622: INFO: Got endpoints: latency-svc-dd86n [217.88286ms]
Jan 28 01:27:46.630: INFO: Created: latency-svc-l7h94
Jan 28 01:27:46.640: INFO: Got endpoints: latency-svc-l7h94 [219.676164ms]
Jan 28 01:27:46.646: INFO: Created: latency-svc-tgw8v
Jan 28 01:27:46.656: INFO: Created: latency-svc-4x2lh
Jan 28 01:27:46.657: INFO: Got endpoints: latency-svc-tgw8v [216.988403ms]
Jan 28 01:27:46.664: INFO: Got endpoints: latency-svc-4x2lh [189.834278ms]
Jan 28 01:27:46.670: INFO: Created: latency-svc-7njhj
Jan 28 01:27:46.683: INFO: Created: latency-svc-lzskp
Jan 28 01:27:46.694: INFO: Got endpoints: latency-svc-lzskp [185.896922ms]
Jan 28 01:27:46.694: INFO: Got endpoints: latency-svc-7njhj [219.946053ms]
Jan 28 01:27:46.701: INFO: Created: latency-svc-k9qhn
Jan 28 01:27:46.710: INFO: Got endpoints: latency-svc-k9qhn [201.246289ms]
Jan 28 01:27:46.718: INFO: Created: latency-svc-mgvxr
Jan 28 01:27:46.729: INFO: Got endpoints: latency-svc-mgvxr [215.78024ms]
Jan 28 01:27:46.730: INFO: Created: latency-svc-86s4m
Jan 28 01:27:46.738: INFO: Got endpoints: latency-svc-86s4m [195.078744ms]
Jan 28 01:27:46.745: INFO: Created: latency-svc-mxmld
Jan 28 01:27:46.755: INFO: Got endpoints: latency-svc-mxmld [210.503449ms]
Jan 28 01:27:46.760: INFO: Created: latency-svc-csz5h
Jan 28 01:27:46.771: INFO: Created: latency-svc-bczpj
Jan 28 01:27:46.771: INFO: Got endpoints: latency-svc-csz5h [221.846469ms]
Jan 28 01:27:46.779: INFO: Got endpoints: latency-svc-bczpj [215.830913ms]
Jan 28 01:27:46.785: INFO: Created: latency-svc-w6f9z
Jan 28 01:27:46.790: INFO: Got endpoints: latency-svc-w6f9z [212.098089ms]
Jan 28 01:27:46.798: INFO: Created: latency-svc-c6nrc
Jan 28 01:27:46.807: INFO: Got endpoints: latency-svc-c6nrc [213.572609ms]
Jan 28 01:27:46.816: INFO: Created: latency-svc-q79v2
Jan 28 01:27:46.823: INFO: Got endpoints: latency-svc-q79v2 [213.638818ms]
Jan 28 01:27:46.831: INFO: Created: latency-svc-jtbs4
Jan 28 01:27:46.840: INFO: Got endpoints: latency-svc-jtbs4 [211.751202ms]
Jan 28 01:27:46.844: INFO: Created: latency-svc-wz7rh
Jan 28 01:27:46.862: INFO: Got endpoints: latency-svc-wz7rh [221.603912ms]
Jan 28 01:27:46.867: INFO: Created: latency-svc-dwnv6
Jan 28 01:27:46.875: INFO: Got endpoints: latency-svc-dwnv6 [217.784028ms]
Jan 28 01:27:46.882: INFO: Created: latency-svc-p4vq5
Jan 28 01:27:46.892: INFO: Got endpoints: latency-svc-p4vq5 [228.044137ms]
Jan 28 01:27:46.896: INFO: Created: latency-svc-28hhf
Jan 28 01:27:46.910: INFO: Got endpoints: latency-svc-28hhf [215.978333ms]
Jan 28 01:27:46.911: INFO: Created: latency-svc-jg8m7
Jan 28 01:27:46.920: INFO: Got endpoints: latency-svc-jg8m7 [225.703509ms]
Jan 28 01:27:46.922: INFO: Created: latency-svc-c5b5j
Jan 28 01:27:46.931: INFO: Got endpoints: latency-svc-c5b5j [220.957652ms]
Jan 28 01:27:46.940: INFO: Created: latency-svc-ccchf
Jan 28 01:27:46.952: INFO: Got endpoints: latency-svc-ccchf [223.677235ms]
Jan 28 01:27:46.952: INFO: Created: latency-svc-wqqz5
Jan 28 01:27:46.963: INFO: Got endpoints: latency-svc-wqqz5 [224.009338ms]
Jan 28 01:27:46.967: INFO: Created: latency-svc-zkfqj
Jan 28 01:27:46.976: INFO: Got endpoints: latency-svc-zkfqj [220.534455ms]
Jan 28 01:27:46.980: INFO: Created: latency-svc-82l75
Jan 28 01:27:46.991: INFO: Got endpoints: latency-svc-82l75 [219.541202ms]
Jan 28 01:27:46.996: INFO: Created: latency-svc-cmj98
Jan 28 01:27:47.011: INFO: Created: latency-svc-7b66c
Jan 28 01:27:47.012: INFO: Got endpoints: latency-svc-cmj98 [233.065297ms]
Jan 28 01:27:47.021: INFO: Got endpoints: latency-svc-7b66c [230.596612ms]
Jan 28 01:27:47.022: INFO: Created: latency-svc-v42lg
Jan 28 01:27:47.032: INFO: Got endpoints: latency-svc-v42lg [225.253722ms]
Jan 28 01:27:47.043: INFO: Created: latency-svc-627tq
Jan 28 01:27:47.058: INFO: Got endpoints: latency-svc-627tq [234.981938ms]
Jan 28 01:27:47.273: INFO: Created: latency-svc-h4dtm
Jan 28 01:27:47.275: INFO: Created: latency-svc-2ltw2
Jan 28 01:27:47.291: INFO: Created: latency-svc-4jkdg
Jan 28 01:27:47.292: INFO: Created: latency-svc-ng2dx
Jan 28 01:27:47.292: INFO: Created: latency-svc-vbwmk
Jan 28 01:27:47.293: INFO: Created: latency-svc-fw82t
Jan 28 01:27:47.293: INFO: Created: latency-svc-5zwrq
Jan 28 01:27:47.293: INFO: Created: latency-svc-9rflb
Jan 28 01:27:47.294: INFO: Created: latency-svc-c4h44
Jan 28 01:27:47.294: INFO: Created: latency-svc-bl8nd
Jan 28 01:27:47.294: INFO: Created: latency-svc-sm9md
Jan 28 01:27:47.295: INFO: Created: latency-svc-dgzlh
Jan 28 01:27:47.295: INFO: Created: latency-svc-27tb8
Jan 28 01:27:47.295: INFO: Created: latency-svc-kj7kx
Jan 28 01:27:47.295: INFO: Got endpoints: latency-svc-2ltw2 [332.459783ms]
Jan 28 01:27:47.295: INFO: Got endpoints: latency-svc-h4dtm [455.259192ms]
Jan 28 01:27:47.295: INFO: Created: latency-svc-h9bcd
Jan 28 01:27:47.298: INFO: Got endpoints: latency-svc-9rflb [285.60394ms]
Jan 28 01:27:47.298: INFO: Got endpoints: latency-svc-4jkdg [265.125027ms]
Jan 28 01:27:47.298: INFO: Got endpoints: latency-svc-vbwmk [306.0759ms]
Jan 28 01:27:47.299: INFO: Got endpoints: latency-svc-5zwrq [241.206382ms]
Jan 28 01:27:47.302: INFO: Got endpoints: latency-svc-fw82t [439.480121ms]
Jan 28 01:27:47.312: INFO: Got endpoints: latency-svc-sm9md [436.54319ms]
Jan 28 01:27:47.319: INFO: Got endpoints: latency-svc-kj7kx [408.703504ms]
Jan 28 01:27:47.319: INFO: Got endpoints: latency-svc-ng2dx [426.477091ms]
Jan 28 01:27:47.320: INFO: Got endpoints: latency-svc-bl8nd [399.476487ms]
Jan 28 01:27:47.320: INFO: Got endpoints: latency-svc-27tb8 [298.273955ms]
Jan 28 01:27:47.333: INFO: Got endpoints: latency-svc-dgzlh [357.020074ms]
Jan 28 01:27:47.333: INFO: Got endpoints: latency-svc-h9bcd [402.0708ms]
Jan 28 01:27:47.334: INFO: Created: latency-svc-wm4sf
Jan 28 01:27:47.335: INFO: Got endpoints: latency-svc-c4h44 [382.033533ms]
Jan 28 01:27:47.338: INFO: Got endpoints: latency-svc-wm4sf [42.979556ms]
Jan 28 01:27:47.349: INFO: Created: latency-svc-gghdk
Jan 28 01:27:47.359: INFO: Got endpoints: latency-svc-gghdk [64.080693ms]
Jan 28 01:27:47.577: INFO: Created: latency-svc-wkvzz
Jan 28 01:27:47.578: INFO: Created: latency-svc-bjptn
Jan 28 01:27:47.578: INFO: Created: latency-svc-sw6cj
Jan 28 01:27:47.578: INFO: Created: latency-svc-9ch5p
Jan 28 01:27:47.579: INFO: Created: latency-svc-rjq6v
Jan 28 01:27:47.579: INFO: Created: latency-svc-9hv6v
Jan 28 01:27:47.579: INFO: Created: latency-svc-2r9hw
Jan 28 01:27:47.579: INFO: Created: latency-svc-2cnhz
Jan 28 01:27:47.579: INFO: Created: latency-svc-kcbs5
Jan 28 01:27:47.580: INFO: Created: latency-svc-96vwv
Jan 28 01:27:47.580: INFO: Created: latency-svc-rnhtn
Jan 28 01:27:47.580: INFO: Created: latency-svc-9qhgw
Jan 28 01:27:47.580: INFO: Created: latency-svc-ncqgv
Jan 28 01:27:47.580: INFO: Created: latency-svc-z9gcf
Jan 28 01:27:47.580: INFO: Created: latency-svc-9dzdl
Jan 28 01:27:47.583: INFO: Got endpoints: latency-svc-bjptn [223.235298ms]
Jan 28 01:27:47.583: INFO: Got endpoints: latency-svc-sw6cj [285.394823ms]
Jan 28 01:27:47.583: INFO: Got endpoints: latency-svc-wkvzz [248.539901ms]
Jan 28 01:27:47.584: INFO: Got endpoints: latency-svc-9ch5p [286.155299ms]
Jan 28 01:27:47.584: INFO: Got endpoints: latency-svc-2cnhz [282.569276ms]
Jan 28 01:27:47.587: INFO: Got endpoints: latency-svc-ncqgv [289.265815ms]
Jan 28 01:27:47.588: INFO: Got endpoints: latency-svc-2r9hw [268.189872ms]
Jan 28 01:27:47.589: INFO: Got endpoints: latency-svc-z9gcf [270.533455ms]
Jan 28 01:27:47.590: INFO: Got endpoints: latency-svc-rjq6v [257.252737ms]
Jan 28 01:27:47.596: INFO: Got endpoints: latency-svc-9hv6v [275.455458ms]
Jan 28 01:27:47.606: INFO: Got endpoints: latency-svc-rnhtn [272.831801ms]
Jan 28 01:27:47.606: INFO: Got endpoints: latency-svc-9dzdl [286.575403ms]
Jan 28 01:27:47.607: INFO: Got endpoints: latency-svc-kcbs5 [295.028333ms]
Jan 28 01:27:47.606: INFO: Got endpoints: latency-svc-9qhgw [307.33883ms]
Jan 28 01:27:47.609: INFO: Got endpoints: latency-svc-96vwv [270.801749ms]
Jan 28 01:27:47.611: INFO: Created: latency-svc-4nmbm
Jan 28 01:27:47.622: INFO: Got endpoints: latency-svc-4nmbm [39.46672ms]
Jan 28 01:27:47.821: INFO: Created: latency-svc-qmtgs
Jan 28 01:27:47.821: INFO: Created: latency-svc-xc9bl
Jan 28 01:27:47.822: INFO: Created: latency-svc-5clmv
Jan 28 01:27:47.823: INFO: Created: latency-svc-pc2pv
Jan 28 01:27:47.824: INFO: Created: latency-svc-k8bj7
Jan 28 01:27:47.824: INFO: Created: latency-svc-jcmkw
Jan 28 01:27:47.825: INFO: Created: latency-svc-vszps
Jan 28 01:27:47.825: INFO: Created: latency-svc-dhg7s
Jan 28 01:27:47.825: INFO: Created: latency-svc-8kclj
Jan 28 01:27:47.831: INFO: Created: latency-svc-vddhd
Jan 28 01:27:47.832: INFO: Created: latency-svc-w9mgq
Jan 28 01:27:47.835: INFO: Created: latency-svc-2b5gr
Jan 28 01:27:47.835: INFO: Got endpoints: latency-svc-qmtgs [252.150927ms]
Jan 28 01:27:47.834: INFO: Created: latency-svc-xnjfr
Jan 28 01:27:47.832: INFO: Created: latency-svc-z5qsc
Jan 28 01:27:47.837: INFO: Created: latency-svc-k5bhx
Jan 28 01:27:47.875: INFO: Got endpoints: latency-svc-z5qsc [266.52163ms]
Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-2b5gr [268.938391ms]
Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-k5bhx [285.812177ms]
Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-k8bj7 [288.987421ms]
Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-dhg7s [280.213794ms]
Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-xc9bl [253.276805ms]
Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-pc2pv [291.453193ms]
Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-jcmkw [292.529283ms]
Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-8kclj [292.088314ms]
Jan 28 01:27:47.879: INFO: Created: latency-svc-pt2kx
Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-vszps [286.802484ms]
Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-5clmv [288.307882ms]
Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-xnjfr [269.59592ms]
Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-vddhd [269.617299ms]
Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-w9mgq [269.306002ms]
Jan 28 01:27:47.880: INFO: Got endpoints: latency-svc-pt2kx [43.941126ms]
Jan 28 01:27:47.904: INFO: Created: latency-svc-f2cnq
Jan 28 01:27:47.915: INFO: Got endpoints: latency-svc-f2cnq [39.130097ms]
Jan 28 01:27:47.921: INFO: Created: latency-svc-g5ckz
Jan 28 01:27:47.933: INFO: Got endpoints: latency-svc-g5ckz [55.911329ms]
Jan 28 01:27:47.939: INFO: Created: latency-svc-rkzdt
Jan 28 01:27:47.950: INFO: Got endpoints: latency-svc-rkzdt [73.166129ms]
Jan 28 01:27:47.951: INFO: Created: latency-svc-fbs98
Jan 28 01:27:47.959: INFO: Got endpoints: latency-svc-fbs98 [81.748808ms]
Jan 28 01:27:47.965: INFO: Created: latency-svc-jjg4k
Jan 28 01:27:47.973: INFO: Got endpoints: latency-svc-jjg4k [95.743663ms]
Jan 28 01:27:47.979: INFO: Created: latency-svc-bc9jw
Jan 28 01:27:47.988: INFO: Got endpoints: latency-svc-bc9jw [109.761765ms]
Jan 28 01:27:48.002: INFO: Created: latency-svc-dktf8
Jan 28 01:27:48.008: INFO: Got endpoints: latency-svc-dktf8 [129.328871ms]
Jan 28 01:27:48.015: INFO: Created: latency-svc-vx59v
Jan 28 01:27:48.026: INFO: Got endpoints: latency-svc-vx59v [148.641791ms]
Jan 28 01:27:48.027: INFO: Created: latency-svc-p7kdk
Jan 28 01:27:48.035: INFO: Got endpoints: latency-svc-p7kdk [155.878772ms]
Jan 28 01:27:48.040: INFO: Created: latency-svc-jqvd2
Jan 28 01:27:48.046: INFO: Got endpoints: latency-svc-jqvd2 [167.051242ms]
Jan 28 01:27:48.052: INFO: Created: latency-svc-mwmf7
Jan 28 01:27:48.060: INFO: Got endpoints: latency-svc-mwmf7 [181.029201ms]
Jan 28 01:27:48.067: INFO: Created: latency-svc-dsj5z
Jan 28 01:27:48.073: INFO: Got endpoints: latency-svc-dsj5z [194.652298ms]
Jan 28 01:27:48.081: INFO: Created: latency-svc-gz9fs
Jan 28 01:27:48.090: INFO: Got endpoints: latency-svc-gz9fs [210.505872ms]
Jan 28 01:27:48.095: INFO: Created: latency-svc-grr2z
Jan 28 01:27:48.104: INFO: Got endpoints: latency-svc-grr2z [224.222581ms]
Jan 28 01:27:48.111: INFO: Created: latency-svc-wb79h
Jan 28 01:27:48.116: INFO: Got endpoints: latency-svc-wb79h [237.004338ms]
Jan 28 01:27:48.124: INFO: Created: latency-svc-q9dhc
Jan 28 01:27:48.135: INFO: Got endpoints: latency-svc-q9dhc [219.77958ms]
Jan 28 01:27:48.138: INFO: Created: latency-svc-pk78w
Jan 28 01:27:48.147: INFO: Got endpoints: latency-svc-pk78w [214.06276ms]
Jan 28 01:27:48.158: INFO: Created: latency-svc-4w7ct
Jan 28 01:27:48.163: INFO: Got endpoints: latency-svc-4w7ct [213.153645ms]
Jan 28 01:27:48.168: INFO: Created: latency-svc-c4lbr
Jan 28 01:27:48.175: INFO: Got endpoints: latency-svc-c4lbr [215.562683ms]
Jan 28 01:27:48.180: INFO: Created: latency-svc-qdjls
Jan 28 01:27:48.188: INFO: Got endpoints: latency-svc-qdjls [215.31062ms]
Jan 28 01:27:48.194: INFO: Created: latency-svc-q72ds
Jan 28 01:27:48.201: INFO: Got endpoints: latency-svc-q72ds [213.689534ms]
Jan 28 01:27:48.207: INFO: Created: latency-svc-l26vx
Jan 28 01:27:48.215: INFO: Got endpoints: latency-svc-l26vx [207.217888ms]
Jan 28 01:27:48.219: INFO: Created: latency-svc-lc7zb
Jan 28 01:27:48.227: INFO: Got endpoints: latency-svc-lc7zb [200.774476ms]
Jan 28 01:27:48.234: INFO: Created: latency-svc-dd9kl
Jan 28 01:27:48.241: INFO: Got endpoints: latency-svc-dd9kl [206.22115ms]
Jan 28 01:27:48.249: INFO: Created: latency-svc-wm76f
Jan 28 01:27:48.255: INFO: Got endpoints: latency-svc-wm76f [209.111063ms]
Jan 28 01:27:48.263: INFO: Created: latency-svc-8v9x2
Jan 28 01:27:48.270: INFO: Got endpoints: latency-svc-8v9x2 [209.779585ms]
Jan 28 01:27:48.277: INFO: Created: latency-svc-4jwwj
Jan 28 01:27:48.284: INFO: Got endpoints: latency-svc-4jwwj [208.23642ms]
Jan 28 01:27:48.293: INFO: Created: latency-svc-6ffl6
Jan 28 01:27:48.301: INFO: Got endpoints: latency-svc-6ffl6 [211.213257ms]
Jan 28 01:27:48.305: INFO: Created: latency-svc-zzqzh
Jan 28 01:27:48.313: INFO: Got endpoints: latency-svc-zzqzh [207.616654ms]
Jan 28 01:27:48.320: INFO: Created: latency-svc-5zgs5
Jan 28 01:27:48.329: INFO: Got endpoints: latency-svc-5zgs5 [212.461575ms]
Jan 28 01:27:48.334: INFO: Created: latency-svc-k7p4b
Jan 28 01:27:48.341: INFO: Got endpoints: latency-svc-k7p4b [205.761064ms]
Jan 28 01:27:48.348: INFO: Created: latency-svc-rr585
Jan 28 01:27:48.364: INFO: Got endpoints: latency-svc-rr585 [217.299055ms]
Jan 28 01:27:48.365: INFO: Created: latency-svc-799xr
Jan 28 01:27:48.377: INFO: Created: latency-svc-ks7wg
Jan 28 01:27:48.377: INFO: Got endpoints: latency-svc-799xr [214.37253ms]
Jan 28 01:27:48.392: INFO: Created: latency-svc-nzxrv
Jan 28 01:27:48.399: INFO: Got endpoints: latency-svc-ks7wg [223.898869ms]
Jan 28 01:27:48.405: INFO: Got endpoints: latency-svc-nzxrv [216.499888ms]
Jan 28 01:27:48.407: INFO: Created: latency-svc-87ztc
Jan 28 01:27:48.411: INFO: Got endpoints: latency-svc-87ztc [209.733189ms]
Jan 28 01:27:48.418: INFO: Created: latency-svc-95b6z
Jan 28 01:27:48.426: INFO: Got endpoints: latency-svc-95b6z [209.995493ms]
Jan 28 01:27:48.431: INFO: Created: latency-svc-7bqxx
Jan 28 01:27:48.440: INFO: Got endpoints: latency-svc-7bqxx [212.640991ms]
Jan 28 01:27:48.444: INFO: Created: latency-svc-l8c6h
Jan 28 01:27:48.455: INFO: Got endpoints: latency-svc-l8c6h [212.890913ms]
Jan 28 01:27:48.458: INFO: Created: latency-svc-qz6rp
Jan 28 01:27:48.466: INFO: Got endpoints: latency-svc-qz6rp [211.356545ms]
Jan 28 01:27:48.472: INFO: Created: latency-svc-sbrr8
Jan 28 01:27:48.480: INFO: Got endpoints: latency-svc-sbrr8 [209.777688ms]
Jan 28 01:27:48.493: INFO: Created: latency-svc-x5llf
Jan 28 01:27:48.495: INFO: Created: latency-svc-98qxj
Jan 28 01:27:48.500: INFO: Got endpoints: latency-svc-x5llf [216.142384ms]
Jan 28 01:27:48.505: INFO: Got endpoints: latency-svc-98qxj [203.593912ms]
Jan 28 01:27:48.515: INFO: Created: latency-svc-qq4df
Jan 28 01:27:48.523: INFO: Got endpoints: latency-svc-qq4df [210.611044ms]
Jan 28 01:27:48.530: INFO: Created: latency-svc-22jn6
Jan 28 01:27:48.534: INFO: Got endpoints: latency-svc-22jn6 [204.983063ms]
Jan 28 01:27:48.542: INFO: Created: latency-svc-jpq2j
Jan 28 01:27:48.551: INFO: Got endpoints: latency-svc-jpq2j [209.663798ms]
Jan 28 01:27:48.559: INFO: Created: latency-svc-jvms2
Jan 28 01:27:48.565: INFO: Got endpoints: latency-svc-jvms2 [200.920691ms]
Jan 28 01:27:48.577: INFO: Created: latency-svc-kl4kp
Jan 28 01:27:48.581: INFO: Got endpoints: latency-svc-kl4kp [203.659537ms]
Jan 28 01:27:48.584: INFO: Created: latency-svc-6cx6p
Jan 28 01:27:48.594: INFO: Got endpoints: latency-svc-6cx6p [194.936801ms]
Jan 28 01:27:48.594: INFO: Latencies: [39.130097ms 39.46672ms 42.979556ms 43.941126ms 55.911329ms 64.080693ms 73.166129ms 81.748808ms 91.636515ms 93.872334ms 95.743663ms 109.761765ms 121.925739ms 122.159384ms 122.81329ms 127.320546ms 129.328871ms 148.641791ms 155.878772ms 167.051242ms 167.417299ms 169.529512ms 177.296538ms 181.029201ms 184.54354ms 185.896922ms 189.834278ms 194.652298ms 194.936801ms 195.078744ms 200.774476ms 200.920691ms 201.246289ms 202.140244ms 203.593912ms 203.659537ms 204.983063ms 205.761064ms 206.22115ms 207.217888ms 207.616654ms 208.23642ms 209.111063ms 209.663798ms 209.733189ms 209.777688ms 209.779585ms 209.995493ms 210.503449ms 210.505872ms 210.611044ms 211.213257ms 211.356545ms 211.751202ms 212.098089ms 212.461575ms 212.640991ms 212.890913ms 213.153645ms 213.572609ms 213.638818ms 213.689534ms 213.809745ms 214.06276ms 214.37253ms 215.31062ms 215.314326ms 215.562683ms 215.78024ms 215.830913ms 215.978333ms 216.142384ms 216.499888ms 216.988403ms 217.207165ms 217.299055ms 217.784028ms 217.88286ms 219.541202ms 219.676164ms 219.77958ms 219.946053ms 220.48354ms 220.534455ms 220.957652ms 221.603912ms 221.846469ms 223.235298ms 223.677235ms 223.898869ms 224.009338ms 224.222581ms 225.253722ms 225.703509ms 225.815825ms 228.044137ms 229.167778ms 229.720255ms 230.596612ms 231.35424ms 233.065297ms 233.346498ms 233.594271ms 234.650489ms 234.981938ms 236.027679ms 236.363091ms 236.561483ms 236.696206ms 237.002984ms 237.004338ms 237.833793ms 237.937493ms 238.31117ms 238.947162ms 238.976053ms 239.23013ms 239.777768ms 240.281082ms 240.880431ms 240.938129ms 240.975428ms 241.206382ms 241.42941ms 241.517529ms 241.727623ms 241.801966ms 243.046691ms 244.194837ms 246.139837ms 247.060737ms 248.539901ms 249.773704ms 250.579043ms 252.150927ms 252.839786ms 253.276805ms 253.594486ms 254.859938ms 255.022995ms 255.275165ms 256.836225ms 257.252737ms 257.311173ms 257.388225ms 257.888144ms 258.666753ms 259.046854ms 259.155594ms 259.568243ms 259.686256ms 260.167525ms 260.177515ms 264.96839ms 265.125027ms 265.272716ms 265.919087ms 266.52163ms 267.018573ms 268.189872ms 268.938391ms 269.306002ms 269.59592ms 269.617299ms 269.661858ms 270.533455ms 270.801749ms 272.328144ms 272.831801ms 275.455458ms 280.213794ms 282.569276ms 285.307496ms 285.394823ms 285.60394ms 285.812177ms 286.155299ms 286.575403ms 286.802484ms 288.307882ms 288.987421ms 289.265815ms 291.453193ms 292.088314ms 292.412484ms 292.529283ms 295.028333ms 298.273955ms 306.0759ms 307.33883ms 332.459783ms 357.020074ms 382.033533ms 399.476487ms 402.0708ms 408.703504ms 426.477091ms 436.54319ms 439.480121ms 455.259192ms]
Jan 28 01:27:48.595: INFO: 50 %ile: 233.065297ms
Jan 28 01:27:48.595: INFO: 90 %ile: 288.987421ms
Jan 28 01:27:48.595: INFO: 99 %ile: 439.480121ms
Jan 28 01:27:48.595: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Jan 28 01:27:48.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7335" for this suite. 01/28/23 01:27:48.615
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":234,"skipped":4208,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.541 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:27:42.094
    Jan 28 01:27:42.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename svc-latency 01/28/23 01:27:42.095
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:27:42.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:27:42.149
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan 28 01:27:42.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-7335 01/28/23 01:27:42.158
    I0128 01:27:42.172069      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7335, replica count: 1
    I0128 01:27:43.223769      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0128 01:27:44.224033      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0128 01:27:45.225255      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 28 01:27:45.358: INFO: Created: latency-svc-7wwv7
    Jan 28 01:27:45.404: INFO: Got endpoints: latency-svc-7wwv7 [78.56677ms]
    Jan 28 01:27:45.461: INFO: Created: latency-svc-xkq8h
    Jan 28 01:27:45.466: INFO: Created: latency-svc-4bfnl
    Jan 28 01:27:45.484: INFO: Created: latency-svc-jcszd
    Jan 28 01:27:45.497: INFO: Created: latency-svc-qh6rd
    Jan 28 01:27:45.497: INFO: Got endpoints: latency-svc-4bfnl [91.636515ms]
    Jan 28 01:27:45.498: INFO: Got endpoints: latency-svc-xkq8h [93.872334ms]
    Jan 28 01:27:45.509: INFO: Created: latency-svc-jfbbx
    Jan 28 01:27:45.524: INFO: Created: latency-svc-64vbm
    Jan 28 01:27:45.527: INFO: Got endpoints: latency-svc-qh6rd [121.925739ms]
    Jan 28 01:27:45.528: INFO: Got endpoints: latency-svc-jfbbx [122.81329ms]
    Jan 28 01:27:45.527: INFO: Got endpoints: latency-svc-jcszd [122.159384ms]
    Jan 28 01:27:45.533: INFO: Got endpoints: latency-svc-64vbm [127.320546ms]
    Jan 28 01:27:45.534: INFO: Created: latency-svc-tmnmb
    Jan 28 01:27:45.568: INFO: Created: latency-svc-98wf9
    Jan 28 01:27:45.572: INFO: Got endpoints: latency-svc-tmnmb [167.417299ms]
    Jan 28 01:27:45.574: INFO: Created: latency-svc-kwpgq
    Jan 28 01:27:45.575: INFO: Got endpoints: latency-svc-98wf9 [169.529512ms]
    Jan 28 01:27:45.582: INFO: Got endpoints: latency-svc-kwpgq [177.296538ms]
    Jan 28 01:27:45.594: INFO: Created: latency-svc-hgv2d
    Jan 28 01:27:45.608: INFO: Got endpoints: latency-svc-hgv2d [202.140244ms]
    Jan 28 01:27:45.611: INFO: Created: latency-svc-dttpv
    Jan 28 01:27:45.622: INFO: Created: latency-svc-5hnwr
    Jan 28 01:27:45.622: INFO: Got endpoints: latency-svc-dttpv [217.207165ms]
    Jan 28 01:27:45.637: INFO: Created: latency-svc-jj5lr
    Jan 28 01:27:45.646: INFO: Got endpoints: latency-svc-jj5lr [241.727623ms]
    Jan 28 01:27:45.646: INFO: Got endpoints: latency-svc-5hnwr [240.938129ms]
    Jan 28 01:27:45.649: INFO: Created: latency-svc-vlshb
    Jan 28 01:27:45.655: INFO: Got endpoints: latency-svc-vlshb [249.773704ms]
    Jan 28 01:27:45.661: INFO: Created: latency-svc-w8mc6
    Jan 28 01:27:45.670: INFO: Got endpoints: latency-svc-w8mc6 [264.96839ms]
    Jan 28 01:27:45.674: INFO: Created: latency-svc-c8b8q
    Jan 28 01:27:45.682: INFO: Got endpoints: latency-svc-c8b8q [184.54354ms]
    Jan 28 01:27:45.718: INFO: Created: latency-svc-hnwrm
    Jan 28 01:27:45.728: INFO: Got endpoints: latency-svc-hnwrm [229.167778ms]
    Jan 28 01:27:45.740: INFO: Created: latency-svc-ggdlq
    Jan 28 01:27:45.742: INFO: Got endpoints: latency-svc-ggdlq [213.809745ms]
    Jan 28 01:27:45.749: INFO: Created: latency-svc-z4zvm
    Jan 28 01:27:45.761: INFO: Got endpoints: latency-svc-z4zvm [233.594271ms]
    Jan 28 01:27:45.762: INFO: Created: latency-svc-zbqr2
    Jan 28 01:27:45.772: INFO: Got endpoints: latency-svc-zbqr2 [243.046691ms]
    Jan 28 01:27:45.779: INFO: Created: latency-svc-mbjsw
    Jan 28 01:27:45.788: INFO: Got endpoints: latency-svc-mbjsw [254.859938ms]
    Jan 28 01:27:45.792: INFO: Created: latency-svc-jg99t
    Jan 28 01:27:45.806: INFO: Created: latency-svc-vmbnt
    Jan 28 01:27:45.808: INFO: Got endpoints: latency-svc-jg99t [236.027679ms]
    Jan 28 01:27:45.813: INFO: Got endpoints: latency-svc-vmbnt [238.31117ms]
    Jan 28 01:27:45.818: INFO: Created: latency-svc-7sld7
    Jan 28 01:27:45.830: INFO: Got endpoints: latency-svc-7sld7 [247.060737ms]
    Jan 28 01:27:45.834: INFO: Created: latency-svc-pgks2
    Jan 28 01:27:45.863: INFO: Got endpoints: latency-svc-pgks2 [255.275165ms]
    Jan 28 01:27:45.869: INFO: Created: latency-svc-slcrx
    Jan 28 01:27:45.876: INFO: Got endpoints: latency-svc-slcrx [253.594486ms]
    Jan 28 01:27:45.882: INFO: Created: latency-svc-kjzmb
    Jan 28 01:27:45.893: INFO: Got endpoints: latency-svc-kjzmb [246.139837ms]
    Jan 28 01:27:45.898: INFO: Created: latency-svc-4k68l
    Jan 28 01:27:45.906: INFO: Got endpoints: latency-svc-4k68l [259.155594ms]
    Jan 28 01:27:45.912: INFO: Created: latency-svc-w9dgj
    Jan 28 01:27:45.921: INFO: Got endpoints: latency-svc-w9dgj [265.919087ms]
    Jan 28 01:27:45.925: INFO: Created: latency-svc-ws8zp
    Jan 28 01:27:45.937: INFO: Got endpoints: latency-svc-ws8zp [267.018573ms]
    Jan 28 01:27:45.943: INFO: Created: latency-svc-2m5pj
    Jan 28 01:27:45.947: INFO: Got endpoints: latency-svc-2m5pj [265.272716ms]
    Jan 28 01:27:45.953: INFO: Created: latency-svc-9cwrk
    Jan 28 01:27:45.963: INFO: Got endpoints: latency-svc-9cwrk [234.650489ms]
    Jan 28 01:27:45.968: INFO: Created: latency-svc-rd5wj
    Jan 28 01:27:45.975: INFO: Got endpoints: latency-svc-rd5wj [233.346498ms]
    Jan 28 01:27:45.984: INFO: Created: latency-svc-cf454
    Jan 28 01:27:45.998: INFO: Got endpoints: latency-svc-cf454 [236.561483ms]
    Jan 28 01:27:46.004: INFO: Created: latency-svc-bx652
    Jan 28 01:27:46.011: INFO: Got endpoints: latency-svc-bx652 [239.777768ms]
    Jan 28 01:27:46.014: INFO: Created: latency-svc-xcdj8
    Jan 28 01:27:46.025: INFO: Got endpoints: latency-svc-xcdj8 [236.696206ms]
    Jan 28 01:27:46.031: INFO: Created: latency-svc-j89k7
    Jan 28 01:27:46.038: INFO: Got endpoints: latency-svc-j89k7 [229.720255ms]
    Jan 28 01:27:46.044: INFO: Created: latency-svc-b6m6n
    Jan 28 01:27:46.052: INFO: Got endpoints: latency-svc-b6m6n [238.976053ms]
    Jan 28 01:27:46.059: INFO: Created: latency-svc-ln8rj
    Jan 28 01:27:46.066: INFO: Got endpoints: latency-svc-ln8rj [236.363091ms]
    Jan 28 01:27:46.069: INFO: Created: latency-svc-gqzc9
    Jan 28 01:27:46.083: INFO: Created: latency-svc-59c5v
    Jan 28 01:27:46.089: INFO: Got endpoints: latency-svc-gqzc9 [225.815825ms]
    Jan 28 01:27:46.092: INFO: Got endpoints: latency-svc-59c5v [215.314326ms]
    Jan 28 01:27:46.105: INFO: Created: latency-svc-jbv4d
    Jan 28 01:27:46.114: INFO: Got endpoints: latency-svc-jbv4d [220.48354ms]
    Jan 28 01:27:46.119: INFO: Created: latency-svc-cd5xw
    Jan 28 01:27:46.147: INFO: Got endpoints: latency-svc-cd5xw [240.975428ms]
    Jan 28 01:27:46.153: INFO: Created: latency-svc-m5wdq
    Jan 28 01:27:46.160: INFO: Got endpoints: latency-svc-m5wdq [238.947162ms]
    Jan 28 01:27:46.169: INFO: Created: latency-svc-9z62k
    Jan 28 01:27:46.180: INFO: Created: latency-svc-4p2kx
    Jan 28 01:27:46.182: INFO: Got endpoints: latency-svc-9z62k [244.194837ms]
    Jan 28 01:27:46.189: INFO: Got endpoints: latency-svc-4p2kx [241.517529ms]
    Jan 28 01:27:46.193: INFO: Created: latency-svc-q6qqx
    Jan 28 01:27:46.204: INFO: Got endpoints: latency-svc-q6qqx [241.42941ms]
    Jan 28 01:27:46.206: INFO: Created: latency-svc-687h8
    Jan 28 01:27:46.214: INFO: Got endpoints: latency-svc-687h8 [239.23013ms]
    Jan 28 01:27:46.247: INFO: Created: latency-svc-xpfxs
    Jan 28 01:27:46.251: INFO: Got endpoints: latency-svc-xpfxs [252.839786ms]
    Jan 28 01:27:46.260: INFO: Created: latency-svc-vnj5d
    Jan 28 01:27:46.269: INFO: Got endpoints: latency-svc-vnj5d [257.388225ms]
    Jan 28 01:27:46.276: INFO: Created: latency-svc-5k449
    Jan 28 01:27:46.284: INFO: Got endpoints: latency-svc-5k449 [259.046854ms]
    Jan 28 01:27:46.290: INFO: Created: latency-svc-brfb4
    Jan 28 01:27:46.309: INFO: Created: latency-svc-mcp67
    Jan 28 01:27:46.311: INFO: Got endpoints: latency-svc-mcp67 [258.666753ms]
    Jan 28 01:27:46.312: INFO: Got endpoints: latency-svc-brfb4 [272.328144ms]
    Jan 28 01:27:46.318: INFO: Created: latency-svc-gkjcl
    Jan 28 01:27:46.326: INFO: Got endpoints: latency-svc-gkjcl [259.686256ms]
    Jan 28 01:27:46.337: INFO: Created: latency-svc-gt46c
    Jan 28 01:27:46.340: INFO: Got endpoints: latency-svc-gt46c [250.579043ms]
    Jan 28 01:27:46.344: INFO: Created: latency-svc-nbwjv
    Jan 28 01:27:46.351: INFO: Got endpoints: latency-svc-nbwjv [259.568243ms]
    Jan 28 01:27:46.358: INFO: Created: latency-svc-n7zq8
    Jan 28 01:27:46.369: INFO: Got endpoints: latency-svc-n7zq8 [255.022995ms]
    Jan 28 01:27:46.392: INFO: Created: latency-svc-4tml5
    Jan 28 01:27:46.404: INFO: Got endpoints: latency-svc-4tml5 [256.836225ms]
    Jan 28 01:27:46.413: INFO: Created: latency-svc-pmkj6
    Jan 28 01:27:46.420: INFO: Got endpoints: latency-svc-pmkj6 [260.167525ms]
    Jan 28 01:27:46.428: INFO: Created: latency-svc-5fcvz
    Jan 28 01:27:46.440: INFO: Got endpoints: latency-svc-5fcvz [257.888144ms]
    Jan 28 01:27:46.443: INFO: Created: latency-svc-rr9dw
    Jan 28 01:27:46.458: INFO: Created: latency-svc-nmrp9
    Jan 28 01:27:46.471: INFO: Created: latency-svc-q4x6x
    Jan 28 01:27:46.474: INFO: Got endpoints: latency-svc-nmrp9 [269.661858ms]
    Jan 28 01:27:46.474: INFO: Got endpoints: latency-svc-rr9dw [285.307496ms]
    Jan 28 01:27:46.485: INFO: Created: latency-svc-pcz7t
    Jan 28 01:27:46.500: INFO: Created: latency-svc-rk6bz
    Jan 28 01:27:46.507: INFO: Got endpoints: latency-svc-q4x6x [292.412484ms]
    Jan 28 01:27:46.508: INFO: Got endpoints: latency-svc-pcz7t [257.311173ms]
    Jan 28 01:27:46.510: INFO: Got endpoints: latency-svc-rk6bz [240.880431ms]
    Jan 28 01:27:46.514: INFO: Created: latency-svc-jhs26
    Jan 28 01:27:46.526: INFO: Created: latency-svc-qmv6v
    Jan 28 01:27:46.541: INFO: Created: latency-svc-jgdwh
    Jan 28 01:27:46.543: INFO: Got endpoints: latency-svc-qmv6v [231.35424ms]
    Jan 28 01:27:46.544: INFO: Got endpoints: latency-svc-jhs26 [260.177515ms]
    Jan 28 01:27:46.549: INFO: Got endpoints: latency-svc-jgdwh [237.833793ms]
    Jan 28 01:27:46.554: INFO: Created: latency-svc-vmjg8
    Jan 28 01:27:46.563: INFO: Got endpoints: latency-svc-vmjg8 [237.002984ms]
    Jan 28 01:27:46.570: INFO: Created: latency-svc-kzhrv
    Jan 28 01:27:46.577: INFO: Got endpoints: latency-svc-kzhrv [237.937493ms]
    Jan 28 01:27:46.585: INFO: Created: latency-svc-wn7rh
    Jan 28 01:27:46.593: INFO: Got endpoints: latency-svc-wn7rh [241.801966ms]
    Jan 28 01:27:46.600: INFO: Created: latency-svc-c829t
    Jan 28 01:27:46.609: INFO: Got endpoints: latency-svc-c829t [240.281082ms]
    Jan 28 01:27:46.614: INFO: Created: latency-svc-dd86n
    Jan 28 01:27:46.622: INFO: Got endpoints: latency-svc-dd86n [217.88286ms]
    Jan 28 01:27:46.630: INFO: Created: latency-svc-l7h94
    Jan 28 01:27:46.640: INFO: Got endpoints: latency-svc-l7h94 [219.676164ms]
    Jan 28 01:27:46.646: INFO: Created: latency-svc-tgw8v
    Jan 28 01:27:46.656: INFO: Created: latency-svc-4x2lh
    Jan 28 01:27:46.657: INFO: Got endpoints: latency-svc-tgw8v [216.988403ms]
    Jan 28 01:27:46.664: INFO: Got endpoints: latency-svc-4x2lh [189.834278ms]
    Jan 28 01:27:46.670: INFO: Created: latency-svc-7njhj
    Jan 28 01:27:46.683: INFO: Created: latency-svc-lzskp
    Jan 28 01:27:46.694: INFO: Got endpoints: latency-svc-lzskp [185.896922ms]
    Jan 28 01:27:46.694: INFO: Got endpoints: latency-svc-7njhj [219.946053ms]
    Jan 28 01:27:46.701: INFO: Created: latency-svc-k9qhn
    Jan 28 01:27:46.710: INFO: Got endpoints: latency-svc-k9qhn [201.246289ms]
    Jan 28 01:27:46.718: INFO: Created: latency-svc-mgvxr
    Jan 28 01:27:46.729: INFO: Got endpoints: latency-svc-mgvxr [215.78024ms]
    Jan 28 01:27:46.730: INFO: Created: latency-svc-86s4m
    Jan 28 01:27:46.738: INFO: Got endpoints: latency-svc-86s4m [195.078744ms]
    Jan 28 01:27:46.745: INFO: Created: latency-svc-mxmld
    Jan 28 01:27:46.755: INFO: Got endpoints: latency-svc-mxmld [210.503449ms]
    Jan 28 01:27:46.760: INFO: Created: latency-svc-csz5h
    Jan 28 01:27:46.771: INFO: Created: latency-svc-bczpj
    Jan 28 01:27:46.771: INFO: Got endpoints: latency-svc-csz5h [221.846469ms]
    Jan 28 01:27:46.779: INFO: Got endpoints: latency-svc-bczpj [215.830913ms]
    Jan 28 01:27:46.785: INFO: Created: latency-svc-w6f9z
    Jan 28 01:27:46.790: INFO: Got endpoints: latency-svc-w6f9z [212.098089ms]
    Jan 28 01:27:46.798: INFO: Created: latency-svc-c6nrc
    Jan 28 01:27:46.807: INFO: Got endpoints: latency-svc-c6nrc [213.572609ms]
    Jan 28 01:27:46.816: INFO: Created: latency-svc-q79v2
    Jan 28 01:27:46.823: INFO: Got endpoints: latency-svc-q79v2 [213.638818ms]
    Jan 28 01:27:46.831: INFO: Created: latency-svc-jtbs4
    Jan 28 01:27:46.840: INFO: Got endpoints: latency-svc-jtbs4 [211.751202ms]
    Jan 28 01:27:46.844: INFO: Created: latency-svc-wz7rh
    Jan 28 01:27:46.862: INFO: Got endpoints: latency-svc-wz7rh [221.603912ms]
    Jan 28 01:27:46.867: INFO: Created: latency-svc-dwnv6
    Jan 28 01:27:46.875: INFO: Got endpoints: latency-svc-dwnv6 [217.784028ms]
    Jan 28 01:27:46.882: INFO: Created: latency-svc-p4vq5
    Jan 28 01:27:46.892: INFO: Got endpoints: latency-svc-p4vq5 [228.044137ms]
    Jan 28 01:27:46.896: INFO: Created: latency-svc-28hhf
    Jan 28 01:27:46.910: INFO: Got endpoints: latency-svc-28hhf [215.978333ms]
    Jan 28 01:27:46.911: INFO: Created: latency-svc-jg8m7
    Jan 28 01:27:46.920: INFO: Got endpoints: latency-svc-jg8m7 [225.703509ms]
    Jan 28 01:27:46.922: INFO: Created: latency-svc-c5b5j
    Jan 28 01:27:46.931: INFO: Got endpoints: latency-svc-c5b5j [220.957652ms]
    Jan 28 01:27:46.940: INFO: Created: latency-svc-ccchf
    Jan 28 01:27:46.952: INFO: Got endpoints: latency-svc-ccchf [223.677235ms]
    Jan 28 01:27:46.952: INFO: Created: latency-svc-wqqz5
    Jan 28 01:27:46.963: INFO: Got endpoints: latency-svc-wqqz5 [224.009338ms]
    Jan 28 01:27:46.967: INFO: Created: latency-svc-zkfqj
    Jan 28 01:27:46.976: INFO: Got endpoints: latency-svc-zkfqj [220.534455ms]
    Jan 28 01:27:46.980: INFO: Created: latency-svc-82l75
    Jan 28 01:27:46.991: INFO: Got endpoints: latency-svc-82l75 [219.541202ms]
    Jan 28 01:27:46.996: INFO: Created: latency-svc-cmj98
    Jan 28 01:27:47.011: INFO: Created: latency-svc-7b66c
    Jan 28 01:27:47.012: INFO: Got endpoints: latency-svc-cmj98 [233.065297ms]
    Jan 28 01:27:47.021: INFO: Got endpoints: latency-svc-7b66c [230.596612ms]
    Jan 28 01:27:47.022: INFO: Created: latency-svc-v42lg
    Jan 28 01:27:47.032: INFO: Got endpoints: latency-svc-v42lg [225.253722ms]
    Jan 28 01:27:47.043: INFO: Created: latency-svc-627tq
    Jan 28 01:27:47.058: INFO: Got endpoints: latency-svc-627tq [234.981938ms]
    Jan 28 01:27:47.273: INFO: Created: latency-svc-h4dtm
    Jan 28 01:27:47.275: INFO: Created: latency-svc-2ltw2
    Jan 28 01:27:47.291: INFO: Created: latency-svc-4jkdg
    Jan 28 01:27:47.292: INFO: Created: latency-svc-ng2dx
    Jan 28 01:27:47.292: INFO: Created: latency-svc-vbwmk
    Jan 28 01:27:47.293: INFO: Created: latency-svc-fw82t
    Jan 28 01:27:47.293: INFO: Created: latency-svc-5zwrq
    Jan 28 01:27:47.293: INFO: Created: latency-svc-9rflb
    Jan 28 01:27:47.294: INFO: Created: latency-svc-c4h44
    Jan 28 01:27:47.294: INFO: Created: latency-svc-bl8nd
    Jan 28 01:27:47.294: INFO: Created: latency-svc-sm9md
    Jan 28 01:27:47.295: INFO: Created: latency-svc-dgzlh
    Jan 28 01:27:47.295: INFO: Created: latency-svc-27tb8
    Jan 28 01:27:47.295: INFO: Created: latency-svc-kj7kx
    Jan 28 01:27:47.295: INFO: Got endpoints: latency-svc-2ltw2 [332.459783ms]
    Jan 28 01:27:47.295: INFO: Got endpoints: latency-svc-h4dtm [455.259192ms]
    Jan 28 01:27:47.295: INFO: Created: latency-svc-h9bcd
    Jan 28 01:27:47.298: INFO: Got endpoints: latency-svc-9rflb [285.60394ms]
    Jan 28 01:27:47.298: INFO: Got endpoints: latency-svc-4jkdg [265.125027ms]
    Jan 28 01:27:47.298: INFO: Got endpoints: latency-svc-vbwmk [306.0759ms]
    Jan 28 01:27:47.299: INFO: Got endpoints: latency-svc-5zwrq [241.206382ms]
    Jan 28 01:27:47.302: INFO: Got endpoints: latency-svc-fw82t [439.480121ms]
    Jan 28 01:27:47.312: INFO: Got endpoints: latency-svc-sm9md [436.54319ms]
    Jan 28 01:27:47.319: INFO: Got endpoints: latency-svc-kj7kx [408.703504ms]
    Jan 28 01:27:47.319: INFO: Got endpoints: latency-svc-ng2dx [426.477091ms]
    Jan 28 01:27:47.320: INFO: Got endpoints: latency-svc-bl8nd [399.476487ms]
    Jan 28 01:27:47.320: INFO: Got endpoints: latency-svc-27tb8 [298.273955ms]
    Jan 28 01:27:47.333: INFO: Got endpoints: latency-svc-dgzlh [357.020074ms]
    Jan 28 01:27:47.333: INFO: Got endpoints: latency-svc-h9bcd [402.0708ms]
    Jan 28 01:27:47.334: INFO: Created: latency-svc-wm4sf
    Jan 28 01:27:47.335: INFO: Got endpoints: latency-svc-c4h44 [382.033533ms]
    Jan 28 01:27:47.338: INFO: Got endpoints: latency-svc-wm4sf [42.979556ms]
    Jan 28 01:27:47.349: INFO: Created: latency-svc-gghdk
    Jan 28 01:27:47.359: INFO: Got endpoints: latency-svc-gghdk [64.080693ms]
    Jan 28 01:27:47.577: INFO: Created: latency-svc-wkvzz
    Jan 28 01:27:47.578: INFO: Created: latency-svc-bjptn
    Jan 28 01:27:47.578: INFO: Created: latency-svc-sw6cj
    Jan 28 01:27:47.578: INFO: Created: latency-svc-9ch5p
    Jan 28 01:27:47.579: INFO: Created: latency-svc-rjq6v
    Jan 28 01:27:47.579: INFO: Created: latency-svc-9hv6v
    Jan 28 01:27:47.579: INFO: Created: latency-svc-2r9hw
    Jan 28 01:27:47.579: INFO: Created: latency-svc-2cnhz
    Jan 28 01:27:47.579: INFO: Created: latency-svc-kcbs5
    Jan 28 01:27:47.580: INFO: Created: latency-svc-96vwv
    Jan 28 01:27:47.580: INFO: Created: latency-svc-rnhtn
    Jan 28 01:27:47.580: INFO: Created: latency-svc-9qhgw
    Jan 28 01:27:47.580: INFO: Created: latency-svc-ncqgv
    Jan 28 01:27:47.580: INFO: Created: latency-svc-z9gcf
    Jan 28 01:27:47.580: INFO: Created: latency-svc-9dzdl
    Jan 28 01:27:47.583: INFO: Got endpoints: latency-svc-bjptn [223.235298ms]
    Jan 28 01:27:47.583: INFO: Got endpoints: latency-svc-sw6cj [285.394823ms]
    Jan 28 01:27:47.583: INFO: Got endpoints: latency-svc-wkvzz [248.539901ms]
    Jan 28 01:27:47.584: INFO: Got endpoints: latency-svc-9ch5p [286.155299ms]
    Jan 28 01:27:47.584: INFO: Got endpoints: latency-svc-2cnhz [282.569276ms]
    Jan 28 01:27:47.587: INFO: Got endpoints: latency-svc-ncqgv [289.265815ms]
    Jan 28 01:27:47.588: INFO: Got endpoints: latency-svc-2r9hw [268.189872ms]
    Jan 28 01:27:47.589: INFO: Got endpoints: latency-svc-z9gcf [270.533455ms]
    Jan 28 01:27:47.590: INFO: Got endpoints: latency-svc-rjq6v [257.252737ms]
    Jan 28 01:27:47.596: INFO: Got endpoints: latency-svc-9hv6v [275.455458ms]
    Jan 28 01:27:47.606: INFO: Got endpoints: latency-svc-rnhtn [272.831801ms]
    Jan 28 01:27:47.606: INFO: Got endpoints: latency-svc-9dzdl [286.575403ms]
    Jan 28 01:27:47.607: INFO: Got endpoints: latency-svc-kcbs5 [295.028333ms]
    Jan 28 01:27:47.606: INFO: Got endpoints: latency-svc-9qhgw [307.33883ms]
    Jan 28 01:27:47.609: INFO: Got endpoints: latency-svc-96vwv [270.801749ms]
    Jan 28 01:27:47.611: INFO: Created: latency-svc-4nmbm
    Jan 28 01:27:47.622: INFO: Got endpoints: latency-svc-4nmbm [39.46672ms]
    Jan 28 01:27:47.821: INFO: Created: latency-svc-qmtgs
    Jan 28 01:27:47.821: INFO: Created: latency-svc-xc9bl
    Jan 28 01:27:47.822: INFO: Created: latency-svc-5clmv
    Jan 28 01:27:47.823: INFO: Created: latency-svc-pc2pv
    Jan 28 01:27:47.824: INFO: Created: latency-svc-k8bj7
    Jan 28 01:27:47.824: INFO: Created: latency-svc-jcmkw
    Jan 28 01:27:47.825: INFO: Created: latency-svc-vszps
    Jan 28 01:27:47.825: INFO: Created: latency-svc-dhg7s
    Jan 28 01:27:47.825: INFO: Created: latency-svc-8kclj
    Jan 28 01:27:47.831: INFO: Created: latency-svc-vddhd
    Jan 28 01:27:47.832: INFO: Created: latency-svc-w9mgq
    Jan 28 01:27:47.835: INFO: Created: latency-svc-2b5gr
    Jan 28 01:27:47.835: INFO: Got endpoints: latency-svc-qmtgs [252.150927ms]
    Jan 28 01:27:47.834: INFO: Created: latency-svc-xnjfr
    Jan 28 01:27:47.832: INFO: Created: latency-svc-z5qsc
    Jan 28 01:27:47.837: INFO: Created: latency-svc-k5bhx
    Jan 28 01:27:47.875: INFO: Got endpoints: latency-svc-z5qsc [266.52163ms]
    Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-2b5gr [268.938391ms]
    Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-k5bhx [285.812177ms]
    Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-k8bj7 [288.987421ms]
    Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-dhg7s [280.213794ms]
    Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-xc9bl [253.276805ms]
    Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-pc2pv [291.453193ms]
    Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-jcmkw [292.529283ms]
    Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-8kclj [292.088314ms]
    Jan 28 01:27:47.879: INFO: Created: latency-svc-pt2kx
    Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-vszps [286.802484ms]
    Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-5clmv [288.307882ms]
    Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-xnjfr [269.59592ms]
    Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-vddhd [269.617299ms]
    Jan 28 01:27:47.876: INFO: Got endpoints: latency-svc-w9mgq [269.306002ms]
    Jan 28 01:27:47.880: INFO: Got endpoints: latency-svc-pt2kx [43.941126ms]
    Jan 28 01:27:47.904: INFO: Created: latency-svc-f2cnq
    Jan 28 01:27:47.915: INFO: Got endpoints: latency-svc-f2cnq [39.130097ms]
    Jan 28 01:27:47.921: INFO: Created: latency-svc-g5ckz
    Jan 28 01:27:47.933: INFO: Got endpoints: latency-svc-g5ckz [55.911329ms]
    Jan 28 01:27:47.939: INFO: Created: latency-svc-rkzdt
    Jan 28 01:27:47.950: INFO: Got endpoints: latency-svc-rkzdt [73.166129ms]
    Jan 28 01:27:47.951: INFO: Created: latency-svc-fbs98
    Jan 28 01:27:47.959: INFO: Got endpoints: latency-svc-fbs98 [81.748808ms]
    Jan 28 01:27:47.965: INFO: Created: latency-svc-jjg4k
    Jan 28 01:27:47.973: INFO: Got endpoints: latency-svc-jjg4k [95.743663ms]
    Jan 28 01:27:47.979: INFO: Created: latency-svc-bc9jw
    Jan 28 01:27:47.988: INFO: Got endpoints: latency-svc-bc9jw [109.761765ms]
    Jan 28 01:27:48.002: INFO: Created: latency-svc-dktf8
    Jan 28 01:27:48.008: INFO: Got endpoints: latency-svc-dktf8 [129.328871ms]
    Jan 28 01:27:48.015: INFO: Created: latency-svc-vx59v
    Jan 28 01:27:48.026: INFO: Got endpoints: latency-svc-vx59v [148.641791ms]
    Jan 28 01:27:48.027: INFO: Created: latency-svc-p7kdk
    Jan 28 01:27:48.035: INFO: Got endpoints: latency-svc-p7kdk [155.878772ms]
    Jan 28 01:27:48.040: INFO: Created: latency-svc-jqvd2
    Jan 28 01:27:48.046: INFO: Got endpoints: latency-svc-jqvd2 [167.051242ms]
    Jan 28 01:27:48.052: INFO: Created: latency-svc-mwmf7
    Jan 28 01:27:48.060: INFO: Got endpoints: latency-svc-mwmf7 [181.029201ms]
    Jan 28 01:27:48.067: INFO: Created: latency-svc-dsj5z
    Jan 28 01:27:48.073: INFO: Got endpoints: latency-svc-dsj5z [194.652298ms]
    Jan 28 01:27:48.081: INFO: Created: latency-svc-gz9fs
    Jan 28 01:27:48.090: INFO: Got endpoints: latency-svc-gz9fs [210.505872ms]
    Jan 28 01:27:48.095: INFO: Created: latency-svc-grr2z
    Jan 28 01:27:48.104: INFO: Got endpoints: latency-svc-grr2z [224.222581ms]
    Jan 28 01:27:48.111: INFO: Created: latency-svc-wb79h
    Jan 28 01:27:48.116: INFO: Got endpoints: latency-svc-wb79h [237.004338ms]
    Jan 28 01:27:48.124: INFO: Created: latency-svc-q9dhc
    Jan 28 01:27:48.135: INFO: Got endpoints: latency-svc-q9dhc [219.77958ms]
    Jan 28 01:27:48.138: INFO: Created: latency-svc-pk78w
    Jan 28 01:27:48.147: INFO: Got endpoints: latency-svc-pk78w [214.06276ms]
    Jan 28 01:27:48.158: INFO: Created: latency-svc-4w7ct
    Jan 28 01:27:48.163: INFO: Got endpoints: latency-svc-4w7ct [213.153645ms]
    Jan 28 01:27:48.168: INFO: Created: latency-svc-c4lbr
    Jan 28 01:27:48.175: INFO: Got endpoints: latency-svc-c4lbr [215.562683ms]
    Jan 28 01:27:48.180: INFO: Created: latency-svc-qdjls
    Jan 28 01:27:48.188: INFO: Got endpoints: latency-svc-qdjls [215.31062ms]
    Jan 28 01:27:48.194: INFO: Created: latency-svc-q72ds
    Jan 28 01:27:48.201: INFO: Got endpoints: latency-svc-q72ds [213.689534ms]
    Jan 28 01:27:48.207: INFO: Created: latency-svc-l26vx
    Jan 28 01:27:48.215: INFO: Got endpoints: latency-svc-l26vx [207.217888ms]
    Jan 28 01:27:48.219: INFO: Created: latency-svc-lc7zb
    Jan 28 01:27:48.227: INFO: Got endpoints: latency-svc-lc7zb [200.774476ms]
    Jan 28 01:27:48.234: INFO: Created: latency-svc-dd9kl
    Jan 28 01:27:48.241: INFO: Got endpoints: latency-svc-dd9kl [206.22115ms]
    Jan 28 01:27:48.249: INFO: Created: latency-svc-wm76f
    Jan 28 01:27:48.255: INFO: Got endpoints: latency-svc-wm76f [209.111063ms]
    Jan 28 01:27:48.263: INFO: Created: latency-svc-8v9x2
    Jan 28 01:27:48.270: INFO: Got endpoints: latency-svc-8v9x2 [209.779585ms]
    Jan 28 01:27:48.277: INFO: Created: latency-svc-4jwwj
    Jan 28 01:27:48.284: INFO: Got endpoints: latency-svc-4jwwj [208.23642ms]
    Jan 28 01:27:48.293: INFO: Created: latency-svc-6ffl6
    Jan 28 01:27:48.301: INFO: Got endpoints: latency-svc-6ffl6 [211.213257ms]
    Jan 28 01:27:48.305: INFO: Created: latency-svc-zzqzh
    Jan 28 01:27:48.313: INFO: Got endpoints: latency-svc-zzqzh [207.616654ms]
    Jan 28 01:27:48.320: INFO: Created: latency-svc-5zgs5
    Jan 28 01:27:48.329: INFO: Got endpoints: latency-svc-5zgs5 [212.461575ms]
    Jan 28 01:27:48.334: INFO: Created: latency-svc-k7p4b
    Jan 28 01:27:48.341: INFO: Got endpoints: latency-svc-k7p4b [205.761064ms]
    Jan 28 01:27:48.348: INFO: Created: latency-svc-rr585
    Jan 28 01:27:48.364: INFO: Got endpoints: latency-svc-rr585 [217.299055ms]
    Jan 28 01:27:48.365: INFO: Created: latency-svc-799xr
    Jan 28 01:27:48.377: INFO: Created: latency-svc-ks7wg
    Jan 28 01:27:48.377: INFO: Got endpoints: latency-svc-799xr [214.37253ms]
    Jan 28 01:27:48.392: INFO: Created: latency-svc-nzxrv
    Jan 28 01:27:48.399: INFO: Got endpoints: latency-svc-ks7wg [223.898869ms]
    Jan 28 01:27:48.405: INFO: Got endpoints: latency-svc-nzxrv [216.499888ms]
    Jan 28 01:27:48.407: INFO: Created: latency-svc-87ztc
    Jan 28 01:27:48.411: INFO: Got endpoints: latency-svc-87ztc [209.733189ms]
    Jan 28 01:27:48.418: INFO: Created: latency-svc-95b6z
    Jan 28 01:27:48.426: INFO: Got endpoints: latency-svc-95b6z [209.995493ms]
    Jan 28 01:27:48.431: INFO: Created: latency-svc-7bqxx
    Jan 28 01:27:48.440: INFO: Got endpoints: latency-svc-7bqxx [212.640991ms]
    Jan 28 01:27:48.444: INFO: Created: latency-svc-l8c6h
    Jan 28 01:27:48.455: INFO: Got endpoints: latency-svc-l8c6h [212.890913ms]
    Jan 28 01:27:48.458: INFO: Created: latency-svc-qz6rp
    Jan 28 01:27:48.466: INFO: Got endpoints: latency-svc-qz6rp [211.356545ms]
    Jan 28 01:27:48.472: INFO: Created: latency-svc-sbrr8
    Jan 28 01:27:48.480: INFO: Got endpoints: latency-svc-sbrr8 [209.777688ms]
    Jan 28 01:27:48.493: INFO: Created: latency-svc-x5llf
    Jan 28 01:27:48.495: INFO: Created: latency-svc-98qxj
    Jan 28 01:27:48.500: INFO: Got endpoints: latency-svc-x5llf [216.142384ms]
    Jan 28 01:27:48.505: INFO: Got endpoints: latency-svc-98qxj [203.593912ms]
    Jan 28 01:27:48.515: INFO: Created: latency-svc-qq4df
    Jan 28 01:27:48.523: INFO: Got endpoints: latency-svc-qq4df [210.611044ms]
    Jan 28 01:27:48.530: INFO: Created: latency-svc-22jn6
    Jan 28 01:27:48.534: INFO: Got endpoints: latency-svc-22jn6 [204.983063ms]
    Jan 28 01:27:48.542: INFO: Created: latency-svc-jpq2j
    Jan 28 01:27:48.551: INFO: Got endpoints: latency-svc-jpq2j [209.663798ms]
    Jan 28 01:27:48.559: INFO: Created: latency-svc-jvms2
    Jan 28 01:27:48.565: INFO: Got endpoints: latency-svc-jvms2 [200.920691ms]
    Jan 28 01:27:48.577: INFO: Created: latency-svc-kl4kp
    Jan 28 01:27:48.581: INFO: Got endpoints: latency-svc-kl4kp [203.659537ms]
    Jan 28 01:27:48.584: INFO: Created: latency-svc-6cx6p
    Jan 28 01:27:48.594: INFO: Got endpoints: latency-svc-6cx6p [194.936801ms]
    Jan 28 01:27:48.594: INFO: Latencies: [39.130097ms 39.46672ms 42.979556ms 43.941126ms 55.911329ms 64.080693ms 73.166129ms 81.748808ms 91.636515ms 93.872334ms 95.743663ms 109.761765ms 121.925739ms 122.159384ms 122.81329ms 127.320546ms 129.328871ms 148.641791ms 155.878772ms 167.051242ms 167.417299ms 169.529512ms 177.296538ms 181.029201ms 184.54354ms 185.896922ms 189.834278ms 194.652298ms 194.936801ms 195.078744ms 200.774476ms 200.920691ms 201.246289ms 202.140244ms 203.593912ms 203.659537ms 204.983063ms 205.761064ms 206.22115ms 207.217888ms 207.616654ms 208.23642ms 209.111063ms 209.663798ms 209.733189ms 209.777688ms 209.779585ms 209.995493ms 210.503449ms 210.505872ms 210.611044ms 211.213257ms 211.356545ms 211.751202ms 212.098089ms 212.461575ms 212.640991ms 212.890913ms 213.153645ms 213.572609ms 213.638818ms 213.689534ms 213.809745ms 214.06276ms 214.37253ms 215.31062ms 215.314326ms 215.562683ms 215.78024ms 215.830913ms 215.978333ms 216.142384ms 216.499888ms 216.988403ms 217.207165ms 217.299055ms 217.784028ms 217.88286ms 219.541202ms 219.676164ms 219.77958ms 219.946053ms 220.48354ms 220.534455ms 220.957652ms 221.603912ms 221.846469ms 223.235298ms 223.677235ms 223.898869ms 224.009338ms 224.222581ms 225.253722ms 225.703509ms 225.815825ms 228.044137ms 229.167778ms 229.720255ms 230.596612ms 231.35424ms 233.065297ms 233.346498ms 233.594271ms 234.650489ms 234.981938ms 236.027679ms 236.363091ms 236.561483ms 236.696206ms 237.002984ms 237.004338ms 237.833793ms 237.937493ms 238.31117ms 238.947162ms 238.976053ms 239.23013ms 239.777768ms 240.281082ms 240.880431ms 240.938129ms 240.975428ms 241.206382ms 241.42941ms 241.517529ms 241.727623ms 241.801966ms 243.046691ms 244.194837ms 246.139837ms 247.060737ms 248.539901ms 249.773704ms 250.579043ms 252.150927ms 252.839786ms 253.276805ms 253.594486ms 254.859938ms 255.022995ms 255.275165ms 256.836225ms 257.252737ms 257.311173ms 257.388225ms 257.888144ms 258.666753ms 259.046854ms 259.155594ms 259.568243ms 259.686256ms 260.167525ms 260.177515ms 264.96839ms 265.125027ms 265.272716ms 265.919087ms 266.52163ms 267.018573ms 268.189872ms 268.938391ms 269.306002ms 269.59592ms 269.617299ms 269.661858ms 270.533455ms 270.801749ms 272.328144ms 272.831801ms 275.455458ms 280.213794ms 282.569276ms 285.307496ms 285.394823ms 285.60394ms 285.812177ms 286.155299ms 286.575403ms 286.802484ms 288.307882ms 288.987421ms 289.265815ms 291.453193ms 292.088314ms 292.412484ms 292.529283ms 295.028333ms 298.273955ms 306.0759ms 307.33883ms 332.459783ms 357.020074ms 382.033533ms 399.476487ms 402.0708ms 408.703504ms 426.477091ms 436.54319ms 439.480121ms 455.259192ms]
    Jan 28 01:27:48.595: INFO: 50 %ile: 233.065297ms
    Jan 28 01:27:48.595: INFO: 90 %ile: 288.987421ms
    Jan 28 01:27:48.595: INFO: 99 %ile: 439.480121ms
    Jan 28 01:27:48.595: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Jan 28 01:27:48.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-7335" for this suite. 01/28/23 01:27:48.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:27:48.639
Jan 28 01:27:48.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename webhook 01/28/23 01:27:48.642
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:27:48.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:27:48.694
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/28/23 01:27:48.744
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:27:49.495
STEP: Deploying the webhook pod 01/28/23 01:27:49.516
STEP: Wait for the deployment to be ready 01/28/23 01:27:49.552
Jan 28 01:27:49.585: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 01:27:51.632: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 27, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 27, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/28/23 01:27:53.646
STEP: Verifying the service has paired with the endpoint 01/28/23 01:27:53.68
Jan 28 01:27:54.681: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 01/28/23 01:27:54.693
STEP: create a pod that should be denied by the webhook 01/28/23 01:27:54.769
STEP: create a pod that causes the webhook to hang 01/28/23 01:27:54.84
STEP: create a configmap that should be denied by the webhook 01/28/23 01:28:04.863
STEP: create a configmap that should be admitted by the webhook 01/28/23 01:28:04.927
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/28/23 01:28:04.975
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/28/23 01:28:05.016
STEP: create a namespace that bypass the webhook 01/28/23 01:28:05.041
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/28/23 01:28:05.068
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:28:05.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1050" for this suite. 01/28/23 01:28:05.184
STEP: Destroying namespace "webhook-1050-markers" for this suite. 01/28/23 01:28:05.203
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":235,"skipped":4218,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.694 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:27:48.639
    Jan 28 01:27:48.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename webhook 01/28/23 01:27:48.642
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:27:48.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:27:48.694
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/28/23 01:27:48.744
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:27:49.495
    STEP: Deploying the webhook pod 01/28/23 01:27:49.516
    STEP: Wait for the deployment to be ready 01/28/23 01:27:49.552
    Jan 28 01:27:49.585: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 28 01:27:51.632: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 27, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 27, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/28/23 01:27:53.646
    STEP: Verifying the service has paired with the endpoint 01/28/23 01:27:53.68
    Jan 28 01:27:54.681: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 01/28/23 01:27:54.693
    STEP: create a pod that should be denied by the webhook 01/28/23 01:27:54.769
    STEP: create a pod that causes the webhook to hang 01/28/23 01:27:54.84
    STEP: create a configmap that should be denied by the webhook 01/28/23 01:28:04.863
    STEP: create a configmap that should be admitted by the webhook 01/28/23 01:28:04.927
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/28/23 01:28:04.975
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/28/23 01:28:05.016
    STEP: create a namespace that bypass the webhook 01/28/23 01:28:05.041
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/28/23 01:28:05.068
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:28:05.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1050" for this suite. 01/28/23 01:28:05.184
    STEP: Destroying namespace "webhook-1050-markers" for this suite. 01/28/23 01:28:05.203
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:28:05.342
Jan 28 01:28:05.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename configmap 01/28/23 01:28:05.343
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:05.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:05.419
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-91b8cea7-ad0a-4749-a708-857b31c373c5 01/28/23 01:28:05.431
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 28 01:28:05.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2655" for this suite. 01/28/23 01:28:05.46
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":236,"skipped":4234,"failed":0}
------------------------------
â€¢ [0.139 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:28:05.342
    Jan 28 01:28:05.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename configmap 01/28/23 01:28:05.343
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:05.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:05.419
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-91b8cea7-ad0a-4749-a708-857b31c373c5 01/28/23 01:28:05.431
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 28 01:28:05.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2655" for this suite. 01/28/23 01:28:05.46
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:28:05.481
Jan 28 01:28:05.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename init-container 01/28/23 01:28:05.486
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:05.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:05.538
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 01/28/23 01:28:05.551
Jan 28 01:28:05.552: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 28 01:28:09.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1692" for this suite. 01/28/23 01:28:09.825
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":237,"skipped":4234,"failed":0}
------------------------------
â€¢ [4.364 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:28:05.481
    Jan 28 01:28:05.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename init-container 01/28/23 01:28:05.486
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:05.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:05.538
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 01/28/23 01:28:05.551
    Jan 28 01:28:05.552: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 28 01:28:09.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-1692" for this suite. 01/28/23 01:28:09.825
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:28:09.858
Jan 28 01:28:09.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 01:28:09.86
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:09.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:09.923
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 01/28/23 01:28:09.936
Jan 28 01:28:09.961: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f93dd233-62c7-4a09-abb1-2d0e3962ddcd" in namespace "projected-6047" to be "Succeeded or Failed"
Jan 28 01:28:09.974: INFO: Pod "downwardapi-volume-f93dd233-62c7-4a09-abb1-2d0e3962ddcd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.08501ms
Jan 28 01:28:12.002: INFO: Pod "downwardapi-volume-f93dd233-62c7-4a09-abb1-2d0e3962ddcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040498448s
Jan 28 01:28:13.985: INFO: Pod "downwardapi-volume-f93dd233-62c7-4a09-abb1-2d0e3962ddcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023854136s
STEP: Saw pod success 01/28/23 01:28:13.986
Jan 28 01:28:13.986: INFO: Pod "downwardapi-volume-f93dd233-62c7-4a09-abb1-2d0e3962ddcd" satisfied condition "Succeeded or Failed"
Jan 28 01:28:14.005: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-f93dd233-62c7-4a09-abb1-2d0e3962ddcd container client-container: <nil>
STEP: delete the pod 01/28/23 01:28:14.038
Jan 28 01:28:14.102: INFO: Waiting for pod downwardapi-volume-f93dd233-62c7-4a09-abb1-2d0e3962ddcd to disappear
Jan 28 01:28:14.114: INFO: Pod downwardapi-volume-f93dd233-62c7-4a09-abb1-2d0e3962ddcd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 28 01:28:14.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6047" for this suite. 01/28/23 01:28:14.142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":238,"skipped":4276,"failed":0}
------------------------------
â€¢ [4.300 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:28:09.858
    Jan 28 01:28:09.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 01:28:09.86
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:09.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:09.923
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 01/28/23 01:28:09.936
    Jan 28 01:28:09.961: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f93dd233-62c7-4a09-abb1-2d0e3962ddcd" in namespace "projected-6047" to be "Succeeded or Failed"
    Jan 28 01:28:09.974: INFO: Pod "downwardapi-volume-f93dd233-62c7-4a09-abb1-2d0e3962ddcd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.08501ms
    Jan 28 01:28:12.002: INFO: Pod "downwardapi-volume-f93dd233-62c7-4a09-abb1-2d0e3962ddcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040498448s
    Jan 28 01:28:13.985: INFO: Pod "downwardapi-volume-f93dd233-62c7-4a09-abb1-2d0e3962ddcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023854136s
    STEP: Saw pod success 01/28/23 01:28:13.986
    Jan 28 01:28:13.986: INFO: Pod "downwardapi-volume-f93dd233-62c7-4a09-abb1-2d0e3962ddcd" satisfied condition "Succeeded or Failed"
    Jan 28 01:28:14.005: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-f93dd233-62c7-4a09-abb1-2d0e3962ddcd container client-container: <nil>
    STEP: delete the pod 01/28/23 01:28:14.038
    Jan 28 01:28:14.102: INFO: Waiting for pod downwardapi-volume-f93dd233-62c7-4a09-abb1-2d0e3962ddcd to disappear
    Jan 28 01:28:14.114: INFO: Pod downwardapi-volume-f93dd233-62c7-4a09-abb1-2d0e3962ddcd no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 28 01:28:14.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6047" for this suite. 01/28/23 01:28:14.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:28:14.174
Jan 28 01:28:14.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename downward-api 01/28/23 01:28:14.175
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:14.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:14.232
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 01/28/23 01:28:14.245
Jan 28 01:28:14.270: INFO: Waiting up to 5m0s for pod "downward-api-800af2f4-1378-4111-b5ad-107ddf6027ae" in namespace "downward-api-8129" to be "Succeeded or Failed"
Jan 28 01:28:14.303: INFO: Pod "downward-api-800af2f4-1378-4111-b5ad-107ddf6027ae": Phase="Pending", Reason="", readiness=false. Elapsed: 32.994139ms
Jan 28 01:28:16.317: INFO: Pod "downward-api-800af2f4-1378-4111-b5ad-107ddf6027ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04735401s
Jan 28 01:28:18.317: INFO: Pod "downward-api-800af2f4-1378-4111-b5ad-107ddf6027ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046796835s
STEP: Saw pod success 01/28/23 01:28:18.317
Jan 28 01:28:18.317: INFO: Pod "downward-api-800af2f4-1378-4111-b5ad-107ddf6027ae" satisfied condition "Succeeded or Failed"
Jan 28 01:28:18.338: INFO: Trying to get logs from node 10.9.20.126 pod downward-api-800af2f4-1378-4111-b5ad-107ddf6027ae container dapi-container: <nil>
STEP: delete the pod 01/28/23 01:28:18.368
Jan 28 01:28:18.398: INFO: Waiting for pod downward-api-800af2f4-1378-4111-b5ad-107ddf6027ae to disappear
Jan 28 01:28:18.409: INFO: Pod downward-api-800af2f4-1378-4111-b5ad-107ddf6027ae no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 28 01:28:18.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8129" for this suite. 01/28/23 01:28:18.425
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":239,"skipped":4317,"failed":0}
------------------------------
â€¢ [4.268 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:28:14.174
    Jan 28 01:28:14.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename downward-api 01/28/23 01:28:14.175
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:14.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:14.232
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 01/28/23 01:28:14.245
    Jan 28 01:28:14.270: INFO: Waiting up to 5m0s for pod "downward-api-800af2f4-1378-4111-b5ad-107ddf6027ae" in namespace "downward-api-8129" to be "Succeeded or Failed"
    Jan 28 01:28:14.303: INFO: Pod "downward-api-800af2f4-1378-4111-b5ad-107ddf6027ae": Phase="Pending", Reason="", readiness=false. Elapsed: 32.994139ms
    Jan 28 01:28:16.317: INFO: Pod "downward-api-800af2f4-1378-4111-b5ad-107ddf6027ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04735401s
    Jan 28 01:28:18.317: INFO: Pod "downward-api-800af2f4-1378-4111-b5ad-107ddf6027ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046796835s
    STEP: Saw pod success 01/28/23 01:28:18.317
    Jan 28 01:28:18.317: INFO: Pod "downward-api-800af2f4-1378-4111-b5ad-107ddf6027ae" satisfied condition "Succeeded or Failed"
    Jan 28 01:28:18.338: INFO: Trying to get logs from node 10.9.20.126 pod downward-api-800af2f4-1378-4111-b5ad-107ddf6027ae container dapi-container: <nil>
    STEP: delete the pod 01/28/23 01:28:18.368
    Jan 28 01:28:18.398: INFO: Waiting for pod downward-api-800af2f4-1378-4111-b5ad-107ddf6027ae to disappear
    Jan 28 01:28:18.409: INFO: Pod downward-api-800af2f4-1378-4111-b5ad-107ddf6027ae no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 28 01:28:18.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8129" for this suite. 01/28/23 01:28:18.425
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:28:18.445
Jan 28 01:28:18.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename proxy 01/28/23 01:28:18.447
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:18.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:18.525
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan 28 01:28:18.538: INFO: Creating pod...
Jan 28 01:28:18.559: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3508" to be "running"
Jan 28 01:28:18.572: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 12.746727ms
Jan 28 01:28:20.585: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025916807s
Jan 28 01:28:22.585: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.025682846s
Jan 28 01:28:22.585: INFO: Pod "agnhost" satisfied condition "running"
Jan 28 01:28:22.585: INFO: Creating service...
Jan 28 01:28:22.620: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/pods/agnhost/proxy/some/path/with/DELETE
Jan 28 01:28:22.664: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 28 01:28:22.664: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/pods/agnhost/proxy/some/path/with/GET
Jan 28 01:28:22.680: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 28 01:28:22.680: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/pods/agnhost/proxy/some/path/with/HEAD
Jan 28 01:28:22.700: INFO: http.Client request:HEAD | StatusCode:200
Jan 28 01:28:22.700: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 28 01:28:22.718: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 28 01:28:22.718: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/pods/agnhost/proxy/some/path/with/PATCH
Jan 28 01:28:22.738: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 28 01:28:22.738: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/pods/agnhost/proxy/some/path/with/POST
Jan 28 01:28:22.755: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 28 01:28:22.755: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/pods/agnhost/proxy/some/path/with/PUT
Jan 28 01:28:22.771: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 28 01:28:22.771: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/services/test-service/proxy/some/path/with/DELETE
Jan 28 01:28:22.793: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 28 01:28:22.793: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/services/test-service/proxy/some/path/with/GET
Jan 28 01:28:22.836: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 28 01:28:22.836: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/services/test-service/proxy/some/path/with/HEAD
Jan 28 01:28:22.859: INFO: http.Client request:HEAD | StatusCode:200
Jan 28 01:28:22.859: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/services/test-service/proxy/some/path/with/OPTIONS
Jan 28 01:28:22.883: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 28 01:28:22.883: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/services/test-service/proxy/some/path/with/PATCH
Jan 28 01:28:22.906: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 28 01:28:22.906: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/services/test-service/proxy/some/path/with/POST
Jan 28 01:28:22.929: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 28 01:28:22.929: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/services/test-service/proxy/some/path/with/PUT
Jan 28 01:28:22.951: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 28 01:28:22.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3508" for this suite. 01/28/23 01:28:22.969
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":240,"skipped":4321,"failed":0}
------------------------------
â€¢ [4.543 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:28:18.445
    Jan 28 01:28:18.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename proxy 01/28/23 01:28:18.447
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:18.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:18.525
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan 28 01:28:18.538: INFO: Creating pod...
    Jan 28 01:28:18.559: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3508" to be "running"
    Jan 28 01:28:18.572: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 12.746727ms
    Jan 28 01:28:20.585: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025916807s
    Jan 28 01:28:22.585: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.025682846s
    Jan 28 01:28:22.585: INFO: Pod "agnhost" satisfied condition "running"
    Jan 28 01:28:22.585: INFO: Creating service...
    Jan 28 01:28:22.620: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/pods/agnhost/proxy/some/path/with/DELETE
    Jan 28 01:28:22.664: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 28 01:28:22.664: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/pods/agnhost/proxy/some/path/with/GET
    Jan 28 01:28:22.680: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 28 01:28:22.680: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/pods/agnhost/proxy/some/path/with/HEAD
    Jan 28 01:28:22.700: INFO: http.Client request:HEAD | StatusCode:200
    Jan 28 01:28:22.700: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan 28 01:28:22.718: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 28 01:28:22.718: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/pods/agnhost/proxy/some/path/with/PATCH
    Jan 28 01:28:22.738: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 28 01:28:22.738: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/pods/agnhost/proxy/some/path/with/POST
    Jan 28 01:28:22.755: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 28 01:28:22.755: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/pods/agnhost/proxy/some/path/with/PUT
    Jan 28 01:28:22.771: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 28 01:28:22.771: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/services/test-service/proxy/some/path/with/DELETE
    Jan 28 01:28:22.793: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 28 01:28:22.793: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/services/test-service/proxy/some/path/with/GET
    Jan 28 01:28:22.836: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 28 01:28:22.836: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/services/test-service/proxy/some/path/with/HEAD
    Jan 28 01:28:22.859: INFO: http.Client request:HEAD | StatusCode:200
    Jan 28 01:28:22.859: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/services/test-service/proxy/some/path/with/OPTIONS
    Jan 28 01:28:22.883: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 28 01:28:22.883: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/services/test-service/proxy/some/path/with/PATCH
    Jan 28 01:28:22.906: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 28 01:28:22.906: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/services/test-service/proxy/some/path/with/POST
    Jan 28 01:28:22.929: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 28 01:28:22.929: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3508/services/test-service/proxy/some/path/with/PUT
    Jan 28 01:28:22.951: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 28 01:28:22.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-3508" for this suite. 01/28/23 01:28:22.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:28:22.992
Jan 28 01:28:22.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename configmap 01/28/23 01:28:22.994
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:23.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:23.052
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-98f5af49-6b1a-462c-8c90-91f9d592c793 01/28/23 01:28:23.064
STEP: Creating a pod to test consume configMaps 01/28/23 01:28:23.078
Jan 28 01:28:23.101: INFO: Waiting up to 5m0s for pod "pod-configmaps-d5e803e6-6118-43f3-b427-3313992e7398" in namespace "configmap-2761" to be "Succeeded or Failed"
Jan 28 01:28:23.119: INFO: Pod "pod-configmaps-d5e803e6-6118-43f3-b427-3313992e7398": Phase="Pending", Reason="", readiness=false. Elapsed: 17.173558ms
Jan 28 01:28:25.133: INFO: Pod "pod-configmaps-d5e803e6-6118-43f3-b427-3313992e7398": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03136926s
Jan 28 01:28:27.132: INFO: Pod "pod-configmaps-d5e803e6-6118-43f3-b427-3313992e7398": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030292397s
STEP: Saw pod success 01/28/23 01:28:27.132
Jan 28 01:28:27.132: INFO: Pod "pod-configmaps-d5e803e6-6118-43f3-b427-3313992e7398" satisfied condition "Succeeded or Failed"
Jan 28 01:28:27.144: INFO: Trying to get logs from node 10.9.20.126 pod pod-configmaps-d5e803e6-6118-43f3-b427-3313992e7398 container agnhost-container: <nil>
STEP: delete the pod 01/28/23 01:28:27.172
Jan 28 01:28:27.209: INFO: Waiting for pod pod-configmaps-d5e803e6-6118-43f3-b427-3313992e7398 to disappear
Jan 28 01:28:27.221: INFO: Pod pod-configmaps-d5e803e6-6118-43f3-b427-3313992e7398 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 28 01:28:27.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2761" for this suite. 01/28/23 01:28:27.237
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":241,"skipped":4348,"failed":0}
------------------------------
â€¢ [4.267 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:28:22.992
    Jan 28 01:28:22.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename configmap 01/28/23 01:28:22.994
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:23.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:23.052
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-98f5af49-6b1a-462c-8c90-91f9d592c793 01/28/23 01:28:23.064
    STEP: Creating a pod to test consume configMaps 01/28/23 01:28:23.078
    Jan 28 01:28:23.101: INFO: Waiting up to 5m0s for pod "pod-configmaps-d5e803e6-6118-43f3-b427-3313992e7398" in namespace "configmap-2761" to be "Succeeded or Failed"
    Jan 28 01:28:23.119: INFO: Pod "pod-configmaps-d5e803e6-6118-43f3-b427-3313992e7398": Phase="Pending", Reason="", readiness=false. Elapsed: 17.173558ms
    Jan 28 01:28:25.133: INFO: Pod "pod-configmaps-d5e803e6-6118-43f3-b427-3313992e7398": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03136926s
    Jan 28 01:28:27.132: INFO: Pod "pod-configmaps-d5e803e6-6118-43f3-b427-3313992e7398": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030292397s
    STEP: Saw pod success 01/28/23 01:28:27.132
    Jan 28 01:28:27.132: INFO: Pod "pod-configmaps-d5e803e6-6118-43f3-b427-3313992e7398" satisfied condition "Succeeded or Failed"
    Jan 28 01:28:27.144: INFO: Trying to get logs from node 10.9.20.126 pod pod-configmaps-d5e803e6-6118-43f3-b427-3313992e7398 container agnhost-container: <nil>
    STEP: delete the pod 01/28/23 01:28:27.172
    Jan 28 01:28:27.209: INFO: Waiting for pod pod-configmaps-d5e803e6-6118-43f3-b427-3313992e7398 to disappear
    Jan 28 01:28:27.221: INFO: Pod pod-configmaps-d5e803e6-6118-43f3-b427-3313992e7398 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 28 01:28:27.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2761" for this suite. 01/28/23 01:28:27.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:28:27.263
Jan 28 01:28:27.263: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename services 01/28/23 01:28:27.265
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:27.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:27.319
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-5504 01/28/23 01:28:27.333
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5504 to expose endpoints map[] 01/28/23 01:28:27.371
Jan 28 01:28:27.423: INFO: successfully validated that service multi-endpoint-test in namespace services-5504 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5504 01/28/23 01:28:27.423
Jan 28 01:28:27.452: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5504" to be "running and ready"
Jan 28 01:28:27.465: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.374911ms
Jan 28 01:28:27.466: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:28:29.479: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027242032s
Jan 28 01:28:29.479: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:28:31.479: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.027590604s
Jan 28 01:28:31.479: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 28 01:28:31.479: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5504 to expose endpoints map[pod1:[100]] 01/28/23 01:28:31.492
Jan 28 01:28:31.537: INFO: successfully validated that service multi-endpoint-test in namespace services-5504 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5504 01/28/23 01:28:31.537
Jan 28 01:28:31.553: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5504" to be "running and ready"
Jan 28 01:28:31.565: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.528868ms
Jan 28 01:28:31.565: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:28:33.597: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044166748s
Jan 28 01:28:33.597: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:28:35.577: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.024135341s
Jan 28 01:28:35.577: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 28 01:28:35.577: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5504 to expose endpoints map[pod1:[100] pod2:[101]] 01/28/23 01:28:35.588
Jan 28 01:28:35.638: INFO: successfully validated that service multi-endpoint-test in namespace services-5504 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/28/23 01:28:35.639
Jan 28 01:28:35.639: INFO: Creating new exec pod
Jan 28 01:28:35.660: INFO: Waiting up to 5m0s for pod "execpods89fz" in namespace "services-5504" to be "running"
Jan 28 01:28:35.709: INFO: Pod "execpods89fz": Phase="Pending", Reason="", readiness=false. Elapsed: 49.211517ms
Jan 28 01:28:37.726: INFO: Pod "execpods89fz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065539489s
Jan 28 01:28:39.722: INFO: Pod "execpods89fz": Phase="Running", Reason="", readiness=true. Elapsed: 4.061714612s
Jan 28 01:28:39.722: INFO: Pod "execpods89fz" satisfied condition "running"
Jan 28 01:28:40.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-5504 exec execpods89fz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jan 28 01:28:41.064: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 28 01:28:41.065: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:28:41.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-5504 exec execpods89fz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.150.108 80'
Jan 28 01:28:41.389: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.150.108 80\nConnection to 172.21.150.108 80 port [tcp/http] succeeded!\n"
Jan 28 01:28:41.390: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:28:41.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-5504 exec execpods89fz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jan 28 01:28:41.724: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 28 01:28:41.724: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:28:41.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-5504 exec execpods89fz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.150.108 81'
Jan 28 01:28:42.060: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.150.108 81\nConnection to 172.21.150.108 81 port [tcp/*] succeeded!\n"
Jan 28 01:28:42.060: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-5504 01/28/23 01:28:42.06
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5504 to expose endpoints map[pod2:[101]] 01/28/23 01:28:42.101
Jan 28 01:28:42.140: INFO: successfully validated that service multi-endpoint-test in namespace services-5504 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5504 01/28/23 01:28:42.14
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5504 to expose endpoints map[] 01/28/23 01:28:42.173
Jan 28 01:28:42.201: INFO: successfully validated that service multi-endpoint-test in namespace services-5504 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 28 01:28:42.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5504" for this suite. 01/28/23 01:28:42.264
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":242,"skipped":4380,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.021 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:28:27.263
    Jan 28 01:28:27.263: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename services 01/28/23 01:28:27.265
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:27.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:27.319
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-5504 01/28/23 01:28:27.333
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5504 to expose endpoints map[] 01/28/23 01:28:27.371
    Jan 28 01:28:27.423: INFO: successfully validated that service multi-endpoint-test in namespace services-5504 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5504 01/28/23 01:28:27.423
    Jan 28 01:28:27.452: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5504" to be "running and ready"
    Jan 28 01:28:27.465: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.374911ms
    Jan 28 01:28:27.466: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:28:29.479: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027242032s
    Jan 28 01:28:29.479: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:28:31.479: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.027590604s
    Jan 28 01:28:31.479: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 28 01:28:31.479: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5504 to expose endpoints map[pod1:[100]] 01/28/23 01:28:31.492
    Jan 28 01:28:31.537: INFO: successfully validated that service multi-endpoint-test in namespace services-5504 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-5504 01/28/23 01:28:31.537
    Jan 28 01:28:31.553: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5504" to be "running and ready"
    Jan 28 01:28:31.565: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.528868ms
    Jan 28 01:28:31.565: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:28:33.597: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044166748s
    Jan 28 01:28:33.597: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:28:35.577: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.024135341s
    Jan 28 01:28:35.577: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 28 01:28:35.577: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5504 to expose endpoints map[pod1:[100] pod2:[101]] 01/28/23 01:28:35.588
    Jan 28 01:28:35.638: INFO: successfully validated that service multi-endpoint-test in namespace services-5504 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/28/23 01:28:35.639
    Jan 28 01:28:35.639: INFO: Creating new exec pod
    Jan 28 01:28:35.660: INFO: Waiting up to 5m0s for pod "execpods89fz" in namespace "services-5504" to be "running"
    Jan 28 01:28:35.709: INFO: Pod "execpods89fz": Phase="Pending", Reason="", readiness=false. Elapsed: 49.211517ms
    Jan 28 01:28:37.726: INFO: Pod "execpods89fz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065539489s
    Jan 28 01:28:39.722: INFO: Pod "execpods89fz": Phase="Running", Reason="", readiness=true. Elapsed: 4.061714612s
    Jan 28 01:28:39.722: INFO: Pod "execpods89fz" satisfied condition "running"
    Jan 28 01:28:40.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-5504 exec execpods89fz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Jan 28 01:28:41.064: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan 28 01:28:41.065: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 01:28:41.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-5504 exec execpods89fz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.150.108 80'
    Jan 28 01:28:41.389: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.150.108 80\nConnection to 172.21.150.108 80 port [tcp/http] succeeded!\n"
    Jan 28 01:28:41.390: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 01:28:41.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-5504 exec execpods89fz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Jan 28 01:28:41.724: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan 28 01:28:41.724: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 01:28:41.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-5504 exec execpods89fz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.150.108 81'
    Jan 28 01:28:42.060: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.150.108 81\nConnection to 172.21.150.108 81 port [tcp/*] succeeded!\n"
    Jan 28 01:28:42.060: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-5504 01/28/23 01:28:42.06
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5504 to expose endpoints map[pod2:[101]] 01/28/23 01:28:42.101
    Jan 28 01:28:42.140: INFO: successfully validated that service multi-endpoint-test in namespace services-5504 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-5504 01/28/23 01:28:42.14
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5504 to expose endpoints map[] 01/28/23 01:28:42.173
    Jan 28 01:28:42.201: INFO: successfully validated that service multi-endpoint-test in namespace services-5504 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 28 01:28:42.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5504" for this suite. 01/28/23 01:28:42.264
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:28:42.301
Jan 28 01:28:42.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename downward-api 01/28/23 01:28:42.303
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:42.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:42.38
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 01/28/23 01:28:42.393
Jan 28 01:28:42.439: INFO: Waiting up to 5m0s for pod "downward-api-3d2eeaec-faad-45d5-97de-083f100d291e" in namespace "downward-api-5490" to be "Succeeded or Failed"
Jan 28 01:28:42.451: INFO: Pod "downward-api-3d2eeaec-faad-45d5-97de-083f100d291e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.910278ms
Jan 28 01:28:44.464: INFO: Pod "downward-api-3d2eeaec-faad-45d5-97de-083f100d291e": Phase="Running", Reason="", readiness=true. Elapsed: 2.025439746s
Jan 28 01:28:46.465: INFO: Pod "downward-api-3d2eeaec-faad-45d5-97de-083f100d291e": Phase="Running", Reason="", readiness=false. Elapsed: 4.026222256s
Jan 28 01:28:48.465: INFO: Pod "downward-api-3d2eeaec-faad-45d5-97de-083f100d291e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02578611s
STEP: Saw pod success 01/28/23 01:28:48.465
Jan 28 01:28:48.465: INFO: Pod "downward-api-3d2eeaec-faad-45d5-97de-083f100d291e" satisfied condition "Succeeded or Failed"
Jan 28 01:28:48.478: INFO: Trying to get logs from node 10.9.20.126 pod downward-api-3d2eeaec-faad-45d5-97de-083f100d291e container dapi-container: <nil>
STEP: delete the pod 01/28/23 01:28:48.509
Jan 28 01:28:48.542: INFO: Waiting for pod downward-api-3d2eeaec-faad-45d5-97de-083f100d291e to disappear
Jan 28 01:28:48.553: INFO: Pod downward-api-3d2eeaec-faad-45d5-97de-083f100d291e no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 28 01:28:48.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5490" for this suite. 01/28/23 01:28:48.573
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":243,"skipped":4470,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.294 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:28:42.301
    Jan 28 01:28:42.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename downward-api 01/28/23 01:28:42.303
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:42.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:42.38
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 01/28/23 01:28:42.393
    Jan 28 01:28:42.439: INFO: Waiting up to 5m0s for pod "downward-api-3d2eeaec-faad-45d5-97de-083f100d291e" in namespace "downward-api-5490" to be "Succeeded or Failed"
    Jan 28 01:28:42.451: INFO: Pod "downward-api-3d2eeaec-faad-45d5-97de-083f100d291e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.910278ms
    Jan 28 01:28:44.464: INFO: Pod "downward-api-3d2eeaec-faad-45d5-97de-083f100d291e": Phase="Running", Reason="", readiness=true. Elapsed: 2.025439746s
    Jan 28 01:28:46.465: INFO: Pod "downward-api-3d2eeaec-faad-45d5-97de-083f100d291e": Phase="Running", Reason="", readiness=false. Elapsed: 4.026222256s
    Jan 28 01:28:48.465: INFO: Pod "downward-api-3d2eeaec-faad-45d5-97de-083f100d291e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02578611s
    STEP: Saw pod success 01/28/23 01:28:48.465
    Jan 28 01:28:48.465: INFO: Pod "downward-api-3d2eeaec-faad-45d5-97de-083f100d291e" satisfied condition "Succeeded or Failed"
    Jan 28 01:28:48.478: INFO: Trying to get logs from node 10.9.20.126 pod downward-api-3d2eeaec-faad-45d5-97de-083f100d291e container dapi-container: <nil>
    STEP: delete the pod 01/28/23 01:28:48.509
    Jan 28 01:28:48.542: INFO: Waiting for pod downward-api-3d2eeaec-faad-45d5-97de-083f100d291e to disappear
    Jan 28 01:28:48.553: INFO: Pod downward-api-3d2eeaec-faad-45d5-97de-083f100d291e no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 28 01:28:48.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5490" for this suite. 01/28/23 01:28:48.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:28:48.604
Jan 28 01:28:48.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/28/23 01:28:48.605
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:48.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:48.666
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/28/23 01:28:48.68
STEP: Creating hostNetwork=false pod 01/28/23 01:28:48.68
Jan 28 01:28:48.703: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-8281" to be "running and ready"
Jan 28 01:28:48.743: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 39.580042ms
Jan 28 01:28:48.743: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:28:50.756: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052112061s
Jan 28 01:28:50.756: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:28:52.756: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.052272272s
Jan 28 01:28:52.756: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan 28 01:28:52.756: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/28/23 01:28:52.768
Jan 28 01:28:52.814: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-8281" to be "running and ready"
Jan 28 01:28:52.826: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.968802ms
Jan 28 01:28:52.826: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:28:54.838: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.024682649s
Jan 28 01:28:54.838: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan 28 01:28:54.839: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/28/23 01:28:54.85
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/28/23 01:28:54.85
Jan 28 01:28:54.851: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:28:54.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:28:54.854: INFO: ExecWithOptions: Clientset creation
Jan 28 01:28:54.854: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 28 01:28:55.073: INFO: Exec stderr: ""
Jan 28 01:28:55.073: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:28:55.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:28:55.075: INFO: ExecWithOptions: Clientset creation
Jan 28 01:28:55.075: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 28 01:28:55.260: INFO: Exec stderr: ""
Jan 28 01:28:55.260: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:28:55.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:28:55.262: INFO: ExecWithOptions: Clientset creation
Jan 28 01:28:55.262: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 28 01:28:55.479: INFO: Exec stderr: ""
Jan 28 01:28:55.479: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:28:55.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:28:55.479: INFO: ExecWithOptions: Clientset creation
Jan 28 01:28:55.480: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 28 01:28:55.679: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/28/23 01:28:55.679
Jan 28 01:28:55.680: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:28:55.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:28:55.681: INFO: ExecWithOptions: Clientset creation
Jan 28 01:28:55.681: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 28 01:28:55.889: INFO: Exec stderr: ""
Jan 28 01:28:55.889: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:28:55.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:28:55.890: INFO: ExecWithOptions: Clientset creation
Jan 28 01:28:55.891: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 28 01:28:56.109: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/28/23 01:28:56.109
Jan 28 01:28:56.110: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:28:56.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:28:56.111: INFO: ExecWithOptions: Clientset creation
Jan 28 01:28:56.111: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 28 01:28:56.330: INFO: Exec stderr: ""
Jan 28 01:28:56.330: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:28:56.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:28:56.333: INFO: ExecWithOptions: Clientset creation
Jan 28 01:28:56.333: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 28 01:28:56.554: INFO: Exec stderr: ""
Jan 28 01:28:56.554: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:28:56.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:28:56.558: INFO: ExecWithOptions: Clientset creation
Jan 28 01:28:56.558: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 28 01:28:56.758: INFO: Exec stderr: ""
Jan 28 01:28:56.758: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:28:56.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:28:56.760: INFO: ExecWithOptions: Clientset creation
Jan 28 01:28:56.760: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 28 01:28:56.962: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Jan 28 01:28:56.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8281" for this suite. 01/28/23 01:28:56.98
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":244,"skipped":4502,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.401 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:28:48.604
    Jan 28 01:28:48.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/28/23 01:28:48.605
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:48.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:48.666
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/28/23 01:28:48.68
    STEP: Creating hostNetwork=false pod 01/28/23 01:28:48.68
    Jan 28 01:28:48.703: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-8281" to be "running and ready"
    Jan 28 01:28:48.743: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 39.580042ms
    Jan 28 01:28:48.743: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:28:50.756: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052112061s
    Jan 28 01:28:50.756: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:28:52.756: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.052272272s
    Jan 28 01:28:52.756: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan 28 01:28:52.756: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/28/23 01:28:52.768
    Jan 28 01:28:52.814: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-8281" to be "running and ready"
    Jan 28 01:28:52.826: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.968802ms
    Jan 28 01:28:52.826: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:28:54.838: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.024682649s
    Jan 28 01:28:54.838: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan 28 01:28:54.839: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/28/23 01:28:54.85
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/28/23 01:28:54.85
    Jan 28 01:28:54.851: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:28:54.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:28:54.854: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:28:54.854: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 28 01:28:55.073: INFO: Exec stderr: ""
    Jan 28 01:28:55.073: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:28:55.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:28:55.075: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:28:55.075: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 28 01:28:55.260: INFO: Exec stderr: ""
    Jan 28 01:28:55.260: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:28:55.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:28:55.262: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:28:55.262: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 28 01:28:55.479: INFO: Exec stderr: ""
    Jan 28 01:28:55.479: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:28:55.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:28:55.479: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:28:55.480: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 28 01:28:55.679: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/28/23 01:28:55.679
    Jan 28 01:28:55.680: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:28:55.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:28:55.681: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:28:55.681: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 28 01:28:55.889: INFO: Exec stderr: ""
    Jan 28 01:28:55.889: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:28:55.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:28:55.890: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:28:55.891: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 28 01:28:56.109: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/28/23 01:28:56.109
    Jan 28 01:28:56.110: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:28:56.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:28:56.111: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:28:56.111: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 28 01:28:56.330: INFO: Exec stderr: ""
    Jan 28 01:28:56.330: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:28:56.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:28:56.333: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:28:56.333: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 28 01:28:56.554: INFO: Exec stderr: ""
    Jan 28 01:28:56.554: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:28:56.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:28:56.558: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:28:56.558: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 28 01:28:56.758: INFO: Exec stderr: ""
    Jan 28 01:28:56.758: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8281 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:28:56.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:28:56.760: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:28:56.760: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8281/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 28 01:28:56.962: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Jan 28 01:28:56.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-8281" for this suite. 01/28/23 01:28:56.98
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:28:57.006
Jan 28 01:28:57.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename job 01/28/23 01:28:57.01
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:57.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:57.063
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 01/28/23 01:28:57.075
STEP: Ensuring job reaches completions 01/28/23 01:28:57.09
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 28 01:29:11.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5937" for this suite. 01/28/23 01:29:11.121
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":245,"skipped":4506,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.134 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:28:57.006
    Jan 28 01:28:57.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename job 01/28/23 01:28:57.01
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:28:57.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:28:57.063
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 01/28/23 01:28:57.075
    STEP: Ensuring job reaches completions 01/28/23 01:28:57.09
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 28 01:29:11.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5937" for this suite. 01/28/23 01:29:11.121
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:29:11.142
Jan 28 01:29:11.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename podtemplate 01/28/23 01:29:11.144
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:11.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:11.199
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 28 01:29:11.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2267" for this suite. 01/28/23 01:29:11.315
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":246,"skipped":4508,"failed":0}
------------------------------
â€¢ [0.192 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:29:11.142
    Jan 28 01:29:11.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename podtemplate 01/28/23 01:29:11.144
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:11.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:11.199
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 28 01:29:11.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-2267" for this suite. 01/28/23 01:29:11.315
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:29:11.337
Jan 28 01:29:11.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename security-context-test 01/28/23 01:29:11.339
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:11.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:11.391
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Jan 28 01:29:11.424: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-7df498c2-47bf-4b2c-8646-5693daec1995" in namespace "security-context-test-7012" to be "Succeeded or Failed"
Jan 28 01:29:11.457: INFO: Pod "busybox-privileged-false-7df498c2-47bf-4b2c-8646-5693daec1995": Phase="Pending", Reason="", readiness=false. Elapsed: 32.668985ms
Jan 28 01:29:13.469: INFO: Pod "busybox-privileged-false-7df498c2-47bf-4b2c-8646-5693daec1995": Phase="Running", Reason="", readiness=true. Elapsed: 2.044366366s
Jan 28 01:29:15.472: INFO: Pod "busybox-privileged-false-7df498c2-47bf-4b2c-8646-5693daec1995": Phase="Running", Reason="", readiness=false. Elapsed: 4.046859518s
Jan 28 01:29:17.469: INFO: Pod "busybox-privileged-false-7df498c2-47bf-4b2c-8646-5693daec1995": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044603114s
Jan 28 01:29:17.470: INFO: Pod "busybox-privileged-false-7df498c2-47bf-4b2c-8646-5693daec1995" satisfied condition "Succeeded or Failed"
Jan 28 01:29:17.502: INFO: Got logs for pod "busybox-privileged-false-7df498c2-47bf-4b2c-8646-5693daec1995": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 28 01:29:17.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7012" for this suite. 01/28/23 01:29:17.52
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":247,"skipped":4523,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.203 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:29:11.337
    Jan 28 01:29:11.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename security-context-test 01/28/23 01:29:11.339
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:11.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:11.391
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Jan 28 01:29:11.424: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-7df498c2-47bf-4b2c-8646-5693daec1995" in namespace "security-context-test-7012" to be "Succeeded or Failed"
    Jan 28 01:29:11.457: INFO: Pod "busybox-privileged-false-7df498c2-47bf-4b2c-8646-5693daec1995": Phase="Pending", Reason="", readiness=false. Elapsed: 32.668985ms
    Jan 28 01:29:13.469: INFO: Pod "busybox-privileged-false-7df498c2-47bf-4b2c-8646-5693daec1995": Phase="Running", Reason="", readiness=true. Elapsed: 2.044366366s
    Jan 28 01:29:15.472: INFO: Pod "busybox-privileged-false-7df498c2-47bf-4b2c-8646-5693daec1995": Phase="Running", Reason="", readiness=false. Elapsed: 4.046859518s
    Jan 28 01:29:17.469: INFO: Pod "busybox-privileged-false-7df498c2-47bf-4b2c-8646-5693daec1995": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044603114s
    Jan 28 01:29:17.470: INFO: Pod "busybox-privileged-false-7df498c2-47bf-4b2c-8646-5693daec1995" satisfied condition "Succeeded or Failed"
    Jan 28 01:29:17.502: INFO: Got logs for pod "busybox-privileged-false-7df498c2-47bf-4b2c-8646-5693daec1995": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 28 01:29:17.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-7012" for this suite. 01/28/23 01:29:17.52
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:29:17.54
Jan 28 01:29:17.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename csistoragecapacity 01/28/23 01:29:17.543
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:17.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:17.595
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/28/23 01:29:17.621
STEP: getting /apis/storage.k8s.io 01/28/23 01:29:17.632
STEP: getting /apis/storage.k8s.io/v1 01/28/23 01:29:17.637
STEP: creating 01/28/23 01:29:17.642
STEP: watching 01/28/23 01:29:17.691
Jan 28 01:29:17.692: INFO: starting watch
STEP: getting 01/28/23 01:29:17.714
STEP: listing in namespace 01/28/23 01:29:17.724
STEP: listing across namespaces 01/28/23 01:29:17.735
STEP: patching 01/28/23 01:29:17.745
STEP: updating 01/28/23 01:29:17.759
Jan 28 01:29:17.772: INFO: waiting for watch events with expected annotations in namespace
Jan 28 01:29:17.772: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/28/23 01:29:17.773
STEP: deleting a collection 01/28/23 01:29:17.837
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Jan 28 01:29:17.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-128" for this suite. 01/28/23 01:29:17.924
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":248,"skipped":4523,"failed":0}
------------------------------
â€¢ [0.403 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:29:17.54
    Jan 28 01:29:17.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename csistoragecapacity 01/28/23 01:29:17.543
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:17.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:17.595
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/28/23 01:29:17.621
    STEP: getting /apis/storage.k8s.io 01/28/23 01:29:17.632
    STEP: getting /apis/storage.k8s.io/v1 01/28/23 01:29:17.637
    STEP: creating 01/28/23 01:29:17.642
    STEP: watching 01/28/23 01:29:17.691
    Jan 28 01:29:17.692: INFO: starting watch
    STEP: getting 01/28/23 01:29:17.714
    STEP: listing in namespace 01/28/23 01:29:17.724
    STEP: listing across namespaces 01/28/23 01:29:17.735
    STEP: patching 01/28/23 01:29:17.745
    STEP: updating 01/28/23 01:29:17.759
    Jan 28 01:29:17.772: INFO: waiting for watch events with expected annotations in namespace
    Jan 28 01:29:17.772: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/28/23 01:29:17.773
    STEP: deleting a collection 01/28/23 01:29:17.837
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Jan 28 01:29:17.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-128" for this suite. 01/28/23 01:29:17.924
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:29:17.945
Jan 28 01:29:17.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename replicaset 01/28/23 01:29:17.947
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:17.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:18.004
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/28/23 01:29:18.022
Jan 28 01:29:18.073: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9158" to be "running and ready"
Jan 28 01:29:18.087: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 13.593263ms
Jan 28 01:29:18.087: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:29:20.101: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028003167s
Jan 28 01:29:20.101: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:29:22.100: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.026418639s
Jan 28 01:29:22.100: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan 28 01:29:22.100: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/28/23 01:29:22.111
STEP: Then the orphan pod is adopted 01/28/23 01:29:22.139
STEP: When the matched label of one of its pods change 01/28/23 01:29:23.174
Jan 28 01:29:23.189: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/28/23 01:29:23.217
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 28 01:29:23.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9158" for this suite. 01/28/23 01:29:23.244
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":249,"skipped":4526,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.319 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:29:17.945
    Jan 28 01:29:17.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename replicaset 01/28/23 01:29:17.947
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:17.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:18.004
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/28/23 01:29:18.022
    Jan 28 01:29:18.073: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9158" to be "running and ready"
    Jan 28 01:29:18.087: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 13.593263ms
    Jan 28 01:29:18.087: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:29:20.101: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028003167s
    Jan 28 01:29:20.101: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:29:22.100: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.026418639s
    Jan 28 01:29:22.100: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan 28 01:29:22.100: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/28/23 01:29:22.111
    STEP: Then the orphan pod is adopted 01/28/23 01:29:22.139
    STEP: When the matched label of one of its pods change 01/28/23 01:29:23.174
    Jan 28 01:29:23.189: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/28/23 01:29:23.217
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 28 01:29:23.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9158" for this suite. 01/28/23 01:29:23.244
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:29:23.276
Jan 28 01:29:23.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename replicaset 01/28/23 01:29:23.277
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:23.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:23.328
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/28/23 01:29:23.34
STEP: Verify that the required pods have come up 01/28/23 01:29:23.355
Jan 28 01:29:23.366: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 28 01:29:28.380: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/28/23 01:29:28.38
Jan 28 01:29:28.395: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/28/23 01:29:28.395
STEP: DeleteCollection of the ReplicaSets 01/28/23 01:29:28.417
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/28/23 01:29:28.44
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 28 01:29:28.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7191" for this suite. 01/28/23 01:29:28.485
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":250,"skipped":4557,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.228 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:29:23.276
    Jan 28 01:29:23.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename replicaset 01/28/23 01:29:23.277
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:23.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:23.328
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/28/23 01:29:23.34
    STEP: Verify that the required pods have come up 01/28/23 01:29:23.355
    Jan 28 01:29:23.366: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan 28 01:29:28.380: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/28/23 01:29:28.38
    Jan 28 01:29:28.395: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/28/23 01:29:28.395
    STEP: DeleteCollection of the ReplicaSets 01/28/23 01:29:28.417
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/28/23 01:29:28.44
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 28 01:29:28.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7191" for this suite. 01/28/23 01:29:28.485
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:29:28.512
Jan 28 01:29:28.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename podtemplate 01/28/23 01:29:28.514
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:28.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:28.577
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/28/23 01:29:28.615
STEP: Replace a pod template 01/28/23 01:29:28.657
Jan 28 01:29:28.710: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 28 01:29:28.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8503" for this suite. 01/28/23 01:29:28.726
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":251,"skipped":4588,"failed":0}
------------------------------
â€¢ [0.232 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:29:28.512
    Jan 28 01:29:28.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename podtemplate 01/28/23 01:29:28.514
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:28.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:28.577
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/28/23 01:29:28.615
    STEP: Replace a pod template 01/28/23 01:29:28.657
    Jan 28 01:29:28.710: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 28 01:29:28.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-8503" for this suite. 01/28/23 01:29:28.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:29:28.767
Jan 28 01:29:28.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename gc 01/28/23 01:29:28.769
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:28.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:28.825
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/28/23 01:29:28.85
STEP: Wait for the Deployment to create new ReplicaSet 01/28/23 01:29:28.865
STEP: delete the deployment 01/28/23 01:29:29.401
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/28/23 01:29:29.423
STEP: Gathering metrics 01/28/23 01:29:30.005
W0128 01:29:30.032312      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 28 01:29:30.032: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 28 01:29:30.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1035" for this suite. 01/28/23 01:29:30.051
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":252,"skipped":4622,"failed":0}
------------------------------
â€¢ [1.301 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:29:28.767
    Jan 28 01:29:28.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename gc 01/28/23 01:29:28.769
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:28.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:28.825
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/28/23 01:29:28.85
    STEP: Wait for the Deployment to create new ReplicaSet 01/28/23 01:29:28.865
    STEP: delete the deployment 01/28/23 01:29:29.401
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/28/23 01:29:29.423
    STEP: Gathering metrics 01/28/23 01:29:30.005
    W0128 01:29:30.032312      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 28 01:29:30.032: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 28 01:29:30.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1035" for this suite. 01/28/23 01:29:30.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:29:30.073
Jan 28 01:29:30.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename events 01/28/23 01:29:30.076
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:30.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:30.129
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/28/23 01:29:30.142
STEP: listing events in all namespaces 01/28/23 01:29:30.16
STEP: listing events in test namespace 01/28/23 01:29:30.176
STEP: listing events with field selection filtering on source 01/28/23 01:29:30.187
STEP: listing events with field selection filtering on reportingController 01/28/23 01:29:30.203
STEP: getting the test event 01/28/23 01:29:30.214
STEP: patching the test event 01/28/23 01:29:30.228
STEP: getting the test event 01/28/23 01:29:30.255
STEP: updating the test event 01/28/23 01:29:30.271
STEP: getting the test event 01/28/23 01:29:30.29
STEP: deleting the test event 01/28/23 01:29:30.301
STEP: listing events in all namespaces 01/28/23 01:29:30.326
STEP: listing events in test namespace 01/28/23 01:29:30.339
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan 28 01:29:30.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4157" for this suite. 01/28/23 01:29:30.368
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":253,"skipped":4644,"failed":0}
------------------------------
â€¢ [0.319 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:29:30.073
    Jan 28 01:29:30.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename events 01/28/23 01:29:30.076
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:30.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:30.129
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/28/23 01:29:30.142
    STEP: listing events in all namespaces 01/28/23 01:29:30.16
    STEP: listing events in test namespace 01/28/23 01:29:30.176
    STEP: listing events with field selection filtering on source 01/28/23 01:29:30.187
    STEP: listing events with field selection filtering on reportingController 01/28/23 01:29:30.203
    STEP: getting the test event 01/28/23 01:29:30.214
    STEP: patching the test event 01/28/23 01:29:30.228
    STEP: getting the test event 01/28/23 01:29:30.255
    STEP: updating the test event 01/28/23 01:29:30.271
    STEP: getting the test event 01/28/23 01:29:30.29
    STEP: deleting the test event 01/28/23 01:29:30.301
    STEP: listing events in all namespaces 01/28/23 01:29:30.326
    STEP: listing events in test namespace 01/28/23 01:29:30.339
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan 28 01:29:30.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4157" for this suite. 01/28/23 01:29:30.368
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:29:30.395
Jan 28 01:29:30.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename prestop 01/28/23 01:29:30.397
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:30.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:30.476
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-6852 01/28/23 01:29:30.489
STEP: Waiting for pods to come up. 01/28/23 01:29:30.512
Jan 28 01:29:30.512: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6852" to be "running"
Jan 28 01:29:30.530: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 18.315178ms
Jan 28 01:29:32.548: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.036317718s
Jan 28 01:29:32.548: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-6852 01/28/23 01:29:32.56
Jan 28 01:29:32.574: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6852" to be "running"
Jan 28 01:29:32.584: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 10.378545ms
Jan 28 01:29:34.596: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.022190276s
Jan 28 01:29:34.597: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/28/23 01:29:34.597
Jan 28 01:29:39.666: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/28/23 01:29:39.666
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Jan 28 01:29:39.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6852" for this suite. 01/28/23 01:29:39.717
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":254,"skipped":4650,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.347 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:29:30.395
    Jan 28 01:29:30.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename prestop 01/28/23 01:29:30.397
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:30.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:30.476
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-6852 01/28/23 01:29:30.489
    STEP: Waiting for pods to come up. 01/28/23 01:29:30.512
    Jan 28 01:29:30.512: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6852" to be "running"
    Jan 28 01:29:30.530: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 18.315178ms
    Jan 28 01:29:32.548: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.036317718s
    Jan 28 01:29:32.548: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-6852 01/28/23 01:29:32.56
    Jan 28 01:29:32.574: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6852" to be "running"
    Jan 28 01:29:32.584: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 10.378545ms
    Jan 28 01:29:34.596: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.022190276s
    Jan 28 01:29:34.597: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/28/23 01:29:34.597
    Jan 28 01:29:39.666: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/28/23 01:29:39.666
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Jan 28 01:29:39.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-6852" for this suite. 01/28/23 01:29:39.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:29:39.746
Jan 28 01:29:39.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubelet-test 01/28/23 01:29:39.748
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:39.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:39.807
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 28 01:29:43.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8805" for this suite. 01/28/23 01:29:43.884
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":255,"skipped":4684,"failed":0}
------------------------------
â€¢ [4.159 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:29:39.746
    Jan 28 01:29:39.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubelet-test 01/28/23 01:29:39.748
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:39.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:39.807
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 28 01:29:43.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-8805" for this suite. 01/28/23 01:29:43.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:29:43.917
Jan 28 01:29:43.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename services 01/28/23 01:29:43.918
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:43.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:43.991
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-9007 01/28/23 01:29:44.004
Jan 28 01:29:44.084: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-9007" to be "running and ready"
Jan 28 01:29:44.095: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 11.177548ms
Jan 28 01:29:44.095: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:29:46.107: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.023236077s
Jan 28 01:29:46.108: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 28 01:29:46.108: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan 28 01:29:46.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 28 01:29:46.430: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 28 01:29:46.430: INFO: stdout: "iptables"
Jan 28 01:29:46.430: INFO: proxyMode: iptables
Jan 28 01:29:46.461: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 28 01:29:46.472: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-9007 01/28/23 01:29:46.472
STEP: creating replication controller affinity-nodeport-timeout in namespace services-9007 01/28/23 01:29:46.515
I0128 01:29:46.528646      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-9007, replica count: 3
I0128 01:29:49.579918      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 01:29:49.623: INFO: Creating new exec pod
Jan 28 01:29:49.638: INFO: Waiting up to 5m0s for pod "execpod-affinitykfg5w" in namespace "services-9007" to be "running"
Jan 28 01:29:49.655: INFO: Pod "execpod-affinitykfg5w": Phase="Pending", Reason="", readiness=false. Elapsed: 17.301389ms
Jan 28 01:29:51.668: INFO: Pod "execpod-affinitykfg5w": Phase="Running", Reason="", readiness=true. Elapsed: 2.030787924s
Jan 28 01:29:51.669: INFO: Pod "execpod-affinitykfg5w" satisfied condition "running"
Jan 28 01:29:52.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec execpod-affinitykfg5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan 28 01:29:53.001: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan 28 01:29:53.001: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:29:53.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec execpod-affinitykfg5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.43.152 80'
Jan 28 01:29:53.357: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.43.152 80\nConnection to 172.21.43.152 80 port [tcp/http] succeeded!\n"
Jan 28 01:29:53.357: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:29:53.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec execpod-affinitykfg5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.72 30340'
Jan 28 01:29:53.708: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.9.20.72 30340\nConnection to 10.9.20.72 30340 port [tcp/*] succeeded!\n"
Jan 28 01:29:53.708: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:29:53.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec execpod-affinitykfg5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.126 30340'
Jan 28 01:29:54.053: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.9.20.126 30340\nConnection to 10.9.20.126 30340 port [tcp/*] succeeded!\n"
Jan 28 01:29:54.053: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:29:54.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec execpod-affinitykfg5w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.9.20.126:30340/ ; done'
Jan 28 01:29:54.594: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n"
Jan 28 01:29:54.594: INFO: stdout: "\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk"
Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
Jan 28 01:29:54.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec execpod-affinitykfg5w -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.9.20.126:30340/'
Jan 28 01:29:54.954: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n"
Jan 28 01:29:54.954: INFO: stdout: "affinity-nodeport-timeout-dxppk"
Jan 28 01:30:14.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec execpod-affinitykfg5w -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.9.20.126:30340/'
Jan 28 01:30:15.286: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n"
Jan 28 01:30:15.286: INFO: stdout: "affinity-nodeport-timeout-dxppk"
Jan 28 01:30:35.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec execpod-affinitykfg5w -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.9.20.126:30340/'
Jan 28 01:30:35.618: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n"
Jan 28 01:30:35.618: INFO: stdout: "affinity-nodeport-timeout-dxppk"
Jan 28 01:30:55.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec execpod-affinitykfg5w -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.9.20.126:30340/'
Jan 28 01:30:55.930: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n"
Jan 28 01:30:55.930: INFO: stdout: "affinity-nodeport-timeout-2ttwn"
Jan 28 01:30:55.930: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-9007, will wait for the garbage collector to delete the pods 01/28/23 01:30:55.983
Jan 28 01:30:56.075: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 28.102243ms
Jan 28 01:30:56.176: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.507552ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 28 01:30:59.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9007" for this suite. 01/28/23 01:30:59.177
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":256,"skipped":4725,"failed":0}
------------------------------
â€¢ [SLOW TEST] [75.281 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:29:43.917
    Jan 28 01:29:43.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename services 01/28/23 01:29:43.918
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:29:43.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:29:43.991
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-9007 01/28/23 01:29:44.004
    Jan 28 01:29:44.084: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-9007" to be "running and ready"
    Jan 28 01:29:44.095: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 11.177548ms
    Jan 28 01:29:44.095: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:29:46.107: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.023236077s
    Jan 28 01:29:46.108: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan 28 01:29:46.108: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan 28 01:29:46.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan 28 01:29:46.430: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan 28 01:29:46.430: INFO: stdout: "iptables"
    Jan 28 01:29:46.430: INFO: proxyMode: iptables
    Jan 28 01:29:46.461: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan 28 01:29:46.472: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-9007 01/28/23 01:29:46.472
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-9007 01/28/23 01:29:46.515
    I0128 01:29:46.528646      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-9007, replica count: 3
    I0128 01:29:49.579918      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 28 01:29:49.623: INFO: Creating new exec pod
    Jan 28 01:29:49.638: INFO: Waiting up to 5m0s for pod "execpod-affinitykfg5w" in namespace "services-9007" to be "running"
    Jan 28 01:29:49.655: INFO: Pod "execpod-affinitykfg5w": Phase="Pending", Reason="", readiness=false. Elapsed: 17.301389ms
    Jan 28 01:29:51.668: INFO: Pod "execpod-affinitykfg5w": Phase="Running", Reason="", readiness=true. Elapsed: 2.030787924s
    Jan 28 01:29:51.669: INFO: Pod "execpod-affinitykfg5w" satisfied condition "running"
    Jan 28 01:29:52.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec execpod-affinitykfg5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Jan 28 01:29:53.001: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Jan 28 01:29:53.001: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 01:29:53.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec execpod-affinitykfg5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.43.152 80'
    Jan 28 01:29:53.357: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.43.152 80\nConnection to 172.21.43.152 80 port [tcp/http] succeeded!\n"
    Jan 28 01:29:53.357: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 01:29:53.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec execpod-affinitykfg5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.72 30340'
    Jan 28 01:29:53.708: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.9.20.72 30340\nConnection to 10.9.20.72 30340 port [tcp/*] succeeded!\n"
    Jan 28 01:29:53.708: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 01:29:53.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec execpod-affinitykfg5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.126 30340'
    Jan 28 01:29:54.053: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.9.20.126 30340\nConnection to 10.9.20.126 30340 port [tcp/*] succeeded!\n"
    Jan 28 01:29:54.053: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 01:29:54.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec execpod-affinitykfg5w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.9.20.126:30340/ ; done'
    Jan 28 01:29:54.594: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n"
    Jan 28 01:29:54.594: INFO: stdout: "\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk\naffinity-nodeport-timeout-dxppk"
    Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
    Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
    Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
    Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
    Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
    Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
    Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
    Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
    Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
    Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
    Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
    Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
    Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
    Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
    Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
    Jan 28 01:29:54.594: INFO: Received response from host: affinity-nodeport-timeout-dxppk
    Jan 28 01:29:54.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec execpod-affinitykfg5w -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.9.20.126:30340/'
    Jan 28 01:29:54.954: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n"
    Jan 28 01:29:54.954: INFO: stdout: "affinity-nodeport-timeout-dxppk"
    Jan 28 01:30:14.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec execpod-affinitykfg5w -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.9.20.126:30340/'
    Jan 28 01:30:15.286: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n"
    Jan 28 01:30:15.286: INFO: stdout: "affinity-nodeport-timeout-dxppk"
    Jan 28 01:30:35.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec execpod-affinitykfg5w -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.9.20.126:30340/'
    Jan 28 01:30:35.618: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n"
    Jan 28 01:30:35.618: INFO: stdout: "affinity-nodeport-timeout-dxppk"
    Jan 28 01:30:55.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-9007 exec execpod-affinitykfg5w -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.9.20.126:30340/'
    Jan 28 01:30:55.930: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.9.20.126:30340/\n"
    Jan 28 01:30:55.930: INFO: stdout: "affinity-nodeport-timeout-2ttwn"
    Jan 28 01:30:55.930: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-9007, will wait for the garbage collector to delete the pods 01/28/23 01:30:55.983
    Jan 28 01:30:56.075: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 28.102243ms
    Jan 28 01:30:56.176: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.507552ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 28 01:30:59.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9007" for this suite. 01/28/23 01:30:59.177
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:30:59.198
Jan 28 01:30:59.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 01:30:59.203
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:30:59.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:30:59.284
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-865b18e6-7756-4674-85b6-d621495ab15f 01/28/23 01:30:59.296
STEP: Creating a pod to test consume secrets 01/28/23 01:30:59.31
Jan 28 01:30:59.330: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6a6a7165-e771-4cf1-82fa-e169267e09b2" in namespace "projected-2441" to be "Succeeded or Failed"
Jan 28 01:30:59.342: INFO: Pod "pod-projected-secrets-6a6a7165-e771-4cf1-82fa-e169267e09b2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.512552ms
Jan 28 01:31:01.360: INFO: Pod "pod-projected-secrets-6a6a7165-e771-4cf1-82fa-e169267e09b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.029734657s
Jan 28 01:31:03.381: INFO: Pod "pod-projected-secrets-6a6a7165-e771-4cf1-82fa-e169267e09b2": Phase="Running", Reason="", readiness=false. Elapsed: 4.050944145s
Jan 28 01:31:05.355: INFO: Pod "pod-projected-secrets-6a6a7165-e771-4cf1-82fa-e169267e09b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024744455s
STEP: Saw pod success 01/28/23 01:31:05.355
Jan 28 01:31:05.355: INFO: Pod "pod-projected-secrets-6a6a7165-e771-4cf1-82fa-e169267e09b2" satisfied condition "Succeeded or Failed"
Jan 28 01:31:05.367: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-secrets-6a6a7165-e771-4cf1-82fa-e169267e09b2 container secret-volume-test: <nil>
STEP: delete the pod 01/28/23 01:31:05.437
Jan 28 01:31:05.470: INFO: Waiting for pod pod-projected-secrets-6a6a7165-e771-4cf1-82fa-e169267e09b2 to disappear
Jan 28 01:31:05.501: INFO: Pod pod-projected-secrets-6a6a7165-e771-4cf1-82fa-e169267e09b2 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 28 01:31:05.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2441" for this suite. 01/28/23 01:31:05.541
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":257,"skipped":4726,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.378 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:30:59.198
    Jan 28 01:30:59.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 01:30:59.203
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:30:59.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:30:59.284
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-865b18e6-7756-4674-85b6-d621495ab15f 01/28/23 01:30:59.296
    STEP: Creating a pod to test consume secrets 01/28/23 01:30:59.31
    Jan 28 01:30:59.330: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6a6a7165-e771-4cf1-82fa-e169267e09b2" in namespace "projected-2441" to be "Succeeded or Failed"
    Jan 28 01:30:59.342: INFO: Pod "pod-projected-secrets-6a6a7165-e771-4cf1-82fa-e169267e09b2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.512552ms
    Jan 28 01:31:01.360: INFO: Pod "pod-projected-secrets-6a6a7165-e771-4cf1-82fa-e169267e09b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.029734657s
    Jan 28 01:31:03.381: INFO: Pod "pod-projected-secrets-6a6a7165-e771-4cf1-82fa-e169267e09b2": Phase="Running", Reason="", readiness=false. Elapsed: 4.050944145s
    Jan 28 01:31:05.355: INFO: Pod "pod-projected-secrets-6a6a7165-e771-4cf1-82fa-e169267e09b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024744455s
    STEP: Saw pod success 01/28/23 01:31:05.355
    Jan 28 01:31:05.355: INFO: Pod "pod-projected-secrets-6a6a7165-e771-4cf1-82fa-e169267e09b2" satisfied condition "Succeeded or Failed"
    Jan 28 01:31:05.367: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-secrets-6a6a7165-e771-4cf1-82fa-e169267e09b2 container secret-volume-test: <nil>
    STEP: delete the pod 01/28/23 01:31:05.437
    Jan 28 01:31:05.470: INFO: Waiting for pod pod-projected-secrets-6a6a7165-e771-4cf1-82fa-e169267e09b2 to disappear
    Jan 28 01:31:05.501: INFO: Pod pod-projected-secrets-6a6a7165-e771-4cf1-82fa-e169267e09b2 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 28 01:31:05.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2441" for this suite. 01/28/23 01:31:05.541
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:31:05.576
Jan 28 01:31:05.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename pod-network-test 01/28/23 01:31:05.578
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:31:05.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:31:05.653
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-9128 01/28/23 01:31:05.663
STEP: creating a selector 01/28/23 01:31:05.664
STEP: Creating the service pods in kubernetes 01/28/23 01:31:05.664
Jan 28 01:31:05.664: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 28 01:31:05.771: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9128" to be "running and ready"
Jan 28 01:31:05.810: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 38.8971ms
Jan 28 01:31:05.810: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:31:07.827: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05609291s
Jan 28 01:31:07.827: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:31:09.822: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.050794838s
Jan 28 01:31:09.822: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:31:11.823: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.052031073s
Jan 28 01:31:11.823: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:31:13.831: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.059251957s
Jan 28 01:31:13.831: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:31:15.823: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.051261243s
Jan 28 01:31:15.823: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:31:17.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.052394449s
Jan 28 01:31:17.824: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:31:19.823: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.051782869s
Jan 28 01:31:19.823: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:31:21.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.053187105s
Jan 28 01:31:21.825: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:31:23.822: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.051119928s
Jan 28 01:31:23.822: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:31:25.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.052516693s
Jan 28 01:31:25.824: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:31:27.823: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.051420092s
Jan 28 01:31:27.823: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 28 01:31:27.823: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 28 01:31:27.834: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9128" to be "running and ready"
Jan 28 01:31:27.845: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.43938ms
Jan 28 01:31:27.845: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 28 01:31:27.845: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 28 01:31:27.874: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9128" to be "running and ready"
Jan 28 01:31:27.885: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.267087ms
Jan 28 01:31:27.885: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 28 01:31:27.885: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/28/23 01:31:27.918
Jan 28 01:31:27.991: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9128" to be "running"
Jan 28 01:31:28.003: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.440971ms
Jan 28 01:31:30.043: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052025111s
Jan 28 01:31:32.015: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.023642381s
Jan 28 01:31:32.015: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 28 01:31:32.026: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9128" to be "running"
Jan 28 01:31:32.041: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 14.623439ms
Jan 28 01:31:32.041: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 28 01:31:32.052: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 28 01:31:32.052: INFO: Going to poll 172.30.12.237 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan 28 01:31:32.063: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.12.237 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9128 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:31:32.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:31:32.064: INFO: ExecWithOptions: Clientset creation
Jan 28 01:31:32.065: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9128/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.12.237+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 28 01:31:33.257: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 28 01:31:33.258: INFO: Going to poll 172.30.185.52 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan 28 01:31:33.270: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.185.52 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9128 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:31:33.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:31:33.272: INFO: ExecWithOptions: Clientset creation
Jan 28 01:31:33.272: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9128/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.185.52+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 28 01:31:34.589: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 28 01:31:34.589: INFO: Going to poll 172.30.84.62 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan 28 01:31:34.601: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.84.62 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9128 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:31:34.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:31:34.602: INFO: ExecWithOptions: Clientset creation
Jan 28 01:31:34.603: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9128/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.84.62+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 28 01:31:35.793: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 28 01:31:35.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9128" for this suite. 01/28/23 01:31:35.812
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":258,"skipped":4729,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.274 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:31:05.576
    Jan 28 01:31:05.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename pod-network-test 01/28/23 01:31:05.578
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:31:05.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:31:05.653
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-9128 01/28/23 01:31:05.663
    STEP: creating a selector 01/28/23 01:31:05.664
    STEP: Creating the service pods in kubernetes 01/28/23 01:31:05.664
    Jan 28 01:31:05.664: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 28 01:31:05.771: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9128" to be "running and ready"
    Jan 28 01:31:05.810: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 38.8971ms
    Jan 28 01:31:05.810: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:31:07.827: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05609291s
    Jan 28 01:31:07.827: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:31:09.822: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.050794838s
    Jan 28 01:31:09.822: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:31:11.823: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.052031073s
    Jan 28 01:31:11.823: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:31:13.831: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.059251957s
    Jan 28 01:31:13.831: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:31:15.823: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.051261243s
    Jan 28 01:31:15.823: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:31:17.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.052394449s
    Jan 28 01:31:17.824: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:31:19.823: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.051782869s
    Jan 28 01:31:19.823: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:31:21.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.053187105s
    Jan 28 01:31:21.825: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:31:23.822: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.051119928s
    Jan 28 01:31:23.822: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:31:25.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.052516693s
    Jan 28 01:31:25.824: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:31:27.823: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.051420092s
    Jan 28 01:31:27.823: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 28 01:31:27.823: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 28 01:31:27.834: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9128" to be "running and ready"
    Jan 28 01:31:27.845: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.43938ms
    Jan 28 01:31:27.845: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 28 01:31:27.845: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 28 01:31:27.874: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9128" to be "running and ready"
    Jan 28 01:31:27.885: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.267087ms
    Jan 28 01:31:27.885: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 28 01:31:27.885: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/28/23 01:31:27.918
    Jan 28 01:31:27.991: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9128" to be "running"
    Jan 28 01:31:28.003: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.440971ms
    Jan 28 01:31:30.043: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052025111s
    Jan 28 01:31:32.015: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.023642381s
    Jan 28 01:31:32.015: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 28 01:31:32.026: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9128" to be "running"
    Jan 28 01:31:32.041: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 14.623439ms
    Jan 28 01:31:32.041: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 28 01:31:32.052: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan 28 01:31:32.052: INFO: Going to poll 172.30.12.237 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan 28 01:31:32.063: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.12.237 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9128 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:31:32.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:31:32.064: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:31:32.065: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9128/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.12.237+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 28 01:31:33.257: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 28 01:31:33.258: INFO: Going to poll 172.30.185.52 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan 28 01:31:33.270: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.185.52 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9128 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:31:33.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:31:33.272: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:31:33.272: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9128/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.185.52+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 28 01:31:34.589: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan 28 01:31:34.589: INFO: Going to poll 172.30.84.62 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan 28 01:31:34.601: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.84.62 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9128 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:31:34.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:31:34.602: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:31:34.603: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9128/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.84.62+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 28 01:31:35.793: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 28 01:31:35.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-9128" for this suite. 01/28/23 01:31:35.812
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:31:35.853
Jan 28 01:31:35.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename secrets 01/28/23 01:31:35.856
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:31:35.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:31:35.939
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-fd3edff8-3047-4c2a-a26a-8f69f8d80573 01/28/23 01:31:35.971
STEP: Creating secret with name s-test-opt-upd-2f3c4d50-b32f-46b3-89cf-0daf7c55f96c 01/28/23 01:31:35.99
STEP: Creating the pod 01/28/23 01:31:36.026
Jan 28 01:31:36.049: INFO: Waiting up to 5m0s for pod "pod-secrets-e82fd932-bf3b-4436-bd2a-9ff1791253d9" in namespace "secrets-7619" to be "running and ready"
Jan 28 01:31:36.060: INFO: Pod "pod-secrets-e82fd932-bf3b-4436-bd2a-9ff1791253d9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.092706ms
Jan 28 01:31:36.060: INFO: The phase of Pod pod-secrets-e82fd932-bf3b-4436-bd2a-9ff1791253d9 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:31:38.074: INFO: Pod "pod-secrets-e82fd932-bf3b-4436-bd2a-9ff1791253d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0254502s
Jan 28 01:31:38.074: INFO: The phase of Pod pod-secrets-e82fd932-bf3b-4436-bd2a-9ff1791253d9 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:31:40.073: INFO: Pod "pod-secrets-e82fd932-bf3b-4436-bd2a-9ff1791253d9": Phase="Running", Reason="", readiness=true. Elapsed: 4.023553476s
Jan 28 01:31:40.073: INFO: The phase of Pod pod-secrets-e82fd932-bf3b-4436-bd2a-9ff1791253d9 is Running (Ready = true)
Jan 28 01:31:40.073: INFO: Pod "pod-secrets-e82fd932-bf3b-4436-bd2a-9ff1791253d9" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-fd3edff8-3047-4c2a-a26a-8f69f8d80573 01/28/23 01:31:40.164
STEP: Updating secret s-test-opt-upd-2f3c4d50-b32f-46b3-89cf-0daf7c55f96c 01/28/23 01:31:40.213
STEP: Creating secret with name s-test-opt-create-129e2069-539a-4877-815c-fe30d93ae5a9 01/28/23 01:31:40.227
STEP: waiting to observe update in volume 01/28/23 01:31:40.269
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 28 01:33:09.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7619" for this suite. 01/28/23 01:33:09.914
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":259,"skipped":4733,"failed":0}
------------------------------
â€¢ [SLOW TEST] [94.079 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:31:35.853
    Jan 28 01:31:35.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename secrets 01/28/23 01:31:35.856
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:31:35.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:31:35.939
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-fd3edff8-3047-4c2a-a26a-8f69f8d80573 01/28/23 01:31:35.971
    STEP: Creating secret with name s-test-opt-upd-2f3c4d50-b32f-46b3-89cf-0daf7c55f96c 01/28/23 01:31:35.99
    STEP: Creating the pod 01/28/23 01:31:36.026
    Jan 28 01:31:36.049: INFO: Waiting up to 5m0s for pod "pod-secrets-e82fd932-bf3b-4436-bd2a-9ff1791253d9" in namespace "secrets-7619" to be "running and ready"
    Jan 28 01:31:36.060: INFO: Pod "pod-secrets-e82fd932-bf3b-4436-bd2a-9ff1791253d9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.092706ms
    Jan 28 01:31:36.060: INFO: The phase of Pod pod-secrets-e82fd932-bf3b-4436-bd2a-9ff1791253d9 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:31:38.074: INFO: Pod "pod-secrets-e82fd932-bf3b-4436-bd2a-9ff1791253d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0254502s
    Jan 28 01:31:38.074: INFO: The phase of Pod pod-secrets-e82fd932-bf3b-4436-bd2a-9ff1791253d9 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:31:40.073: INFO: Pod "pod-secrets-e82fd932-bf3b-4436-bd2a-9ff1791253d9": Phase="Running", Reason="", readiness=true. Elapsed: 4.023553476s
    Jan 28 01:31:40.073: INFO: The phase of Pod pod-secrets-e82fd932-bf3b-4436-bd2a-9ff1791253d9 is Running (Ready = true)
    Jan 28 01:31:40.073: INFO: Pod "pod-secrets-e82fd932-bf3b-4436-bd2a-9ff1791253d9" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-fd3edff8-3047-4c2a-a26a-8f69f8d80573 01/28/23 01:31:40.164
    STEP: Updating secret s-test-opt-upd-2f3c4d50-b32f-46b3-89cf-0daf7c55f96c 01/28/23 01:31:40.213
    STEP: Creating secret with name s-test-opt-create-129e2069-539a-4877-815c-fe30d93ae5a9 01/28/23 01:31:40.227
    STEP: waiting to observe update in volume 01/28/23 01:31:40.269
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 28 01:33:09.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7619" for this suite. 01/28/23 01:33:09.914
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:33:09.933
Jan 28 01:33:09.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename webhook 01/28/23 01:33:09.935
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:33:09.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:33:09.997
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/28/23 01:33:10.053
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:33:10.412
STEP: Deploying the webhook pod 01/28/23 01:33:10.432
STEP: Wait for the deployment to be ready 01/28/23 01:33:10.461
Jan 28 01:33:10.490: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 01:33:12.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 33, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 33, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 33, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 33, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/28/23 01:33:14.544
STEP: Verifying the service has paired with the endpoint 01/28/23 01:33:14.582
Jan 28 01:33:15.583: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/28/23 01:33:15.595
STEP: create a pod that should be updated by the webhook 01/28/23 01:33:15.681
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:33:15.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2353" for this suite. 01/28/23 01:33:15.804
STEP: Destroying namespace "webhook-2353-markers" for this suite. 01/28/23 01:33:15.822
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":260,"skipped":4736,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.019 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:33:09.933
    Jan 28 01:33:09.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename webhook 01/28/23 01:33:09.935
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:33:09.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:33:09.997
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/28/23 01:33:10.053
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:33:10.412
    STEP: Deploying the webhook pod 01/28/23 01:33:10.432
    STEP: Wait for the deployment to be ready 01/28/23 01:33:10.461
    Jan 28 01:33:10.490: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 28 01:33:12.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 33, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 33, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 33, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 33, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/28/23 01:33:14.544
    STEP: Verifying the service has paired with the endpoint 01/28/23 01:33:14.582
    Jan 28 01:33:15.583: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/28/23 01:33:15.595
    STEP: create a pod that should be updated by the webhook 01/28/23 01:33:15.681
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:33:15.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2353" for this suite. 01/28/23 01:33:15.804
    STEP: Destroying namespace "webhook-2353-markers" for this suite. 01/28/23 01:33:15.822
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:33:15.954
Jan 28 01:33:15.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename gc 01/28/23 01:33:15.957
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:33:15.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:33:16.008
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan 28 01:33:16.089: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"2e435bfd-4fb0-4137-b491-3088c7efcc2e", Controller:(*bool)(0xc00489420e), BlockOwnerDeletion:(*bool)(0xc00489420f)}}
Jan 28 01:33:16.122: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"050707eb-9251-4f14-8908-d4e1fe846ce1", Controller:(*bool)(0xc0048944ae), BlockOwnerDeletion:(*bool)(0xc0048944af)}}
Jan 28 01:33:16.135: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"422ded3c-b474-46ce-88f8-0540dc1df853", Controller:(*bool)(0xc003e71d66), BlockOwnerDeletion:(*bool)(0xc003e71d67)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 28 01:33:21.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4831" for this suite. 01/28/23 01:33:21.207
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":261,"skipped":4751,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.272 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:33:15.954
    Jan 28 01:33:15.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename gc 01/28/23 01:33:15.957
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:33:15.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:33:16.008
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan 28 01:33:16.089: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"2e435bfd-4fb0-4137-b491-3088c7efcc2e", Controller:(*bool)(0xc00489420e), BlockOwnerDeletion:(*bool)(0xc00489420f)}}
    Jan 28 01:33:16.122: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"050707eb-9251-4f14-8908-d4e1fe846ce1", Controller:(*bool)(0xc0048944ae), BlockOwnerDeletion:(*bool)(0xc0048944af)}}
    Jan 28 01:33:16.135: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"422ded3c-b474-46ce-88f8-0540dc1df853", Controller:(*bool)(0xc003e71d66), BlockOwnerDeletion:(*bool)(0xc003e71d67)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 28 01:33:21.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4831" for this suite. 01/28/23 01:33:21.207
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:33:21.229
Jan 28 01:33:21.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename dns 01/28/23 01:33:21.231
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:33:21.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:33:21.312
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/28/23 01:33:21.325
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5546 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5546;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5546 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5546;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5546.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5546.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5546.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5546.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5546.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5546.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5546.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5546.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5546.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5546.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5546.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5546.svc;check="$$(dig +notcp +noall +answer +search 66.48.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.48.66_udp@PTR;check="$$(dig +tcp +noall +answer +search 66.48.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.48.66_tcp@PTR;sleep 1; done
 01/28/23 01:33:21.378
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5546 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5546;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5546 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5546;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5546.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5546.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5546.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5546.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5546.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5546.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5546.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5546.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5546.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5546.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5546.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5546.svc;check="$$(dig +notcp +noall +answer +search 66.48.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.48.66_udp@PTR;check="$$(dig +tcp +noall +answer +search 66.48.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.48.66_tcp@PTR;sleep 1; done
 01/28/23 01:33:21.378
STEP: creating a pod to probe DNS 01/28/23 01:33:21.379
STEP: submitting the pod to kubernetes 01/28/23 01:33:21.38
Jan 28 01:33:21.452: INFO: Waiting up to 15m0s for pod "dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0" in namespace "dns-5546" to be "running"
Jan 28 01:33:21.465: INFO: Pod "dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.366936ms
Jan 28 01:33:23.509: INFO: Pod "dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056382487s
Jan 28 01:33:25.481: INFO: Pod "dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0": Phase="Running", Reason="", readiness=true. Elapsed: 4.028350921s
Jan 28 01:33:25.481: INFO: Pod "dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0" satisfied condition "running"
STEP: retrieving the pod 01/28/23 01:33:25.481
STEP: looking for the results for each expected name from probers 01/28/23 01:33:25.493
Jan 28 01:33:25.553: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:25.570: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:25.587: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:25.606: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:25.621: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:25.638: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:25.655: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:25.674: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:25.758: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:25.774: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:25.797: INFO: Unable to read jessie_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:25.814: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:25.830: INFO: Unable to read jessie_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:25.848: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:25.863: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:25.892: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:25.962: INFO: Lookups using dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5546 wheezy_tcp@dns-test-service.dns-5546 wheezy_udp@dns-test-service.dns-5546.svc wheezy_tcp@dns-test-service.dns-5546.svc wheezy_udp@_http._tcp.dns-test-service.dns-5546.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5546.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5546 jessie_tcp@dns-test-service.dns-5546 jessie_udp@dns-test-service.dns-5546.svc jessie_tcp@dns-test-service.dns-5546.svc jessie_udp@_http._tcp.dns-test-service.dns-5546.svc jessie_tcp@_http._tcp.dns-test-service.dns-5546.svc]

Jan 28 01:33:30.982: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:31.005: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:31.023: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:31.045: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:31.072: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:31.092: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:31.109: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:31.246: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:31.262: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:31.278: INFO: Unable to read jessie_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:31.296: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:31.312: INFO: Unable to read jessie_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:31.329: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:31.450: INFO: Lookups using dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5546 wheezy_tcp@dns-test-service.dns-5546 wheezy_udp@dns-test-service.dns-5546.svc wheezy_tcp@dns-test-service.dns-5546.svc wheezy_udp@_http._tcp.dns-test-service.dns-5546.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5546 jessie_tcp@dns-test-service.dns-5546 jessie_udp@dns-test-service.dns-5546.svc jessie_tcp@dns-test-service.dns-5546.svc]

Jan 28 01:33:35.983: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:36.005: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:36.022: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:36.039: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:36.054: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:36.070: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:36.184: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:36.200: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:36.217: INFO: Unable to read jessie_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:36.233: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:36.248: INFO: Unable to read jessie_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:36.264: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:36.379: INFO: Lookups using dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5546 wheezy_tcp@dns-test-service.dns-5546 wheezy_udp@dns-test-service.dns-5546.svc wheezy_tcp@dns-test-service.dns-5546.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5546 jessie_tcp@dns-test-service.dns-5546 jessie_udp@dns-test-service.dns-5546.svc jessie_tcp@dns-test-service.dns-5546.svc]

Jan 28 01:33:40.981: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:40.999: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:41.017: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:41.036: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:41.053: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:41.072: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:41.194: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:41.210: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:41.227: INFO: Unable to read jessie_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:41.243: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:41.260: INFO: Unable to read jessie_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:41.277: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:41.381: INFO: Lookups using dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5546 wheezy_tcp@dns-test-service.dns-5546 wheezy_udp@dns-test-service.dns-5546.svc wheezy_tcp@dns-test-service.dns-5546.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5546 jessie_tcp@dns-test-service.dns-5546 jessie_udp@dns-test-service.dns-5546.svc jessie_tcp@dns-test-service.dns-5546.svc]

Jan 28 01:33:45.984: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:46.002: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:46.019: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:46.064: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:46.082: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:46.103: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:46.230: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:46.252: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:46.269: INFO: Unable to read jessie_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:46.285: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:46.300: INFO: Unable to read jessie_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:46.318: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:46.418: INFO: Lookups using dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5546 wheezy_tcp@dns-test-service.dns-5546 wheezy_udp@dns-test-service.dns-5546.svc wheezy_tcp@dns-test-service.dns-5546.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5546 jessie_tcp@dns-test-service.dns-5546 jessie_udp@dns-test-service.dns-5546.svc jessie_tcp@dns-test-service.dns-5546.svc]

Jan 28 01:33:50.982: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:50.999: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:51.016: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:51.036: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:51.054: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:51.072: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:51.216: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:51.236: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:51.253: INFO: Unable to read jessie_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:51.270: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:51.313: INFO: Unable to read jessie_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:51.331: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
Jan 28 01:33:51.434: INFO: Lookups using dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5546 wheezy_tcp@dns-test-service.dns-5546 wheezy_udp@dns-test-service.dns-5546.svc wheezy_tcp@dns-test-service.dns-5546.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5546 jessie_tcp@dns-test-service.dns-5546 jessie_udp@dns-test-service.dns-5546.svc jessie_tcp@dns-test-service.dns-5546.svc]

Jan 28 01:33:56.427: INFO: DNS probes using dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0 succeeded

STEP: deleting the pod 01/28/23 01:33:56.427
STEP: deleting the test service 01/28/23 01:33:56.469
STEP: deleting the test headless service 01/28/23 01:33:56.514
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 28 01:33:56.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5546" for this suite. 01/28/23 01:33:56.584
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":262,"skipped":4757,"failed":0}
------------------------------
â€¢ [SLOW TEST] [35.374 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:33:21.229
    Jan 28 01:33:21.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename dns 01/28/23 01:33:21.231
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:33:21.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:33:21.312
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/28/23 01:33:21.325
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5546 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5546;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5546 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5546;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5546.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5546.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5546.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5546.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5546.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5546.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5546.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5546.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5546.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5546.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5546.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5546.svc;check="$$(dig +notcp +noall +answer +search 66.48.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.48.66_udp@PTR;check="$$(dig +tcp +noall +answer +search 66.48.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.48.66_tcp@PTR;sleep 1; done
     01/28/23 01:33:21.378
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5546 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5546;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5546 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5546;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5546.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5546.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5546.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5546.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5546.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5546.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5546.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5546.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5546.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5546.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5546.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5546.svc;check="$$(dig +notcp +noall +answer +search 66.48.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.48.66_udp@PTR;check="$$(dig +tcp +noall +answer +search 66.48.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.48.66_tcp@PTR;sleep 1; done
     01/28/23 01:33:21.378
    STEP: creating a pod to probe DNS 01/28/23 01:33:21.379
    STEP: submitting the pod to kubernetes 01/28/23 01:33:21.38
    Jan 28 01:33:21.452: INFO: Waiting up to 15m0s for pod "dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0" in namespace "dns-5546" to be "running"
    Jan 28 01:33:21.465: INFO: Pod "dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.366936ms
    Jan 28 01:33:23.509: INFO: Pod "dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056382487s
    Jan 28 01:33:25.481: INFO: Pod "dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0": Phase="Running", Reason="", readiness=true. Elapsed: 4.028350921s
    Jan 28 01:33:25.481: INFO: Pod "dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0" satisfied condition "running"
    STEP: retrieving the pod 01/28/23 01:33:25.481
    STEP: looking for the results for each expected name from probers 01/28/23 01:33:25.493
    Jan 28 01:33:25.553: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:25.570: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:25.587: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:25.606: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:25.621: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:25.638: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:25.655: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:25.674: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:25.758: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:25.774: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:25.797: INFO: Unable to read jessie_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:25.814: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:25.830: INFO: Unable to read jessie_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:25.848: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:25.863: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:25.892: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:25.962: INFO: Lookups using dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5546 wheezy_tcp@dns-test-service.dns-5546 wheezy_udp@dns-test-service.dns-5546.svc wheezy_tcp@dns-test-service.dns-5546.svc wheezy_udp@_http._tcp.dns-test-service.dns-5546.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5546.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5546 jessie_tcp@dns-test-service.dns-5546 jessie_udp@dns-test-service.dns-5546.svc jessie_tcp@dns-test-service.dns-5546.svc jessie_udp@_http._tcp.dns-test-service.dns-5546.svc jessie_tcp@_http._tcp.dns-test-service.dns-5546.svc]

    Jan 28 01:33:30.982: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:31.005: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:31.023: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:31.045: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:31.072: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:31.092: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:31.109: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:31.246: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:31.262: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:31.278: INFO: Unable to read jessie_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:31.296: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:31.312: INFO: Unable to read jessie_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:31.329: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:31.450: INFO: Lookups using dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5546 wheezy_tcp@dns-test-service.dns-5546 wheezy_udp@dns-test-service.dns-5546.svc wheezy_tcp@dns-test-service.dns-5546.svc wheezy_udp@_http._tcp.dns-test-service.dns-5546.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5546 jessie_tcp@dns-test-service.dns-5546 jessie_udp@dns-test-service.dns-5546.svc jessie_tcp@dns-test-service.dns-5546.svc]

    Jan 28 01:33:35.983: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:36.005: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:36.022: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:36.039: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:36.054: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:36.070: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:36.184: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:36.200: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:36.217: INFO: Unable to read jessie_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:36.233: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:36.248: INFO: Unable to read jessie_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:36.264: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:36.379: INFO: Lookups using dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5546 wheezy_tcp@dns-test-service.dns-5546 wheezy_udp@dns-test-service.dns-5546.svc wheezy_tcp@dns-test-service.dns-5546.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5546 jessie_tcp@dns-test-service.dns-5546 jessie_udp@dns-test-service.dns-5546.svc jessie_tcp@dns-test-service.dns-5546.svc]

    Jan 28 01:33:40.981: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:40.999: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:41.017: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:41.036: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:41.053: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:41.072: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:41.194: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:41.210: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:41.227: INFO: Unable to read jessie_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:41.243: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:41.260: INFO: Unable to read jessie_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:41.277: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:41.381: INFO: Lookups using dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5546 wheezy_tcp@dns-test-service.dns-5546 wheezy_udp@dns-test-service.dns-5546.svc wheezy_tcp@dns-test-service.dns-5546.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5546 jessie_tcp@dns-test-service.dns-5546 jessie_udp@dns-test-service.dns-5546.svc jessie_tcp@dns-test-service.dns-5546.svc]

    Jan 28 01:33:45.984: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:46.002: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:46.019: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:46.064: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:46.082: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:46.103: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:46.230: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:46.252: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:46.269: INFO: Unable to read jessie_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:46.285: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:46.300: INFO: Unable to read jessie_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:46.318: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:46.418: INFO: Lookups using dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5546 wheezy_tcp@dns-test-service.dns-5546 wheezy_udp@dns-test-service.dns-5546.svc wheezy_tcp@dns-test-service.dns-5546.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5546 jessie_tcp@dns-test-service.dns-5546 jessie_udp@dns-test-service.dns-5546.svc jessie_tcp@dns-test-service.dns-5546.svc]

    Jan 28 01:33:50.982: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:50.999: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:51.016: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:51.036: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:51.054: INFO: Unable to read wheezy_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:51.072: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:51.216: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:51.236: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:51.253: INFO: Unable to read jessie_udp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:51.270: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546 from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:51.313: INFO: Unable to read jessie_udp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:51.331: INFO: Unable to read jessie_tcp@dns-test-service.dns-5546.svc from pod dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0: the server could not find the requested resource (get pods dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0)
    Jan 28 01:33:51.434: INFO: Lookups using dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5546 wheezy_tcp@dns-test-service.dns-5546 wheezy_udp@dns-test-service.dns-5546.svc wheezy_tcp@dns-test-service.dns-5546.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5546 jessie_tcp@dns-test-service.dns-5546 jessie_udp@dns-test-service.dns-5546.svc jessie_tcp@dns-test-service.dns-5546.svc]

    Jan 28 01:33:56.427: INFO: DNS probes using dns-5546/dns-test-a8a40ceb-2c19-41f9-949a-742fcf85d3e0 succeeded

    STEP: deleting the pod 01/28/23 01:33:56.427
    STEP: deleting the test service 01/28/23 01:33:56.469
    STEP: deleting the test headless service 01/28/23 01:33:56.514
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 28 01:33:56.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5546" for this suite. 01/28/23 01:33:56.584
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:33:56.606
Jan 28 01:33:56.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 01:33:56.608
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:33:56.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:33:56.663
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-663736fe-5c20-42fa-8fb0-a9a93f76a327 01/28/23 01:33:56.678
STEP: Creating secret with name secret-projected-all-test-volume-7c23ac10-dc8f-426b-96cd-a83b2d08af41 01/28/23 01:33:56.691
STEP: Creating a pod to test Check all projections for projected volume plugin 01/28/23 01:33:56.706
Jan 28 01:33:56.727: INFO: Waiting up to 5m0s for pod "projected-volume-43519460-58fa-4d4a-ba02-65787238c774" in namespace "projected-1417" to be "Succeeded or Failed"
Jan 28 01:33:56.739: INFO: Pod "projected-volume-43519460-58fa-4d4a-ba02-65787238c774": Phase="Pending", Reason="", readiness=false. Elapsed: 11.677714ms
Jan 28 01:33:58.753: INFO: Pod "projected-volume-43519460-58fa-4d4a-ba02-65787238c774": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025178303s
Jan 28 01:34:00.752: INFO: Pod "projected-volume-43519460-58fa-4d4a-ba02-65787238c774": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024402942s
Jan 28 01:34:02.753: INFO: Pod "projected-volume-43519460-58fa-4d4a-ba02-65787238c774": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025852902s
STEP: Saw pod success 01/28/23 01:34:02.753
Jan 28 01:34:02.754: INFO: Pod "projected-volume-43519460-58fa-4d4a-ba02-65787238c774" satisfied condition "Succeeded or Failed"
Jan 28 01:34:02.767: INFO: Trying to get logs from node 10.9.20.126 pod projected-volume-43519460-58fa-4d4a-ba02-65787238c774 container projected-all-volume-test: <nil>
STEP: delete the pod 01/28/23 01:34:02.804
Jan 28 01:34:02.844: INFO: Waiting for pod projected-volume-43519460-58fa-4d4a-ba02-65787238c774 to disappear
Jan 28 01:34:02.872: INFO: Pod projected-volume-43519460-58fa-4d4a-ba02-65787238c774 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Jan 28 01:34:02.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1417" for this suite. 01/28/23 01:34:02.899
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":263,"skipped":4787,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.314 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:33:56.606
    Jan 28 01:33:56.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 01:33:56.608
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:33:56.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:33:56.663
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-663736fe-5c20-42fa-8fb0-a9a93f76a327 01/28/23 01:33:56.678
    STEP: Creating secret with name secret-projected-all-test-volume-7c23ac10-dc8f-426b-96cd-a83b2d08af41 01/28/23 01:33:56.691
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/28/23 01:33:56.706
    Jan 28 01:33:56.727: INFO: Waiting up to 5m0s for pod "projected-volume-43519460-58fa-4d4a-ba02-65787238c774" in namespace "projected-1417" to be "Succeeded or Failed"
    Jan 28 01:33:56.739: INFO: Pod "projected-volume-43519460-58fa-4d4a-ba02-65787238c774": Phase="Pending", Reason="", readiness=false. Elapsed: 11.677714ms
    Jan 28 01:33:58.753: INFO: Pod "projected-volume-43519460-58fa-4d4a-ba02-65787238c774": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025178303s
    Jan 28 01:34:00.752: INFO: Pod "projected-volume-43519460-58fa-4d4a-ba02-65787238c774": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024402942s
    Jan 28 01:34:02.753: INFO: Pod "projected-volume-43519460-58fa-4d4a-ba02-65787238c774": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025852902s
    STEP: Saw pod success 01/28/23 01:34:02.753
    Jan 28 01:34:02.754: INFO: Pod "projected-volume-43519460-58fa-4d4a-ba02-65787238c774" satisfied condition "Succeeded or Failed"
    Jan 28 01:34:02.767: INFO: Trying to get logs from node 10.9.20.126 pod projected-volume-43519460-58fa-4d4a-ba02-65787238c774 container projected-all-volume-test: <nil>
    STEP: delete the pod 01/28/23 01:34:02.804
    Jan 28 01:34:02.844: INFO: Waiting for pod projected-volume-43519460-58fa-4d4a-ba02-65787238c774 to disappear
    Jan 28 01:34:02.872: INFO: Pod projected-volume-43519460-58fa-4d4a-ba02-65787238c774 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Jan 28 01:34:02.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1417" for this suite. 01/28/23 01:34:02.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:34:02.925
Jan 28 01:34:02.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename namespaces 01/28/23 01:34:02.927
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:02.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:02.988
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 01/28/23 01:34:03
Jan 28 01:34:03.013: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/28/23 01:34:03.013
Jan 28 01:34:03.029: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/28/23 01:34:03.029
Jan 28 01:34:03.060: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 28 01:34:03.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4383" for this suite. 01/28/23 01:34:03.093
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":264,"skipped":4806,"failed":0}
------------------------------
â€¢ [0.187 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:34:02.925
    Jan 28 01:34:02.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename namespaces 01/28/23 01:34:02.927
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:02.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:02.988
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 01/28/23 01:34:03
    Jan 28 01:34:03.013: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/28/23 01:34:03.013
    Jan 28 01:34:03.029: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/28/23 01:34:03.029
    Jan 28 01:34:03.060: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 01:34:03.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-4383" for this suite. 01/28/23 01:34:03.093
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:34:03.112
Jan 28 01:34:03.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 01:34:03.114
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:03.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:03.159
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 01/28/23 01:34:03.172
Jan 28 01:34:03.195: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54ba97ea-7d18-4e1d-8e73-011617f92931" in namespace "projected-2862" to be "Succeeded or Failed"
Jan 28 01:34:03.212: INFO: Pod "downwardapi-volume-54ba97ea-7d18-4e1d-8e73-011617f92931": Phase="Pending", Reason="", readiness=false. Elapsed: 16.921229ms
Jan 28 01:34:05.224: INFO: Pod "downwardapi-volume-54ba97ea-7d18-4e1d-8e73-011617f92931": Phase="Running", Reason="", readiness=true. Elapsed: 2.028421039s
Jan 28 01:34:07.228: INFO: Pod "downwardapi-volume-54ba97ea-7d18-4e1d-8e73-011617f92931": Phase="Running", Reason="", readiness=false. Elapsed: 4.032025206s
Jan 28 01:34:09.226: INFO: Pod "downwardapi-volume-54ba97ea-7d18-4e1d-8e73-011617f92931": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030288491s
STEP: Saw pod success 01/28/23 01:34:09.226
Jan 28 01:34:09.227: INFO: Pod "downwardapi-volume-54ba97ea-7d18-4e1d-8e73-011617f92931" satisfied condition "Succeeded or Failed"
Jan 28 01:34:09.239: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-54ba97ea-7d18-4e1d-8e73-011617f92931 container client-container: <nil>
STEP: delete the pod 01/28/23 01:34:09.269
Jan 28 01:34:09.304: INFO: Waiting for pod downwardapi-volume-54ba97ea-7d18-4e1d-8e73-011617f92931 to disappear
Jan 28 01:34:09.315: INFO: Pod downwardapi-volume-54ba97ea-7d18-4e1d-8e73-011617f92931 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 28 01:34:09.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2862" for this suite. 01/28/23 01:34:09.338
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":265,"skipped":4806,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.247 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:34:03.112
    Jan 28 01:34:03.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 01:34:03.114
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:03.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:03.159
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 01/28/23 01:34:03.172
    Jan 28 01:34:03.195: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54ba97ea-7d18-4e1d-8e73-011617f92931" in namespace "projected-2862" to be "Succeeded or Failed"
    Jan 28 01:34:03.212: INFO: Pod "downwardapi-volume-54ba97ea-7d18-4e1d-8e73-011617f92931": Phase="Pending", Reason="", readiness=false. Elapsed: 16.921229ms
    Jan 28 01:34:05.224: INFO: Pod "downwardapi-volume-54ba97ea-7d18-4e1d-8e73-011617f92931": Phase="Running", Reason="", readiness=true. Elapsed: 2.028421039s
    Jan 28 01:34:07.228: INFO: Pod "downwardapi-volume-54ba97ea-7d18-4e1d-8e73-011617f92931": Phase="Running", Reason="", readiness=false. Elapsed: 4.032025206s
    Jan 28 01:34:09.226: INFO: Pod "downwardapi-volume-54ba97ea-7d18-4e1d-8e73-011617f92931": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030288491s
    STEP: Saw pod success 01/28/23 01:34:09.226
    Jan 28 01:34:09.227: INFO: Pod "downwardapi-volume-54ba97ea-7d18-4e1d-8e73-011617f92931" satisfied condition "Succeeded or Failed"
    Jan 28 01:34:09.239: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-54ba97ea-7d18-4e1d-8e73-011617f92931 container client-container: <nil>
    STEP: delete the pod 01/28/23 01:34:09.269
    Jan 28 01:34:09.304: INFO: Waiting for pod downwardapi-volume-54ba97ea-7d18-4e1d-8e73-011617f92931 to disappear
    Jan 28 01:34:09.315: INFO: Pod downwardapi-volume-54ba97ea-7d18-4e1d-8e73-011617f92931 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 28 01:34:09.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2862" for this suite. 01/28/23 01:34:09.338
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:34:09.362
Jan 28 01:34:09.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubelet-test 01/28/23 01:34:09.364
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:09.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:09.415
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan 28 01:34:09.449: INFO: Waiting up to 5m0s for pod "busybox-scheduling-69074301-a83f-4ddc-a0d1-cbf528386622" in namespace "kubelet-test-5894" to be "running and ready"
Jan 28 01:34:09.460: INFO: Pod "busybox-scheduling-69074301-a83f-4ddc-a0d1-cbf528386622": Phase="Pending", Reason="", readiness=false. Elapsed: 11.325506ms
Jan 28 01:34:09.461: INFO: The phase of Pod busybox-scheduling-69074301-a83f-4ddc-a0d1-cbf528386622 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:34:11.485: INFO: Pod "busybox-scheduling-69074301-a83f-4ddc-a0d1-cbf528386622": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036353711s
Jan 28 01:34:11.486: INFO: The phase of Pod busybox-scheduling-69074301-a83f-4ddc-a0d1-cbf528386622 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:34:13.474: INFO: Pod "busybox-scheduling-69074301-a83f-4ddc-a0d1-cbf528386622": Phase="Running", Reason="", readiness=true. Elapsed: 4.025126869s
Jan 28 01:34:13.474: INFO: The phase of Pod busybox-scheduling-69074301-a83f-4ddc-a0d1-cbf528386622 is Running (Ready = true)
Jan 28 01:34:13.474: INFO: Pod "busybox-scheduling-69074301-a83f-4ddc-a0d1-cbf528386622" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 28 01:34:13.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5894" for this suite. 01/28/23 01:34:13.53
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":266,"skipped":4808,"failed":0}
------------------------------
â€¢ [4.186 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:34:09.362
    Jan 28 01:34:09.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubelet-test 01/28/23 01:34:09.364
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:09.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:09.415
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan 28 01:34:09.449: INFO: Waiting up to 5m0s for pod "busybox-scheduling-69074301-a83f-4ddc-a0d1-cbf528386622" in namespace "kubelet-test-5894" to be "running and ready"
    Jan 28 01:34:09.460: INFO: Pod "busybox-scheduling-69074301-a83f-4ddc-a0d1-cbf528386622": Phase="Pending", Reason="", readiness=false. Elapsed: 11.325506ms
    Jan 28 01:34:09.461: INFO: The phase of Pod busybox-scheduling-69074301-a83f-4ddc-a0d1-cbf528386622 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:34:11.485: INFO: Pod "busybox-scheduling-69074301-a83f-4ddc-a0d1-cbf528386622": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036353711s
    Jan 28 01:34:11.486: INFO: The phase of Pod busybox-scheduling-69074301-a83f-4ddc-a0d1-cbf528386622 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:34:13.474: INFO: Pod "busybox-scheduling-69074301-a83f-4ddc-a0d1-cbf528386622": Phase="Running", Reason="", readiness=true. Elapsed: 4.025126869s
    Jan 28 01:34:13.474: INFO: The phase of Pod busybox-scheduling-69074301-a83f-4ddc-a0d1-cbf528386622 is Running (Ready = true)
    Jan 28 01:34:13.474: INFO: Pod "busybox-scheduling-69074301-a83f-4ddc-a0d1-cbf528386622" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 28 01:34:13.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5894" for this suite. 01/28/23 01:34:13.53
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:34:13.554
Jan 28 01:34:13.555: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename security-context-test 01/28/23 01:34:13.557
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:13.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:13.624
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Jan 28 01:34:13.686: INFO: Waiting up to 5m0s for pod "busybox-user-65534-d5527455-0218-4819-8dab-a5ec082f0f40" in namespace "security-context-test-4362" to be "Succeeded or Failed"
Jan 28 01:34:13.700: INFO: Pod "busybox-user-65534-d5527455-0218-4819-8dab-a5ec082f0f40": Phase="Pending", Reason="", readiness=false. Elapsed: 13.801526ms
Jan 28 01:34:15.713: INFO: Pod "busybox-user-65534-d5527455-0218-4819-8dab-a5ec082f0f40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026861882s
Jan 28 01:34:17.713: INFO: Pod "busybox-user-65534-d5527455-0218-4819-8dab-a5ec082f0f40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026422917s
Jan 28 01:34:19.712: INFO: Pod "busybox-user-65534-d5527455-0218-4819-8dab-a5ec082f0f40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026111279s
Jan 28 01:34:19.712: INFO: Pod "busybox-user-65534-d5527455-0218-4819-8dab-a5ec082f0f40" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 28 01:34:19.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4362" for this suite. 01/28/23 01:34:19.737
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":267,"skipped":4829,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.201 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:34:13.554
    Jan 28 01:34:13.555: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename security-context-test 01/28/23 01:34:13.557
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:13.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:13.624
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Jan 28 01:34:13.686: INFO: Waiting up to 5m0s for pod "busybox-user-65534-d5527455-0218-4819-8dab-a5ec082f0f40" in namespace "security-context-test-4362" to be "Succeeded or Failed"
    Jan 28 01:34:13.700: INFO: Pod "busybox-user-65534-d5527455-0218-4819-8dab-a5ec082f0f40": Phase="Pending", Reason="", readiness=false. Elapsed: 13.801526ms
    Jan 28 01:34:15.713: INFO: Pod "busybox-user-65534-d5527455-0218-4819-8dab-a5ec082f0f40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026861882s
    Jan 28 01:34:17.713: INFO: Pod "busybox-user-65534-d5527455-0218-4819-8dab-a5ec082f0f40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026422917s
    Jan 28 01:34:19.712: INFO: Pod "busybox-user-65534-d5527455-0218-4819-8dab-a5ec082f0f40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026111279s
    Jan 28 01:34:19.712: INFO: Pod "busybox-user-65534-d5527455-0218-4819-8dab-a5ec082f0f40" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 28 01:34:19.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-4362" for this suite. 01/28/23 01:34:19.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:34:19.767
Jan 28 01:34:19.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename pods 01/28/23 01:34:19.769
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:19.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:19.818
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Jan 28 01:34:19.857: INFO: Waiting up to 5m0s for pod "server-envvars-deb1cc22-6547-4ff1-ab36-69952febce75" in namespace "pods-9911" to be "running and ready"
Jan 28 01:34:19.868: INFO: Pod "server-envvars-deb1cc22-6547-4ff1-ab36-69952febce75": Phase="Pending", Reason="", readiness=false. Elapsed: 11.614292ms
Jan 28 01:34:19.868: INFO: The phase of Pod server-envvars-deb1cc22-6547-4ff1-ab36-69952febce75 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:34:21.881: INFO: Pod "server-envvars-deb1cc22-6547-4ff1-ab36-69952febce75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02448543s
Jan 28 01:34:21.881: INFO: The phase of Pod server-envvars-deb1cc22-6547-4ff1-ab36-69952febce75 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:34:23.881: INFO: Pod "server-envvars-deb1cc22-6547-4ff1-ab36-69952febce75": Phase="Running", Reason="", readiness=true. Elapsed: 4.024559509s
Jan 28 01:34:23.881: INFO: The phase of Pod server-envvars-deb1cc22-6547-4ff1-ab36-69952febce75 is Running (Ready = true)
Jan 28 01:34:23.881: INFO: Pod "server-envvars-deb1cc22-6547-4ff1-ab36-69952febce75" satisfied condition "running and ready"
Jan 28 01:34:23.950: INFO: Waiting up to 5m0s for pod "client-envvars-770b62c4-1b84-40a3-b71d-579f1cadbd82" in namespace "pods-9911" to be "Succeeded or Failed"
Jan 28 01:34:23.963: INFO: Pod "client-envvars-770b62c4-1b84-40a3-b71d-579f1cadbd82": Phase="Pending", Reason="", readiness=false. Elapsed: 12.877425ms
Jan 28 01:34:25.975: INFO: Pod "client-envvars-770b62c4-1b84-40a3-b71d-579f1cadbd82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024989467s
Jan 28 01:34:27.975: INFO: Pod "client-envvars-770b62c4-1b84-40a3-b71d-579f1cadbd82": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025190899s
Jan 28 01:34:29.975: INFO: Pod "client-envvars-770b62c4-1b84-40a3-b71d-579f1cadbd82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024822132s
STEP: Saw pod success 01/28/23 01:34:29.975
Jan 28 01:34:29.975: INFO: Pod "client-envvars-770b62c4-1b84-40a3-b71d-579f1cadbd82" satisfied condition "Succeeded or Failed"
Jan 28 01:34:29.989: INFO: Trying to get logs from node 10.9.20.72 pod client-envvars-770b62c4-1b84-40a3-b71d-579f1cadbd82 container env3cont: <nil>
STEP: delete the pod 01/28/23 01:34:30.101
Jan 28 01:34:30.128: INFO: Waiting for pod client-envvars-770b62c4-1b84-40a3-b71d-579f1cadbd82 to disappear
Jan 28 01:34:30.156: INFO: Pod client-envvars-770b62c4-1b84-40a3-b71d-579f1cadbd82 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 28 01:34:30.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9911" for this suite. 01/28/23 01:34:30.178
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":268,"skipped":4913,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.430 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:34:19.767
    Jan 28 01:34:19.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename pods 01/28/23 01:34:19.769
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:19.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:19.818
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Jan 28 01:34:19.857: INFO: Waiting up to 5m0s for pod "server-envvars-deb1cc22-6547-4ff1-ab36-69952febce75" in namespace "pods-9911" to be "running and ready"
    Jan 28 01:34:19.868: INFO: Pod "server-envvars-deb1cc22-6547-4ff1-ab36-69952febce75": Phase="Pending", Reason="", readiness=false. Elapsed: 11.614292ms
    Jan 28 01:34:19.868: INFO: The phase of Pod server-envvars-deb1cc22-6547-4ff1-ab36-69952febce75 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:34:21.881: INFO: Pod "server-envvars-deb1cc22-6547-4ff1-ab36-69952febce75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02448543s
    Jan 28 01:34:21.881: INFO: The phase of Pod server-envvars-deb1cc22-6547-4ff1-ab36-69952febce75 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:34:23.881: INFO: Pod "server-envvars-deb1cc22-6547-4ff1-ab36-69952febce75": Phase="Running", Reason="", readiness=true. Elapsed: 4.024559509s
    Jan 28 01:34:23.881: INFO: The phase of Pod server-envvars-deb1cc22-6547-4ff1-ab36-69952febce75 is Running (Ready = true)
    Jan 28 01:34:23.881: INFO: Pod "server-envvars-deb1cc22-6547-4ff1-ab36-69952febce75" satisfied condition "running and ready"
    Jan 28 01:34:23.950: INFO: Waiting up to 5m0s for pod "client-envvars-770b62c4-1b84-40a3-b71d-579f1cadbd82" in namespace "pods-9911" to be "Succeeded or Failed"
    Jan 28 01:34:23.963: INFO: Pod "client-envvars-770b62c4-1b84-40a3-b71d-579f1cadbd82": Phase="Pending", Reason="", readiness=false. Elapsed: 12.877425ms
    Jan 28 01:34:25.975: INFO: Pod "client-envvars-770b62c4-1b84-40a3-b71d-579f1cadbd82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024989467s
    Jan 28 01:34:27.975: INFO: Pod "client-envvars-770b62c4-1b84-40a3-b71d-579f1cadbd82": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025190899s
    Jan 28 01:34:29.975: INFO: Pod "client-envvars-770b62c4-1b84-40a3-b71d-579f1cadbd82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024822132s
    STEP: Saw pod success 01/28/23 01:34:29.975
    Jan 28 01:34:29.975: INFO: Pod "client-envvars-770b62c4-1b84-40a3-b71d-579f1cadbd82" satisfied condition "Succeeded or Failed"
    Jan 28 01:34:29.989: INFO: Trying to get logs from node 10.9.20.72 pod client-envvars-770b62c4-1b84-40a3-b71d-579f1cadbd82 container env3cont: <nil>
    STEP: delete the pod 01/28/23 01:34:30.101
    Jan 28 01:34:30.128: INFO: Waiting for pod client-envvars-770b62c4-1b84-40a3-b71d-579f1cadbd82 to disappear
    Jan 28 01:34:30.156: INFO: Pod client-envvars-770b62c4-1b84-40a3-b71d-579f1cadbd82 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 28 01:34:30.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9911" for this suite. 01/28/23 01:34:30.178
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:34:30.199
Jan 28 01:34:30.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename subpath 01/28/23 01:34:30.202
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:30.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:30.264
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/28/23 01:34:30.274
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-v6cq 01/28/23 01:34:30.302
STEP: Creating a pod to test atomic-volume-subpath 01/28/23 01:34:30.303
Jan 28 01:34:30.324: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-v6cq" in namespace "subpath-1299" to be "Succeeded or Failed"
Jan 28 01:34:30.337: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Pending", Reason="", readiness=false. Elapsed: 13.601051ms
Jan 28 01:34:32.350: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026054504s
Jan 28 01:34:34.352: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 4.027997626s
Jan 28 01:34:36.352: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 6.028350145s
Jan 28 01:34:38.354: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 8.029896857s
Jan 28 01:34:40.351: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 10.027598552s
Jan 28 01:34:42.351: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 12.027575639s
Jan 28 01:34:44.351: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 14.027697148s
Jan 28 01:34:46.352: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 16.028000043s
Jan 28 01:34:48.354: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 18.029979123s
Jan 28 01:34:50.350: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 20.026359317s
Jan 28 01:34:52.352: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 22.028473217s
Jan 28 01:34:54.352: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=false. Elapsed: 24.028319788s
Jan 28 01:34:56.351: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.026912173s
STEP: Saw pod success 01/28/23 01:34:56.351
Jan 28 01:34:56.351: INFO: Pod "pod-subpath-test-projected-v6cq" satisfied condition "Succeeded or Failed"
Jan 28 01:34:56.363: INFO: Trying to get logs from node 10.9.20.72 pod pod-subpath-test-projected-v6cq container test-container-subpath-projected-v6cq: <nil>
STEP: delete the pod 01/28/23 01:34:56.397
Jan 28 01:34:56.454: INFO: Waiting for pod pod-subpath-test-projected-v6cq to disappear
Jan 28 01:34:56.465: INFO: Pod pod-subpath-test-projected-v6cq no longer exists
STEP: Deleting pod pod-subpath-test-projected-v6cq 01/28/23 01:34:56.465
Jan 28 01:34:56.465: INFO: Deleting pod "pod-subpath-test-projected-v6cq" in namespace "subpath-1299"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 28 01:34:56.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1299" for this suite. 01/28/23 01:34:56.5
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":269,"skipped":4915,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.320 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:34:30.199
    Jan 28 01:34:30.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename subpath 01/28/23 01:34:30.202
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:30.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:30.264
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/28/23 01:34:30.274
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-v6cq 01/28/23 01:34:30.302
    STEP: Creating a pod to test atomic-volume-subpath 01/28/23 01:34:30.303
    Jan 28 01:34:30.324: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-v6cq" in namespace "subpath-1299" to be "Succeeded or Failed"
    Jan 28 01:34:30.337: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Pending", Reason="", readiness=false. Elapsed: 13.601051ms
    Jan 28 01:34:32.350: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026054504s
    Jan 28 01:34:34.352: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 4.027997626s
    Jan 28 01:34:36.352: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 6.028350145s
    Jan 28 01:34:38.354: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 8.029896857s
    Jan 28 01:34:40.351: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 10.027598552s
    Jan 28 01:34:42.351: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 12.027575639s
    Jan 28 01:34:44.351: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 14.027697148s
    Jan 28 01:34:46.352: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 16.028000043s
    Jan 28 01:34:48.354: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 18.029979123s
    Jan 28 01:34:50.350: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 20.026359317s
    Jan 28 01:34:52.352: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=true. Elapsed: 22.028473217s
    Jan 28 01:34:54.352: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Running", Reason="", readiness=false. Elapsed: 24.028319788s
    Jan 28 01:34:56.351: INFO: Pod "pod-subpath-test-projected-v6cq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.026912173s
    STEP: Saw pod success 01/28/23 01:34:56.351
    Jan 28 01:34:56.351: INFO: Pod "pod-subpath-test-projected-v6cq" satisfied condition "Succeeded or Failed"
    Jan 28 01:34:56.363: INFO: Trying to get logs from node 10.9.20.72 pod pod-subpath-test-projected-v6cq container test-container-subpath-projected-v6cq: <nil>
    STEP: delete the pod 01/28/23 01:34:56.397
    Jan 28 01:34:56.454: INFO: Waiting for pod pod-subpath-test-projected-v6cq to disappear
    Jan 28 01:34:56.465: INFO: Pod pod-subpath-test-projected-v6cq no longer exists
    STEP: Deleting pod pod-subpath-test-projected-v6cq 01/28/23 01:34:56.465
    Jan 28 01:34:56.465: INFO: Deleting pod "pod-subpath-test-projected-v6cq" in namespace "subpath-1299"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 28 01:34:56.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-1299" for this suite. 01/28/23 01:34:56.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:34:56.532
Jan 28 01:34:56.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename runtimeclass 01/28/23 01:34:56.533
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:56.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:56.613
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan 28 01:34:56.663: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9430 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 28 01:34:56.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9430" for this suite. 01/28/23 01:34:56.722
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":270,"skipped":4968,"failed":0}
------------------------------
â€¢ [0.211 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:34:56.532
    Jan 28 01:34:56.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename runtimeclass 01/28/23 01:34:56.533
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:56.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:56.613
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan 28 01:34:56.663: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9430 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 28 01:34:56.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9430" for this suite. 01/28/23 01:34:56.722
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:34:56.744
Jan 28 01:34:56.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename events 01/28/23 01:34:56.746
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:56.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:56.802
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/28/23 01:34:56.814
STEP: listing all events in all namespaces 01/28/23 01:34:56.828
STEP: patching the test event 01/28/23 01:34:56.853
STEP: fetching the test event 01/28/23 01:34:56.874
STEP: updating the test event 01/28/23 01:34:56.887
STEP: getting the test event 01/28/23 01:34:56.921
STEP: deleting the test event 01/28/23 01:34:56.936
STEP: listing all events in all namespaces 01/28/23 01:34:56.964
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan 28 01:34:56.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-991" for this suite. 01/28/23 01:34:57.004
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":271,"skipped":4971,"failed":0}
------------------------------
â€¢ [0.281 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:34:56.744
    Jan 28 01:34:56.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename events 01/28/23 01:34:56.746
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:56.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:56.802
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/28/23 01:34:56.814
    STEP: listing all events in all namespaces 01/28/23 01:34:56.828
    STEP: patching the test event 01/28/23 01:34:56.853
    STEP: fetching the test event 01/28/23 01:34:56.874
    STEP: updating the test event 01/28/23 01:34:56.887
    STEP: getting the test event 01/28/23 01:34:56.921
    STEP: deleting the test event 01/28/23 01:34:56.936
    STEP: listing all events in all namespaces 01/28/23 01:34:56.964
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan 28 01:34:56.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-991" for this suite. 01/28/23 01:34:57.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:34:57.035
Jan 28 01:34:57.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename watch 01/28/23 01:34:57.038
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:57.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:57.119
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/28/23 01:34:57.132
STEP: modifying the configmap once 01/28/23 01:34:57.147
STEP: modifying the configmap a second time 01/28/23 01:34:57.178
STEP: deleting the configmap 01/28/23 01:34:57.204
STEP: creating a watch on configmaps from the resource version returned by the first update 01/28/23 01:34:57.223
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/28/23 01:34:57.23
Jan 28 01:34:57.230: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6332  1da3b20b-ccb9-4fe5-938c-0b4eaafa67e7 41039 0 2023-01-28 01:34:57 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-28 01:34:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 01:34:57.231: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6332  1da3b20b-ccb9-4fe5-938c-0b4eaafa67e7 41040 0 2023-01-28 01:34:57 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-28 01:34:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 28 01:34:57.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6332" for this suite. 01/28/23 01:34:57.265
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":272,"skipped":5007,"failed":0}
------------------------------
â€¢ [0.250 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:34:57.035
    Jan 28 01:34:57.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename watch 01/28/23 01:34:57.038
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:57.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:57.119
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/28/23 01:34:57.132
    STEP: modifying the configmap once 01/28/23 01:34:57.147
    STEP: modifying the configmap a second time 01/28/23 01:34:57.178
    STEP: deleting the configmap 01/28/23 01:34:57.204
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/28/23 01:34:57.223
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/28/23 01:34:57.23
    Jan 28 01:34:57.230: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6332  1da3b20b-ccb9-4fe5-938c-0b4eaafa67e7 41039 0 2023-01-28 01:34:57 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-28 01:34:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 28 01:34:57.231: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6332  1da3b20b-ccb9-4fe5-938c-0b4eaafa67e7 41040 0 2023-01-28 01:34:57 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-28 01:34:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 28 01:34:57.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6332" for this suite. 01/28/23 01:34:57.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:34:57.288
Jan 28 01:34:57.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename services 01/28/23 01:34:57.29
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:57.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:57.345
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-1111 01/28/23 01:34:57.359
STEP: creating replication controller nodeport-test in namespace services-1111 01/28/23 01:34:57.407
I0128 01:34:57.423962      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1111, replica count: 2
I0128 01:35:00.478538      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 01:35:00.479: INFO: Creating new exec pod
Jan 28 01:35:00.501: INFO: Waiting up to 5m0s for pod "execpodmtwch" in namespace "services-1111" to be "running"
Jan 28 01:35:00.513: INFO: Pod "execpodmtwch": Phase="Pending", Reason="", readiness=false. Elapsed: 12.682852ms
Jan 28 01:35:02.529: INFO: Pod "execpodmtwch": Phase="Running", Reason="", readiness=true. Elapsed: 2.027911598s
Jan 28 01:35:02.529: INFO: Pod "execpodmtwch" satisfied condition "running"
Jan 28 01:35:03.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-1111 exec execpodmtwch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 28 01:35:03.900: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 28 01:35:03.900: INFO: stdout: "nodeport-test-55n64"
Jan 28 01:35:03.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-1111 exec execpodmtwch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.220.127 80'
Jan 28 01:35:04.243: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.220.127 80\nConnection to 172.21.220.127 80 port [tcp/http] succeeded!\n"
Jan 28 01:35:04.243: INFO: stdout: "nodeport-test-ph4hj"
Jan 28 01:35:04.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-1111 exec execpodmtwch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.75 31180'
Jan 28 01:35:04.566: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.9.20.75 31180\nConnection to 10.9.20.75 31180 port [tcp/*] succeeded!\n"
Jan 28 01:35:04.566: INFO: stdout: "nodeport-test-55n64"
Jan 28 01:35:04.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-1111 exec execpodmtwch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.72 31180'
Jan 28 01:35:04.872: INFO: stderr: "+ echo+  hostNamenc\n -v -t -w 2 10.9.20.72 31180\nConnection to 10.9.20.72 31180 port [tcp/*] succeeded!\n"
Jan 28 01:35:04.872: INFO: stdout: ""
Jan 28 01:35:05.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-1111 exec execpodmtwch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.72 31180'
Jan 28 01:35:06.178: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.9.20.72 31180\nConnection to 10.9.20.72 31180 port [tcp/*] succeeded!\n"
Jan 28 01:35:06.178: INFO: stdout: "nodeport-test-ph4hj"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 28 01:35:06.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1111" for this suite. 01/28/23 01:35:06.197
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":273,"skipped":5021,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.926 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:34:57.288
    Jan 28 01:34:57.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename services 01/28/23 01:34:57.29
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:34:57.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:34:57.345
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-1111 01/28/23 01:34:57.359
    STEP: creating replication controller nodeport-test in namespace services-1111 01/28/23 01:34:57.407
    I0128 01:34:57.423962      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1111, replica count: 2
    I0128 01:35:00.478538      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 28 01:35:00.479: INFO: Creating new exec pod
    Jan 28 01:35:00.501: INFO: Waiting up to 5m0s for pod "execpodmtwch" in namespace "services-1111" to be "running"
    Jan 28 01:35:00.513: INFO: Pod "execpodmtwch": Phase="Pending", Reason="", readiness=false. Elapsed: 12.682852ms
    Jan 28 01:35:02.529: INFO: Pod "execpodmtwch": Phase="Running", Reason="", readiness=true. Elapsed: 2.027911598s
    Jan 28 01:35:02.529: INFO: Pod "execpodmtwch" satisfied condition "running"
    Jan 28 01:35:03.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-1111 exec execpodmtwch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 28 01:35:03.900: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 28 01:35:03.900: INFO: stdout: "nodeport-test-55n64"
    Jan 28 01:35:03.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-1111 exec execpodmtwch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.220.127 80'
    Jan 28 01:35:04.243: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.220.127 80\nConnection to 172.21.220.127 80 port [tcp/http] succeeded!\n"
    Jan 28 01:35:04.243: INFO: stdout: "nodeport-test-ph4hj"
    Jan 28 01:35:04.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-1111 exec execpodmtwch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.75 31180'
    Jan 28 01:35:04.566: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.9.20.75 31180\nConnection to 10.9.20.75 31180 port [tcp/*] succeeded!\n"
    Jan 28 01:35:04.566: INFO: stdout: "nodeport-test-55n64"
    Jan 28 01:35:04.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-1111 exec execpodmtwch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.72 31180'
    Jan 28 01:35:04.872: INFO: stderr: "+ echo+  hostNamenc\n -v -t -w 2 10.9.20.72 31180\nConnection to 10.9.20.72 31180 port [tcp/*] succeeded!\n"
    Jan 28 01:35:04.872: INFO: stdout: ""
    Jan 28 01:35:05.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-1111 exec execpodmtwch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.72 31180'
    Jan 28 01:35:06.178: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.9.20.72 31180\nConnection to 10.9.20.72 31180 port [tcp/*] succeeded!\n"
    Jan 28 01:35:06.178: INFO: stdout: "nodeport-test-ph4hj"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 28 01:35:06.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1111" for this suite. 01/28/23 01:35:06.197
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:35:06.22
Jan 28 01:35:06.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename downward-api 01/28/23 01:35:06.222
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:35:06.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:35:06.272
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 01/28/23 01:35:06.284
Jan 28 01:35:06.333: INFO: Waiting up to 5m0s for pod "labelsupdatebb00ba17-97fb-4cec-a914-6a70ce9f6031" in namespace "downward-api-1452" to be "running and ready"
Jan 28 01:35:06.343: INFO: Pod "labelsupdatebb00ba17-97fb-4cec-a914-6a70ce9f6031": Phase="Pending", Reason="", readiness=false. Elapsed: 10.53454ms
Jan 28 01:35:06.343: INFO: The phase of Pod labelsupdatebb00ba17-97fb-4cec-a914-6a70ce9f6031 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:35:08.357: INFO: Pod "labelsupdatebb00ba17-97fb-4cec-a914-6a70ce9f6031": Phase="Running", Reason="", readiness=true. Elapsed: 2.024137638s
Jan 28 01:35:08.357: INFO: The phase of Pod labelsupdatebb00ba17-97fb-4cec-a914-6a70ce9f6031 is Running (Ready = true)
Jan 28 01:35:08.357: INFO: Pod "labelsupdatebb00ba17-97fb-4cec-a914-6a70ce9f6031" satisfied condition "running and ready"
Jan 28 01:35:08.924: INFO: Successfully updated pod "labelsupdatebb00ba17-97fb-4cec-a914-6a70ce9f6031"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 28 01:35:10.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1452" for this suite. 01/28/23 01:35:11.003
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":274,"skipped":5038,"failed":0}
------------------------------
â€¢ [4.802 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:35:06.22
    Jan 28 01:35:06.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename downward-api 01/28/23 01:35:06.222
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:35:06.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:35:06.272
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 01/28/23 01:35:06.284
    Jan 28 01:35:06.333: INFO: Waiting up to 5m0s for pod "labelsupdatebb00ba17-97fb-4cec-a914-6a70ce9f6031" in namespace "downward-api-1452" to be "running and ready"
    Jan 28 01:35:06.343: INFO: Pod "labelsupdatebb00ba17-97fb-4cec-a914-6a70ce9f6031": Phase="Pending", Reason="", readiness=false. Elapsed: 10.53454ms
    Jan 28 01:35:06.343: INFO: The phase of Pod labelsupdatebb00ba17-97fb-4cec-a914-6a70ce9f6031 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:35:08.357: INFO: Pod "labelsupdatebb00ba17-97fb-4cec-a914-6a70ce9f6031": Phase="Running", Reason="", readiness=true. Elapsed: 2.024137638s
    Jan 28 01:35:08.357: INFO: The phase of Pod labelsupdatebb00ba17-97fb-4cec-a914-6a70ce9f6031 is Running (Ready = true)
    Jan 28 01:35:08.357: INFO: Pod "labelsupdatebb00ba17-97fb-4cec-a914-6a70ce9f6031" satisfied condition "running and ready"
    Jan 28 01:35:08.924: INFO: Successfully updated pod "labelsupdatebb00ba17-97fb-4cec-a914-6a70ce9f6031"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 28 01:35:10.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1452" for this suite. 01/28/23 01:35:11.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:35:11.027
Jan 28 01:35:11.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename container-runtime 01/28/23 01:35:11.029
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:35:11.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:35:11.086
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 01/28/23 01:35:11.097
STEP: wait for the container to reach Failed 01/28/23 01:35:11.119
STEP: get the container status 01/28/23 01:35:16.197
STEP: the container should be terminated 01/28/23 01:35:16.208
STEP: the termination message should be set 01/28/23 01:35:16.209
Jan 28 01:35:16.209: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/28/23 01:35:16.209
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 28 01:35:16.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1827" for this suite. 01/28/23 01:35:16.273
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":275,"skipped":5073,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.264 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:35:11.027
    Jan 28 01:35:11.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename container-runtime 01/28/23 01:35:11.029
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:35:11.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:35:11.086
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 01/28/23 01:35:11.097
    STEP: wait for the container to reach Failed 01/28/23 01:35:11.119
    STEP: get the container status 01/28/23 01:35:16.197
    STEP: the container should be terminated 01/28/23 01:35:16.208
    STEP: the termination message should be set 01/28/23 01:35:16.209
    Jan 28 01:35:16.209: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/28/23 01:35:16.209
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 28 01:35:16.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1827" for this suite. 01/28/23 01:35:16.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:35:16.298
Jan 28 01:35:16.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename security-context-test 01/28/23 01:35:16.301
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:35:16.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:35:16.356
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Jan 28 01:35:16.389: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-a9527245-4b32-43e8-9f8a-3d8606cae7ee" in namespace "security-context-test-8413" to be "Succeeded or Failed"
Jan 28 01:35:16.401: INFO: Pod "busybox-readonly-false-a9527245-4b32-43e8-9f8a-3d8606cae7ee": Phase="Pending", Reason="", readiness=false. Elapsed: 11.791397ms
Jan 28 01:35:18.413: INFO: Pod "busybox-readonly-false-a9527245-4b32-43e8-9f8a-3d8606cae7ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023885982s
Jan 28 01:35:20.413: INFO: Pod "busybox-readonly-false-a9527245-4b32-43e8-9f8a-3d8606cae7ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023530346s
Jan 28 01:35:20.413: INFO: Pod "busybox-readonly-false-a9527245-4b32-43e8-9f8a-3d8606cae7ee" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 28 01:35:20.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8413" for this suite. 01/28/23 01:35:20.431
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":276,"skipped":5090,"failed":0}
------------------------------
â€¢ [4.153 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:35:16.298
    Jan 28 01:35:16.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename security-context-test 01/28/23 01:35:16.301
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:35:16.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:35:16.356
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Jan 28 01:35:16.389: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-a9527245-4b32-43e8-9f8a-3d8606cae7ee" in namespace "security-context-test-8413" to be "Succeeded or Failed"
    Jan 28 01:35:16.401: INFO: Pod "busybox-readonly-false-a9527245-4b32-43e8-9f8a-3d8606cae7ee": Phase="Pending", Reason="", readiness=false. Elapsed: 11.791397ms
    Jan 28 01:35:18.413: INFO: Pod "busybox-readonly-false-a9527245-4b32-43e8-9f8a-3d8606cae7ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023885982s
    Jan 28 01:35:20.413: INFO: Pod "busybox-readonly-false-a9527245-4b32-43e8-9f8a-3d8606cae7ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023530346s
    Jan 28 01:35:20.413: INFO: Pod "busybox-readonly-false-a9527245-4b32-43e8-9f8a-3d8606cae7ee" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 28 01:35:20.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-8413" for this suite. 01/28/23 01:35:20.431
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:35:20.458
Jan 28 01:35:20.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubectl 01/28/23 01:35:20.46
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:35:20.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:35:20.512
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/28/23 01:35:20.523
Jan 28 01:35:20.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-4367 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Jan 28 01:35:20.674: INFO: stderr: ""
Jan 28 01:35:20.674: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/28/23 01:35:20.674
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Jan 28 01:35:20.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-4367 delete pods e2e-test-httpd-pod'
Jan 28 01:35:23.298: INFO: stderr: ""
Jan 28 01:35:23.298: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 28 01:35:23.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4367" for this suite. 01/28/23 01:35:23.318
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":277,"skipped":5113,"failed":0}
------------------------------
â€¢ [2.884 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:35:20.458
    Jan 28 01:35:20.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubectl 01/28/23 01:35:20.46
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:35:20.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:35:20.512
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/28/23 01:35:20.523
    Jan 28 01:35:20.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-4367 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Jan 28 01:35:20.674: INFO: stderr: ""
    Jan 28 01:35:20.674: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/28/23 01:35:20.674
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Jan 28 01:35:20.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-4367 delete pods e2e-test-httpd-pod'
    Jan 28 01:35:23.298: INFO: stderr: ""
    Jan 28 01:35:23.298: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 28 01:35:23.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4367" for this suite. 01/28/23 01:35:23.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:35:23.349
Jan 28 01:35:23.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubectl 01/28/23 01:35:23.351
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:35:23.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:35:23.406
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/28/23 01:35:23.42
Jan 28 01:35:23.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-4128 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 28 01:35:23.530: INFO: stderr: ""
Jan 28 01:35:23.530: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/28/23 01:35:23.53
Jan 28 01:35:23.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-4128 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jan 28 01:35:24.598: INFO: stderr: ""
Jan 28 01:35:24.598: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/28/23 01:35:24.598
Jan 28 01:35:24.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-4128 delete pods e2e-test-httpd-pod'
Jan 28 01:35:26.319: INFO: stderr: ""
Jan 28 01:35:26.319: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 28 01:35:26.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4128" for this suite. 01/28/23 01:35:26.339
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":278,"skipped":5144,"failed":0}
------------------------------
â€¢ [3.009 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:35:23.349
    Jan 28 01:35:23.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubectl 01/28/23 01:35:23.351
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:35:23.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:35:23.406
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/28/23 01:35:23.42
    Jan 28 01:35:23.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-4128 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 28 01:35:23.530: INFO: stderr: ""
    Jan 28 01:35:23.530: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/28/23 01:35:23.53
    Jan 28 01:35:23.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-4128 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Jan 28 01:35:24.598: INFO: stderr: ""
    Jan 28 01:35:24.598: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/28/23 01:35:24.598
    Jan 28 01:35:24.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-4128 delete pods e2e-test-httpd-pod'
    Jan 28 01:35:26.319: INFO: stderr: ""
    Jan 28 01:35:26.319: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 28 01:35:26.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4128" for this suite. 01/28/23 01:35:26.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:35:26.359
Jan 28 01:35:26.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename statefulset 01/28/23 01:35:26.362
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:35:26.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:35:26.41
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4424 01/28/23 01:35:26.422
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 01/28/23 01:35:26.466
STEP: Creating pod with conflicting port in namespace statefulset-4424 01/28/23 01:35:26.49
STEP: Waiting until pod test-pod will start running in namespace statefulset-4424 01/28/23 01:35:26.511
Jan 28 01:35:26.512: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-4424" to be "running"
Jan 28 01:35:26.523: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.455098ms
Jan 28 01:35:28.536: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024810653s
Jan 28 01:35:30.535: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.023152477s
Jan 28 01:35:30.535: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-4424 01/28/23 01:35:30.535
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4424 01/28/23 01:35:30.553
Jan 28 01:35:30.586: INFO: Observed stateful pod in namespace: statefulset-4424, name: ss-0, uid: 39c65cd7-b3d5-4416-bf15-04ff627d924a, status phase: Pending. Waiting for statefulset controller to delete.
Jan 28 01:35:30.621: INFO: Observed stateful pod in namespace: statefulset-4424, name: ss-0, uid: 39c65cd7-b3d5-4416-bf15-04ff627d924a, status phase: Failed. Waiting for statefulset controller to delete.
Jan 28 01:35:30.640: INFO: Observed stateful pod in namespace: statefulset-4424, name: ss-0, uid: 39c65cd7-b3d5-4416-bf15-04ff627d924a, status phase: Failed. Waiting for statefulset controller to delete.
Jan 28 01:35:30.645: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4424
STEP: Removing pod with conflicting port in namespace statefulset-4424 01/28/23 01:35:30.645
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4424 and will be in running state 01/28/23 01:35:30.671
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 28 01:35:34.706: INFO: Deleting all statefulset in ns statefulset-4424
Jan 28 01:35:34.720: INFO: Scaling statefulset ss to 0
Jan 28 01:35:44.775: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 01:35:44.787: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 28 01:35:44.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4424" for this suite. 01/28/23 01:35:44.85
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":279,"skipped":5150,"failed":0}
------------------------------
â€¢ [SLOW TEST] [18.511 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:35:26.359
    Jan 28 01:35:26.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename statefulset 01/28/23 01:35:26.362
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:35:26.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:35:26.41
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4424 01/28/23 01:35:26.422
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 01/28/23 01:35:26.466
    STEP: Creating pod with conflicting port in namespace statefulset-4424 01/28/23 01:35:26.49
    STEP: Waiting until pod test-pod will start running in namespace statefulset-4424 01/28/23 01:35:26.511
    Jan 28 01:35:26.512: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-4424" to be "running"
    Jan 28 01:35:26.523: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.455098ms
    Jan 28 01:35:28.536: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024810653s
    Jan 28 01:35:30.535: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.023152477s
    Jan 28 01:35:30.535: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-4424 01/28/23 01:35:30.535
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4424 01/28/23 01:35:30.553
    Jan 28 01:35:30.586: INFO: Observed stateful pod in namespace: statefulset-4424, name: ss-0, uid: 39c65cd7-b3d5-4416-bf15-04ff627d924a, status phase: Pending. Waiting for statefulset controller to delete.
    Jan 28 01:35:30.621: INFO: Observed stateful pod in namespace: statefulset-4424, name: ss-0, uid: 39c65cd7-b3d5-4416-bf15-04ff627d924a, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 28 01:35:30.640: INFO: Observed stateful pod in namespace: statefulset-4424, name: ss-0, uid: 39c65cd7-b3d5-4416-bf15-04ff627d924a, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 28 01:35:30.645: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4424
    STEP: Removing pod with conflicting port in namespace statefulset-4424 01/28/23 01:35:30.645
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4424 and will be in running state 01/28/23 01:35:30.671
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 28 01:35:34.706: INFO: Deleting all statefulset in ns statefulset-4424
    Jan 28 01:35:34.720: INFO: Scaling statefulset ss to 0
    Jan 28 01:35:44.775: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 28 01:35:44.787: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 28 01:35:44.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4424" for this suite. 01/28/23 01:35:44.85
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:35:44.883
Jan 28 01:35:44.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename pod-network-test 01/28/23 01:35:44.884
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:35:44.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:35:44.936
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-7218 01/28/23 01:35:44.948
STEP: creating a selector 01/28/23 01:35:44.948
STEP: Creating the service pods in kubernetes 01/28/23 01:35:44.948
Jan 28 01:35:44.949: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 28 01:35:45.048: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7218" to be "running and ready"
Jan 28 01:35:45.101: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 52.822993ms
Jan 28 01:35:45.102: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:35:47.115: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066851355s
Jan 28 01:35:47.115: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:35:49.114: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.065675456s
Jan 28 01:35:49.114: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:35:51.114: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.065923024s
Jan 28 01:35:51.114: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:35:53.114: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.065461099s
Jan 28 01:35:53.114: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:35:55.116: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.067260218s
Jan 28 01:35:55.116: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:35:57.115: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.066944319s
Jan 28 01:35:57.116: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:35:59.120: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.071115501s
Jan 28 01:35:59.120: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:36:01.114: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.06532418s
Jan 28 01:36:01.114: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:36:03.114: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.065746692s
Jan 28 01:36:03.114: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:36:05.114: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.065214636s
Jan 28 01:36:05.114: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:36:07.116: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.067364647s
Jan 28 01:36:07.116: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 28 01:36:07.116: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 28 01:36:07.128: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7218" to be "running and ready"
Jan 28 01:36:07.139: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.903185ms
Jan 28 01:36:07.139: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 28 01:36:07.139: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 28 01:36:07.150: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7218" to be "running and ready"
Jan 28 01:36:07.162: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.248015ms
Jan 28 01:36:07.162: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 28 01:36:07.162: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/28/23 01:36:07.173
Jan 28 01:36:07.187: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7218" to be "running"
Jan 28 01:36:07.198: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.115089ms
Jan 28 01:36:09.211: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023924053s
Jan 28 01:36:11.210: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.023201476s
Jan 28 01:36:11.211: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 28 01:36:11.223: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 28 01:36:11.223: INFO: Breadth first check of 172.30.12.238 on host 10.9.20.126...
Jan 28 01:36:11.234: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.12.196:9080/dial?request=hostname&protocol=udp&host=172.30.12.238&port=8081&tries=1'] Namespace:pod-network-test-7218 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:36:11.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:36:11.235: INFO: ExecWithOptions: Clientset creation
Jan 28 01:36:11.235: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7218/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.12.196%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.12.238%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 28 01:36:11.467: INFO: Waiting for responses: map[]
Jan 28 01:36:11.467: INFO: reached 172.30.12.238 after 0/1 tries
Jan 28 01:36:11.467: INFO: Breadth first check of 172.30.185.62 on host 10.9.20.72...
Jan 28 01:36:11.479: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.12.196:9080/dial?request=hostname&protocol=udp&host=172.30.185.62&port=8081&tries=1'] Namespace:pod-network-test-7218 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:36:11.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:36:11.480: INFO: ExecWithOptions: Clientset creation
Jan 28 01:36:11.480: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7218/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.12.196%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.185.62%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 28 01:36:11.694: INFO: Waiting for responses: map[]
Jan 28 01:36:11.694: INFO: reached 172.30.185.62 after 0/1 tries
Jan 28 01:36:11.694: INFO: Breadth first check of 172.30.84.21 on host 10.9.20.75...
Jan 28 01:36:11.705: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.12.196:9080/dial?request=hostname&protocol=udp&host=172.30.84.21&port=8081&tries=1'] Namespace:pod-network-test-7218 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:36:11.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:36:11.706: INFO: ExecWithOptions: Clientset creation
Jan 28 01:36:11.706: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7218/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.12.196%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.84.21%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 28 01:36:11.936: INFO: Waiting for responses: map[]
Jan 28 01:36:11.936: INFO: reached 172.30.84.21 after 0/1 tries
Jan 28 01:36:11.936: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 28 01:36:11.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7218" for this suite. 01/28/23 01:36:11.956
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":280,"skipped":5183,"failed":0}
------------------------------
â€¢ [SLOW TEST] [27.093 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:35:44.883
    Jan 28 01:35:44.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename pod-network-test 01/28/23 01:35:44.884
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:35:44.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:35:44.936
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-7218 01/28/23 01:35:44.948
    STEP: creating a selector 01/28/23 01:35:44.948
    STEP: Creating the service pods in kubernetes 01/28/23 01:35:44.948
    Jan 28 01:35:44.949: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 28 01:35:45.048: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7218" to be "running and ready"
    Jan 28 01:35:45.101: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 52.822993ms
    Jan 28 01:35:45.102: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:35:47.115: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066851355s
    Jan 28 01:35:47.115: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:35:49.114: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.065675456s
    Jan 28 01:35:49.114: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:35:51.114: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.065923024s
    Jan 28 01:35:51.114: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:35:53.114: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.065461099s
    Jan 28 01:35:53.114: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:35:55.116: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.067260218s
    Jan 28 01:35:55.116: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:35:57.115: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.066944319s
    Jan 28 01:35:57.116: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:35:59.120: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.071115501s
    Jan 28 01:35:59.120: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:36:01.114: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.06532418s
    Jan 28 01:36:01.114: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:36:03.114: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.065746692s
    Jan 28 01:36:03.114: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:36:05.114: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.065214636s
    Jan 28 01:36:05.114: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:36:07.116: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.067364647s
    Jan 28 01:36:07.116: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 28 01:36:07.116: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 28 01:36:07.128: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7218" to be "running and ready"
    Jan 28 01:36:07.139: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.903185ms
    Jan 28 01:36:07.139: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 28 01:36:07.139: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 28 01:36:07.150: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7218" to be "running and ready"
    Jan 28 01:36:07.162: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.248015ms
    Jan 28 01:36:07.162: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 28 01:36:07.162: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/28/23 01:36:07.173
    Jan 28 01:36:07.187: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7218" to be "running"
    Jan 28 01:36:07.198: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.115089ms
    Jan 28 01:36:09.211: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023924053s
    Jan 28 01:36:11.210: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.023201476s
    Jan 28 01:36:11.211: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 28 01:36:11.223: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan 28 01:36:11.223: INFO: Breadth first check of 172.30.12.238 on host 10.9.20.126...
    Jan 28 01:36:11.234: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.12.196:9080/dial?request=hostname&protocol=udp&host=172.30.12.238&port=8081&tries=1'] Namespace:pod-network-test-7218 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:36:11.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:36:11.235: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:36:11.235: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7218/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.12.196%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.12.238%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 28 01:36:11.467: INFO: Waiting for responses: map[]
    Jan 28 01:36:11.467: INFO: reached 172.30.12.238 after 0/1 tries
    Jan 28 01:36:11.467: INFO: Breadth first check of 172.30.185.62 on host 10.9.20.72...
    Jan 28 01:36:11.479: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.12.196:9080/dial?request=hostname&protocol=udp&host=172.30.185.62&port=8081&tries=1'] Namespace:pod-network-test-7218 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:36:11.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:36:11.480: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:36:11.480: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7218/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.12.196%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.185.62%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 28 01:36:11.694: INFO: Waiting for responses: map[]
    Jan 28 01:36:11.694: INFO: reached 172.30.185.62 after 0/1 tries
    Jan 28 01:36:11.694: INFO: Breadth first check of 172.30.84.21 on host 10.9.20.75...
    Jan 28 01:36:11.705: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.12.196:9080/dial?request=hostname&protocol=udp&host=172.30.84.21&port=8081&tries=1'] Namespace:pod-network-test-7218 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:36:11.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:36:11.706: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:36:11.706: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7218/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.12.196%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.84.21%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 28 01:36:11.936: INFO: Waiting for responses: map[]
    Jan 28 01:36:11.936: INFO: reached 172.30.84.21 after 0/1 tries
    Jan 28 01:36:11.936: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 28 01:36:11.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-7218" for this suite. 01/28/23 01:36:11.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:36:11.984
Jan 28 01:36:11.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename events 01/28/23 01:36:11.986
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:36:12.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:36:12.034
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/28/23 01:36:12.045
Jan 28 01:36:12.059: INFO: created test-event-1
Jan 28 01:36:12.072: INFO: created test-event-2
Jan 28 01:36:12.086: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/28/23 01:36:12.086
STEP: delete collection of events 01/28/23 01:36:12.098
Jan 28 01:36:12.099: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/28/23 01:36:12.173
Jan 28 01:36:12.173: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan 28 01:36:12.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6442" for this suite. 01/28/23 01:36:12.203
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":281,"skipped":5217,"failed":0}
------------------------------
â€¢ [0.237 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:36:11.984
    Jan 28 01:36:11.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename events 01/28/23 01:36:11.986
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:36:12.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:36:12.034
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/28/23 01:36:12.045
    Jan 28 01:36:12.059: INFO: created test-event-1
    Jan 28 01:36:12.072: INFO: created test-event-2
    Jan 28 01:36:12.086: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/28/23 01:36:12.086
    STEP: delete collection of events 01/28/23 01:36:12.098
    Jan 28 01:36:12.099: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/28/23 01:36:12.173
    Jan 28 01:36:12.173: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan 28 01:36:12.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6442" for this suite. 01/28/23 01:36:12.203
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:36:12.229
Jan 28 01:36:12.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename services 01/28/23 01:36:12.232
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:36:12.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:36:12.289
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-6549 01/28/23 01:36:12.302
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6549 to expose endpoints map[] 01/28/23 01:36:12.339
Jan 28 01:36:12.424: INFO: successfully validated that service endpoint-test2 in namespace services-6549 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6549 01/28/23 01:36:12.424
Jan 28 01:36:12.444: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6549" to be "running and ready"
Jan 28 01:36:12.460: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.09704ms
Jan 28 01:36:12.460: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:36:14.483: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038895737s
Jan 28 01:36:14.483: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:36:16.472: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.028109857s
Jan 28 01:36:16.472: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 28 01:36:16.472: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6549 to expose endpoints map[pod1:[80]] 01/28/23 01:36:16.486
Jan 28 01:36:16.523: INFO: successfully validated that service endpoint-test2 in namespace services-6549 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/28/23 01:36:16.523
Jan 28 01:36:16.523: INFO: Creating new exec pod
Jan 28 01:36:16.537: INFO: Waiting up to 5m0s for pod "execpodfhkn8" in namespace "services-6549" to be "running"
Jan 28 01:36:16.548: INFO: Pod "execpodfhkn8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.234487ms
Jan 28 01:36:18.562: INFO: Pod "execpodfhkn8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024408093s
Jan 28 01:36:20.562: INFO: Pod "execpodfhkn8": Phase="Running", Reason="", readiness=true. Elapsed: 4.025232506s
Jan 28 01:36:20.562: INFO: Pod "execpodfhkn8" satisfied condition "running"
Jan 28 01:36:21.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6549 exec execpodfhkn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 28 01:36:21.901: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 28 01:36:21.901: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:36:21.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6549 exec execpodfhkn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.43.193 80'
Jan 28 01:36:22.227: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.43.193 80\nConnection to 172.21.43.193 80 port [tcp/http] succeeded!\n"
Jan 28 01:36:22.227: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-6549 01/28/23 01:36:22.227
Jan 28 01:36:22.242: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6549" to be "running and ready"
Jan 28 01:36:22.291: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 49.269341ms
Jan 28 01:36:22.292: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:36:24.303: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.061246794s
Jan 28 01:36:24.303: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 28 01:36:24.303: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6549 to expose endpoints map[pod1:[80] pod2:[80]] 01/28/23 01:36:24.315
Jan 28 01:36:24.368: INFO: successfully validated that service endpoint-test2 in namespace services-6549 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/28/23 01:36:24.368
Jan 28 01:36:25.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6549 exec execpodfhkn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 28 01:36:25.715: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 28 01:36:25.715: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:36:25.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6549 exec execpodfhkn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.43.193 80'
Jan 28 01:36:26.038: INFO: stderr: "+ + nc -vecho -t -w 2 hostName 172.21.43.193 80\n\nConnection to 172.21.43.193 80 port [tcp/http] succeeded!\n"
Jan 28 01:36:26.038: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-6549 01/28/23 01:36:26.038
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6549 to expose endpoints map[pod2:[80]] 01/28/23 01:36:26.071
Jan 28 01:36:26.118: INFO: successfully validated that service endpoint-test2 in namespace services-6549 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/28/23 01:36:26.119
Jan 28 01:36:27.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6549 exec execpodfhkn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 28 01:36:27.455: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 28 01:36:27.455: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:36:27.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6549 exec execpodfhkn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.43.193 80'
Jan 28 01:36:28.765: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.43.193 80\nConnection to 172.21.43.193 80 port [tcp/http] succeeded!\n"
Jan 28 01:36:28.765: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-6549 01/28/23 01:36:28.765
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6549 to expose endpoints map[] 01/28/23 01:36:28.792
Jan 28 01:36:28.826: INFO: successfully validated that service endpoint-test2 in namespace services-6549 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 28 01:36:28.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6549" for this suite. 01/28/23 01:36:28.886
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":282,"skipped":5223,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.677 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:36:12.229
    Jan 28 01:36:12.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename services 01/28/23 01:36:12.232
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:36:12.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:36:12.289
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-6549 01/28/23 01:36:12.302
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6549 to expose endpoints map[] 01/28/23 01:36:12.339
    Jan 28 01:36:12.424: INFO: successfully validated that service endpoint-test2 in namespace services-6549 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-6549 01/28/23 01:36:12.424
    Jan 28 01:36:12.444: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6549" to be "running and ready"
    Jan 28 01:36:12.460: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.09704ms
    Jan 28 01:36:12.460: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:36:14.483: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038895737s
    Jan 28 01:36:14.483: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:36:16.472: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.028109857s
    Jan 28 01:36:16.472: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 28 01:36:16.472: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6549 to expose endpoints map[pod1:[80]] 01/28/23 01:36:16.486
    Jan 28 01:36:16.523: INFO: successfully validated that service endpoint-test2 in namespace services-6549 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/28/23 01:36:16.523
    Jan 28 01:36:16.523: INFO: Creating new exec pod
    Jan 28 01:36:16.537: INFO: Waiting up to 5m0s for pod "execpodfhkn8" in namespace "services-6549" to be "running"
    Jan 28 01:36:16.548: INFO: Pod "execpodfhkn8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.234487ms
    Jan 28 01:36:18.562: INFO: Pod "execpodfhkn8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024408093s
    Jan 28 01:36:20.562: INFO: Pod "execpodfhkn8": Phase="Running", Reason="", readiness=true. Elapsed: 4.025232506s
    Jan 28 01:36:20.562: INFO: Pod "execpodfhkn8" satisfied condition "running"
    Jan 28 01:36:21.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6549 exec execpodfhkn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 28 01:36:21.901: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 28 01:36:21.901: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 01:36:21.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6549 exec execpodfhkn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.43.193 80'
    Jan 28 01:36:22.227: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.43.193 80\nConnection to 172.21.43.193 80 port [tcp/http] succeeded!\n"
    Jan 28 01:36:22.227: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-6549 01/28/23 01:36:22.227
    Jan 28 01:36:22.242: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6549" to be "running and ready"
    Jan 28 01:36:22.291: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 49.269341ms
    Jan 28 01:36:22.292: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:36:24.303: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.061246794s
    Jan 28 01:36:24.303: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 28 01:36:24.303: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6549 to expose endpoints map[pod1:[80] pod2:[80]] 01/28/23 01:36:24.315
    Jan 28 01:36:24.368: INFO: successfully validated that service endpoint-test2 in namespace services-6549 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/28/23 01:36:24.368
    Jan 28 01:36:25.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6549 exec execpodfhkn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 28 01:36:25.715: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 28 01:36:25.715: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 01:36:25.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6549 exec execpodfhkn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.43.193 80'
    Jan 28 01:36:26.038: INFO: stderr: "+ + nc -vecho -t -w 2 hostName 172.21.43.193 80\n\nConnection to 172.21.43.193 80 port [tcp/http] succeeded!\n"
    Jan 28 01:36:26.038: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-6549 01/28/23 01:36:26.038
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6549 to expose endpoints map[pod2:[80]] 01/28/23 01:36:26.071
    Jan 28 01:36:26.118: INFO: successfully validated that service endpoint-test2 in namespace services-6549 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/28/23 01:36:26.119
    Jan 28 01:36:27.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6549 exec execpodfhkn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 28 01:36:27.455: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 28 01:36:27.455: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 01:36:27.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6549 exec execpodfhkn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.43.193 80'
    Jan 28 01:36:28.765: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.43.193 80\nConnection to 172.21.43.193 80 port [tcp/http] succeeded!\n"
    Jan 28 01:36:28.765: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-6549 01/28/23 01:36:28.765
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6549 to expose endpoints map[] 01/28/23 01:36:28.792
    Jan 28 01:36:28.826: INFO: successfully validated that service endpoint-test2 in namespace services-6549 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 28 01:36:28.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6549" for this suite. 01/28/23 01:36:28.886
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:36:28.911
Jan 28 01:36:28.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename configmap 01/28/23 01:36:28.913
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:36:28.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:36:28.964
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-cdec26ad-8474-402b-9f1a-0019d077d2c3 01/28/23 01:36:29.04
STEP: Creating the pod 01/28/23 01:36:29.055
Jan 28 01:36:29.077: INFO: Waiting up to 5m0s for pod "pod-configmaps-53932ce7-9a76-40f1-892c-95f558daeb12" in namespace "configmap-6855" to be "running and ready"
Jan 28 01:36:29.087: INFO: Pod "pod-configmaps-53932ce7-9a76-40f1-892c-95f558daeb12": Phase="Pending", Reason="", readiness=false. Elapsed: 10.64567ms
Jan 28 01:36:29.087: INFO: The phase of Pod pod-configmaps-53932ce7-9a76-40f1-892c-95f558daeb12 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:36:31.099: INFO: Pod "pod-configmaps-53932ce7-9a76-40f1-892c-95f558daeb12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02218608s
Jan 28 01:36:31.099: INFO: The phase of Pod pod-configmaps-53932ce7-9a76-40f1-892c-95f558daeb12 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:36:33.101: INFO: Pod "pod-configmaps-53932ce7-9a76-40f1-892c-95f558daeb12": Phase="Running", Reason="", readiness=true. Elapsed: 4.024681775s
Jan 28 01:36:33.101: INFO: The phase of Pod pod-configmaps-53932ce7-9a76-40f1-892c-95f558daeb12 is Running (Ready = true)
Jan 28 01:36:33.101: INFO: Pod "pod-configmaps-53932ce7-9a76-40f1-892c-95f558daeb12" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-cdec26ad-8474-402b-9f1a-0019d077d2c3 01/28/23 01:36:33.14
STEP: waiting to observe update in volume 01/28/23 01:36:33.159
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 28 01:37:55.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6855" for this suite. 01/28/23 01:37:55.082
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":283,"skipped":5247,"failed":0}
------------------------------
â€¢ [SLOW TEST] [86.191 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:36:28.911
    Jan 28 01:36:28.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename configmap 01/28/23 01:36:28.913
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:36:28.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:36:28.964
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-cdec26ad-8474-402b-9f1a-0019d077d2c3 01/28/23 01:36:29.04
    STEP: Creating the pod 01/28/23 01:36:29.055
    Jan 28 01:36:29.077: INFO: Waiting up to 5m0s for pod "pod-configmaps-53932ce7-9a76-40f1-892c-95f558daeb12" in namespace "configmap-6855" to be "running and ready"
    Jan 28 01:36:29.087: INFO: Pod "pod-configmaps-53932ce7-9a76-40f1-892c-95f558daeb12": Phase="Pending", Reason="", readiness=false. Elapsed: 10.64567ms
    Jan 28 01:36:29.087: INFO: The phase of Pod pod-configmaps-53932ce7-9a76-40f1-892c-95f558daeb12 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:36:31.099: INFO: Pod "pod-configmaps-53932ce7-9a76-40f1-892c-95f558daeb12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02218608s
    Jan 28 01:36:31.099: INFO: The phase of Pod pod-configmaps-53932ce7-9a76-40f1-892c-95f558daeb12 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:36:33.101: INFO: Pod "pod-configmaps-53932ce7-9a76-40f1-892c-95f558daeb12": Phase="Running", Reason="", readiness=true. Elapsed: 4.024681775s
    Jan 28 01:36:33.101: INFO: The phase of Pod pod-configmaps-53932ce7-9a76-40f1-892c-95f558daeb12 is Running (Ready = true)
    Jan 28 01:36:33.101: INFO: Pod "pod-configmaps-53932ce7-9a76-40f1-892c-95f558daeb12" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-cdec26ad-8474-402b-9f1a-0019d077d2c3 01/28/23 01:36:33.14
    STEP: waiting to observe update in volume 01/28/23 01:36:33.159
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 28 01:37:55.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6855" for this suite. 01/28/23 01:37:55.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:37:55.103
Jan 28 01:37:55.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename configmap 01/28/23 01:37:55.106
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:37:55.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:37:55.2
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-93d03992-4c91-4410-ab1a-f0f84374fe94 01/28/23 01:37:55.212
STEP: Creating a pod to test consume configMaps 01/28/23 01:37:55.257
Jan 28 01:37:55.330: INFO: Waiting up to 5m0s for pod "pod-configmaps-e5394e12-488c-439b-8693-299a8d6c0afa" in namespace "configmap-8567" to be "Succeeded or Failed"
Jan 28 01:37:55.361: INFO: Pod "pod-configmaps-e5394e12-488c-439b-8693-299a8d6c0afa": Phase="Pending", Reason="", readiness=false. Elapsed: 30.954715ms
Jan 28 01:37:57.373: INFO: Pod "pod-configmaps-e5394e12-488c-439b-8693-299a8d6c0afa": Phase="Running", Reason="", readiness=true. Elapsed: 2.042421858s
Jan 28 01:37:59.380: INFO: Pod "pod-configmaps-e5394e12-488c-439b-8693-299a8d6c0afa": Phase="Running", Reason="", readiness=false. Elapsed: 4.049728034s
Jan 28 01:38:01.421: INFO: Pod "pod-configmaps-e5394e12-488c-439b-8693-299a8d6c0afa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.090454844s
STEP: Saw pod success 01/28/23 01:38:01.421
Jan 28 01:38:01.422: INFO: Pod "pod-configmaps-e5394e12-488c-439b-8693-299a8d6c0afa" satisfied condition "Succeeded or Failed"
Jan 28 01:38:01.462: INFO: Trying to get logs from node 10.9.20.126 pod pod-configmaps-e5394e12-488c-439b-8693-299a8d6c0afa container configmap-volume-test: <nil>
STEP: delete the pod 01/28/23 01:38:01.544
Jan 28 01:38:01.612: INFO: Waiting for pod pod-configmaps-e5394e12-488c-439b-8693-299a8d6c0afa to disappear
Jan 28 01:38:01.646: INFO: Pod pod-configmaps-e5394e12-488c-439b-8693-299a8d6c0afa no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 28 01:38:01.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8567" for this suite. 01/28/23 01:38:01.69
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":284,"skipped":5255,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.679 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:37:55.103
    Jan 28 01:37:55.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename configmap 01/28/23 01:37:55.106
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:37:55.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:37:55.2
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-93d03992-4c91-4410-ab1a-f0f84374fe94 01/28/23 01:37:55.212
    STEP: Creating a pod to test consume configMaps 01/28/23 01:37:55.257
    Jan 28 01:37:55.330: INFO: Waiting up to 5m0s for pod "pod-configmaps-e5394e12-488c-439b-8693-299a8d6c0afa" in namespace "configmap-8567" to be "Succeeded or Failed"
    Jan 28 01:37:55.361: INFO: Pod "pod-configmaps-e5394e12-488c-439b-8693-299a8d6c0afa": Phase="Pending", Reason="", readiness=false. Elapsed: 30.954715ms
    Jan 28 01:37:57.373: INFO: Pod "pod-configmaps-e5394e12-488c-439b-8693-299a8d6c0afa": Phase="Running", Reason="", readiness=true. Elapsed: 2.042421858s
    Jan 28 01:37:59.380: INFO: Pod "pod-configmaps-e5394e12-488c-439b-8693-299a8d6c0afa": Phase="Running", Reason="", readiness=false. Elapsed: 4.049728034s
    Jan 28 01:38:01.421: INFO: Pod "pod-configmaps-e5394e12-488c-439b-8693-299a8d6c0afa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.090454844s
    STEP: Saw pod success 01/28/23 01:38:01.421
    Jan 28 01:38:01.422: INFO: Pod "pod-configmaps-e5394e12-488c-439b-8693-299a8d6c0afa" satisfied condition "Succeeded or Failed"
    Jan 28 01:38:01.462: INFO: Trying to get logs from node 10.9.20.126 pod pod-configmaps-e5394e12-488c-439b-8693-299a8d6c0afa container configmap-volume-test: <nil>
    STEP: delete the pod 01/28/23 01:38:01.544
    Jan 28 01:38:01.612: INFO: Waiting for pod pod-configmaps-e5394e12-488c-439b-8693-299a8d6c0afa to disappear
    Jan 28 01:38:01.646: INFO: Pod pod-configmaps-e5394e12-488c-439b-8693-299a8d6c0afa no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 28 01:38:01.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8567" for this suite. 01/28/23 01:38:01.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:38:01.783
Jan 28 01:38:01.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename custom-resource-definition 01/28/23 01:38:01.784
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:38:01.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:38:01.854
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan 28 01:38:01.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:38:03.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9593" for this suite. 01/28/23 01:38:03.024
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":285,"skipped":5263,"failed":0}
------------------------------
â€¢ [1.259 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:38:01.783
    Jan 28 01:38:01.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename custom-resource-definition 01/28/23 01:38:01.784
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:38:01.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:38:01.854
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan 28 01:38:01.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:38:03.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9593" for this suite. 01/28/23 01:38:03.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:38:03.061
Jan 28 01:38:03.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubectl 01/28/23 01:38:03.062
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:38:03.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:38:03.167
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 01/28/23 01:38:03.179
Jan 28 01:38:03.179: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-7244 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/28/23 01:38:03.254
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 28 01:38:03.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7244" for this suite. 01/28/23 01:38:03.341
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":286,"skipped":5309,"failed":0}
------------------------------
â€¢ [0.299 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:38:03.061
    Jan 28 01:38:03.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubectl 01/28/23 01:38:03.062
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:38:03.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:38:03.167
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 01/28/23 01:38:03.179
    Jan 28 01:38:03.179: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-7244 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/28/23 01:38:03.254
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 28 01:38:03.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7244" for this suite. 01/28/23 01:38:03.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:38:03.361
Jan 28 01:38:03.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename webhook 01/28/23 01:38:03.363
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:38:03.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:38:03.481
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/28/23 01:38:03.611
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:38:03.881
STEP: Deploying the webhook pod 01/28/23 01:38:03.916
STEP: Wait for the deployment to be ready 01/28/23 01:38:03.99
Jan 28 01:38:04.135: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 38, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 38, 4, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 38, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 38, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:38:06.174: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 38, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 38, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 38, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 38, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/28/23 01:38:08.163
STEP: Verifying the service has paired with the endpoint 01/28/23 01:38:08.225
Jan 28 01:38:09.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 01/28/23 01:38:09.238
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/28/23 01:38:09.244
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/28/23 01:38:09.244
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/28/23 01:38:09.245
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/28/23 01:38:09.252
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/28/23 01:38:09.252
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/28/23 01:38:09.257
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:38:09.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7834" for this suite. 01/28/23 01:38:09.329
STEP: Destroying namespace "webhook-7834-markers" for this suite. 01/28/23 01:38:09.37
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":287,"skipped":5330,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.226 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:38:03.361
    Jan 28 01:38:03.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename webhook 01/28/23 01:38:03.363
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:38:03.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:38:03.481
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/28/23 01:38:03.611
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:38:03.881
    STEP: Deploying the webhook pod 01/28/23 01:38:03.916
    STEP: Wait for the deployment to be ready 01/28/23 01:38:03.99
    Jan 28 01:38:04.135: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 38, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 38, 4, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 38, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 38, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:38:06.174: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 38, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 38, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 38, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 38, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/28/23 01:38:08.163
    STEP: Verifying the service has paired with the endpoint 01/28/23 01:38:08.225
    Jan 28 01:38:09.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 01/28/23 01:38:09.238
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/28/23 01:38:09.244
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/28/23 01:38:09.244
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/28/23 01:38:09.245
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/28/23 01:38:09.252
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/28/23 01:38:09.252
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/28/23 01:38:09.257
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:38:09.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7834" for this suite. 01/28/23 01:38:09.329
    STEP: Destroying namespace "webhook-7834-markers" for this suite. 01/28/23 01:38:09.37
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:38:09.591
Jan 28 01:38:09.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename taint-multiple-pods 01/28/23 01:38:09.594
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:38:09.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:38:09.653
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jan 28 01:38:09.667: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 28 01:39:09.823: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Jan 28 01:39:09.838: INFO: Starting informer...
STEP: Starting pods... 01/28/23 01:39:09.839
Jan 28 01:39:10.116: INFO: Pod1 is running on 10.9.20.126. Tainting Node
Jan 28 01:39:10.163: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8517" to be "running"
Jan 28 01:39:10.175: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.434482ms
Jan 28 01:39:12.186: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023401973s
Jan 28 01:39:14.187: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.023666048s
Jan 28 01:39:14.187: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan 28 01:39:14.187: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8517" to be "running"
Jan 28 01:39:14.198: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 10.912013ms
Jan 28 01:39:14.198: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan 28 01:39:14.199: INFO: Pod2 is running on 10.9.20.126. Tainting Node
STEP: Trying to apply a taint on the Node 01/28/23 01:39:14.199
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/28/23 01:39:14.241
STEP: Waiting for Pod1 and Pod2 to be deleted 01/28/23 01:39:14.286
Jan 28 01:39:20.429: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 28 01:39:40.545: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/28/23 01:39:40.612
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Jan 28 01:39:40.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8517" for this suite. 01/28/23 01:39:40.681
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":288,"skipped":5335,"failed":0}
------------------------------
â€¢ [SLOW TEST] [91.107 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:38:09.591
    Jan 28 01:38:09.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename taint-multiple-pods 01/28/23 01:38:09.594
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:38:09.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:38:09.653
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Jan 28 01:38:09.667: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 28 01:39:09.823: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Jan 28 01:39:09.838: INFO: Starting informer...
    STEP: Starting pods... 01/28/23 01:39:09.839
    Jan 28 01:39:10.116: INFO: Pod1 is running on 10.9.20.126. Tainting Node
    Jan 28 01:39:10.163: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8517" to be "running"
    Jan 28 01:39:10.175: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.434482ms
    Jan 28 01:39:12.186: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023401973s
    Jan 28 01:39:14.187: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.023666048s
    Jan 28 01:39:14.187: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan 28 01:39:14.187: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8517" to be "running"
    Jan 28 01:39:14.198: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 10.912013ms
    Jan 28 01:39:14.198: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan 28 01:39:14.199: INFO: Pod2 is running on 10.9.20.126. Tainting Node
    STEP: Trying to apply a taint on the Node 01/28/23 01:39:14.199
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/28/23 01:39:14.241
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/28/23 01:39:14.286
    Jan 28 01:39:20.429: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan 28 01:39:40.545: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/28/23 01:39:40.612
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 01:39:40.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-8517" for this suite. 01/28/23 01:39:40.681
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:39:40.699
Jan 28 01:39:40.700: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename namespaces 01/28/23 01:39:40.702
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:39:40.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:39:40.761
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 01/28/23 01:39:40.803
STEP: patching the Namespace 01/28/23 01:39:40.843
STEP: get the Namespace and ensuring it has the label 01/28/23 01:39:40.858
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 28 01:39:40.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1716" for this suite. 01/28/23 01:39:40.92
STEP: Destroying namespace "nspatchtest-cfba1180-2417-4052-992e-e6c9fbf48393-84" for this suite. 01/28/23 01:39:40.964
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":289,"skipped":5338,"failed":0}
------------------------------
â€¢ [0.284 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:39:40.699
    Jan 28 01:39:40.700: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename namespaces 01/28/23 01:39:40.702
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:39:40.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:39:40.761
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 01/28/23 01:39:40.803
    STEP: patching the Namespace 01/28/23 01:39:40.843
    STEP: get the Namespace and ensuring it has the label 01/28/23 01:39:40.858
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 01:39:40.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-1716" for this suite. 01/28/23 01:39:40.92
    STEP: Destroying namespace "nspatchtest-cfba1180-2417-4052-992e-e6c9fbf48393-84" for this suite. 01/28/23 01:39:40.964
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:39:40.984
Jan 28 01:39:40.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename limitrange 01/28/23 01:39:40.987
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:39:41.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:39:41.083
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 01/28/23 01:39:41.098
STEP: Setting up watch 01/28/23 01:39:41.099
STEP: Submitting a LimitRange 01/28/23 01:39:41.211
STEP: Verifying LimitRange creation was observed 01/28/23 01:39:41.234
STEP: Fetching the LimitRange to ensure it has proper values 01/28/23 01:39:41.235
Jan 28 01:39:41.246: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 28 01:39:41.246: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/28/23 01:39:41.246
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/28/23 01:39:41.261
Jan 28 01:39:41.276: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 28 01:39:41.276: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/28/23 01:39:41.276
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/28/23 01:39:41.29
Jan 28 01:39:41.331: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 28 01:39:41.331: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/28/23 01:39:41.331
STEP: Failing to create a Pod with more than max resources 01/28/23 01:39:41.341
STEP: Updating a LimitRange 01/28/23 01:39:41.348
STEP: Verifying LimitRange updating is effective 01/28/23 01:39:41.362
STEP: Creating a Pod with less than former min resources 01/28/23 01:39:43.374
STEP: Failing to create a Pod with more than max resources 01/28/23 01:39:43.39
STEP: Deleting a LimitRange 01/28/23 01:39:43.397
STEP: Verifying the LimitRange was deleted 01/28/23 01:39:43.415
Jan 28 01:39:48.428: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/28/23 01:39:48.428
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Jan 28 01:39:48.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-6626" for this suite. 01/28/23 01:39:48.486
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":290,"skipped":5341,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.521 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:39:40.984
    Jan 28 01:39:40.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename limitrange 01/28/23 01:39:40.987
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:39:41.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:39:41.083
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 01/28/23 01:39:41.098
    STEP: Setting up watch 01/28/23 01:39:41.099
    STEP: Submitting a LimitRange 01/28/23 01:39:41.211
    STEP: Verifying LimitRange creation was observed 01/28/23 01:39:41.234
    STEP: Fetching the LimitRange to ensure it has proper values 01/28/23 01:39:41.235
    Jan 28 01:39:41.246: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 28 01:39:41.246: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/28/23 01:39:41.246
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/28/23 01:39:41.261
    Jan 28 01:39:41.276: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 28 01:39:41.276: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/28/23 01:39:41.276
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/28/23 01:39:41.29
    Jan 28 01:39:41.331: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan 28 01:39:41.331: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/28/23 01:39:41.331
    STEP: Failing to create a Pod with more than max resources 01/28/23 01:39:41.341
    STEP: Updating a LimitRange 01/28/23 01:39:41.348
    STEP: Verifying LimitRange updating is effective 01/28/23 01:39:41.362
    STEP: Creating a Pod with less than former min resources 01/28/23 01:39:43.374
    STEP: Failing to create a Pod with more than max resources 01/28/23 01:39:43.39
    STEP: Deleting a LimitRange 01/28/23 01:39:43.397
    STEP: Verifying the LimitRange was deleted 01/28/23 01:39:43.415
    Jan 28 01:39:48.428: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/28/23 01:39:48.428
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Jan 28 01:39:48.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-6626" for this suite. 01/28/23 01:39:48.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:39:48.508
Jan 28 01:39:48.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename custom-resource-definition 01/28/23 01:39:48.51
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:39:48.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:39:48.586
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/28/23 01:39:48.6
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/28/23 01:39:48.606
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/28/23 01:39:48.606
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/28/23 01:39:48.606
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/28/23 01:39:48.612
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/28/23 01:39:48.612
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/28/23 01:39:48.618
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:39:48.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3160" for this suite. 01/28/23 01:39:48.672
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":291,"skipped":5349,"failed":0}
------------------------------
â€¢ [0.182 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:39:48.508
    Jan 28 01:39:48.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename custom-resource-definition 01/28/23 01:39:48.51
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:39:48.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:39:48.586
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/28/23 01:39:48.6
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/28/23 01:39:48.606
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/28/23 01:39:48.606
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/28/23 01:39:48.606
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/28/23 01:39:48.612
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/28/23 01:39:48.612
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/28/23 01:39:48.618
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:39:48.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-3160" for this suite. 01/28/23 01:39:48.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:39:48.703
Jan 28 01:39:48.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename runtimeclass 01/28/23 01:39:48.705
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:39:48.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:39:48.758
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 28 01:39:48.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3141" for this suite. 01/28/23 01:39:48.815
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":292,"skipped":5366,"failed":0}
------------------------------
â€¢ [0.131 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:39:48.703
    Jan 28 01:39:48.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename runtimeclass 01/28/23 01:39:48.705
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:39:48.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:39:48.758
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 28 01:39:48.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3141" for this suite. 01/28/23 01:39:48.815
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:39:48.851
Jan 28 01:39:48.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename downward-api 01/28/23 01:39:48.852
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:39:48.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:39:48.978
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 01/28/23 01:39:48.99
Jan 28 01:39:49.011: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99b38759-938c-4d69-9563-ed919de3774d" in namespace "downward-api-4052" to be "Succeeded or Failed"
Jan 28 01:39:49.023: INFO: Pod "downwardapi-volume-99b38759-938c-4d69-9563-ed919de3774d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.920515ms
Jan 28 01:39:51.037: INFO: Pod "downwardapi-volume-99b38759-938c-4d69-9563-ed919de3774d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025246592s
Jan 28 01:39:53.037: INFO: Pod "downwardapi-volume-99b38759-938c-4d69-9563-ed919de3774d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025846938s
Jan 28 01:39:55.063: INFO: Pod "downwardapi-volume-99b38759-938c-4d69-9563-ed919de3774d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051347421s
STEP: Saw pod success 01/28/23 01:39:55.063
Jan 28 01:39:55.063: INFO: Pod "downwardapi-volume-99b38759-938c-4d69-9563-ed919de3774d" satisfied condition "Succeeded or Failed"
Jan 28 01:39:55.074: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-99b38759-938c-4d69-9563-ed919de3774d container client-container: <nil>
STEP: delete the pod 01/28/23 01:39:55.163
Jan 28 01:39:55.192: INFO: Waiting for pod downwardapi-volume-99b38759-938c-4d69-9563-ed919de3774d to disappear
Jan 28 01:39:55.203: INFO: Pod downwardapi-volume-99b38759-938c-4d69-9563-ed919de3774d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 28 01:39:55.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4052" for this suite. 01/28/23 01:39:55.225
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":293,"skipped":5393,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.403 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:39:48.851
    Jan 28 01:39:48.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename downward-api 01/28/23 01:39:48.852
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:39:48.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:39:48.978
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 01/28/23 01:39:48.99
    Jan 28 01:39:49.011: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99b38759-938c-4d69-9563-ed919de3774d" in namespace "downward-api-4052" to be "Succeeded or Failed"
    Jan 28 01:39:49.023: INFO: Pod "downwardapi-volume-99b38759-938c-4d69-9563-ed919de3774d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.920515ms
    Jan 28 01:39:51.037: INFO: Pod "downwardapi-volume-99b38759-938c-4d69-9563-ed919de3774d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025246592s
    Jan 28 01:39:53.037: INFO: Pod "downwardapi-volume-99b38759-938c-4d69-9563-ed919de3774d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025846938s
    Jan 28 01:39:55.063: INFO: Pod "downwardapi-volume-99b38759-938c-4d69-9563-ed919de3774d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051347421s
    STEP: Saw pod success 01/28/23 01:39:55.063
    Jan 28 01:39:55.063: INFO: Pod "downwardapi-volume-99b38759-938c-4d69-9563-ed919de3774d" satisfied condition "Succeeded or Failed"
    Jan 28 01:39:55.074: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-99b38759-938c-4d69-9563-ed919de3774d container client-container: <nil>
    STEP: delete the pod 01/28/23 01:39:55.163
    Jan 28 01:39:55.192: INFO: Waiting for pod downwardapi-volume-99b38759-938c-4d69-9563-ed919de3774d to disappear
    Jan 28 01:39:55.203: INFO: Pod downwardapi-volume-99b38759-938c-4d69-9563-ed919de3774d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 28 01:39:55.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4052" for this suite. 01/28/23 01:39:55.225
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:39:55.26
Jan 28 01:39:55.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename emptydir 01/28/23 01:39:55.261
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:39:55.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:39:55.311
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/28/23 01:39:55.331
Jan 28 01:39:55.354: INFO: Waiting up to 5m0s for pod "pod-0caf0a50-ada7-48e1-b111-d41bd1d59d8b" in namespace "emptydir-1284" to be "Succeeded or Failed"
Jan 28 01:39:55.366: INFO: Pod "pod-0caf0a50-ada7-48e1-b111-d41bd1d59d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.23535ms
Jan 28 01:39:57.378: INFO: Pod "pod-0caf0a50-ada7-48e1-b111-d41bd1d59d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023542776s
Jan 28 01:39:59.379: INFO: Pod "pod-0caf0a50-ada7-48e1-b111-d41bd1d59d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024633776s
Jan 28 01:40:01.379: INFO: Pod "pod-0caf0a50-ada7-48e1-b111-d41bd1d59d8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024272878s
STEP: Saw pod success 01/28/23 01:40:01.379
Jan 28 01:40:01.379: INFO: Pod "pod-0caf0a50-ada7-48e1-b111-d41bd1d59d8b" satisfied condition "Succeeded or Failed"
Jan 28 01:40:01.390: INFO: Trying to get logs from node 10.9.20.126 pod pod-0caf0a50-ada7-48e1-b111-d41bd1d59d8b container test-container: <nil>
STEP: delete the pod 01/28/23 01:40:01.418
Jan 28 01:40:01.446: INFO: Waiting for pod pod-0caf0a50-ada7-48e1-b111-d41bd1d59d8b to disappear
Jan 28 01:40:01.456: INFO: Pod pod-0caf0a50-ada7-48e1-b111-d41bd1d59d8b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 28 01:40:01.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1284" for this suite. 01/28/23 01:40:01.483
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":294,"skipped":5413,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.244 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:39:55.26
    Jan 28 01:39:55.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename emptydir 01/28/23 01:39:55.261
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:39:55.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:39:55.311
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/28/23 01:39:55.331
    Jan 28 01:39:55.354: INFO: Waiting up to 5m0s for pod "pod-0caf0a50-ada7-48e1-b111-d41bd1d59d8b" in namespace "emptydir-1284" to be "Succeeded or Failed"
    Jan 28 01:39:55.366: INFO: Pod "pod-0caf0a50-ada7-48e1-b111-d41bd1d59d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.23535ms
    Jan 28 01:39:57.378: INFO: Pod "pod-0caf0a50-ada7-48e1-b111-d41bd1d59d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023542776s
    Jan 28 01:39:59.379: INFO: Pod "pod-0caf0a50-ada7-48e1-b111-d41bd1d59d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024633776s
    Jan 28 01:40:01.379: INFO: Pod "pod-0caf0a50-ada7-48e1-b111-d41bd1d59d8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024272878s
    STEP: Saw pod success 01/28/23 01:40:01.379
    Jan 28 01:40:01.379: INFO: Pod "pod-0caf0a50-ada7-48e1-b111-d41bd1d59d8b" satisfied condition "Succeeded or Failed"
    Jan 28 01:40:01.390: INFO: Trying to get logs from node 10.9.20.126 pod pod-0caf0a50-ada7-48e1-b111-d41bd1d59d8b container test-container: <nil>
    STEP: delete the pod 01/28/23 01:40:01.418
    Jan 28 01:40:01.446: INFO: Waiting for pod pod-0caf0a50-ada7-48e1-b111-d41bd1d59d8b to disappear
    Jan 28 01:40:01.456: INFO: Pod pod-0caf0a50-ada7-48e1-b111-d41bd1d59d8b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 28 01:40:01.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1284" for this suite. 01/28/23 01:40:01.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:40:01.519
Jan 28 01:40:01.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename resourcequota 01/28/23 01:40:01.52
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:40:01.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:40:01.587
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 01/28/23 01:40:01.598
STEP: Ensuring ResourceQuota status is calculated 01/28/23 01:40:01.612
STEP: Creating a ResourceQuota with not terminating scope 01/28/23 01:40:03.625
STEP: Ensuring ResourceQuota status is calculated 01/28/23 01:40:03.637
STEP: Creating a long running pod 01/28/23 01:40:05.651
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/28/23 01:40:05.714
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/28/23 01:40:07.726
STEP: Deleting the pod 01/28/23 01:40:09.743
STEP: Ensuring resource quota status released the pod usage 01/28/23 01:40:09.768
STEP: Creating a terminating pod 01/28/23 01:40:11.781
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/28/23 01:40:11.811
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/28/23 01:40:13.831
STEP: Deleting the pod 01/28/23 01:40:15.846
STEP: Ensuring resource quota status released the pod usage 01/28/23 01:40:15.879
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 28 01:40:17.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6727" for this suite. 01/28/23 01:40:17.913
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":295,"skipped":5445,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.414 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:40:01.519
    Jan 28 01:40:01.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename resourcequota 01/28/23 01:40:01.52
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:40:01.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:40:01.587
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 01/28/23 01:40:01.598
    STEP: Ensuring ResourceQuota status is calculated 01/28/23 01:40:01.612
    STEP: Creating a ResourceQuota with not terminating scope 01/28/23 01:40:03.625
    STEP: Ensuring ResourceQuota status is calculated 01/28/23 01:40:03.637
    STEP: Creating a long running pod 01/28/23 01:40:05.651
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/28/23 01:40:05.714
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/28/23 01:40:07.726
    STEP: Deleting the pod 01/28/23 01:40:09.743
    STEP: Ensuring resource quota status released the pod usage 01/28/23 01:40:09.768
    STEP: Creating a terminating pod 01/28/23 01:40:11.781
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/28/23 01:40:11.811
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/28/23 01:40:13.831
    STEP: Deleting the pod 01/28/23 01:40:15.846
    STEP: Ensuring resource quota status released the pod usage 01/28/23 01:40:15.879
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 28 01:40:17.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6727" for this suite. 01/28/23 01:40:17.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:40:17.938
Jan 28 01:40:17.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename sched-preemption 01/28/23 01:40:17.939
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:40:17.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:40:17.991
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 28 01:40:18.097: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 28 01:41:18.237: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 01/28/23 01:41:18.251
Jan 28 01:41:18.325: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 28 01:41:18.344: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 28 01:41:18.401: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 28 01:41:18.417: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 28 01:41:18.464: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 28 01:41:18.549: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/28/23 01:41:18.549
Jan 28 01:41:18.549: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3283" to be "running"
Jan 28 01:41:18.563: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 13.916625ms
Jan 28 01:41:20.575: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025228551s
Jan 28 01:41:22.587: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037690012s
Jan 28 01:41:24.588: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039003312s
Jan 28 01:41:26.603: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.053865007s
Jan 28 01:41:28.578: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.028492017s
Jan 28 01:41:30.574: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.025021336s
Jan 28 01:41:32.575: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.026124881s
Jan 28 01:41:32.576: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 28 01:41:32.576: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3283" to be "running"
Jan 28 01:41:32.587: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.326534ms
Jan 28 01:41:32.587: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 28 01:41:32.587: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3283" to be "running"
Jan 28 01:41:32.597: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.50193ms
Jan 28 01:41:32.597: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 28 01:41:32.597: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3283" to be "running"
Jan 28 01:41:32.606: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.406057ms
Jan 28 01:41:32.606: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 28 01:41:32.606: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3283" to be "running"
Jan 28 01:41:32.617: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.750849ms
Jan 28 01:41:32.617: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 28 01:41:32.617: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3283" to be "running"
Jan 28 01:41:32.627: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.302194ms
Jan 28 01:41:32.627: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/28/23 01:41:32.627
Jan 28 01:41:32.678: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan 28 01:41:32.686: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.290276ms
Jan 28 01:41:34.697: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019302259s
Jan 28 01:41:36.724: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.046658544s
Jan 28 01:41:36.725: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 28 01:41:36.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3283" for this suite. 01/28/23 01:41:36.897
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":296,"skipped":5491,"failed":0}
------------------------------
â€¢ [SLOW TEST] [79.132 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:40:17.938
    Jan 28 01:40:17.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename sched-preemption 01/28/23 01:40:17.939
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:40:17.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:40:17.991
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 28 01:40:18.097: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 28 01:41:18.237: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 01/28/23 01:41:18.251
    Jan 28 01:41:18.325: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 28 01:41:18.344: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 28 01:41:18.401: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 28 01:41:18.417: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan 28 01:41:18.464: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan 28 01:41:18.549: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/28/23 01:41:18.549
    Jan 28 01:41:18.549: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3283" to be "running"
    Jan 28 01:41:18.563: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 13.916625ms
    Jan 28 01:41:20.575: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025228551s
    Jan 28 01:41:22.587: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037690012s
    Jan 28 01:41:24.588: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039003312s
    Jan 28 01:41:26.603: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.053865007s
    Jan 28 01:41:28.578: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.028492017s
    Jan 28 01:41:30.574: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.025021336s
    Jan 28 01:41:32.575: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.026124881s
    Jan 28 01:41:32.576: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 28 01:41:32.576: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3283" to be "running"
    Jan 28 01:41:32.587: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.326534ms
    Jan 28 01:41:32.587: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 28 01:41:32.587: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3283" to be "running"
    Jan 28 01:41:32.597: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.50193ms
    Jan 28 01:41:32.597: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 28 01:41:32.597: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3283" to be "running"
    Jan 28 01:41:32.606: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.406057ms
    Jan 28 01:41:32.606: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 28 01:41:32.606: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3283" to be "running"
    Jan 28 01:41:32.617: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.750849ms
    Jan 28 01:41:32.617: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 28 01:41:32.617: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3283" to be "running"
    Jan 28 01:41:32.627: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.302194ms
    Jan 28 01:41:32.627: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/28/23 01:41:32.627
    Jan 28 01:41:32.678: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan 28 01:41:32.686: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.290276ms
    Jan 28 01:41:34.697: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019302259s
    Jan 28 01:41:36.724: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.046658544s
    Jan 28 01:41:36.725: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 01:41:36.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-3283" for this suite. 01/28/23 01:41:36.897
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:41:37.073
Jan 28 01:41:37.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename cronjob 01/28/23 01:41:37.075
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:41:37.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:41:37.127
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/28/23 01:41:37.141
STEP: Ensuring a job is scheduled 01/28/23 01:41:37.158
STEP: Ensuring exactly one is scheduled 01/28/23 01:42:01.168
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/28/23 01:42:01.178
STEP: Ensuring no more jobs are scheduled 01/28/23 01:42:01.189
STEP: Removing cronjob 01/28/23 01:47:01.209
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 28 01:47:01.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5325" for this suite. 01/28/23 01:47:01.252
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":297,"skipped":5507,"failed":0}
------------------------------
â€¢ [SLOW TEST] [324.192 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:41:37.073
    Jan 28 01:41:37.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename cronjob 01/28/23 01:41:37.075
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:41:37.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:41:37.127
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/28/23 01:41:37.141
    STEP: Ensuring a job is scheduled 01/28/23 01:41:37.158
    STEP: Ensuring exactly one is scheduled 01/28/23 01:42:01.168
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/28/23 01:42:01.178
    STEP: Ensuring no more jobs are scheduled 01/28/23 01:42:01.189
    STEP: Removing cronjob 01/28/23 01:47:01.209
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 28 01:47:01.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5325" for this suite. 01/28/23 01:47:01.252
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:47:01.269
Jan 28 01:47:01.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename endpointslice 01/28/23 01:47:01.271
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:47:01.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:47:01.324
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Jan 28 01:47:01.377: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
Jan 28 01:47:01.377: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 28 01:47:01.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8140" for this suite. 01/28/23 01:47:01.401
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":298,"skipped":5550,"failed":0}
------------------------------
â€¢ [0.150 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:47:01.269
    Jan 28 01:47:01.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename endpointslice 01/28/23 01:47:01.271
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:47:01.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:47:01.324
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Jan 28 01:47:01.377: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
    Jan 28 01:47:01.377: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 28 01:47:01.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-8140" for this suite. 01/28/23 01:47:01.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:47:01.426
Jan 28 01:47:01.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename endpointslice 01/28/23 01:47:01.427
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:47:01.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:47:01.482
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 01/28/23 01:47:01.499
STEP: getting /apis/discovery.k8s.io 01/28/23 01:47:01.511
STEP: getting /apis/discovery.k8s.iov1 01/28/23 01:47:01.518
STEP: creating 01/28/23 01:47:01.525
STEP: getting 01/28/23 01:47:01.572
STEP: listing 01/28/23 01:47:01.585
STEP: watching 01/28/23 01:47:01.598
Jan 28 01:47:01.599: INFO: starting watch
STEP: cluster-wide listing 01/28/23 01:47:01.606
STEP: cluster-wide watching 01/28/23 01:47:01.62
Jan 28 01:47:01.620: INFO: starting watch
STEP: patching 01/28/23 01:47:01.626
STEP: updating 01/28/23 01:47:01.652
Jan 28 01:47:01.725: INFO: waiting for watch events with expected annotations
Jan 28 01:47:01.725: INFO: saw patched and updated annotations
STEP: deleting 01/28/23 01:47:01.726
STEP: deleting a collection 01/28/23 01:47:01.79
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 28 01:47:01.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7422" for this suite. 01/28/23 01:47:01.871
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":299,"skipped":5583,"failed":0}
------------------------------
â€¢ [0.462 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:47:01.426
    Jan 28 01:47:01.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename endpointslice 01/28/23 01:47:01.427
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:47:01.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:47:01.482
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 01/28/23 01:47:01.499
    STEP: getting /apis/discovery.k8s.io 01/28/23 01:47:01.511
    STEP: getting /apis/discovery.k8s.iov1 01/28/23 01:47:01.518
    STEP: creating 01/28/23 01:47:01.525
    STEP: getting 01/28/23 01:47:01.572
    STEP: listing 01/28/23 01:47:01.585
    STEP: watching 01/28/23 01:47:01.598
    Jan 28 01:47:01.599: INFO: starting watch
    STEP: cluster-wide listing 01/28/23 01:47:01.606
    STEP: cluster-wide watching 01/28/23 01:47:01.62
    Jan 28 01:47:01.620: INFO: starting watch
    STEP: patching 01/28/23 01:47:01.626
    STEP: updating 01/28/23 01:47:01.652
    Jan 28 01:47:01.725: INFO: waiting for watch events with expected annotations
    Jan 28 01:47:01.725: INFO: saw patched and updated annotations
    STEP: deleting 01/28/23 01:47:01.726
    STEP: deleting a collection 01/28/23 01:47:01.79
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 28 01:47:01.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-7422" for this suite. 01/28/23 01:47:01.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:47:01.897
Jan 28 01:47:01.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename deployment 01/28/23 01:47:01.899
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:47:01.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:47:01.958
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan 28 01:47:01.973: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 28 01:47:02.011: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 28 01:47:07.033: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/28/23 01:47:07.033
Jan 28 01:47:07.034: INFO: Creating deployment "test-rolling-update-deployment"
Jan 28 01:47:07.048: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 28 01:47:07.082: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 28 01:47:09.118: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 28 01:47:09.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 47, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 47, 7, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 47, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 47, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:47:11.148: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 28 01:47:11.191: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2077  7b80d5f6-ca18-40c7-a2b8-4e661a5adfcb 43128 1 2023-01-28 01:47:07 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-28 01:47:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:47:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004fa1028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-28 01:47:07 +0000 UTC,LastTransitionTime:2023-01-28 01:47:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-28 01:47:09 +0000 UTC,LastTransitionTime:2023-01-28 01:47:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 28 01:47:11.207: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-2077  79da5313-897b-49c4-bc05-d8d68ac0127c 43118 1 2023-01-28 01:47:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 7b80d5f6-ca18-40c7-a2b8-4e661a5adfcb 0xc005e881c7 0xc005e881c8}] [] [{kube-controller-manager Update apps/v1 2023-01-28 01:47:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b80d5f6-ca18-40c7-a2b8-4e661a5adfcb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:47:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e88278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 28 01:47:11.207: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 28 01:47:11.207: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2077  1d6fc2db-1245-4fc0-9789-0d9d81d4efd3 43127 2 2023-01-28 01:47:01 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 7b80d5f6-ca18-40c7-a2b8-4e661a5adfcb 0xc005e88097 0xc005e88098}] [] [{e2e.test Update apps/v1 2023-01-28 01:47:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:47:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b80d5f6-ca18-40c7-a2b8-4e661a5adfcb\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:47:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005e88158 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 28 01:47:11.219: INFO: Pod "test-rolling-update-deployment-78f575d8ff-4chfc" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-4chfc test-rolling-update-deployment-78f575d8ff- deployment-2077  e57b1c29-6829-4355-ac71-5e8d7108e17b 43117 0 2023-01-28 01:47:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:7248d5fb22cdd5ad8a78285697a12c166f77d1fe484dfe10c8fb49de6487e0d5 cni.projectcalico.org/podIP:172.30.12.223/32 cni.projectcalico.org/podIPs:172.30.12.223/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 79da5313-897b-49c4-bc05-d8d68ac0127c 0xc004fa1457 0xc004fa1458}] [] [{kube-controller-manager Update v1 2023-01-28 01:47:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79da5313-897b-49c4-bc05-d8d68ac0127c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:47:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:47:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2srj5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2srj5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:47:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:47:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:47:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:47:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:172.30.12.223,StartTime:2023-01-28 01:47:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:47:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://5d5ab64053d22b4521caa80f8cf38c157c27a4fb6ffc608656fb33e7342e52e3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.12.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 28 01:47:11.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2077" for this suite. 01/28/23 01:47:11.247
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":300,"skipped":5602,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.368 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:47:01.897
    Jan 28 01:47:01.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename deployment 01/28/23 01:47:01.899
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:47:01.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:47:01.958
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan 28 01:47:01.973: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan 28 01:47:02.011: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 28 01:47:07.033: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/28/23 01:47:07.033
    Jan 28 01:47:07.034: INFO: Creating deployment "test-rolling-update-deployment"
    Jan 28 01:47:07.048: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan 28 01:47:07.082: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan 28 01:47:09.118: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan 28 01:47:09.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 47, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 47, 7, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 47, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 47, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 28 01:47:11.148: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 28 01:47:11.191: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2077  7b80d5f6-ca18-40c7-a2b8-4e661a5adfcb 43128 1 2023-01-28 01:47:07 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-28 01:47:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:47:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004fa1028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-28 01:47:07 +0000 UTC,LastTransitionTime:2023-01-28 01:47:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-28 01:47:09 +0000 UTC,LastTransitionTime:2023-01-28 01:47:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 28 01:47:11.207: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-2077  79da5313-897b-49c4-bc05-d8d68ac0127c 43118 1 2023-01-28 01:47:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 7b80d5f6-ca18-40c7-a2b8-4e661a5adfcb 0xc005e881c7 0xc005e881c8}] [] [{kube-controller-manager Update apps/v1 2023-01-28 01:47:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b80d5f6-ca18-40c7-a2b8-4e661a5adfcb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:47:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e88278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 28 01:47:11.207: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan 28 01:47:11.207: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2077  1d6fc2db-1245-4fc0-9789-0d9d81d4efd3 43127 2 2023-01-28 01:47:01 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 7b80d5f6-ca18-40c7-a2b8-4e661a5adfcb 0xc005e88097 0xc005e88098}] [] [{e2e.test Update apps/v1 2023-01-28 01:47:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:47:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b80d5f6-ca18-40c7-a2b8-4e661a5adfcb\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:47:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005e88158 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 28 01:47:11.219: INFO: Pod "test-rolling-update-deployment-78f575d8ff-4chfc" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-4chfc test-rolling-update-deployment-78f575d8ff- deployment-2077  e57b1c29-6829-4355-ac71-5e8d7108e17b 43117 0 2023-01-28 01:47:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:7248d5fb22cdd5ad8a78285697a12c166f77d1fe484dfe10c8fb49de6487e0d5 cni.projectcalico.org/podIP:172.30.12.223/32 cni.projectcalico.org/podIPs:172.30.12.223/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 79da5313-897b-49c4-bc05-d8d68ac0127c 0xc004fa1457 0xc004fa1458}] [] [{kube-controller-manager Update v1 2023-01-28 01:47:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79da5313-897b-49c4-bc05-d8d68ac0127c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:47:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:47:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2srj5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2srj5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:47:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:47:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:47:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:47:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:172.30.12.223,StartTime:2023-01-28 01:47:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:47:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://5d5ab64053d22b4521caa80f8cf38c157c27a4fb6ffc608656fb33e7342e52e3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.12.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 28 01:47:11.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2077" for this suite. 01/28/23 01:47:11.247
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:47:11.268
Jan 28 01:47:11.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename disruption 01/28/23 01:47:11.27
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:47:11.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:47:11.323
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 01/28/23 01:47:11.335
STEP: Waiting for the pdb to be processed 01/28/23 01:47:11.35
STEP: updating the pdb 01/28/23 01:47:11.363
STEP: Waiting for the pdb to be processed 01/28/23 01:47:11.39
STEP: patching the pdb 01/28/23 01:47:11.402
STEP: Waiting for the pdb to be processed 01/28/23 01:47:11.439
STEP: Waiting for the pdb to be deleted 01/28/23 01:47:11.477
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 28 01:47:11.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5790" for this suite. 01/28/23 01:47:11.514
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":301,"skipped":5607,"failed":0}
------------------------------
â€¢ [0.265 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:47:11.268
    Jan 28 01:47:11.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename disruption 01/28/23 01:47:11.27
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:47:11.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:47:11.323
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 01/28/23 01:47:11.335
    STEP: Waiting for the pdb to be processed 01/28/23 01:47:11.35
    STEP: updating the pdb 01/28/23 01:47:11.363
    STEP: Waiting for the pdb to be processed 01/28/23 01:47:11.39
    STEP: patching the pdb 01/28/23 01:47:11.402
    STEP: Waiting for the pdb to be processed 01/28/23 01:47:11.439
    STEP: Waiting for the pdb to be deleted 01/28/23 01:47:11.477
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 28 01:47:11.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5790" for this suite. 01/28/23 01:47:11.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:47:11.544
Jan 28 01:47:11.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename webhook 01/28/23 01:47:11.545
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:47:11.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:47:11.604
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/28/23 01:47:11.662
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:47:12.169
STEP: Deploying the webhook pod 01/28/23 01:47:12.2
STEP: Wait for the deployment to be ready 01/28/23 01:47:12.232
Jan 28 01:47:12.268: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 01:47:14.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 47, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 47, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 47, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 47, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/28/23 01:47:16.348
STEP: Verifying the service has paired with the endpoint 01/28/23 01:47:16.375
Jan 28 01:47:17.375: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 01/28/23 01:47:17.391
STEP: create a pod 01/28/23 01:47:17.505
Jan 28 01:47:17.528: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2318" to be "running"
Jan 28 01:47:17.542: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.41636ms
Jan 28 01:47:19.555: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026553114s
Jan 28 01:47:19.555: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/28/23 01:47:19.555
Jan 28 01:47:19.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=webhook-2318 attach --namespace=webhook-2318 to-be-attached-pod -i -c=container1'
Jan 28 01:47:19.760: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:47:19.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2318" for this suite. 01/28/23 01:47:19.801
STEP: Destroying namespace "webhook-2318-markers" for this suite. 01/28/23 01:47:19.815
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":302,"skipped":5636,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.401 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:47:11.544
    Jan 28 01:47:11.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename webhook 01/28/23 01:47:11.545
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:47:11.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:47:11.604
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/28/23 01:47:11.662
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:47:12.169
    STEP: Deploying the webhook pod 01/28/23 01:47:12.2
    STEP: Wait for the deployment to be ready 01/28/23 01:47:12.232
    Jan 28 01:47:12.268: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 28 01:47:14.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 47, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 47, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 47, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 47, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/28/23 01:47:16.348
    STEP: Verifying the service has paired with the endpoint 01/28/23 01:47:16.375
    Jan 28 01:47:17.375: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 01/28/23 01:47:17.391
    STEP: create a pod 01/28/23 01:47:17.505
    Jan 28 01:47:17.528: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2318" to be "running"
    Jan 28 01:47:17.542: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.41636ms
    Jan 28 01:47:19.555: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026553114s
    Jan 28 01:47:19.555: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/28/23 01:47:19.555
    Jan 28 01:47:19.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=webhook-2318 attach --namespace=webhook-2318 to-be-attached-pod -i -c=container1'
    Jan 28 01:47:19.760: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:47:19.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2318" for this suite. 01/28/23 01:47:19.801
    STEP: Destroying namespace "webhook-2318-markers" for this suite. 01/28/23 01:47:19.815
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:47:19.949
Jan 28 01:47:19.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename emptydir 01/28/23 01:47:19.951
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:47:19.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:47:20.007
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 01/28/23 01:47:20.022
Jan 28 01:47:20.078: INFO: Waiting up to 5m0s for pod "pod-4cfb455d-7ef6-409a-aaca-09f8ff74cb81" in namespace "emptydir-9690" to be "Succeeded or Failed"
Jan 28 01:47:20.095: INFO: Pod "pod-4cfb455d-7ef6-409a-aaca-09f8ff74cb81": Phase="Pending", Reason="", readiness=false. Elapsed: 17.207723ms
Jan 28 01:47:22.109: INFO: Pod "pod-4cfb455d-7ef6-409a-aaca-09f8ff74cb81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031343304s
Jan 28 01:47:24.110: INFO: Pod "pod-4cfb455d-7ef6-409a-aaca-09f8ff74cb81": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03164583s
Jan 28 01:47:26.108: INFO: Pod "pod-4cfb455d-7ef6-409a-aaca-09f8ff74cb81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029702436s
STEP: Saw pod success 01/28/23 01:47:26.108
Jan 28 01:47:26.108: INFO: Pod "pod-4cfb455d-7ef6-409a-aaca-09f8ff74cb81" satisfied condition "Succeeded or Failed"
Jan 28 01:47:26.118: INFO: Trying to get logs from node 10.9.20.126 pod pod-4cfb455d-7ef6-409a-aaca-09f8ff74cb81 container test-container: <nil>
STEP: delete the pod 01/28/23 01:47:26.189
Jan 28 01:47:26.226: INFO: Waiting for pod pod-4cfb455d-7ef6-409a-aaca-09f8ff74cb81 to disappear
Jan 28 01:47:26.235: INFO: Pod pod-4cfb455d-7ef6-409a-aaca-09f8ff74cb81 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 28 01:47:26.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9690" for this suite. 01/28/23 01:47:26.252
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":303,"skipped":5645,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.318 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:47:19.949
    Jan 28 01:47:19.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename emptydir 01/28/23 01:47:19.951
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:47:19.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:47:20.007
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 01/28/23 01:47:20.022
    Jan 28 01:47:20.078: INFO: Waiting up to 5m0s for pod "pod-4cfb455d-7ef6-409a-aaca-09f8ff74cb81" in namespace "emptydir-9690" to be "Succeeded or Failed"
    Jan 28 01:47:20.095: INFO: Pod "pod-4cfb455d-7ef6-409a-aaca-09f8ff74cb81": Phase="Pending", Reason="", readiness=false. Elapsed: 17.207723ms
    Jan 28 01:47:22.109: INFO: Pod "pod-4cfb455d-7ef6-409a-aaca-09f8ff74cb81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031343304s
    Jan 28 01:47:24.110: INFO: Pod "pod-4cfb455d-7ef6-409a-aaca-09f8ff74cb81": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03164583s
    Jan 28 01:47:26.108: INFO: Pod "pod-4cfb455d-7ef6-409a-aaca-09f8ff74cb81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029702436s
    STEP: Saw pod success 01/28/23 01:47:26.108
    Jan 28 01:47:26.108: INFO: Pod "pod-4cfb455d-7ef6-409a-aaca-09f8ff74cb81" satisfied condition "Succeeded or Failed"
    Jan 28 01:47:26.118: INFO: Trying to get logs from node 10.9.20.126 pod pod-4cfb455d-7ef6-409a-aaca-09f8ff74cb81 container test-container: <nil>
    STEP: delete the pod 01/28/23 01:47:26.189
    Jan 28 01:47:26.226: INFO: Waiting for pod pod-4cfb455d-7ef6-409a-aaca-09f8ff74cb81 to disappear
    Jan 28 01:47:26.235: INFO: Pod pod-4cfb455d-7ef6-409a-aaca-09f8ff74cb81 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 28 01:47:26.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9690" for this suite. 01/28/23 01:47:26.252
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:47:26.267
Jan 28 01:47:26.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename pod-network-test 01/28/23 01:47:26.269
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:47:26.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:47:26.326
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-5807 01/28/23 01:47:26.341
STEP: creating a selector 01/28/23 01:47:26.342
STEP: Creating the service pods in kubernetes 01/28/23 01:47:26.342
Jan 28 01:47:26.343: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 28 01:47:26.433: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5807" to be "running and ready"
Jan 28 01:47:26.445: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.659935ms
Jan 28 01:47:26.445: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:47:28.458: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024980419s
Jan 28 01:47:28.459: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:47:30.462: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.028700786s
Jan 28 01:47:30.462: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:47:32.456: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.022864281s
Jan 28 01:47:32.456: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:47:34.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.025142733s
Jan 28 01:47:34.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:47:36.460: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.026502032s
Jan 28 01:47:36.460: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:47:38.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.024477968s
Jan 28 01:47:38.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:47:40.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.024705244s
Jan 28 01:47:40.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:47:42.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.023958268s
Jan 28 01:47:42.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:47:44.459: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.02592538s
Jan 28 01:47:44.459: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:47:46.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.0246097s
Jan 28 01:47:46.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 28 01:47:48.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.02414496s
Jan 28 01:47:48.457: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 28 01:47:48.458: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 28 01:47:48.469: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5807" to be "running and ready"
Jan 28 01:47:48.481: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 11.633092ms
Jan 28 01:47:48.481: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 28 01:47:48.481: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 28 01:47:48.494: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5807" to be "running and ready"
Jan 28 01:47:48.507: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 12.962383ms
Jan 28 01:47:48.508: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 28 01:47:48.508: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/28/23 01:47:48.52
Jan 28 01:47:48.537: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5807" to be "running"
Jan 28 01:47:48.552: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.865693ms
Jan 28 01:47:50.565: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027790165s
Jan 28 01:47:52.566: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.028093061s
Jan 28 01:47:52.566: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 28 01:47:52.580: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 28 01:47:52.580: INFO: Breadth first check of 172.30.12.229 on host 10.9.20.126...
Jan 28 01:47:52.592: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.12.230:9080/dial?request=hostname&protocol=http&host=172.30.12.229&port=8083&tries=1'] Namespace:pod-network-test-5807 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:47:52.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:47:52.593: INFO: ExecWithOptions: Clientset creation
Jan 28 01:47:52.593: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5807/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.12.230%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.12.229%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 28 01:47:52.808: INFO: Waiting for responses: map[]
Jan 28 01:47:52.808: INFO: reached 172.30.12.229 after 0/1 tries
Jan 28 01:47:52.808: INFO: Breadth first check of 172.30.185.14 on host 10.9.20.72...
Jan 28 01:47:52.823: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.12.230:9080/dial?request=hostname&protocol=http&host=172.30.185.14&port=8083&tries=1'] Namespace:pod-network-test-5807 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:47:52.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:47:52.824: INFO: ExecWithOptions: Clientset creation
Jan 28 01:47:52.824: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5807/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.12.230%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.185.14%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 28 01:47:53.052: INFO: Waiting for responses: map[]
Jan 28 01:47:53.052: INFO: reached 172.30.185.14 after 0/1 tries
Jan 28 01:47:53.053: INFO: Breadth first check of 172.30.84.27 on host 10.9.20.75...
Jan 28 01:47:53.064: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.12.230:9080/dial?request=hostname&protocol=http&host=172.30.84.27&port=8083&tries=1'] Namespace:pod-network-test-5807 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:47:53.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:47:53.065: INFO: ExecWithOptions: Clientset creation
Jan 28 01:47:53.065: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5807/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.12.230%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.84.27%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 28 01:47:53.275: INFO: Waiting for responses: map[]
Jan 28 01:47:53.276: INFO: reached 172.30.84.27 after 0/1 tries
Jan 28 01:47:53.276: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 28 01:47:53.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5807" for this suite. 01/28/23 01:47:53.298
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":304,"skipped":5645,"failed":0}
------------------------------
â€¢ [SLOW TEST] [27.051 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:47:26.267
    Jan 28 01:47:26.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename pod-network-test 01/28/23 01:47:26.269
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:47:26.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:47:26.326
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-5807 01/28/23 01:47:26.341
    STEP: creating a selector 01/28/23 01:47:26.342
    STEP: Creating the service pods in kubernetes 01/28/23 01:47:26.342
    Jan 28 01:47:26.343: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 28 01:47:26.433: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5807" to be "running and ready"
    Jan 28 01:47:26.445: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.659935ms
    Jan 28 01:47:26.445: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:47:28.458: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024980419s
    Jan 28 01:47:28.459: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:47:30.462: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.028700786s
    Jan 28 01:47:30.462: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:47:32.456: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.022864281s
    Jan 28 01:47:32.456: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:47:34.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.025142733s
    Jan 28 01:47:34.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:47:36.460: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.026502032s
    Jan 28 01:47:36.460: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:47:38.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.024477968s
    Jan 28 01:47:38.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:47:40.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.024705244s
    Jan 28 01:47:40.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:47:42.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.023958268s
    Jan 28 01:47:42.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:47:44.459: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.02592538s
    Jan 28 01:47:44.459: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:47:46.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.0246097s
    Jan 28 01:47:46.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 28 01:47:48.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.02414496s
    Jan 28 01:47:48.457: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 28 01:47:48.458: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 28 01:47:48.469: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5807" to be "running and ready"
    Jan 28 01:47:48.481: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 11.633092ms
    Jan 28 01:47:48.481: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 28 01:47:48.481: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 28 01:47:48.494: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5807" to be "running and ready"
    Jan 28 01:47:48.507: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 12.962383ms
    Jan 28 01:47:48.508: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 28 01:47:48.508: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/28/23 01:47:48.52
    Jan 28 01:47:48.537: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5807" to be "running"
    Jan 28 01:47:48.552: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.865693ms
    Jan 28 01:47:50.565: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027790165s
    Jan 28 01:47:52.566: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.028093061s
    Jan 28 01:47:52.566: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 28 01:47:52.580: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan 28 01:47:52.580: INFO: Breadth first check of 172.30.12.229 on host 10.9.20.126...
    Jan 28 01:47:52.592: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.12.230:9080/dial?request=hostname&protocol=http&host=172.30.12.229&port=8083&tries=1'] Namespace:pod-network-test-5807 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:47:52.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:47:52.593: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:47:52.593: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5807/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.12.230%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.12.229%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 28 01:47:52.808: INFO: Waiting for responses: map[]
    Jan 28 01:47:52.808: INFO: reached 172.30.12.229 after 0/1 tries
    Jan 28 01:47:52.808: INFO: Breadth first check of 172.30.185.14 on host 10.9.20.72...
    Jan 28 01:47:52.823: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.12.230:9080/dial?request=hostname&protocol=http&host=172.30.185.14&port=8083&tries=1'] Namespace:pod-network-test-5807 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:47:52.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:47:52.824: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:47:52.824: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5807/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.12.230%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.185.14%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 28 01:47:53.052: INFO: Waiting for responses: map[]
    Jan 28 01:47:53.052: INFO: reached 172.30.185.14 after 0/1 tries
    Jan 28 01:47:53.053: INFO: Breadth first check of 172.30.84.27 on host 10.9.20.75...
    Jan 28 01:47:53.064: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.12.230:9080/dial?request=hostname&protocol=http&host=172.30.84.27&port=8083&tries=1'] Namespace:pod-network-test-5807 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:47:53.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:47:53.065: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:47:53.065: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5807/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.12.230%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.84.27%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 28 01:47:53.275: INFO: Waiting for responses: map[]
    Jan 28 01:47:53.276: INFO: reached 172.30.84.27 after 0/1 tries
    Jan 28 01:47:53.276: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 28 01:47:53.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-5807" for this suite. 01/28/23 01:47:53.298
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:47:53.323
Jan 28 01:47:53.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename services 01/28/23 01:47:53.325
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:47:53.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:47:53.384
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-6161 01/28/23 01:47:53.396
STEP: creating service affinity-nodeport-transition in namespace services-6161 01/28/23 01:47:53.397
STEP: creating replication controller affinity-nodeport-transition in namespace services-6161 01/28/23 01:47:53.446
I0128 01:47:53.465009      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6161, replica count: 3
I0128 01:47:56.519018      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 01:47:56.557: INFO: Creating new exec pod
Jan 28 01:47:56.579: INFO: Waiting up to 5m0s for pod "execpod-affinity6h7wg" in namespace "services-6161" to be "running"
Jan 28 01:47:56.590: INFO: Pod "execpod-affinity6h7wg": Phase="Pending", Reason="", readiness=false. Elapsed: 11.364679ms
Jan 28 01:47:58.608: INFO: Pod "execpod-affinity6h7wg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028849933s
Jan 28 01:48:00.605: INFO: Pod "execpod-affinity6h7wg": Phase="Running", Reason="", readiness=true. Elapsed: 4.025681608s
Jan 28 01:48:00.605: INFO: Pod "execpod-affinity6h7wg" satisfied condition "running"
Jan 28 01:48:01.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6161 exec execpod-affinity6h7wg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan 28 01:48:02.075: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 28 01:48:02.075: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:48:02.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6161 exec execpod-affinity6h7wg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.137.208 80'
Jan 28 01:48:02.446: INFO: stderr: "+ nc -v -t -w 2 172.21.137.208 80\n+ echo hostName\nConnection to 172.21.137.208 80 port [tcp/http] succeeded!\n"
Jan 28 01:48:02.446: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:48:02.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6161 exec execpod-affinity6h7wg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.72 31929'
Jan 28 01:48:02.765: INFO: stderr: "+ echo+  hostName\nnc -v -t -w 2 10.9.20.72 31929\nConnection to 10.9.20.72 31929 port [tcp/*] succeeded!\n"
Jan 28 01:48:02.766: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:48:02.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6161 exec execpod-affinity6h7wg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.75 31929'
Jan 28 01:48:03.107: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.9.20.75 31929\nConnection to 10.9.20.75 31929 port [tcp/*] succeeded!\n"
Jan 28 01:48:03.107: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:48:03.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6161 exec execpod-affinity6h7wg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.9.20.126:31929/ ; done'
Jan 28 01:48:03.632: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n"
Jan 28 01:48:03.632: INFO: stdout: "\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g"
Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:33.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6161 exec execpod-affinity6h7wg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.9.20.126:31929/ ; done'
Jan 28 01:48:34.115: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n"
Jan 28 01:48:34.115: INFO: stdout: "\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-vfzx7\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-vfzx7\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-td28r"
Jan 28 01:48:34.115: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:34.115: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.115: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:34.115: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:34.115: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-vfzx7
Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-vfzx7
Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-cgr8g
Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6161 exec execpod-affinity6h7wg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.9.20.126:31929/ ; done'
Jan 28 01:48:34.643: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n"
Jan 28 01:48:34.643: INFO: stdout: "\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r"
Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
Jan 28 01:48:34.643: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6161, will wait for the garbage collector to delete the pods 01/28/23 01:48:34.672
Jan 28 01:48:34.757: INFO: Deleting ReplicationController affinity-nodeport-transition took: 21.592905ms
Jan 28 01:48:34.859: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.053426ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 28 01:48:37.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6161" for this suite. 01/28/23 01:48:37.74
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":305,"skipped":5658,"failed":0}
------------------------------
â€¢ [SLOW TEST] [44.433 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:47:53.323
    Jan 28 01:47:53.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename services 01/28/23 01:47:53.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:47:53.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:47:53.384
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-6161 01/28/23 01:47:53.396
    STEP: creating service affinity-nodeport-transition in namespace services-6161 01/28/23 01:47:53.397
    STEP: creating replication controller affinity-nodeport-transition in namespace services-6161 01/28/23 01:47:53.446
    I0128 01:47:53.465009      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6161, replica count: 3
    I0128 01:47:56.519018      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 28 01:47:56.557: INFO: Creating new exec pod
    Jan 28 01:47:56.579: INFO: Waiting up to 5m0s for pod "execpod-affinity6h7wg" in namespace "services-6161" to be "running"
    Jan 28 01:47:56.590: INFO: Pod "execpod-affinity6h7wg": Phase="Pending", Reason="", readiness=false. Elapsed: 11.364679ms
    Jan 28 01:47:58.608: INFO: Pod "execpod-affinity6h7wg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028849933s
    Jan 28 01:48:00.605: INFO: Pod "execpod-affinity6h7wg": Phase="Running", Reason="", readiness=true. Elapsed: 4.025681608s
    Jan 28 01:48:00.605: INFO: Pod "execpod-affinity6h7wg" satisfied condition "running"
    Jan 28 01:48:01.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6161 exec execpod-affinity6h7wg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Jan 28 01:48:02.075: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan 28 01:48:02.075: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 01:48:02.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6161 exec execpod-affinity6h7wg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.137.208 80'
    Jan 28 01:48:02.446: INFO: stderr: "+ nc -v -t -w 2 172.21.137.208 80\n+ echo hostName\nConnection to 172.21.137.208 80 port [tcp/http] succeeded!\n"
    Jan 28 01:48:02.446: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 01:48:02.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6161 exec execpod-affinity6h7wg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.72 31929'
    Jan 28 01:48:02.765: INFO: stderr: "+ echo+  hostName\nnc -v -t -w 2 10.9.20.72 31929\nConnection to 10.9.20.72 31929 port [tcp/*] succeeded!\n"
    Jan 28 01:48:02.766: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 01:48:02.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6161 exec execpod-affinity6h7wg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.75 31929'
    Jan 28 01:48:03.107: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.9.20.75 31929\nConnection to 10.9.20.75 31929 port [tcp/*] succeeded!\n"
    Jan 28 01:48:03.107: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 01:48:03.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6161 exec execpod-affinity6h7wg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.9.20.126:31929/ ; done'
    Jan 28 01:48:03.632: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n"
    Jan 28 01:48:03.632: INFO: stdout: "\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g"
    Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:03.632: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:33.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6161 exec execpod-affinity6h7wg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.9.20.126:31929/ ; done'
    Jan 28 01:48:34.115: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n"
    Jan 28 01:48:34.115: INFO: stdout: "\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-vfzx7\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-vfzx7\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-cgr8g\naffinity-nodeport-transition-td28r"
    Jan 28 01:48:34.115: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:34.115: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.115: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:34.115: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:34.115: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-vfzx7
    Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-vfzx7
    Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-cgr8g
    Jan 28 01:48:34.116: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-6161 exec execpod-affinity6h7wg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.9.20.126:31929/ ; done'
    Jan 28 01:48:34.643: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:31929/\n"
    Jan 28 01:48:34.643: INFO: stdout: "\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r\naffinity-nodeport-transition-td28r"
    Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.643: INFO: Received response from host: affinity-nodeport-transition-td28r
    Jan 28 01:48:34.643: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6161, will wait for the garbage collector to delete the pods 01/28/23 01:48:34.672
    Jan 28 01:48:34.757: INFO: Deleting ReplicationController affinity-nodeport-transition took: 21.592905ms
    Jan 28 01:48:34.859: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.053426ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 28 01:48:37.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6161" for this suite. 01/28/23 01:48:37.74
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:48:37.76
Jan 28 01:48:37.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 01:48:37.763
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:48:37.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:48:37.826
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-a30d7d09-171c-4e7b-940b-5e55e840020c 01/28/23 01:48:37.841
STEP: Creating a pod to test consume configMaps 01/28/23 01:48:37.857
Jan 28 01:48:37.883: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f737913a-900e-4fa0-8c65-e095345ab50c" in namespace "projected-1090" to be "Succeeded or Failed"
Jan 28 01:48:37.895: INFO: Pod "pod-projected-configmaps-f737913a-900e-4fa0-8c65-e095345ab50c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.316204ms
Jan 28 01:48:39.912: INFO: Pod "pod-projected-configmaps-f737913a-900e-4fa0-8c65-e095345ab50c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028740231s
Jan 28 01:48:41.910: INFO: Pod "pod-projected-configmaps-f737913a-900e-4fa0-8c65-e095345ab50c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026219686s
STEP: Saw pod success 01/28/23 01:48:41.91
Jan 28 01:48:41.910: INFO: Pod "pod-projected-configmaps-f737913a-900e-4fa0-8c65-e095345ab50c" satisfied condition "Succeeded or Failed"
Jan 28 01:48:41.921: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-configmaps-f737913a-900e-4fa0-8c65-e095345ab50c container agnhost-container: <nil>
STEP: delete the pod 01/28/23 01:48:41.95
Jan 28 01:48:41.974: INFO: Waiting for pod pod-projected-configmaps-f737913a-900e-4fa0-8c65-e095345ab50c to disappear
Jan 28 01:48:41.986: INFO: Pod pod-projected-configmaps-f737913a-900e-4fa0-8c65-e095345ab50c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 28 01:48:41.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1090" for this suite. 01/28/23 01:48:42.005
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":306,"skipped":5663,"failed":0}
------------------------------
â€¢ [4.265 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:48:37.76
    Jan 28 01:48:37.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 01:48:37.763
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:48:37.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:48:37.826
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-a30d7d09-171c-4e7b-940b-5e55e840020c 01/28/23 01:48:37.841
    STEP: Creating a pod to test consume configMaps 01/28/23 01:48:37.857
    Jan 28 01:48:37.883: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f737913a-900e-4fa0-8c65-e095345ab50c" in namespace "projected-1090" to be "Succeeded or Failed"
    Jan 28 01:48:37.895: INFO: Pod "pod-projected-configmaps-f737913a-900e-4fa0-8c65-e095345ab50c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.316204ms
    Jan 28 01:48:39.912: INFO: Pod "pod-projected-configmaps-f737913a-900e-4fa0-8c65-e095345ab50c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028740231s
    Jan 28 01:48:41.910: INFO: Pod "pod-projected-configmaps-f737913a-900e-4fa0-8c65-e095345ab50c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026219686s
    STEP: Saw pod success 01/28/23 01:48:41.91
    Jan 28 01:48:41.910: INFO: Pod "pod-projected-configmaps-f737913a-900e-4fa0-8c65-e095345ab50c" satisfied condition "Succeeded or Failed"
    Jan 28 01:48:41.921: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-configmaps-f737913a-900e-4fa0-8c65-e095345ab50c container agnhost-container: <nil>
    STEP: delete the pod 01/28/23 01:48:41.95
    Jan 28 01:48:41.974: INFO: Waiting for pod pod-projected-configmaps-f737913a-900e-4fa0-8c65-e095345ab50c to disappear
    Jan 28 01:48:41.986: INFO: Pod pod-projected-configmaps-f737913a-900e-4fa0-8c65-e095345ab50c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 28 01:48:41.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1090" for this suite. 01/28/23 01:48:42.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:48:42.029
Jan 28 01:48:42.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename emptydir 01/28/23 01:48:42.031
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:48:42.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:48:42.083
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 01/28/23 01:48:42.096
Jan 28 01:48:42.115: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-941ef9dc-bf93-4dee-a6df-8a35c2dbe5d5" in namespace "emptydir-1998" to be "running"
Jan 28 01:48:42.141: INFO: Pod "pod-sharedvolume-941ef9dc-bf93-4dee-a6df-8a35c2dbe5d5": Phase="Pending", Reason="", readiness=false. Elapsed: 25.810306ms
Jan 28 01:48:44.154: INFO: Pod "pod-sharedvolume-941ef9dc-bf93-4dee-a6df-8a35c2dbe5d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.038940417s
Jan 28 01:48:44.154: INFO: Pod "pod-sharedvolume-941ef9dc-bf93-4dee-a6df-8a35c2dbe5d5" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/28/23 01:48:44.155
Jan 28 01:48:44.155: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1998 PodName:pod-sharedvolume-941ef9dc-bf93-4dee-a6df-8a35c2dbe5d5 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:48:44.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:48:44.158: INFO: ExecWithOptions: Clientset creation
Jan 28 01:48:44.159: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-1998/pods/pod-sharedvolume-941ef9dc-bf93-4dee-a6df-8a35c2dbe5d5/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 28 01:48:44.347: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 28 01:48:44.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1998" for this suite. 01/28/23 01:48:44.368
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":307,"skipped":5673,"failed":0}
------------------------------
â€¢ [2.358 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:48:42.029
    Jan 28 01:48:42.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename emptydir 01/28/23 01:48:42.031
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:48:42.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:48:42.083
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 01/28/23 01:48:42.096
    Jan 28 01:48:42.115: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-941ef9dc-bf93-4dee-a6df-8a35c2dbe5d5" in namespace "emptydir-1998" to be "running"
    Jan 28 01:48:42.141: INFO: Pod "pod-sharedvolume-941ef9dc-bf93-4dee-a6df-8a35c2dbe5d5": Phase="Pending", Reason="", readiness=false. Elapsed: 25.810306ms
    Jan 28 01:48:44.154: INFO: Pod "pod-sharedvolume-941ef9dc-bf93-4dee-a6df-8a35c2dbe5d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.038940417s
    Jan 28 01:48:44.154: INFO: Pod "pod-sharedvolume-941ef9dc-bf93-4dee-a6df-8a35c2dbe5d5" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/28/23 01:48:44.155
    Jan 28 01:48:44.155: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1998 PodName:pod-sharedvolume-941ef9dc-bf93-4dee-a6df-8a35c2dbe5d5 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 28 01:48:44.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:48:44.158: INFO: ExecWithOptions: Clientset creation
    Jan 28 01:48:44.159: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-1998/pods/pod-sharedvolume-941ef9dc-bf93-4dee-a6df-8a35c2dbe5d5/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan 28 01:48:44.347: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 28 01:48:44.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1998" for this suite. 01/28/23 01:48:44.368
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:48:44.39
Jan 28 01:48:44.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename server-version 01/28/23 01:48:44.393
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:48:44.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:48:44.451
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/28/23 01:48:44.468
STEP: Confirm major version 01/28/23 01:48:44.476
Jan 28 01:48:44.476: INFO: Major version: 1
STEP: Confirm minor version 01/28/23 01:48:44.476
Jan 28 01:48:44.477: INFO: cleanMinorVersion: 25
Jan 28 01:48:44.477: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Jan 28 01:48:44.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-6757" for this suite. 01/28/23 01:48:44.498
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":308,"skipped":5685,"failed":0}
------------------------------
â€¢ [0.130 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:48:44.39
    Jan 28 01:48:44.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename server-version 01/28/23 01:48:44.393
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:48:44.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:48:44.451
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/28/23 01:48:44.468
    STEP: Confirm major version 01/28/23 01:48:44.476
    Jan 28 01:48:44.476: INFO: Major version: 1
    STEP: Confirm minor version 01/28/23 01:48:44.476
    Jan 28 01:48:44.477: INFO: cleanMinorVersion: 25
    Jan 28 01:48:44.477: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Jan 28 01:48:44.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-6757" for this suite. 01/28/23 01:48:44.498
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:48:44.521
Jan 28 01:48:44.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubectl 01/28/23 01:48:44.523
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:48:44.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:48:44.589
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 01/28/23 01:48:44.605
Jan 28 01:48:44.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 create -f -'
Jan 28 01:48:44.920: INFO: stderr: ""
Jan 28 01:48:44.920: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/28/23 01:48:44.92
Jan 28 01:48:44.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 28 01:48:45.059: INFO: stderr: ""
Jan 28 01:48:45.059: INFO: stdout: "update-demo-nautilus-b4rvg update-demo-nautilus-brlvz "
Jan 28 01:48:45.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 01:48:45.154: INFO: stderr: ""
Jan 28 01:48:45.154: INFO: stdout: ""
Jan 28 01:48:45.154: INFO: update-demo-nautilus-b4rvg is created but not running
Jan 28 01:48:50.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 28 01:48:50.263: INFO: stderr: ""
Jan 28 01:48:50.263: INFO: stdout: "update-demo-nautilus-b4rvg update-demo-nautilus-brlvz "
Jan 28 01:48:50.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 01:48:50.372: INFO: stderr: ""
Jan 28 01:48:50.372: INFO: stdout: ""
Jan 28 01:48:50.372: INFO: update-demo-nautilus-b4rvg is created but not running
Jan 28 01:48:55.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 28 01:48:55.493: INFO: stderr: ""
Jan 28 01:48:55.493: INFO: stdout: "update-demo-nautilus-b4rvg update-demo-nautilus-brlvz "
Jan 28 01:48:55.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 01:48:55.588: INFO: stderr: ""
Jan 28 01:48:55.588: INFO: stdout: ""
Jan 28 01:48:55.588: INFO: update-demo-nautilus-b4rvg is created but not running
Jan 28 01:49:00.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 28 01:49:00.710: INFO: stderr: ""
Jan 28 01:49:00.710: INFO: stdout: "update-demo-nautilus-b4rvg update-demo-nautilus-brlvz "
Jan 28 01:49:00.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 01:49:00.819: INFO: stderr: ""
Jan 28 01:49:00.819: INFO: stdout: "true"
Jan 28 01:49:00.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 28 01:49:00.919: INFO: stderr: ""
Jan 28 01:49:00.919: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 28 01:49:00.919: INFO: validating pod update-demo-nautilus-b4rvg
Jan 28 01:49:00.970: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 28 01:49:00.970: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 28 01:49:00.970: INFO: update-demo-nautilus-b4rvg is verified up and running
Jan 28 01:49:00.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-brlvz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 01:49:01.085: INFO: stderr: ""
Jan 28 01:49:01.085: INFO: stdout: ""
Jan 28 01:49:01.085: INFO: update-demo-nautilus-brlvz is created but not running
Jan 28 01:49:06.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 28 01:49:06.221: INFO: stderr: ""
Jan 28 01:49:06.221: INFO: stdout: "update-demo-nautilus-b4rvg update-demo-nautilus-brlvz "
Jan 28 01:49:06.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 01:49:06.325: INFO: stderr: ""
Jan 28 01:49:06.325: INFO: stdout: "true"
Jan 28 01:49:06.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 28 01:49:06.413: INFO: stderr: ""
Jan 28 01:49:06.414: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 28 01:49:06.414: INFO: validating pod update-demo-nautilus-b4rvg
Jan 28 01:49:06.433: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 28 01:49:06.433: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 28 01:49:06.433: INFO: update-demo-nautilus-b4rvg is verified up and running
Jan 28 01:49:06.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-brlvz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 01:49:06.549: INFO: stderr: ""
Jan 28 01:49:06.549: INFO: stdout: "true"
Jan 28 01:49:06.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-brlvz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 28 01:49:06.648: INFO: stderr: ""
Jan 28 01:49:06.648: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 28 01:49:06.648: INFO: validating pod update-demo-nautilus-brlvz
Jan 28 01:49:06.719: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 28 01:49:06.719: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 28 01:49:06.719: INFO: update-demo-nautilus-brlvz is verified up and running
STEP: scaling down the replication controller 01/28/23 01:49:06.719
Jan 28 01:49:06.723: INFO: scanned /root for discovery docs: <nil>
Jan 28 01:49:06.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 28 01:49:07.886: INFO: stderr: ""
Jan 28 01:49:07.886: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/28/23 01:49:07.886
Jan 28 01:49:07.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 28 01:49:08.000: INFO: stderr: ""
Jan 28 01:49:08.000: INFO: stdout: "update-demo-nautilus-b4rvg update-demo-nautilus-brlvz "
STEP: Replicas for name=update-demo: expected=1 actual=2 01/28/23 01:49:08
Jan 28 01:49:13.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 28 01:49:13.116: INFO: stderr: ""
Jan 28 01:49:13.116: INFO: stdout: "update-demo-nautilus-b4rvg "
Jan 28 01:49:13.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 01:49:13.238: INFO: stderr: ""
Jan 28 01:49:13.238: INFO: stdout: "true"
Jan 28 01:49:13.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 28 01:49:13.350: INFO: stderr: ""
Jan 28 01:49:13.351: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 28 01:49:13.351: INFO: validating pod update-demo-nautilus-b4rvg
Jan 28 01:49:13.367: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 28 01:49:13.367: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 28 01:49:13.367: INFO: update-demo-nautilus-b4rvg is verified up and running
STEP: scaling up the replication controller 01/28/23 01:49:13.367
Jan 28 01:49:13.372: INFO: scanned /root for discovery docs: <nil>
Jan 28 01:49:13.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 28 01:49:14.543: INFO: stderr: ""
Jan 28 01:49:14.543: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/28/23 01:49:14.543
Jan 28 01:49:14.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 28 01:49:14.664: INFO: stderr: ""
Jan 28 01:49:14.665: INFO: stdout: "update-demo-nautilus-b4rvg update-demo-nautilus-xbntw "
Jan 28 01:49:14.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 01:49:14.766: INFO: stderr: ""
Jan 28 01:49:14.766: INFO: stdout: "true"
Jan 28 01:49:14.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 28 01:49:14.862: INFO: stderr: ""
Jan 28 01:49:14.862: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 28 01:49:14.862: INFO: validating pod update-demo-nautilus-b4rvg
Jan 28 01:49:14.879: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 28 01:49:14.879: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 28 01:49:14.879: INFO: update-demo-nautilus-b4rvg is verified up and running
Jan 28 01:49:14.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-xbntw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 01:49:14.991: INFO: stderr: ""
Jan 28 01:49:14.991: INFO: stdout: ""
Jan 28 01:49:14.991: INFO: update-demo-nautilus-xbntw is created but not running
Jan 28 01:49:19.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 28 01:49:20.126: INFO: stderr: ""
Jan 28 01:49:20.126: INFO: stdout: "update-demo-nautilus-b4rvg update-demo-nautilus-xbntw "
Jan 28 01:49:20.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 01:49:20.233: INFO: stderr: ""
Jan 28 01:49:20.233: INFO: stdout: "true"
Jan 28 01:49:20.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 28 01:49:20.351: INFO: stderr: ""
Jan 28 01:49:20.351: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 28 01:49:20.351: INFO: validating pod update-demo-nautilus-b4rvg
Jan 28 01:49:20.371: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 28 01:49:20.371: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 28 01:49:20.371: INFO: update-demo-nautilus-b4rvg is verified up and running
Jan 28 01:49:20.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-xbntw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 01:49:20.483: INFO: stderr: ""
Jan 28 01:49:20.483: INFO: stdout: "true"
Jan 28 01:49:20.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-xbntw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 28 01:49:20.591: INFO: stderr: ""
Jan 28 01:49:20.591: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 28 01:49:20.591: INFO: validating pod update-demo-nautilus-xbntw
Jan 28 01:49:20.630: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 28 01:49:20.630: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 28 01:49:20.630: INFO: update-demo-nautilus-xbntw is verified up and running
STEP: using delete to clean up resources 01/28/23 01:49:20.63
Jan 28 01:49:20.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 delete --grace-period=0 --force -f -'
Jan 28 01:49:20.771: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 28 01:49:20.771: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 28 01:49:20.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get rc,svc -l name=update-demo --no-headers'
Jan 28 01:49:20.894: INFO: stderr: "No resources found in kubectl-3569 namespace.\n"
Jan 28 01:49:20.894: INFO: stdout: ""
Jan 28 01:49:20.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 28 01:49:20.987: INFO: stderr: ""
Jan 28 01:49:20.987: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 28 01:49:20.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3569" for this suite. 01/28/23 01:49:21.008
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":309,"skipped":5689,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.510 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:48:44.521
    Jan 28 01:48:44.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubectl 01/28/23 01:48:44.523
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:48:44.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:48:44.589
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 01/28/23 01:48:44.605
    Jan 28 01:48:44.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 create -f -'
    Jan 28 01:48:44.920: INFO: stderr: ""
    Jan 28 01:48:44.920: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/28/23 01:48:44.92
    Jan 28 01:48:44.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 28 01:48:45.059: INFO: stderr: ""
    Jan 28 01:48:45.059: INFO: stdout: "update-demo-nautilus-b4rvg update-demo-nautilus-brlvz "
    Jan 28 01:48:45.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 28 01:48:45.154: INFO: stderr: ""
    Jan 28 01:48:45.154: INFO: stdout: ""
    Jan 28 01:48:45.154: INFO: update-demo-nautilus-b4rvg is created but not running
    Jan 28 01:48:50.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 28 01:48:50.263: INFO: stderr: ""
    Jan 28 01:48:50.263: INFO: stdout: "update-demo-nautilus-b4rvg update-demo-nautilus-brlvz "
    Jan 28 01:48:50.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 28 01:48:50.372: INFO: stderr: ""
    Jan 28 01:48:50.372: INFO: stdout: ""
    Jan 28 01:48:50.372: INFO: update-demo-nautilus-b4rvg is created but not running
    Jan 28 01:48:55.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 28 01:48:55.493: INFO: stderr: ""
    Jan 28 01:48:55.493: INFO: stdout: "update-demo-nautilus-b4rvg update-demo-nautilus-brlvz "
    Jan 28 01:48:55.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 28 01:48:55.588: INFO: stderr: ""
    Jan 28 01:48:55.588: INFO: stdout: ""
    Jan 28 01:48:55.588: INFO: update-demo-nautilus-b4rvg is created but not running
    Jan 28 01:49:00.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 28 01:49:00.710: INFO: stderr: ""
    Jan 28 01:49:00.710: INFO: stdout: "update-demo-nautilus-b4rvg update-demo-nautilus-brlvz "
    Jan 28 01:49:00.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 28 01:49:00.819: INFO: stderr: ""
    Jan 28 01:49:00.819: INFO: stdout: "true"
    Jan 28 01:49:00.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 28 01:49:00.919: INFO: stderr: ""
    Jan 28 01:49:00.919: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 28 01:49:00.919: INFO: validating pod update-demo-nautilus-b4rvg
    Jan 28 01:49:00.970: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 28 01:49:00.970: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 28 01:49:00.970: INFO: update-demo-nautilus-b4rvg is verified up and running
    Jan 28 01:49:00.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-brlvz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 28 01:49:01.085: INFO: stderr: ""
    Jan 28 01:49:01.085: INFO: stdout: ""
    Jan 28 01:49:01.085: INFO: update-demo-nautilus-brlvz is created but not running
    Jan 28 01:49:06.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 28 01:49:06.221: INFO: stderr: ""
    Jan 28 01:49:06.221: INFO: stdout: "update-demo-nautilus-b4rvg update-demo-nautilus-brlvz "
    Jan 28 01:49:06.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 28 01:49:06.325: INFO: stderr: ""
    Jan 28 01:49:06.325: INFO: stdout: "true"
    Jan 28 01:49:06.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 28 01:49:06.413: INFO: stderr: ""
    Jan 28 01:49:06.414: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 28 01:49:06.414: INFO: validating pod update-demo-nautilus-b4rvg
    Jan 28 01:49:06.433: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 28 01:49:06.433: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 28 01:49:06.433: INFO: update-demo-nautilus-b4rvg is verified up and running
    Jan 28 01:49:06.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-brlvz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 28 01:49:06.549: INFO: stderr: ""
    Jan 28 01:49:06.549: INFO: stdout: "true"
    Jan 28 01:49:06.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-brlvz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 28 01:49:06.648: INFO: stderr: ""
    Jan 28 01:49:06.648: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 28 01:49:06.648: INFO: validating pod update-demo-nautilus-brlvz
    Jan 28 01:49:06.719: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 28 01:49:06.719: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 28 01:49:06.719: INFO: update-demo-nautilus-brlvz is verified up and running
    STEP: scaling down the replication controller 01/28/23 01:49:06.719
    Jan 28 01:49:06.723: INFO: scanned /root for discovery docs: <nil>
    Jan 28 01:49:06.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan 28 01:49:07.886: INFO: stderr: ""
    Jan 28 01:49:07.886: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/28/23 01:49:07.886
    Jan 28 01:49:07.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 28 01:49:08.000: INFO: stderr: ""
    Jan 28 01:49:08.000: INFO: stdout: "update-demo-nautilus-b4rvg update-demo-nautilus-brlvz "
    STEP: Replicas for name=update-demo: expected=1 actual=2 01/28/23 01:49:08
    Jan 28 01:49:13.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 28 01:49:13.116: INFO: stderr: ""
    Jan 28 01:49:13.116: INFO: stdout: "update-demo-nautilus-b4rvg "
    Jan 28 01:49:13.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 28 01:49:13.238: INFO: stderr: ""
    Jan 28 01:49:13.238: INFO: stdout: "true"
    Jan 28 01:49:13.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 28 01:49:13.350: INFO: stderr: ""
    Jan 28 01:49:13.351: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 28 01:49:13.351: INFO: validating pod update-demo-nautilus-b4rvg
    Jan 28 01:49:13.367: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 28 01:49:13.367: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 28 01:49:13.367: INFO: update-demo-nautilus-b4rvg is verified up and running
    STEP: scaling up the replication controller 01/28/23 01:49:13.367
    Jan 28 01:49:13.372: INFO: scanned /root for discovery docs: <nil>
    Jan 28 01:49:13.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan 28 01:49:14.543: INFO: stderr: ""
    Jan 28 01:49:14.543: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/28/23 01:49:14.543
    Jan 28 01:49:14.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 28 01:49:14.664: INFO: stderr: ""
    Jan 28 01:49:14.665: INFO: stdout: "update-demo-nautilus-b4rvg update-demo-nautilus-xbntw "
    Jan 28 01:49:14.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 28 01:49:14.766: INFO: stderr: ""
    Jan 28 01:49:14.766: INFO: stdout: "true"
    Jan 28 01:49:14.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 28 01:49:14.862: INFO: stderr: ""
    Jan 28 01:49:14.862: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 28 01:49:14.862: INFO: validating pod update-demo-nautilus-b4rvg
    Jan 28 01:49:14.879: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 28 01:49:14.879: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 28 01:49:14.879: INFO: update-demo-nautilus-b4rvg is verified up and running
    Jan 28 01:49:14.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-xbntw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 28 01:49:14.991: INFO: stderr: ""
    Jan 28 01:49:14.991: INFO: stdout: ""
    Jan 28 01:49:14.991: INFO: update-demo-nautilus-xbntw is created but not running
    Jan 28 01:49:19.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 28 01:49:20.126: INFO: stderr: ""
    Jan 28 01:49:20.126: INFO: stdout: "update-demo-nautilus-b4rvg update-demo-nautilus-xbntw "
    Jan 28 01:49:20.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 28 01:49:20.233: INFO: stderr: ""
    Jan 28 01:49:20.233: INFO: stdout: "true"
    Jan 28 01:49:20.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-b4rvg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 28 01:49:20.351: INFO: stderr: ""
    Jan 28 01:49:20.351: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 28 01:49:20.351: INFO: validating pod update-demo-nautilus-b4rvg
    Jan 28 01:49:20.371: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 28 01:49:20.371: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 28 01:49:20.371: INFO: update-demo-nautilus-b4rvg is verified up and running
    Jan 28 01:49:20.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-xbntw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 28 01:49:20.483: INFO: stderr: ""
    Jan 28 01:49:20.483: INFO: stdout: "true"
    Jan 28 01:49:20.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods update-demo-nautilus-xbntw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 28 01:49:20.591: INFO: stderr: ""
    Jan 28 01:49:20.591: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 28 01:49:20.591: INFO: validating pod update-demo-nautilus-xbntw
    Jan 28 01:49:20.630: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 28 01:49:20.630: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 28 01:49:20.630: INFO: update-demo-nautilus-xbntw is verified up and running
    STEP: using delete to clean up resources 01/28/23 01:49:20.63
    Jan 28 01:49:20.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 delete --grace-period=0 --force -f -'
    Jan 28 01:49:20.771: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 28 01:49:20.771: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 28 01:49:20.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get rc,svc -l name=update-demo --no-headers'
    Jan 28 01:49:20.894: INFO: stderr: "No resources found in kubectl-3569 namespace.\n"
    Jan 28 01:49:20.894: INFO: stdout: ""
    Jan 28 01:49:20.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3569 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 28 01:49:20.987: INFO: stderr: ""
    Jan 28 01:49:20.987: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 28 01:49:20.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3569" for this suite. 01/28/23 01:49:21.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:49:21.034
Jan 28 01:49:21.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename svcaccounts 01/28/23 01:49:21.035
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:49:21.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:49:21.093
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Jan 28 01:49:21.123: INFO: Got root ca configmap in namespace "svcaccounts-2736"
Jan 28 01:49:21.143: INFO: Deleted root ca configmap in namespace "svcaccounts-2736"
STEP: waiting for a new root ca configmap created 01/28/23 01:49:21.644
Jan 28 01:49:21.661: INFO: Recreated root ca configmap in namespace "svcaccounts-2736"
Jan 28 01:49:21.673: INFO: Updated root ca configmap in namespace "svcaccounts-2736"
STEP: waiting for the root ca configmap reconciled 01/28/23 01:49:22.174
Jan 28 01:49:22.186: INFO: Reconciled root ca configmap in namespace "svcaccounts-2736"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 28 01:49:22.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2736" for this suite. 01/28/23 01:49:22.208
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":310,"skipped":5711,"failed":0}
------------------------------
â€¢ [1.204 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:49:21.034
    Jan 28 01:49:21.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename svcaccounts 01/28/23 01:49:21.035
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:49:21.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:49:21.093
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Jan 28 01:49:21.123: INFO: Got root ca configmap in namespace "svcaccounts-2736"
    Jan 28 01:49:21.143: INFO: Deleted root ca configmap in namespace "svcaccounts-2736"
    STEP: waiting for a new root ca configmap created 01/28/23 01:49:21.644
    Jan 28 01:49:21.661: INFO: Recreated root ca configmap in namespace "svcaccounts-2736"
    Jan 28 01:49:21.673: INFO: Updated root ca configmap in namespace "svcaccounts-2736"
    STEP: waiting for the root ca configmap reconciled 01/28/23 01:49:22.174
    Jan 28 01:49:22.186: INFO: Reconciled root ca configmap in namespace "svcaccounts-2736"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 28 01:49:22.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2736" for this suite. 01/28/23 01:49:22.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:49:22.239
Jan 28 01:49:22.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename conformance-tests 01/28/23 01:49:22.242
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:49:22.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:49:22.302
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/28/23 01:49:22.314
Jan 28 01:49:22.314: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Jan 28 01:49:22.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-7554" for this suite. 01/28/23 01:49:22.362
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":311,"skipped":5721,"failed":0}
------------------------------
â€¢ [0.140 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:49:22.239
    Jan 28 01:49:22.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename conformance-tests 01/28/23 01:49:22.242
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:49:22.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:49:22.302
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/28/23 01:49:22.314
    Jan 28 01:49:22.314: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Jan 28 01:49:22.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-7554" for this suite. 01/28/23 01:49:22.362
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:49:22.388
Jan 28 01:49:22.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 01:49:22.39
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:49:22.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:49:22.442
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 01/28/23 01:49:22.458
Jan 28 01:49:22.486: INFO: Waiting up to 5m0s for pod "downwardapi-volume-96da4069-03a6-4901-aafd-0a3052eb7805" in namespace "projected-4485" to be "Succeeded or Failed"
Jan 28 01:49:22.497: INFO: Pod "downwardapi-volume-96da4069-03a6-4901-aafd-0a3052eb7805": Phase="Pending", Reason="", readiness=false. Elapsed: 10.420618ms
Jan 28 01:49:24.510: INFO: Pod "downwardapi-volume-96da4069-03a6-4901-aafd-0a3052eb7805": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023368498s
Jan 28 01:49:26.508: INFO: Pod "downwardapi-volume-96da4069-03a6-4901-aafd-0a3052eb7805": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021791387s
STEP: Saw pod success 01/28/23 01:49:26.508
Jan 28 01:49:26.509: INFO: Pod "downwardapi-volume-96da4069-03a6-4901-aafd-0a3052eb7805" satisfied condition "Succeeded or Failed"
Jan 28 01:49:26.522: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-96da4069-03a6-4901-aafd-0a3052eb7805 container client-container: <nil>
STEP: delete the pod 01/28/23 01:49:26.55
Jan 28 01:49:26.590: INFO: Waiting for pod downwardapi-volume-96da4069-03a6-4901-aafd-0a3052eb7805 to disappear
Jan 28 01:49:26.602: INFO: Pod downwardapi-volume-96da4069-03a6-4901-aafd-0a3052eb7805 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 28 01:49:26.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4485" for this suite. 01/28/23 01:49:26.621
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":312,"skipped":5802,"failed":0}
------------------------------
â€¢ [4.253 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:49:22.388
    Jan 28 01:49:22.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 01:49:22.39
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:49:22.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:49:22.442
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 01/28/23 01:49:22.458
    Jan 28 01:49:22.486: INFO: Waiting up to 5m0s for pod "downwardapi-volume-96da4069-03a6-4901-aafd-0a3052eb7805" in namespace "projected-4485" to be "Succeeded or Failed"
    Jan 28 01:49:22.497: INFO: Pod "downwardapi-volume-96da4069-03a6-4901-aafd-0a3052eb7805": Phase="Pending", Reason="", readiness=false. Elapsed: 10.420618ms
    Jan 28 01:49:24.510: INFO: Pod "downwardapi-volume-96da4069-03a6-4901-aafd-0a3052eb7805": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023368498s
    Jan 28 01:49:26.508: INFO: Pod "downwardapi-volume-96da4069-03a6-4901-aafd-0a3052eb7805": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021791387s
    STEP: Saw pod success 01/28/23 01:49:26.508
    Jan 28 01:49:26.509: INFO: Pod "downwardapi-volume-96da4069-03a6-4901-aafd-0a3052eb7805" satisfied condition "Succeeded or Failed"
    Jan 28 01:49:26.522: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-96da4069-03a6-4901-aafd-0a3052eb7805 container client-container: <nil>
    STEP: delete the pod 01/28/23 01:49:26.55
    Jan 28 01:49:26.590: INFO: Waiting for pod downwardapi-volume-96da4069-03a6-4901-aafd-0a3052eb7805 to disappear
    Jan 28 01:49:26.602: INFO: Pod downwardapi-volume-96da4069-03a6-4901-aafd-0a3052eb7805 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 28 01:49:26.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4485" for this suite. 01/28/23 01:49:26.621
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:49:26.641
Jan 28 01:49:26.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename gc 01/28/23 01:49:26.644
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:49:26.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:49:26.705
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/28/23 01:49:26.737
STEP: create the rc2 01/28/23 01:49:26.761
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/28/23 01:49:31.792
STEP: delete the rc simpletest-rc-to-be-deleted 01/28/23 01:49:32.755
STEP: wait for the rc to be deleted 01/28/23 01:49:32.776
STEP: Gathering metrics 01/28/23 01:49:37.854
W0128 01:49:37.912242      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 28 01:49:37.912: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 28 01:49:37.912: INFO: Deleting pod "simpletest-rc-to-be-deleted-2cdsd" in namespace "gc-2127"
Jan 28 01:49:37.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lncs" in namespace "gc-2127"
Jan 28 01:49:38.014: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wmkx" in namespace "gc-2127"
Jan 28 01:49:38.072: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xnm5" in namespace "gc-2127"
Jan 28 01:49:38.098: INFO: Deleting pod "simpletest-rc-to-be-deleted-4d2gq" in namespace "gc-2127"
Jan 28 01:49:38.118: INFO: Deleting pod "simpletest-rc-to-be-deleted-55dwd" in namespace "gc-2127"
Jan 28 01:49:38.140: INFO: Deleting pod "simpletest-rc-to-be-deleted-5lctz" in namespace "gc-2127"
Jan 28 01:49:38.191: INFO: Deleting pod "simpletest-rc-to-be-deleted-66jqj" in namespace "gc-2127"
Jan 28 01:49:38.221: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rxrt" in namespace "gc-2127"
Jan 28 01:49:38.244: INFO: Deleting pod "simpletest-rc-to-be-deleted-77k2c" in namespace "gc-2127"
Jan 28 01:49:38.294: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bm8t" in namespace "gc-2127"
Jan 28 01:49:38.323: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bncm" in namespace "gc-2127"
Jan 28 01:49:38.343: INFO: Deleting pod "simpletest-rc-to-be-deleted-88q2s" in namespace "gc-2127"
Jan 28 01:49:38.362: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bhrq" in namespace "gc-2127"
Jan 28 01:49:38.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-9z625" in namespace "gc-2127"
Jan 28 01:49:38.425: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5l4t" in namespace "gc-2127"
Jan 28 01:49:38.448: INFO: Deleting pod "simpletest-rc-to-be-deleted-b64f6" in namespace "gc-2127"
Jan 28 01:49:38.476: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgpft" in namespace "gc-2127"
Jan 28 01:49:38.496: INFO: Deleting pod "simpletest-rc-to-be-deleted-bks48" in namespace "gc-2127"
Jan 28 01:49:38.521: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7rpf" in namespace "gc-2127"
Jan 28 01:49:38.544: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmtlq" in namespace "gc-2127"
Jan 28 01:49:38.637: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqplb" in namespace "gc-2127"
Jan 28 01:49:38.656: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvr9v" in namespace "gc-2127"
Jan 28 01:49:38.679: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6l9g" in namespace "gc-2127"
Jan 28 01:49:38.709: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7n49" in namespace "gc-2127"
Jan 28 01:49:38.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-f586b" in namespace "gc-2127"
Jan 28 01:49:38.754: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhb29" in namespace "gc-2127"
Jan 28 01:49:38.812: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjh27" in namespace "gc-2127"
Jan 28 01:49:38.835: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnvf9" in namespace "gc-2127"
Jan 28 01:49:38.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqhsp" in namespace "gc-2127"
Jan 28 01:49:38.906: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxmd6" in namespace "gc-2127"
Jan 28 01:49:38.948: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5dws" in namespace "gc-2127"
Jan 28 01:49:39.001: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9h9w" in namespace "gc-2127"
Jan 28 01:49:39.024: INFO: Deleting pod "simpletest-rc-to-be-deleted-hbgsg" in namespace "gc-2127"
Jan 28 01:49:39.067: INFO: Deleting pod "simpletest-rc-to-be-deleted-hblrs" in namespace "gc-2127"
Jan 28 01:49:39.097: INFO: Deleting pod "simpletest-rc-to-be-deleted-hd7t5" in namespace "gc-2127"
Jan 28 01:49:39.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-hkh6t" in namespace "gc-2127"
Jan 28 01:49:39.146: INFO: Deleting pod "simpletest-rc-to-be-deleted-hp2h8" in namespace "gc-2127"
Jan 28 01:49:39.169: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqkr4" in namespace "gc-2127"
Jan 28 01:49:39.224: INFO: Deleting pod "simpletest-rc-to-be-deleted-jd55m" in namespace "gc-2127"
Jan 28 01:49:39.241: INFO: Deleting pod "simpletest-rc-to-be-deleted-jfx89" in namespace "gc-2127"
Jan 28 01:49:39.269: INFO: Deleting pod "simpletest-rc-to-be-deleted-jnv5h" in namespace "gc-2127"
Jan 28 01:49:39.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-jnxfx" in namespace "gc-2127"
Jan 28 01:49:39.346: INFO: Deleting pod "simpletest-rc-to-be-deleted-kbbpm" in namespace "gc-2127"
Jan 28 01:49:39.369: INFO: Deleting pod "simpletest-rc-to-be-deleted-kfcsh" in namespace "gc-2127"
Jan 28 01:49:39.432: INFO: Deleting pod "simpletest-rc-to-be-deleted-khcxm" in namespace "gc-2127"
Jan 28 01:49:39.484: INFO: Deleting pod "simpletest-rc-to-be-deleted-kznb9" in namespace "gc-2127"
Jan 28 01:49:39.572: INFO: Deleting pod "simpletest-rc-to-be-deleted-lhtzq" in namespace "gc-2127"
Jan 28 01:49:39.608: INFO: Deleting pod "simpletest-rc-to-be-deleted-lj95j" in namespace "gc-2127"
Jan 28 01:49:39.655: INFO: Deleting pod "simpletest-rc-to-be-deleted-ljj6q" in namespace "gc-2127"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 28 01:49:39.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2127" for this suite. 01/28/23 01:49:39.694
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":313,"skipped":5803,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.076 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:49:26.641
    Jan 28 01:49:26.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename gc 01/28/23 01:49:26.644
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:49:26.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:49:26.705
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/28/23 01:49:26.737
    STEP: create the rc2 01/28/23 01:49:26.761
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/28/23 01:49:31.792
    STEP: delete the rc simpletest-rc-to-be-deleted 01/28/23 01:49:32.755
    STEP: wait for the rc to be deleted 01/28/23 01:49:32.776
    STEP: Gathering metrics 01/28/23 01:49:37.854
    W0128 01:49:37.912242      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 28 01:49:37.912: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 28 01:49:37.912: INFO: Deleting pod "simpletest-rc-to-be-deleted-2cdsd" in namespace "gc-2127"
    Jan 28 01:49:37.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lncs" in namespace "gc-2127"
    Jan 28 01:49:38.014: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wmkx" in namespace "gc-2127"
    Jan 28 01:49:38.072: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xnm5" in namespace "gc-2127"
    Jan 28 01:49:38.098: INFO: Deleting pod "simpletest-rc-to-be-deleted-4d2gq" in namespace "gc-2127"
    Jan 28 01:49:38.118: INFO: Deleting pod "simpletest-rc-to-be-deleted-55dwd" in namespace "gc-2127"
    Jan 28 01:49:38.140: INFO: Deleting pod "simpletest-rc-to-be-deleted-5lctz" in namespace "gc-2127"
    Jan 28 01:49:38.191: INFO: Deleting pod "simpletest-rc-to-be-deleted-66jqj" in namespace "gc-2127"
    Jan 28 01:49:38.221: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rxrt" in namespace "gc-2127"
    Jan 28 01:49:38.244: INFO: Deleting pod "simpletest-rc-to-be-deleted-77k2c" in namespace "gc-2127"
    Jan 28 01:49:38.294: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bm8t" in namespace "gc-2127"
    Jan 28 01:49:38.323: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bncm" in namespace "gc-2127"
    Jan 28 01:49:38.343: INFO: Deleting pod "simpletest-rc-to-be-deleted-88q2s" in namespace "gc-2127"
    Jan 28 01:49:38.362: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bhrq" in namespace "gc-2127"
    Jan 28 01:49:38.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-9z625" in namespace "gc-2127"
    Jan 28 01:49:38.425: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5l4t" in namespace "gc-2127"
    Jan 28 01:49:38.448: INFO: Deleting pod "simpletest-rc-to-be-deleted-b64f6" in namespace "gc-2127"
    Jan 28 01:49:38.476: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgpft" in namespace "gc-2127"
    Jan 28 01:49:38.496: INFO: Deleting pod "simpletest-rc-to-be-deleted-bks48" in namespace "gc-2127"
    Jan 28 01:49:38.521: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7rpf" in namespace "gc-2127"
    Jan 28 01:49:38.544: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmtlq" in namespace "gc-2127"
    Jan 28 01:49:38.637: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqplb" in namespace "gc-2127"
    Jan 28 01:49:38.656: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvr9v" in namespace "gc-2127"
    Jan 28 01:49:38.679: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6l9g" in namespace "gc-2127"
    Jan 28 01:49:38.709: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7n49" in namespace "gc-2127"
    Jan 28 01:49:38.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-f586b" in namespace "gc-2127"
    Jan 28 01:49:38.754: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhb29" in namespace "gc-2127"
    Jan 28 01:49:38.812: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjh27" in namespace "gc-2127"
    Jan 28 01:49:38.835: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnvf9" in namespace "gc-2127"
    Jan 28 01:49:38.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqhsp" in namespace "gc-2127"
    Jan 28 01:49:38.906: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxmd6" in namespace "gc-2127"
    Jan 28 01:49:38.948: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5dws" in namespace "gc-2127"
    Jan 28 01:49:39.001: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9h9w" in namespace "gc-2127"
    Jan 28 01:49:39.024: INFO: Deleting pod "simpletest-rc-to-be-deleted-hbgsg" in namespace "gc-2127"
    Jan 28 01:49:39.067: INFO: Deleting pod "simpletest-rc-to-be-deleted-hblrs" in namespace "gc-2127"
    Jan 28 01:49:39.097: INFO: Deleting pod "simpletest-rc-to-be-deleted-hd7t5" in namespace "gc-2127"
    Jan 28 01:49:39.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-hkh6t" in namespace "gc-2127"
    Jan 28 01:49:39.146: INFO: Deleting pod "simpletest-rc-to-be-deleted-hp2h8" in namespace "gc-2127"
    Jan 28 01:49:39.169: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqkr4" in namespace "gc-2127"
    Jan 28 01:49:39.224: INFO: Deleting pod "simpletest-rc-to-be-deleted-jd55m" in namespace "gc-2127"
    Jan 28 01:49:39.241: INFO: Deleting pod "simpletest-rc-to-be-deleted-jfx89" in namespace "gc-2127"
    Jan 28 01:49:39.269: INFO: Deleting pod "simpletest-rc-to-be-deleted-jnv5h" in namespace "gc-2127"
    Jan 28 01:49:39.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-jnxfx" in namespace "gc-2127"
    Jan 28 01:49:39.346: INFO: Deleting pod "simpletest-rc-to-be-deleted-kbbpm" in namespace "gc-2127"
    Jan 28 01:49:39.369: INFO: Deleting pod "simpletest-rc-to-be-deleted-kfcsh" in namespace "gc-2127"
    Jan 28 01:49:39.432: INFO: Deleting pod "simpletest-rc-to-be-deleted-khcxm" in namespace "gc-2127"
    Jan 28 01:49:39.484: INFO: Deleting pod "simpletest-rc-to-be-deleted-kznb9" in namespace "gc-2127"
    Jan 28 01:49:39.572: INFO: Deleting pod "simpletest-rc-to-be-deleted-lhtzq" in namespace "gc-2127"
    Jan 28 01:49:39.608: INFO: Deleting pod "simpletest-rc-to-be-deleted-lj95j" in namespace "gc-2127"
    Jan 28 01:49:39.655: INFO: Deleting pod "simpletest-rc-to-be-deleted-ljj6q" in namespace "gc-2127"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 28 01:49:39.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2127" for this suite. 01/28/23 01:49:39.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:49:39.719
Jan 28 01:49:39.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename containers 01/28/23 01:49:39.725
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:49:39.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:49:39.778
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 01/28/23 01:49:39.787
Jan 28 01:49:39.803: INFO: Waiting up to 5m0s for pod "client-containers-1a0ca9c1-5322-48b4-8d3e-b5dc2728c795" in namespace "containers-1761" to be "Succeeded or Failed"
Jan 28 01:49:39.814: INFO: Pod "client-containers-1a0ca9c1-5322-48b4-8d3e-b5dc2728c795": Phase="Pending", Reason="", readiness=false. Elapsed: 10.926836ms
Jan 28 01:49:41.828: INFO: Pod "client-containers-1a0ca9c1-5322-48b4-8d3e-b5dc2728c795": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024151897s
Jan 28 01:49:43.830: INFO: Pod "client-containers-1a0ca9c1-5322-48b4-8d3e-b5dc2728c795": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026790515s
Jan 28 01:49:45.852: INFO: Pod "client-containers-1a0ca9c1-5322-48b4-8d3e-b5dc2728c795": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048954066s
STEP: Saw pod success 01/28/23 01:49:45.852
Jan 28 01:49:45.853: INFO: Pod "client-containers-1a0ca9c1-5322-48b4-8d3e-b5dc2728c795" satisfied condition "Succeeded or Failed"
Jan 28 01:49:45.864: INFO: Trying to get logs from node 10.9.20.126 pod client-containers-1a0ca9c1-5322-48b4-8d3e-b5dc2728c795 container agnhost-container: <nil>
STEP: delete the pod 01/28/23 01:49:45.894
Jan 28 01:49:45.946: INFO: Waiting for pod client-containers-1a0ca9c1-5322-48b4-8d3e-b5dc2728c795 to disappear
Jan 28 01:49:45.956: INFO: Pod client-containers-1a0ca9c1-5322-48b4-8d3e-b5dc2728c795 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 28 01:49:45.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1761" for this suite. 01/28/23 01:49:45.97
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":314,"skipped":5813,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.305 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:49:39.719
    Jan 28 01:49:39.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename containers 01/28/23 01:49:39.725
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:49:39.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:49:39.778
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 01/28/23 01:49:39.787
    Jan 28 01:49:39.803: INFO: Waiting up to 5m0s for pod "client-containers-1a0ca9c1-5322-48b4-8d3e-b5dc2728c795" in namespace "containers-1761" to be "Succeeded or Failed"
    Jan 28 01:49:39.814: INFO: Pod "client-containers-1a0ca9c1-5322-48b4-8d3e-b5dc2728c795": Phase="Pending", Reason="", readiness=false. Elapsed: 10.926836ms
    Jan 28 01:49:41.828: INFO: Pod "client-containers-1a0ca9c1-5322-48b4-8d3e-b5dc2728c795": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024151897s
    Jan 28 01:49:43.830: INFO: Pod "client-containers-1a0ca9c1-5322-48b4-8d3e-b5dc2728c795": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026790515s
    Jan 28 01:49:45.852: INFO: Pod "client-containers-1a0ca9c1-5322-48b4-8d3e-b5dc2728c795": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048954066s
    STEP: Saw pod success 01/28/23 01:49:45.852
    Jan 28 01:49:45.853: INFO: Pod "client-containers-1a0ca9c1-5322-48b4-8d3e-b5dc2728c795" satisfied condition "Succeeded or Failed"
    Jan 28 01:49:45.864: INFO: Trying to get logs from node 10.9.20.126 pod client-containers-1a0ca9c1-5322-48b4-8d3e-b5dc2728c795 container agnhost-container: <nil>
    STEP: delete the pod 01/28/23 01:49:45.894
    Jan 28 01:49:45.946: INFO: Waiting for pod client-containers-1a0ca9c1-5322-48b4-8d3e-b5dc2728c795 to disappear
    Jan 28 01:49:45.956: INFO: Pod client-containers-1a0ca9c1-5322-48b4-8d3e-b5dc2728c795 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 28 01:49:45.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-1761" for this suite. 01/28/23 01:49:45.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:49:46.029
Jan 28 01:49:46.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename configmap 01/28/23 01:49:46.03
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:49:46.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:49:46.116
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-573/configmap-test-af999dd9-6611-4ff2-a774-bbba1fbc9e89 01/28/23 01:49:46.132
STEP: Creating a pod to test consume configMaps 01/28/23 01:49:46.146
Jan 28 01:49:46.162: INFO: Waiting up to 5m0s for pod "pod-configmaps-a259fa59-7e95-461b-9cd0-ce1c26474267" in namespace "configmap-573" to be "Succeeded or Failed"
Jan 28 01:49:46.175: INFO: Pod "pod-configmaps-a259fa59-7e95-461b-9cd0-ce1c26474267": Phase="Pending", Reason="", readiness=false. Elapsed: 12.653555ms
Jan 28 01:49:48.188: INFO: Pod "pod-configmaps-a259fa59-7e95-461b-9cd0-ce1c26474267": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02632273s
Jan 28 01:49:50.187: INFO: Pod "pod-configmaps-a259fa59-7e95-461b-9cd0-ce1c26474267": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024480924s
STEP: Saw pod success 01/28/23 01:49:50.187
Jan 28 01:49:50.187: INFO: Pod "pod-configmaps-a259fa59-7e95-461b-9cd0-ce1c26474267" satisfied condition "Succeeded or Failed"
Jan 28 01:49:50.200: INFO: Trying to get logs from node 10.9.20.126 pod pod-configmaps-a259fa59-7e95-461b-9cd0-ce1c26474267 container env-test: <nil>
STEP: delete the pod 01/28/23 01:49:50.223
Jan 28 01:49:50.245: INFO: Waiting for pod pod-configmaps-a259fa59-7e95-461b-9cd0-ce1c26474267 to disappear
Jan 28 01:49:50.254: INFO: Pod pod-configmaps-a259fa59-7e95-461b-9cd0-ce1c26474267 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 28 01:49:50.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-573" for this suite. 01/28/23 01:49:50.27
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":315,"skipped":5858,"failed":0}
------------------------------
â€¢ [4.256 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:49:46.029
    Jan 28 01:49:46.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename configmap 01/28/23 01:49:46.03
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:49:46.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:49:46.116
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-573/configmap-test-af999dd9-6611-4ff2-a774-bbba1fbc9e89 01/28/23 01:49:46.132
    STEP: Creating a pod to test consume configMaps 01/28/23 01:49:46.146
    Jan 28 01:49:46.162: INFO: Waiting up to 5m0s for pod "pod-configmaps-a259fa59-7e95-461b-9cd0-ce1c26474267" in namespace "configmap-573" to be "Succeeded or Failed"
    Jan 28 01:49:46.175: INFO: Pod "pod-configmaps-a259fa59-7e95-461b-9cd0-ce1c26474267": Phase="Pending", Reason="", readiness=false. Elapsed: 12.653555ms
    Jan 28 01:49:48.188: INFO: Pod "pod-configmaps-a259fa59-7e95-461b-9cd0-ce1c26474267": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02632273s
    Jan 28 01:49:50.187: INFO: Pod "pod-configmaps-a259fa59-7e95-461b-9cd0-ce1c26474267": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024480924s
    STEP: Saw pod success 01/28/23 01:49:50.187
    Jan 28 01:49:50.187: INFO: Pod "pod-configmaps-a259fa59-7e95-461b-9cd0-ce1c26474267" satisfied condition "Succeeded or Failed"
    Jan 28 01:49:50.200: INFO: Trying to get logs from node 10.9.20.126 pod pod-configmaps-a259fa59-7e95-461b-9cd0-ce1c26474267 container env-test: <nil>
    STEP: delete the pod 01/28/23 01:49:50.223
    Jan 28 01:49:50.245: INFO: Waiting for pod pod-configmaps-a259fa59-7e95-461b-9cd0-ce1c26474267 to disappear
    Jan 28 01:49:50.254: INFO: Pod pod-configmaps-a259fa59-7e95-461b-9cd0-ce1c26474267 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 28 01:49:50.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-573" for this suite. 01/28/23 01:49:50.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:49:50.293
Jan 28 01:49:50.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubelet-test 01/28/23 01:49:50.295
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:49:50.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:49:50.396
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan 28 01:49:50.464: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsf08f246e-245f-4cc6-9c96-1685c33ee7e3" in namespace "kubelet-test-7842" to be "running and ready"
Jan 28 01:49:50.475: INFO: Pod "busybox-readonly-fsf08f246e-245f-4cc6-9c96-1685c33ee7e3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.937823ms
Jan 28 01:49:50.476: INFO: The phase of Pod busybox-readonly-fsf08f246e-245f-4cc6-9c96-1685c33ee7e3 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:49:52.513: INFO: Pod "busybox-readonly-fsf08f246e-245f-4cc6-9c96-1685c33ee7e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048094978s
Jan 28 01:49:52.513: INFO: The phase of Pod busybox-readonly-fsf08f246e-245f-4cc6-9c96-1685c33ee7e3 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:49:54.490: INFO: Pod "busybox-readonly-fsf08f246e-245f-4cc6-9c96-1685c33ee7e3": Phase="Running", Reason="", readiness=true. Elapsed: 4.025853579s
Jan 28 01:49:54.490: INFO: The phase of Pod busybox-readonly-fsf08f246e-245f-4cc6-9c96-1685c33ee7e3 is Running (Ready = true)
Jan 28 01:49:54.490: INFO: Pod "busybox-readonly-fsf08f246e-245f-4cc6-9c96-1685c33ee7e3" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 28 01:49:54.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7842" for this suite. 01/28/23 01:49:54.582
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":316,"skipped":5880,"failed":0}
------------------------------
â€¢ [4.303 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:49:50.293
    Jan 28 01:49:50.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubelet-test 01/28/23 01:49:50.295
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:49:50.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:49:50.396
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan 28 01:49:50.464: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsf08f246e-245f-4cc6-9c96-1685c33ee7e3" in namespace "kubelet-test-7842" to be "running and ready"
    Jan 28 01:49:50.475: INFO: Pod "busybox-readonly-fsf08f246e-245f-4cc6-9c96-1685c33ee7e3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.937823ms
    Jan 28 01:49:50.476: INFO: The phase of Pod busybox-readonly-fsf08f246e-245f-4cc6-9c96-1685c33ee7e3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:49:52.513: INFO: Pod "busybox-readonly-fsf08f246e-245f-4cc6-9c96-1685c33ee7e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048094978s
    Jan 28 01:49:52.513: INFO: The phase of Pod busybox-readonly-fsf08f246e-245f-4cc6-9c96-1685c33ee7e3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:49:54.490: INFO: Pod "busybox-readonly-fsf08f246e-245f-4cc6-9c96-1685c33ee7e3": Phase="Running", Reason="", readiness=true. Elapsed: 4.025853579s
    Jan 28 01:49:54.490: INFO: The phase of Pod busybox-readonly-fsf08f246e-245f-4cc6-9c96-1685c33ee7e3 is Running (Ready = true)
    Jan 28 01:49:54.490: INFO: Pod "busybox-readonly-fsf08f246e-245f-4cc6-9c96-1685c33ee7e3" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 28 01:49:54.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7842" for this suite. 01/28/23 01:49:54.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:49:54.601
Jan 28 01:49:54.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename svcaccounts 01/28/23 01:49:54.603
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:49:54.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:49:54.668
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Jan 28 01:49:54.757: INFO: created pod pod-service-account-defaultsa
Jan 28 01:49:54.757: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 28 01:49:54.767: INFO: created pod pod-service-account-mountsa
Jan 28 01:49:54.767: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 28 01:49:54.811: INFO: created pod pod-service-account-nomountsa
Jan 28 01:49:54.811: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 28 01:49:54.825: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 28 01:49:54.825: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 28 01:49:54.848: INFO: created pod pod-service-account-mountsa-mountspec
Jan 28 01:49:54.848: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 28 01:49:54.860: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 28 01:49:54.860: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 28 01:49:54.930: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 28 01:49:54.930: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 28 01:49:54.955: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 28 01:49:54.955: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 28 01:49:54.969: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 28 01:49:54.969: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 28 01:49:54.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4177" for this suite. 01/28/23 01:49:54.992
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":317,"skipped":5909,"failed":0}
------------------------------
â€¢ [0.431 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:49:54.601
    Jan 28 01:49:54.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename svcaccounts 01/28/23 01:49:54.603
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:49:54.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:49:54.668
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Jan 28 01:49:54.757: INFO: created pod pod-service-account-defaultsa
    Jan 28 01:49:54.757: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan 28 01:49:54.767: INFO: created pod pod-service-account-mountsa
    Jan 28 01:49:54.767: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan 28 01:49:54.811: INFO: created pod pod-service-account-nomountsa
    Jan 28 01:49:54.811: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan 28 01:49:54.825: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan 28 01:49:54.825: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan 28 01:49:54.848: INFO: created pod pod-service-account-mountsa-mountspec
    Jan 28 01:49:54.848: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan 28 01:49:54.860: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan 28 01:49:54.860: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan 28 01:49:54.930: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan 28 01:49:54.930: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan 28 01:49:54.955: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan 28 01:49:54.955: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan 28 01:49:54.969: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan 28 01:49:54.969: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 28 01:49:54.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4177" for this suite. 01/28/23 01:49:54.992
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:49:55.032
Jan 28 01:49:55.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename crd-watch 01/28/23 01:49:55.033
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:49:55.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:49:55.15
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan 28 01:49:55.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Creating first CR  01/28/23 01:49:57.838
Jan 28 01:49:57.857: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-28T01:49:57Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-28T01:49:57Z]] name:name1 resourceVersion:46080 uid:434594a7-dc88-416c-863c-e260f636d66d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/28/23 01:50:07.858
Jan 28 01:50:07.907: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-28T01:50:07Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-28T01:50:07Z]] name:name2 resourceVersion:46168 uid:88f75eb9-771c-419d-898f-92028147150e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/28/23 01:50:17.913
Jan 28 01:50:17.931: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-28T01:49:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-28T01:50:17Z]] name:name1 resourceVersion:46176 uid:434594a7-dc88-416c-863c-e260f636d66d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/28/23 01:50:27.932
Jan 28 01:50:27.954: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-28T01:50:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-28T01:50:27Z]] name:name2 resourceVersion:46186 uid:88f75eb9-771c-419d-898f-92028147150e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/28/23 01:50:37.954
Jan 28 01:50:37.982: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-28T01:49:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-28T01:50:17Z]] name:name1 resourceVersion:46205 uid:434594a7-dc88-416c-863c-e260f636d66d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/28/23 01:50:47.983
Jan 28 01:50:48.018: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-28T01:50:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-28T01:50:27Z]] name:name2 resourceVersion:46215 uid:88f75eb9-771c-419d-898f-92028147150e] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:50:58.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-3579" for this suite. 01/28/23 01:50:58.594
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":318,"skipped":5909,"failed":0}
------------------------------
â€¢ [SLOW TEST] [63.582 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:49:55.032
    Jan 28 01:49:55.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename crd-watch 01/28/23 01:49:55.033
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:49:55.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:49:55.15
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan 28 01:49:55.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Creating first CR  01/28/23 01:49:57.838
    Jan 28 01:49:57.857: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-28T01:49:57Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-28T01:49:57Z]] name:name1 resourceVersion:46080 uid:434594a7-dc88-416c-863c-e260f636d66d] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/28/23 01:50:07.858
    Jan 28 01:50:07.907: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-28T01:50:07Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-28T01:50:07Z]] name:name2 resourceVersion:46168 uid:88f75eb9-771c-419d-898f-92028147150e] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/28/23 01:50:17.913
    Jan 28 01:50:17.931: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-28T01:49:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-28T01:50:17Z]] name:name1 resourceVersion:46176 uid:434594a7-dc88-416c-863c-e260f636d66d] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/28/23 01:50:27.932
    Jan 28 01:50:27.954: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-28T01:50:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-28T01:50:27Z]] name:name2 resourceVersion:46186 uid:88f75eb9-771c-419d-898f-92028147150e] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/28/23 01:50:37.954
    Jan 28 01:50:37.982: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-28T01:49:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-28T01:50:17Z]] name:name1 resourceVersion:46205 uid:434594a7-dc88-416c-863c-e260f636d66d] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/28/23 01:50:47.983
    Jan 28 01:50:48.018: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-28T01:50:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-28T01:50:27Z]] name:name2 resourceVersion:46215 uid:88f75eb9-771c-419d-898f-92028147150e] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:50:58.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-3579" for this suite. 01/28/23 01:50:58.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:50:58.624
Jan 28 01:50:58.625: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename secrets 01/28/23 01:50:58.626
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:50:58.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:50:58.699
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 28 01:50:58.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-86" for this suite. 01/28/23 01:50:58.853
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":319,"skipped":5922,"failed":0}
------------------------------
â€¢ [0.247 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:50:58.624
    Jan 28 01:50:58.625: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename secrets 01/28/23 01:50:58.626
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:50:58.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:50:58.699
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 28 01:50:58.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-86" for this suite. 01/28/23 01:50:58.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:50:58.883
Jan 28 01:50:58.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename pods 01/28/23 01:50:58.886
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:50:58.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:50:58.947
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 01/28/23 01:50:58.961
Jan 28 01:50:58.987: INFO: Waiting up to 5m0s for pod "pod-cm2fv" in namespace "pods-9264" to be "running"
Jan 28 01:50:58.998: INFO: Pod "pod-cm2fv": Phase="Pending", Reason="", readiness=false. Elapsed: 11.200448ms
Jan 28 01:51:01.015: INFO: Pod "pod-cm2fv": Phase="Running", Reason="", readiness=true. Elapsed: 2.027936144s
Jan 28 01:51:01.015: INFO: Pod "pod-cm2fv" satisfied condition "running"
STEP: patching /status 01/28/23 01:51:01.015
Jan 28 01:51:01.037: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 28 01:51:01.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9264" for this suite. 01/28/23 01:51:01.054
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":320,"skipped":5943,"failed":0}
------------------------------
â€¢ [2.200 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:50:58.883
    Jan 28 01:50:58.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename pods 01/28/23 01:50:58.886
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:50:58.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:50:58.947
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 01/28/23 01:50:58.961
    Jan 28 01:50:58.987: INFO: Waiting up to 5m0s for pod "pod-cm2fv" in namespace "pods-9264" to be "running"
    Jan 28 01:50:58.998: INFO: Pod "pod-cm2fv": Phase="Pending", Reason="", readiness=false. Elapsed: 11.200448ms
    Jan 28 01:51:01.015: INFO: Pod "pod-cm2fv": Phase="Running", Reason="", readiness=true. Elapsed: 2.027936144s
    Jan 28 01:51:01.015: INFO: Pod "pod-cm2fv" satisfied condition "running"
    STEP: patching /status 01/28/23 01:51:01.015
    Jan 28 01:51:01.037: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 28 01:51:01.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9264" for this suite. 01/28/23 01:51:01.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:51:01.098
Jan 28 01:51:01.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubectl 01/28/23 01:51:01.1
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:01.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:01.144
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 01/28/23 01:51:01.153
Jan 28 01:51:01.154: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 28 01:51:01.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 create -f -'
Jan 28 01:51:02.182: INFO: stderr: ""
Jan 28 01:51:02.182: INFO: stdout: "service/agnhost-replica created\n"
Jan 28 01:51:02.182: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 28 01:51:02.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 create -f -'
Jan 28 01:51:03.158: INFO: stderr: ""
Jan 28 01:51:03.158: INFO: stdout: "service/agnhost-primary created\n"
Jan 28 01:51:03.158: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 28 01:51:03.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 create -f -'
Jan 28 01:51:03.468: INFO: stderr: ""
Jan 28 01:51:03.468: INFO: stdout: "service/frontend created\n"
Jan 28 01:51:03.468: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 28 01:51:03.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 create -f -'
Jan 28 01:51:04.509: INFO: stderr: ""
Jan 28 01:51:04.509: INFO: stdout: "deployment.apps/frontend created\n"
Jan 28 01:51:04.509: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 28 01:51:04.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 create -f -'
Jan 28 01:51:04.812: INFO: stderr: ""
Jan 28 01:51:04.812: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 28 01:51:04.812: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 28 01:51:04.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 create -f -'
Jan 28 01:51:05.078: INFO: stderr: ""
Jan 28 01:51:05.078: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/28/23 01:51:05.078
Jan 28 01:51:05.078: INFO: Waiting for all frontend pods to be Running.
Jan 28 01:51:10.130: INFO: Waiting for frontend to serve content.
Jan 28 01:51:10.224: INFO: Trying to add a new entry to the guestbook.
Jan 28 01:51:10.277: INFO: Verifying that added entry can be retrieved.
Jan 28 01:51:10.333: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources 01/28/23 01:51:15.391
Jan 28 01:51:15.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 delete --grace-period=0 --force -f -'
Jan 28 01:51:15.544: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 28 01:51:15.544: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/28/23 01:51:15.544
Jan 28 01:51:15.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 delete --grace-period=0 --force -f -'
Jan 28 01:51:15.735: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 28 01:51:15.735: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/28/23 01:51:15.735
Jan 28 01:51:15.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 delete --grace-period=0 --force -f -'
Jan 28 01:51:15.879: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 28 01:51:15.879: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/28/23 01:51:15.879
Jan 28 01:51:15.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 delete --grace-period=0 --force -f -'
Jan 28 01:51:16.011: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 28 01:51:16.011: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/28/23 01:51:16.011
Jan 28 01:51:16.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 delete --grace-period=0 --force -f -'
Jan 28 01:51:16.162: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 28 01:51:16.162: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/28/23 01:51:16.162
Jan 28 01:51:16.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 delete --grace-period=0 --force -f -'
Jan 28 01:51:16.304: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 28 01:51:16.304: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 28 01:51:16.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3594" for this suite. 01/28/23 01:51:16.318
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":321,"skipped":5966,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.235 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:51:01.098
    Jan 28 01:51:01.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubectl 01/28/23 01:51:01.1
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:01.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:01.144
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 01/28/23 01:51:01.153
    Jan 28 01:51:01.154: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan 28 01:51:01.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 create -f -'
    Jan 28 01:51:02.182: INFO: stderr: ""
    Jan 28 01:51:02.182: INFO: stdout: "service/agnhost-replica created\n"
    Jan 28 01:51:02.182: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan 28 01:51:02.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 create -f -'
    Jan 28 01:51:03.158: INFO: stderr: ""
    Jan 28 01:51:03.158: INFO: stdout: "service/agnhost-primary created\n"
    Jan 28 01:51:03.158: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan 28 01:51:03.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 create -f -'
    Jan 28 01:51:03.468: INFO: stderr: ""
    Jan 28 01:51:03.468: INFO: stdout: "service/frontend created\n"
    Jan 28 01:51:03.468: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan 28 01:51:03.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 create -f -'
    Jan 28 01:51:04.509: INFO: stderr: ""
    Jan 28 01:51:04.509: INFO: stdout: "deployment.apps/frontend created\n"
    Jan 28 01:51:04.509: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 28 01:51:04.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 create -f -'
    Jan 28 01:51:04.812: INFO: stderr: ""
    Jan 28 01:51:04.812: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan 28 01:51:04.812: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 28 01:51:04.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 create -f -'
    Jan 28 01:51:05.078: INFO: stderr: ""
    Jan 28 01:51:05.078: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/28/23 01:51:05.078
    Jan 28 01:51:05.078: INFO: Waiting for all frontend pods to be Running.
    Jan 28 01:51:10.130: INFO: Waiting for frontend to serve content.
    Jan 28 01:51:10.224: INFO: Trying to add a new entry to the guestbook.
    Jan 28 01:51:10.277: INFO: Verifying that added entry can be retrieved.
    Jan 28 01:51:10.333: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
    STEP: using delete to clean up resources 01/28/23 01:51:15.391
    Jan 28 01:51:15.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 delete --grace-period=0 --force -f -'
    Jan 28 01:51:15.544: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 28 01:51:15.544: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/28/23 01:51:15.544
    Jan 28 01:51:15.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 delete --grace-period=0 --force -f -'
    Jan 28 01:51:15.735: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 28 01:51:15.735: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/28/23 01:51:15.735
    Jan 28 01:51:15.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 delete --grace-period=0 --force -f -'
    Jan 28 01:51:15.879: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 28 01:51:15.879: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/28/23 01:51:15.879
    Jan 28 01:51:15.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 delete --grace-period=0 --force -f -'
    Jan 28 01:51:16.011: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 28 01:51:16.011: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/28/23 01:51:16.011
    Jan 28 01:51:16.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 delete --grace-period=0 --force -f -'
    Jan 28 01:51:16.162: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 28 01:51:16.162: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/28/23 01:51:16.162
    Jan 28 01:51:16.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3594 delete --grace-period=0 --force -f -'
    Jan 28 01:51:16.304: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 28 01:51:16.304: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 28 01:51:16.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3594" for this suite. 01/28/23 01:51:16.318
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:51:16.333
Jan 28 01:51:16.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename secrets 01/28/23 01:51:16.334
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:16.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:16.378
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-7e05afc5-c53f-40d6-b352-fe798ff85690 01/28/23 01:51:16.383
STEP: Creating a pod to test consume secrets 01/28/23 01:51:16.395
Jan 28 01:51:16.412: INFO: Waiting up to 5m0s for pod "pod-secrets-8fa8338f-c7fb-4f10-8496-2435c787dbeb" in namespace "secrets-5171" to be "Succeeded or Failed"
Jan 28 01:51:16.426: INFO: Pod "pod-secrets-8fa8338f-c7fb-4f10-8496-2435c787dbeb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.092327ms
Jan 28 01:51:18.435: INFO: Pod "pod-secrets-8fa8338f-c7fb-4f10-8496-2435c787dbeb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023428637s
Jan 28 01:51:20.437: INFO: Pod "pod-secrets-8fa8338f-c7fb-4f10-8496-2435c787dbeb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024687547s
Jan 28 01:51:22.436: INFO: Pod "pod-secrets-8fa8338f-c7fb-4f10-8496-2435c787dbeb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024442213s
STEP: Saw pod success 01/28/23 01:51:22.436
Jan 28 01:51:22.437: INFO: Pod "pod-secrets-8fa8338f-c7fb-4f10-8496-2435c787dbeb" satisfied condition "Succeeded or Failed"
Jan 28 01:51:22.446: INFO: Trying to get logs from node 10.9.20.126 pod pod-secrets-8fa8338f-c7fb-4f10-8496-2435c787dbeb container secret-volume-test: <nil>
STEP: delete the pod 01/28/23 01:51:22.527
Jan 28 01:51:22.558: INFO: Waiting for pod pod-secrets-8fa8338f-c7fb-4f10-8496-2435c787dbeb to disappear
Jan 28 01:51:22.593: INFO: Pod pod-secrets-8fa8338f-c7fb-4f10-8496-2435c787dbeb no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 28 01:51:22.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5171" for this suite. 01/28/23 01:51:22.609
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":322,"skipped":5967,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.291 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:51:16.333
    Jan 28 01:51:16.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename secrets 01/28/23 01:51:16.334
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:16.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:16.378
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-7e05afc5-c53f-40d6-b352-fe798ff85690 01/28/23 01:51:16.383
    STEP: Creating a pod to test consume secrets 01/28/23 01:51:16.395
    Jan 28 01:51:16.412: INFO: Waiting up to 5m0s for pod "pod-secrets-8fa8338f-c7fb-4f10-8496-2435c787dbeb" in namespace "secrets-5171" to be "Succeeded or Failed"
    Jan 28 01:51:16.426: INFO: Pod "pod-secrets-8fa8338f-c7fb-4f10-8496-2435c787dbeb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.092327ms
    Jan 28 01:51:18.435: INFO: Pod "pod-secrets-8fa8338f-c7fb-4f10-8496-2435c787dbeb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023428637s
    Jan 28 01:51:20.437: INFO: Pod "pod-secrets-8fa8338f-c7fb-4f10-8496-2435c787dbeb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024687547s
    Jan 28 01:51:22.436: INFO: Pod "pod-secrets-8fa8338f-c7fb-4f10-8496-2435c787dbeb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024442213s
    STEP: Saw pod success 01/28/23 01:51:22.436
    Jan 28 01:51:22.437: INFO: Pod "pod-secrets-8fa8338f-c7fb-4f10-8496-2435c787dbeb" satisfied condition "Succeeded or Failed"
    Jan 28 01:51:22.446: INFO: Trying to get logs from node 10.9.20.126 pod pod-secrets-8fa8338f-c7fb-4f10-8496-2435c787dbeb container secret-volume-test: <nil>
    STEP: delete the pod 01/28/23 01:51:22.527
    Jan 28 01:51:22.558: INFO: Waiting for pod pod-secrets-8fa8338f-c7fb-4f10-8496-2435c787dbeb to disappear
    Jan 28 01:51:22.593: INFO: Pod pod-secrets-8fa8338f-c7fb-4f10-8496-2435c787dbeb no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 28 01:51:22.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5171" for this suite. 01/28/23 01:51:22.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:51:22.639
Jan 28 01:51:22.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename webhook 01/28/23 01:51:22.64
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:22.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:22.703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/28/23 01:51:22.743
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:51:23.418
STEP: Deploying the webhook pod 01/28/23 01:51:23.439
STEP: Wait for the deployment to be ready 01/28/23 01:51:23.466
Jan 28 01:51:23.488: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 01:51:25.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 51, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 51, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/28/23 01:51:27.529
STEP: Verifying the service has paired with the endpoint 01/28/23 01:51:27.559
Jan 28 01:51:28.561: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 01/28/23 01:51:28.57
STEP: Creating a custom resource definition that should be denied by the webhook 01/28/23 01:51:28.644
Jan 28 01:51:28.644: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:51:28.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3488" for this suite. 01/28/23 01:51:28.731
STEP: Destroying namespace "webhook-3488-markers" for this suite. 01/28/23 01:51:28.749
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":323,"skipped":6015,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.229 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:51:22.639
    Jan 28 01:51:22.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename webhook 01/28/23 01:51:22.64
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:22.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:22.703
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/28/23 01:51:22.743
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/28/23 01:51:23.418
    STEP: Deploying the webhook pod 01/28/23 01:51:23.439
    STEP: Wait for the deployment to be ready 01/28/23 01:51:23.466
    Jan 28 01:51:23.488: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 28 01:51:25.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 51, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 51, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/28/23 01:51:27.529
    STEP: Verifying the service has paired with the endpoint 01/28/23 01:51:27.559
    Jan 28 01:51:28.561: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/28/23 01:51:28.57
    STEP: Creating a custom resource definition that should be denied by the webhook 01/28/23 01:51:28.644
    Jan 28 01:51:28.644: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:51:28.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3488" for this suite. 01/28/23 01:51:28.731
    STEP: Destroying namespace "webhook-3488-markers" for this suite. 01/28/23 01:51:28.749
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:51:28.878
Jan 28 01:51:28.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename disruption 01/28/23 01:51:28.88
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:28.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:28.962
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:51:28.968
Jan 28 01:51:28.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename disruption-2 01/28/23 01:51:28.972
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:29.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:29.018
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 01/28/23 01:51:29.037
STEP: Waiting for the pdb to be processed 01/28/23 01:51:29.06
STEP: Waiting for the pdb to be processed 01/28/23 01:51:29.08
STEP: listing a collection of PDBs across all namespaces 01/28/23 01:51:29.114
STEP: listing a collection of PDBs in namespace disruption-183 01/28/23 01:51:29.148
STEP: deleting a collection of PDBs 01/28/23 01:51:29.188
STEP: Waiting for the PDB collection to be deleted 01/28/23 01:51:29.258
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Jan 28 01:51:29.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-866" for this suite. 01/28/23 01:51:29.285
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 28 01:51:29.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-183" for this suite. 01/28/23 01:51:29.323
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":324,"skipped":6017,"failed":0}
------------------------------
â€¢ [0.463 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:51:28.878
    Jan 28 01:51:28.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename disruption 01/28/23 01:51:28.88
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:28.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:28.962
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:51:28.968
    Jan 28 01:51:28.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename disruption-2 01/28/23 01:51:28.972
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:29.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:29.018
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 01/28/23 01:51:29.037
    STEP: Waiting for the pdb to be processed 01/28/23 01:51:29.06
    STEP: Waiting for the pdb to be processed 01/28/23 01:51:29.08
    STEP: listing a collection of PDBs across all namespaces 01/28/23 01:51:29.114
    STEP: listing a collection of PDBs in namespace disruption-183 01/28/23 01:51:29.148
    STEP: deleting a collection of PDBs 01/28/23 01:51:29.188
    STEP: Waiting for the PDB collection to be deleted 01/28/23 01:51:29.258
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Jan 28 01:51:29.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-866" for this suite. 01/28/23 01:51:29.285
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 28 01:51:29.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-183" for this suite. 01/28/23 01:51:29.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:51:29.345
Jan 28 01:51:29.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename svcaccounts 01/28/23 01:51:29.348
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:29.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:29.398
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 01/28/23 01:51:29.436
STEP: watching for the ServiceAccount to be added 01/28/23 01:51:29.459
STEP: patching the ServiceAccount 01/28/23 01:51:29.463
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/28/23 01:51:29.477
STEP: deleting the ServiceAccount 01/28/23 01:51:29.491
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 28 01:51:29.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1526" for this suite. 01/28/23 01:51:29.558
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":325,"skipped":6028,"failed":0}
------------------------------
â€¢ [0.231 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:51:29.345
    Jan 28 01:51:29.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename svcaccounts 01/28/23 01:51:29.348
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:29.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:29.398
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 01/28/23 01:51:29.436
    STEP: watching for the ServiceAccount to be added 01/28/23 01:51:29.459
    STEP: patching the ServiceAccount 01/28/23 01:51:29.463
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/28/23 01:51:29.477
    STEP: deleting the ServiceAccount 01/28/23 01:51:29.491
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 28 01:51:29.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1526" for this suite. 01/28/23 01:51:29.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:51:29.587
Jan 28 01:51:29.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename controllerrevisions 01/28/23 01:51:29.589
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:29.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:29.642
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-hbplp-daemon-set" 01/28/23 01:51:29.705
STEP: Check that daemon pods launch on every node of the cluster. 01/28/23 01:51:29.717
Jan 28 01:51:29.748: INFO: Number of nodes with available pods controlled by daemonset e2e-hbplp-daemon-set: 0
Jan 28 01:51:29.748: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 01:51:30.771: INFO: Number of nodes with available pods controlled by daemonset e2e-hbplp-daemon-set: 0
Jan 28 01:51:30.771: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
Jan 28 01:51:31.799: INFO: Number of nodes with available pods controlled by daemonset e2e-hbplp-daemon-set: 2
Jan 28 01:51:31.799: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
Jan 28 01:51:32.802: INFO: Number of nodes with available pods controlled by daemonset e2e-hbplp-daemon-set: 3
Jan 28 01:51:32.802: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-hbplp-daemon-set
STEP: Confirm DaemonSet "e2e-hbplp-daemon-set" successfully created with "daemonset-name=e2e-hbplp-daemon-set" label 01/28/23 01:51:32.811
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-hbplp-daemon-set" 01/28/23 01:51:32.833
Jan 28 01:51:32.848: INFO: Located ControllerRevision: "e2e-hbplp-daemon-set-69755bf69c"
STEP: Patching ControllerRevision "e2e-hbplp-daemon-set-69755bf69c" 01/28/23 01:51:32.858
Jan 28 01:51:32.875: INFO: e2e-hbplp-daemon-set-69755bf69c has been patched
STEP: Create a new ControllerRevision 01/28/23 01:51:32.875
Jan 28 01:51:32.890: INFO: Created ControllerRevision: e2e-hbplp-daemon-set-7d4c5c5f6f
STEP: Confirm that there are two ControllerRevisions 01/28/23 01:51:32.89
Jan 28 01:51:32.890: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 28 01:51:32.900: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-hbplp-daemon-set-69755bf69c" 01/28/23 01:51:32.9
STEP: Confirm that there is only one ControllerRevision 01/28/23 01:51:32.918
Jan 28 01:51:32.918: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 28 01:51:32.928: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-hbplp-daemon-set-7d4c5c5f6f" 01/28/23 01:51:32.938
Jan 28 01:51:32.961: INFO: e2e-hbplp-daemon-set-7d4c5c5f6f has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/28/23 01:51:32.961
W0128 01:51:32.975240      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/28/23 01:51:32.975
Jan 28 01:51:32.976: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 28 01:51:33.986: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 28 01:51:33.997: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-hbplp-daemon-set-7d4c5c5f6f=updated" 01/28/23 01:51:33.998
STEP: Confirm that there is only one ControllerRevision 01/28/23 01:51:34.024
Jan 28 01:51:34.025: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 28 01:51:34.041: INFO: Found 1 ControllerRevisions
Jan 28 01:51:34.051: INFO: ControllerRevision "e2e-hbplp-daemon-set-84b4d5c8fb" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-hbplp-daemon-set" 01/28/23 01:51:34.061
STEP: deleting DaemonSet.extensions e2e-hbplp-daemon-set in namespace controllerrevisions-6094, will wait for the garbage collector to delete the pods 01/28/23 01:51:34.062
Jan 28 01:51:34.145: INFO: Deleting DaemonSet.extensions e2e-hbplp-daemon-set took: 20.462895ms
Jan 28 01:51:34.246: INFO: Terminating DaemonSet.extensions e2e-hbplp-daemon-set pods took: 101.348403ms
Jan 28 01:51:35.857: INFO: Number of nodes with available pods controlled by daemonset e2e-hbplp-daemon-set: 0
Jan 28 01:51:35.857: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-hbplp-daemon-set
Jan 28 01:51:35.868: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46797"},"items":null}

Jan 28 01:51:35.880: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46797"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Jan 28 01:51:35.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-6094" for this suite. 01/28/23 01:51:35.95
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":326,"skipped":6044,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.380 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:51:29.587
    Jan 28 01:51:29.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename controllerrevisions 01/28/23 01:51:29.589
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:29.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:29.642
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-hbplp-daemon-set" 01/28/23 01:51:29.705
    STEP: Check that daemon pods launch on every node of the cluster. 01/28/23 01:51:29.717
    Jan 28 01:51:29.748: INFO: Number of nodes with available pods controlled by daemonset e2e-hbplp-daemon-set: 0
    Jan 28 01:51:29.748: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 01:51:30.771: INFO: Number of nodes with available pods controlled by daemonset e2e-hbplp-daemon-set: 0
    Jan 28 01:51:30.771: INFO: Node 10.9.20.126 is running 0 daemon pod, expected 1
    Jan 28 01:51:31.799: INFO: Number of nodes with available pods controlled by daemonset e2e-hbplp-daemon-set: 2
    Jan 28 01:51:31.799: INFO: Node 10.9.20.72 is running 0 daemon pod, expected 1
    Jan 28 01:51:32.802: INFO: Number of nodes with available pods controlled by daemonset e2e-hbplp-daemon-set: 3
    Jan 28 01:51:32.802: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-hbplp-daemon-set
    STEP: Confirm DaemonSet "e2e-hbplp-daemon-set" successfully created with "daemonset-name=e2e-hbplp-daemon-set" label 01/28/23 01:51:32.811
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-hbplp-daemon-set" 01/28/23 01:51:32.833
    Jan 28 01:51:32.848: INFO: Located ControllerRevision: "e2e-hbplp-daemon-set-69755bf69c"
    STEP: Patching ControllerRevision "e2e-hbplp-daemon-set-69755bf69c" 01/28/23 01:51:32.858
    Jan 28 01:51:32.875: INFO: e2e-hbplp-daemon-set-69755bf69c has been patched
    STEP: Create a new ControllerRevision 01/28/23 01:51:32.875
    Jan 28 01:51:32.890: INFO: Created ControllerRevision: e2e-hbplp-daemon-set-7d4c5c5f6f
    STEP: Confirm that there are two ControllerRevisions 01/28/23 01:51:32.89
    Jan 28 01:51:32.890: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 28 01:51:32.900: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-hbplp-daemon-set-69755bf69c" 01/28/23 01:51:32.9
    STEP: Confirm that there is only one ControllerRevision 01/28/23 01:51:32.918
    Jan 28 01:51:32.918: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 28 01:51:32.928: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-hbplp-daemon-set-7d4c5c5f6f" 01/28/23 01:51:32.938
    Jan 28 01:51:32.961: INFO: e2e-hbplp-daemon-set-7d4c5c5f6f has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/28/23 01:51:32.961
    W0128 01:51:32.975240      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/28/23 01:51:32.975
    Jan 28 01:51:32.976: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 28 01:51:33.986: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 28 01:51:33.997: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-hbplp-daemon-set-7d4c5c5f6f=updated" 01/28/23 01:51:33.998
    STEP: Confirm that there is only one ControllerRevision 01/28/23 01:51:34.024
    Jan 28 01:51:34.025: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 28 01:51:34.041: INFO: Found 1 ControllerRevisions
    Jan 28 01:51:34.051: INFO: ControllerRevision "e2e-hbplp-daemon-set-84b4d5c8fb" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-hbplp-daemon-set" 01/28/23 01:51:34.061
    STEP: deleting DaemonSet.extensions e2e-hbplp-daemon-set in namespace controllerrevisions-6094, will wait for the garbage collector to delete the pods 01/28/23 01:51:34.062
    Jan 28 01:51:34.145: INFO: Deleting DaemonSet.extensions e2e-hbplp-daemon-set took: 20.462895ms
    Jan 28 01:51:34.246: INFO: Terminating DaemonSet.extensions e2e-hbplp-daemon-set pods took: 101.348403ms
    Jan 28 01:51:35.857: INFO: Number of nodes with available pods controlled by daemonset e2e-hbplp-daemon-set: 0
    Jan 28 01:51:35.857: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-hbplp-daemon-set
    Jan 28 01:51:35.868: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46797"},"items":null}

    Jan 28 01:51:35.880: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46797"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 01:51:35.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-6094" for this suite. 01/28/23 01:51:35.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:51:35.973
Jan 28 01:51:35.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename services 01/28/23 01:51:35.974
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:36.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:36.014
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3288 01/28/23 01:51:36.019
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/28/23 01:51:36.056
STEP: creating service externalsvc in namespace services-3288 01/28/23 01:51:36.057
STEP: creating replication controller externalsvc in namespace services-3288 01/28/23 01:51:36.085
I0128 01:51:36.096703      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3288, replica count: 2
I0128 01:51:39.147286      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/28/23 01:51:39.158
Jan 28 01:51:39.214: INFO: Creating new exec pod
Jan 28 01:51:39.234: INFO: Waiting up to 5m0s for pod "execpodx8rmx" in namespace "services-3288" to be "running"
Jan 28 01:51:39.244: INFO: Pod "execpodx8rmx": Phase="Pending", Reason="", readiness=false. Elapsed: 9.995382ms
Jan 28 01:51:41.254: INFO: Pod "execpodx8rmx": Phase="Running", Reason="", readiness=true. Elapsed: 2.019408551s
Jan 28 01:51:41.254: INFO: Pod "execpodx8rmx" satisfied condition "running"
Jan 28 01:51:41.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-3288 exec execpodx8rmx -- /bin/sh -x -c nslookup nodeport-service.services-3288.svc.cluster.local'
Jan 28 01:51:41.682: INFO: stderr: "+ nslookup nodeport-service.services-3288.svc.cluster.local\n"
Jan 28 01:51:41.682: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-3288.svc.cluster.local\tcanonical name = externalsvc.services-3288.svc.cluster.local.\nName:\texternalsvc.services-3288.svc.cluster.local\nAddress: 172.21.1.154\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3288, will wait for the garbage collector to delete the pods 01/28/23 01:51:41.682
Jan 28 01:51:41.758: INFO: Deleting ReplicationController externalsvc took: 17.342254ms
Jan 28 01:51:41.859: INFO: Terminating ReplicationController externalsvc pods took: 100.97751ms
Jan 28 01:51:44.321: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 28 01:51:44.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3288" for this suite. 01/28/23 01:51:44.368
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":327,"skipped":6067,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.411 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:51:35.973
    Jan 28 01:51:35.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename services 01/28/23 01:51:35.974
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:36.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:36.014
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-3288 01/28/23 01:51:36.019
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/28/23 01:51:36.056
    STEP: creating service externalsvc in namespace services-3288 01/28/23 01:51:36.057
    STEP: creating replication controller externalsvc in namespace services-3288 01/28/23 01:51:36.085
    I0128 01:51:36.096703      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3288, replica count: 2
    I0128 01:51:39.147286      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/28/23 01:51:39.158
    Jan 28 01:51:39.214: INFO: Creating new exec pod
    Jan 28 01:51:39.234: INFO: Waiting up to 5m0s for pod "execpodx8rmx" in namespace "services-3288" to be "running"
    Jan 28 01:51:39.244: INFO: Pod "execpodx8rmx": Phase="Pending", Reason="", readiness=false. Elapsed: 9.995382ms
    Jan 28 01:51:41.254: INFO: Pod "execpodx8rmx": Phase="Running", Reason="", readiness=true. Elapsed: 2.019408551s
    Jan 28 01:51:41.254: INFO: Pod "execpodx8rmx" satisfied condition "running"
    Jan 28 01:51:41.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-3288 exec execpodx8rmx -- /bin/sh -x -c nslookup nodeport-service.services-3288.svc.cluster.local'
    Jan 28 01:51:41.682: INFO: stderr: "+ nslookup nodeport-service.services-3288.svc.cluster.local\n"
    Jan 28 01:51:41.682: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-3288.svc.cluster.local\tcanonical name = externalsvc.services-3288.svc.cluster.local.\nName:\texternalsvc.services-3288.svc.cluster.local\nAddress: 172.21.1.154\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3288, will wait for the garbage collector to delete the pods 01/28/23 01:51:41.682
    Jan 28 01:51:41.758: INFO: Deleting ReplicationController externalsvc took: 17.342254ms
    Jan 28 01:51:41.859: INFO: Terminating ReplicationController externalsvc pods took: 100.97751ms
    Jan 28 01:51:44.321: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 28 01:51:44.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3288" for this suite. 01/28/23 01:51:44.368
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:51:44.39
Jan 28 01:51:44.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubectl 01/28/23 01:51:44.393
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:44.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:44.437
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 01/28/23 01:51:44.445
Jan 28 01:51:44.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 create -f -'
Jan 28 01:51:44.749: INFO: stderr: ""
Jan 28 01:51:44.749: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/28/23 01:51:44.749
Jan 28 01:51:44.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 28 01:51:44.866: INFO: stderr: ""
Jan 28 01:51:44.866: INFO: stdout: "update-demo-nautilus-khsxm update-demo-nautilus-scsdq "
Jan 28 01:51:44.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 get pods update-demo-nautilus-khsxm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 01:51:44.989: INFO: stderr: ""
Jan 28 01:51:44.989: INFO: stdout: ""
Jan 28 01:51:44.989: INFO: update-demo-nautilus-khsxm is created but not running
Jan 28 01:51:49.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 28 01:51:50.131: INFO: stderr: ""
Jan 28 01:51:50.131: INFO: stdout: "update-demo-nautilus-khsxm update-demo-nautilus-scsdq "
Jan 28 01:51:50.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 get pods update-demo-nautilus-khsxm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 01:51:50.228: INFO: stderr: ""
Jan 28 01:51:50.228: INFO: stdout: "true"
Jan 28 01:51:50.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 get pods update-demo-nautilus-khsxm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 28 01:51:50.340: INFO: stderr: ""
Jan 28 01:51:50.340: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 28 01:51:50.340: INFO: validating pod update-demo-nautilus-khsxm
Jan 28 01:51:50.388: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 28 01:51:50.388: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 28 01:51:50.388: INFO: update-demo-nautilus-khsxm is verified up and running
Jan 28 01:51:50.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 get pods update-demo-nautilus-scsdq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 01:51:50.500: INFO: stderr: ""
Jan 28 01:51:50.500: INFO: stdout: "true"
Jan 28 01:51:50.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 get pods update-demo-nautilus-scsdq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 28 01:51:50.607: INFO: stderr: ""
Jan 28 01:51:50.607: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 28 01:51:50.607: INFO: validating pod update-demo-nautilus-scsdq
Jan 28 01:51:50.670: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 28 01:51:50.670: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 28 01:51:50.670: INFO: update-demo-nautilus-scsdq is verified up and running
STEP: using delete to clean up resources 01/28/23 01:51:50.67
Jan 28 01:51:50.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 delete --grace-period=0 --force -f -'
Jan 28 01:51:50.803: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 28 01:51:50.803: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 28 01:51:50.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 get rc,svc -l name=update-demo --no-headers'
Jan 28 01:51:50.945: INFO: stderr: "No resources found in kubectl-9433 namespace.\n"
Jan 28 01:51:50.945: INFO: stdout: ""
Jan 28 01:51:50.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 28 01:51:51.081: INFO: stderr: ""
Jan 28 01:51:51.081: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 28 01:51:51.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9433" for this suite. 01/28/23 01:51:51.094
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":328,"skipped":6073,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.720 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:51:44.39
    Jan 28 01:51:44.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubectl 01/28/23 01:51:44.393
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:44.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:44.437
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 01/28/23 01:51:44.445
    Jan 28 01:51:44.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 create -f -'
    Jan 28 01:51:44.749: INFO: stderr: ""
    Jan 28 01:51:44.749: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/28/23 01:51:44.749
    Jan 28 01:51:44.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 28 01:51:44.866: INFO: stderr: ""
    Jan 28 01:51:44.866: INFO: stdout: "update-demo-nautilus-khsxm update-demo-nautilus-scsdq "
    Jan 28 01:51:44.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 get pods update-demo-nautilus-khsxm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 28 01:51:44.989: INFO: stderr: ""
    Jan 28 01:51:44.989: INFO: stdout: ""
    Jan 28 01:51:44.989: INFO: update-demo-nautilus-khsxm is created but not running
    Jan 28 01:51:49.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 28 01:51:50.131: INFO: stderr: ""
    Jan 28 01:51:50.131: INFO: stdout: "update-demo-nautilus-khsxm update-demo-nautilus-scsdq "
    Jan 28 01:51:50.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 get pods update-demo-nautilus-khsxm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 28 01:51:50.228: INFO: stderr: ""
    Jan 28 01:51:50.228: INFO: stdout: "true"
    Jan 28 01:51:50.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 get pods update-demo-nautilus-khsxm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 28 01:51:50.340: INFO: stderr: ""
    Jan 28 01:51:50.340: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 28 01:51:50.340: INFO: validating pod update-demo-nautilus-khsxm
    Jan 28 01:51:50.388: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 28 01:51:50.388: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 28 01:51:50.388: INFO: update-demo-nautilus-khsxm is verified up and running
    Jan 28 01:51:50.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 get pods update-demo-nautilus-scsdq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 28 01:51:50.500: INFO: stderr: ""
    Jan 28 01:51:50.500: INFO: stdout: "true"
    Jan 28 01:51:50.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 get pods update-demo-nautilus-scsdq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 28 01:51:50.607: INFO: stderr: ""
    Jan 28 01:51:50.607: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 28 01:51:50.607: INFO: validating pod update-demo-nautilus-scsdq
    Jan 28 01:51:50.670: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 28 01:51:50.670: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 28 01:51:50.670: INFO: update-demo-nautilus-scsdq is verified up and running
    STEP: using delete to clean up resources 01/28/23 01:51:50.67
    Jan 28 01:51:50.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 delete --grace-period=0 --force -f -'
    Jan 28 01:51:50.803: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 28 01:51:50.803: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 28 01:51:50.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 get rc,svc -l name=update-demo --no-headers'
    Jan 28 01:51:50.945: INFO: stderr: "No resources found in kubectl-9433 namespace.\n"
    Jan 28 01:51:50.945: INFO: stdout: ""
    Jan 28 01:51:50.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-9433 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 28 01:51:51.081: INFO: stderr: ""
    Jan 28 01:51:51.081: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 28 01:51:51.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9433" for this suite. 01/28/23 01:51:51.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:51:51.127
Jan 28 01:51:51.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename sysctl 01/28/23 01:51:51.129
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:51.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:51.174
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/28/23 01:51:51.181
STEP: Watching for error events or started pod 01/28/23 01:51:51.201
STEP: Waiting for pod completion 01/28/23 01:51:53.215
Jan 28 01:51:53.215: INFO: Waiting up to 3m0s for pod "sysctl-4d272d4e-71d6-4326-9568-7493c7bbf9a4" in namespace "sysctl-2872" to be "completed"
Jan 28 01:51:53.225: INFO: Pod "sysctl-4d272d4e-71d6-4326-9568-7493c7bbf9a4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.085762ms
Jan 28 01:51:55.235: INFO: Pod "sysctl-4d272d4e-71d6-4326-9568-7493c7bbf9a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019126725s
Jan 28 01:51:57.235: INFO: Pod "sysctl-4d272d4e-71d6-4326-9568-7493c7bbf9a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020050504s
Jan 28 01:51:57.236: INFO: Pod "sysctl-4d272d4e-71d6-4326-9568-7493c7bbf9a4" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/28/23 01:51:57.245
STEP: Getting logs from the pod 01/28/23 01:51:57.245
STEP: Checking that the sysctl is actually updated 01/28/23 01:51:57.291
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 28 01:51:57.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-2872" for this suite. 01/28/23 01:51:57.305
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":329,"skipped":6115,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.196 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:51:51.127
    Jan 28 01:51:51.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename sysctl 01/28/23 01:51:51.129
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:51.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:51.174
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/28/23 01:51:51.181
    STEP: Watching for error events or started pod 01/28/23 01:51:51.201
    STEP: Waiting for pod completion 01/28/23 01:51:53.215
    Jan 28 01:51:53.215: INFO: Waiting up to 3m0s for pod "sysctl-4d272d4e-71d6-4326-9568-7493c7bbf9a4" in namespace "sysctl-2872" to be "completed"
    Jan 28 01:51:53.225: INFO: Pod "sysctl-4d272d4e-71d6-4326-9568-7493c7bbf9a4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.085762ms
    Jan 28 01:51:55.235: INFO: Pod "sysctl-4d272d4e-71d6-4326-9568-7493c7bbf9a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019126725s
    Jan 28 01:51:57.235: INFO: Pod "sysctl-4d272d4e-71d6-4326-9568-7493c7bbf9a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020050504s
    Jan 28 01:51:57.236: INFO: Pod "sysctl-4d272d4e-71d6-4326-9568-7493c7bbf9a4" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/28/23 01:51:57.245
    STEP: Getting logs from the pod 01/28/23 01:51:57.245
    STEP: Checking that the sysctl is actually updated 01/28/23 01:51:57.291
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 28 01:51:57.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-2872" for this suite. 01/28/23 01:51:57.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:51:57.34
Jan 28 01:51:57.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename container-probe 01/28/23 01:51:57.342
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:57.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:57.394
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 28 01:52:57.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9566" for this suite. 01/28/23 01:52:57.445
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":330,"skipped":6162,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.122 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:51:57.34
    Jan 28 01:51:57.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename container-probe 01/28/23 01:51:57.342
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:51:57.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:51:57.394
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 28 01:52:57.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9566" for this suite. 01/28/23 01:52:57.445
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:52:57.463
Jan 28 01:52:57.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename emptydir 01/28/23 01:52:57.467
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:52:57.512
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:52:57.518
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/28/23 01:52:57.524
Jan 28 01:52:57.543: INFO: Waiting up to 5m0s for pod "pod-d7b98846-adf1-4e44-9d9d-57c70e01a7a3" in namespace "emptydir-9892" to be "Succeeded or Failed"
Jan 28 01:52:57.551: INFO: Pod "pod-d7b98846-adf1-4e44-9d9d-57c70e01a7a3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.61471ms
Jan 28 01:52:59.561: INFO: Pod "pod-d7b98846-adf1-4e44-9d9d-57c70e01a7a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.018850134s
Jan 28 01:53:01.561: INFO: Pod "pod-d7b98846-adf1-4e44-9d9d-57c70e01a7a3": Phase="Running", Reason="", readiness=false. Elapsed: 4.0186506s
Jan 28 01:53:03.561: INFO: Pod "pod-d7b98846-adf1-4e44-9d9d-57c70e01a7a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018701801s
STEP: Saw pod success 01/28/23 01:53:03.561
Jan 28 01:53:03.562: INFO: Pod "pod-d7b98846-adf1-4e44-9d9d-57c70e01a7a3" satisfied condition "Succeeded or Failed"
Jan 28 01:53:03.573: INFO: Trying to get logs from node 10.9.20.126 pod pod-d7b98846-adf1-4e44-9d9d-57c70e01a7a3 container test-container: <nil>
STEP: delete the pod 01/28/23 01:53:03.61
Jan 28 01:53:03.647: INFO: Waiting for pod pod-d7b98846-adf1-4e44-9d9d-57c70e01a7a3 to disappear
Jan 28 01:53:03.669: INFO: Pod pod-d7b98846-adf1-4e44-9d9d-57c70e01a7a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 28 01:53:03.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9892" for this suite. 01/28/23 01:53:03.68
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":331,"skipped":6162,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.232 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:52:57.463
    Jan 28 01:52:57.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename emptydir 01/28/23 01:52:57.467
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:52:57.512
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:52:57.518
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/28/23 01:52:57.524
    Jan 28 01:52:57.543: INFO: Waiting up to 5m0s for pod "pod-d7b98846-adf1-4e44-9d9d-57c70e01a7a3" in namespace "emptydir-9892" to be "Succeeded or Failed"
    Jan 28 01:52:57.551: INFO: Pod "pod-d7b98846-adf1-4e44-9d9d-57c70e01a7a3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.61471ms
    Jan 28 01:52:59.561: INFO: Pod "pod-d7b98846-adf1-4e44-9d9d-57c70e01a7a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.018850134s
    Jan 28 01:53:01.561: INFO: Pod "pod-d7b98846-adf1-4e44-9d9d-57c70e01a7a3": Phase="Running", Reason="", readiness=false. Elapsed: 4.0186506s
    Jan 28 01:53:03.561: INFO: Pod "pod-d7b98846-adf1-4e44-9d9d-57c70e01a7a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018701801s
    STEP: Saw pod success 01/28/23 01:53:03.561
    Jan 28 01:53:03.562: INFO: Pod "pod-d7b98846-adf1-4e44-9d9d-57c70e01a7a3" satisfied condition "Succeeded or Failed"
    Jan 28 01:53:03.573: INFO: Trying to get logs from node 10.9.20.126 pod pod-d7b98846-adf1-4e44-9d9d-57c70e01a7a3 container test-container: <nil>
    STEP: delete the pod 01/28/23 01:53:03.61
    Jan 28 01:53:03.647: INFO: Waiting for pod pod-d7b98846-adf1-4e44-9d9d-57c70e01a7a3 to disappear
    Jan 28 01:53:03.669: INFO: Pod pod-d7b98846-adf1-4e44-9d9d-57c70e01a7a3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 28 01:53:03.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9892" for this suite. 01/28/23 01:53:03.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:53:03.709
Jan 28 01:53:03.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename services 01/28/23 01:53:03.725
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:53:03.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:53:03.775
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-7383 01/28/23 01:53:03.783
STEP: creating service affinity-clusterip-transition in namespace services-7383 01/28/23 01:53:03.783
STEP: creating replication controller affinity-clusterip-transition in namespace services-7383 01/28/23 01:53:03.819
I0128 01:53:03.832368      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-7383, replica count: 3
I0128 01:53:06.884490      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 01:53:06.907: INFO: Creating new exec pod
Jan 28 01:53:06.927: INFO: Waiting up to 5m0s for pod "execpod-affinityjsv2w" in namespace "services-7383" to be "running"
Jan 28 01:53:06.939: INFO: Pod "execpod-affinityjsv2w": Phase="Pending", Reason="", readiness=false. Elapsed: 12.307547ms
Jan 28 01:53:08.949: INFO: Pod "execpod-affinityjsv2w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022073923s
Jan 28 01:53:10.949: INFO: Pod "execpod-affinityjsv2w": Phase="Running", Reason="", readiness=true. Elapsed: 4.022124657s
Jan 28 01:53:10.949: INFO: Pod "execpod-affinityjsv2w" satisfied condition "running"
Jan 28 01:53:11.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-7383 exec execpod-affinityjsv2w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan 28 01:53:12.294: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 28 01:53:12.294: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:53:12.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-7383 exec execpod-affinityjsv2w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.199.202 80'
Jan 28 01:53:12.553: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.199.202 80\nConnection to 172.21.199.202 80 port [tcp/http] succeeded!\n"
Jan 28 01:53:12.553: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:53:12.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-7383 exec execpod-affinityjsv2w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.199.202:80/ ; done'
Jan 28 01:53:13.064: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n"
Jan 28 01:53:13.064: INFO: stdout: "\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5"
Jan 28 01:53:13.064: INFO: Received response from host: affinity-clusterip-transition-4v6k5
Jan 28 01:53:13.064: INFO: Received response from host: affinity-clusterip-transition-4v6k5
Jan 28 01:53:13.064: INFO: Received response from host: affinity-clusterip-transition-4v6k5
Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
Jan 28 01:53:43.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-7383 exec execpod-affinityjsv2w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.199.202:80/ ; done'
Jan 28 01:53:43.544: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n"
Jan 28 01:53:43.544: INFO: stdout: "\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-n4sw2"
Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-m5xxf
Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-4v6k5
Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-4v6k5
Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-m5xxf
Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-m5xxf
Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
Jan 28 01:53:43.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-7383 exec execpod-affinityjsv2w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.199.202:80/ ; done'
Jan 28 01:53:44.066: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n"
Jan 28 01:53:44.066: INFO: stdout: "\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf"
Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
Jan 28 01:53:44.066: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7383, will wait for the garbage collector to delete the pods 01/28/23 01:53:44.126
Jan 28 01:53:44.203: INFO: Deleting ReplicationController affinity-clusterip-transition took: 16.685984ms
Jan 28 01:53:44.304: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.694462ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 28 01:53:46.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7383" for this suite. 01/28/23 01:53:46.963
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":332,"skipped":6181,"failed":0}
------------------------------
â€¢ [SLOW TEST] [43.271 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:53:03.709
    Jan 28 01:53:03.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename services 01/28/23 01:53:03.725
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:53:03.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:53:03.775
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-7383 01/28/23 01:53:03.783
    STEP: creating service affinity-clusterip-transition in namespace services-7383 01/28/23 01:53:03.783
    STEP: creating replication controller affinity-clusterip-transition in namespace services-7383 01/28/23 01:53:03.819
    I0128 01:53:03.832368      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-7383, replica count: 3
    I0128 01:53:06.884490      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 28 01:53:06.907: INFO: Creating new exec pod
    Jan 28 01:53:06.927: INFO: Waiting up to 5m0s for pod "execpod-affinityjsv2w" in namespace "services-7383" to be "running"
    Jan 28 01:53:06.939: INFO: Pod "execpod-affinityjsv2w": Phase="Pending", Reason="", readiness=false. Elapsed: 12.307547ms
    Jan 28 01:53:08.949: INFO: Pod "execpod-affinityjsv2w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022073923s
    Jan 28 01:53:10.949: INFO: Pod "execpod-affinityjsv2w": Phase="Running", Reason="", readiness=true. Elapsed: 4.022124657s
    Jan 28 01:53:10.949: INFO: Pod "execpod-affinityjsv2w" satisfied condition "running"
    Jan 28 01:53:11.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-7383 exec execpod-affinityjsv2w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Jan 28 01:53:12.294: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan 28 01:53:12.294: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 01:53:12.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-7383 exec execpod-affinityjsv2w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.199.202 80'
    Jan 28 01:53:12.553: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.199.202 80\nConnection to 172.21.199.202 80 port [tcp/http] succeeded!\n"
    Jan 28 01:53:12.553: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 01:53:12.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-7383 exec execpod-affinityjsv2w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.199.202:80/ ; done'
    Jan 28 01:53:13.064: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n"
    Jan 28 01:53:13.064: INFO: stdout: "\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5"
    Jan 28 01:53:13.064: INFO: Received response from host: affinity-clusterip-transition-4v6k5
    Jan 28 01:53:13.064: INFO: Received response from host: affinity-clusterip-transition-4v6k5
    Jan 28 01:53:13.064: INFO: Received response from host: affinity-clusterip-transition-4v6k5
    Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
    Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
    Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
    Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
    Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
    Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
    Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
    Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
    Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
    Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
    Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
    Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
    Jan 28 01:53:13.065: INFO: Received response from host: affinity-clusterip-transition-4v6k5
    Jan 28 01:53:43.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-7383 exec execpod-affinityjsv2w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.199.202:80/ ; done'
    Jan 28 01:53:43.544: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n"
    Jan 28 01:53:43.544: INFO: stdout: "\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-4v6k5\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-n4sw2\naffinity-clusterip-transition-n4sw2"
    Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
    Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
    Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-m5xxf
    Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
    Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
    Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
    Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
    Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
    Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-4v6k5
    Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-4v6k5
    Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-m5xxf
    Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
    Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-m5xxf
    Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
    Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
    Jan 28 01:53:43.544: INFO: Received response from host: affinity-clusterip-transition-n4sw2
    Jan 28 01:53:43.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-7383 exec execpod-affinityjsv2w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.199.202:80/ ; done'
    Jan 28 01:53:44.066: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.202:80/\n"
    Jan 28 01:53:44.066: INFO: stdout: "\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf\naffinity-clusterip-transition-m5xxf"
    Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
    Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
    Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
    Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
    Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
    Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
    Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
    Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
    Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
    Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
    Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
    Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
    Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
    Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
    Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
    Jan 28 01:53:44.066: INFO: Received response from host: affinity-clusterip-transition-m5xxf
    Jan 28 01:53:44.066: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7383, will wait for the garbage collector to delete the pods 01/28/23 01:53:44.126
    Jan 28 01:53:44.203: INFO: Deleting ReplicationController affinity-clusterip-transition took: 16.685984ms
    Jan 28 01:53:44.304: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.694462ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 28 01:53:46.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7383" for this suite. 01/28/23 01:53:46.963
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:53:46.99
Jan 28 01:53:46.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubectl 01/28/23 01:53:46.992
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:53:47.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:53:47.04
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 01/28/23 01:53:47.046
Jan 28 01:53:47.047: INFO: namespace kubectl-3176
Jan 28 01:53:47.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3176 create -f -'
Jan 28 01:53:47.398: INFO: stderr: ""
Jan 28 01:53:47.398: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/28/23 01:53:47.398
Jan 28 01:53:48.410: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 01:53:48.410: INFO: Found 0 / 1
Jan 28 01:53:49.408: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 01:53:49.408: INFO: Found 0 / 1
Jan 28 01:53:50.415: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 01:53:50.415: INFO: Found 1 / 1
Jan 28 01:53:50.415: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 28 01:53:50.424: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 01:53:50.424: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 28 01:53:50.425: INFO: wait on agnhost-primary startup in kubectl-3176 
Jan 28 01:53:50.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3176 logs agnhost-primary-n7knb agnhost-primary'
Jan 28 01:53:50.608: INFO: stderr: ""
Jan 28 01:53:50.608: INFO: stdout: "Paused\n"
STEP: exposing RC 01/28/23 01:53:50.608
Jan 28 01:53:50.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3176 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 28 01:53:50.774: INFO: stderr: ""
Jan 28 01:53:50.774: INFO: stdout: "service/rm2 exposed\n"
Jan 28 01:53:50.784: INFO: Service rm2 in namespace kubectl-3176 found.
STEP: exposing service 01/28/23 01:53:52.809
Jan 28 01:53:52.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3176 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 28 01:53:52.967: INFO: stderr: ""
Jan 28 01:53:52.967: INFO: stdout: "service/rm3 exposed\n"
Jan 28 01:53:52.977: INFO: Service rm3 in namespace kubectl-3176 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 28 01:53:54.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3176" for this suite. 01/28/23 01:53:55.01
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":333,"skipped":6196,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.039 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:53:46.99
    Jan 28 01:53:46.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubectl 01/28/23 01:53:46.992
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:53:47.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:53:47.04
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 01/28/23 01:53:47.046
    Jan 28 01:53:47.047: INFO: namespace kubectl-3176
    Jan 28 01:53:47.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3176 create -f -'
    Jan 28 01:53:47.398: INFO: stderr: ""
    Jan 28 01:53:47.398: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/28/23 01:53:47.398
    Jan 28 01:53:48.410: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 28 01:53:48.410: INFO: Found 0 / 1
    Jan 28 01:53:49.408: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 28 01:53:49.408: INFO: Found 0 / 1
    Jan 28 01:53:50.415: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 28 01:53:50.415: INFO: Found 1 / 1
    Jan 28 01:53:50.415: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 28 01:53:50.424: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 28 01:53:50.424: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 28 01:53:50.425: INFO: wait on agnhost-primary startup in kubectl-3176 
    Jan 28 01:53:50.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3176 logs agnhost-primary-n7knb agnhost-primary'
    Jan 28 01:53:50.608: INFO: stderr: ""
    Jan 28 01:53:50.608: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/28/23 01:53:50.608
    Jan 28 01:53:50.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3176 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan 28 01:53:50.774: INFO: stderr: ""
    Jan 28 01:53:50.774: INFO: stdout: "service/rm2 exposed\n"
    Jan 28 01:53:50.784: INFO: Service rm2 in namespace kubectl-3176 found.
    STEP: exposing service 01/28/23 01:53:52.809
    Jan 28 01:53:52.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-3176 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan 28 01:53:52.967: INFO: stderr: ""
    Jan 28 01:53:52.967: INFO: stdout: "service/rm3 exposed\n"
    Jan 28 01:53:52.977: INFO: Service rm3 in namespace kubectl-3176 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 28 01:53:54.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3176" for this suite. 01/28/23 01:53:55.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:53:55.038
Jan 28 01:53:55.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename cronjob 01/28/23 01:53:55.04
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:53:55.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:53:55.086
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/28/23 01:53:55.093
STEP: Ensuring more than one job is running at a time 01/28/23 01:53:55.105
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/28/23 01:55:01.116
STEP: Removing cronjob 01/28/23 01:55:01.126
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 28 01:55:01.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1774" for this suite. 01/28/23 01:55:01.161
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":334,"skipped":6248,"failed":0}
------------------------------
â€¢ [SLOW TEST] [66.140 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:53:55.038
    Jan 28 01:53:55.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename cronjob 01/28/23 01:53:55.04
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:53:55.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:53:55.086
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/28/23 01:53:55.093
    STEP: Ensuring more than one job is running at a time 01/28/23 01:53:55.105
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/28/23 01:55:01.116
    STEP: Removing cronjob 01/28/23 01:55:01.126
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 28 01:55:01.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-1774" for this suite. 01/28/23 01:55:01.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:55:01.183
Jan 28 01:55:01.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename emptydir 01/28/23 01:55:01.185
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:55:01.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:55:01.238
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/28/23 01:55:01.263
Jan 28 01:55:01.287: INFO: Waiting up to 5m0s for pod "pod-cd5b6bf6-0645-482b-a5d7-36856019a269" in namespace "emptydir-4841" to be "Succeeded or Failed"
Jan 28 01:55:01.297: INFO: Pod "pod-cd5b6bf6-0645-482b-a5d7-36856019a269": Phase="Pending", Reason="", readiness=false. Elapsed: 10.302456ms
Jan 28 01:55:03.308: INFO: Pod "pod-cd5b6bf6-0645-482b-a5d7-36856019a269": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021496355s
Jan 28 01:55:05.307: INFO: Pod "pod-cd5b6bf6-0645-482b-a5d7-36856019a269": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020432187s
Jan 28 01:55:07.324: INFO: Pod "pod-cd5b6bf6-0645-482b-a5d7-36856019a269": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037168668s
STEP: Saw pod success 01/28/23 01:55:07.324
Jan 28 01:55:07.324: INFO: Pod "pod-cd5b6bf6-0645-482b-a5d7-36856019a269" satisfied condition "Succeeded or Failed"
Jan 28 01:55:07.334: INFO: Trying to get logs from node 10.9.20.126 pod pod-cd5b6bf6-0645-482b-a5d7-36856019a269 container test-container: <nil>
STEP: delete the pod 01/28/23 01:55:07.404
Jan 28 01:55:07.428: INFO: Waiting for pod pod-cd5b6bf6-0645-482b-a5d7-36856019a269 to disappear
Jan 28 01:55:07.436: INFO: Pod pod-cd5b6bf6-0645-482b-a5d7-36856019a269 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 28 01:55:07.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4841" for this suite. 01/28/23 01:55:07.45
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":335,"skipped":6254,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.284 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:55:01.183
    Jan 28 01:55:01.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename emptydir 01/28/23 01:55:01.185
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:55:01.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:55:01.238
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/28/23 01:55:01.263
    Jan 28 01:55:01.287: INFO: Waiting up to 5m0s for pod "pod-cd5b6bf6-0645-482b-a5d7-36856019a269" in namespace "emptydir-4841" to be "Succeeded or Failed"
    Jan 28 01:55:01.297: INFO: Pod "pod-cd5b6bf6-0645-482b-a5d7-36856019a269": Phase="Pending", Reason="", readiness=false. Elapsed: 10.302456ms
    Jan 28 01:55:03.308: INFO: Pod "pod-cd5b6bf6-0645-482b-a5d7-36856019a269": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021496355s
    Jan 28 01:55:05.307: INFO: Pod "pod-cd5b6bf6-0645-482b-a5d7-36856019a269": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020432187s
    Jan 28 01:55:07.324: INFO: Pod "pod-cd5b6bf6-0645-482b-a5d7-36856019a269": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037168668s
    STEP: Saw pod success 01/28/23 01:55:07.324
    Jan 28 01:55:07.324: INFO: Pod "pod-cd5b6bf6-0645-482b-a5d7-36856019a269" satisfied condition "Succeeded or Failed"
    Jan 28 01:55:07.334: INFO: Trying to get logs from node 10.9.20.126 pod pod-cd5b6bf6-0645-482b-a5d7-36856019a269 container test-container: <nil>
    STEP: delete the pod 01/28/23 01:55:07.404
    Jan 28 01:55:07.428: INFO: Waiting for pod pod-cd5b6bf6-0645-482b-a5d7-36856019a269 to disappear
    Jan 28 01:55:07.436: INFO: Pod pod-cd5b6bf6-0645-482b-a5d7-36856019a269 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 28 01:55:07.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4841" for this suite. 01/28/23 01:55:07.45
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:55:07.469
Jan 28 01:55:07.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 01:55:07.471
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:55:07.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:55:07.51
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-cff150c2-4ea5-4cde-978a-db5bb99ea14d 01/28/23 01:55:07.52
STEP: Creating a pod to test consume configMaps 01/28/23 01:55:07.531
Jan 28 01:55:07.549: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7e1561fd-4f84-4f85-9821-6061de4ac7c1" in namespace "projected-2760" to be "Succeeded or Failed"
Jan 28 01:55:07.558: INFO: Pod "pod-projected-configmaps-7e1561fd-4f84-4f85-9821-6061de4ac7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.506627ms
Jan 28 01:55:09.568: INFO: Pod "pod-projected-configmaps-7e1561fd-4f84-4f85-9821-6061de4ac7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019280729s
Jan 28 01:55:11.568: INFO: Pod "pod-projected-configmaps-7e1561fd-4f84-4f85-9821-6061de4ac7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018881077s
Jan 28 01:55:13.579: INFO: Pod "pod-projected-configmaps-7e1561fd-4f84-4f85-9821-6061de4ac7c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029689441s
STEP: Saw pod success 01/28/23 01:55:13.579
Jan 28 01:55:13.579: INFO: Pod "pod-projected-configmaps-7e1561fd-4f84-4f85-9821-6061de4ac7c1" satisfied condition "Succeeded or Failed"
Jan 28 01:55:13.589: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-configmaps-7e1561fd-4f84-4f85-9821-6061de4ac7c1 container agnhost-container: <nil>
STEP: delete the pod 01/28/23 01:55:13.616
Jan 28 01:55:13.649: INFO: Waiting for pod pod-projected-configmaps-7e1561fd-4f84-4f85-9821-6061de4ac7c1 to disappear
Jan 28 01:55:13.659: INFO: Pod pod-projected-configmaps-7e1561fd-4f84-4f85-9821-6061de4ac7c1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 28 01:55:13.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2760" for this suite. 01/28/23 01:55:13.672
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":336,"skipped":6257,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.218 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:55:07.469
    Jan 28 01:55:07.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 01:55:07.471
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:55:07.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:55:07.51
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-cff150c2-4ea5-4cde-978a-db5bb99ea14d 01/28/23 01:55:07.52
    STEP: Creating a pod to test consume configMaps 01/28/23 01:55:07.531
    Jan 28 01:55:07.549: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7e1561fd-4f84-4f85-9821-6061de4ac7c1" in namespace "projected-2760" to be "Succeeded or Failed"
    Jan 28 01:55:07.558: INFO: Pod "pod-projected-configmaps-7e1561fd-4f84-4f85-9821-6061de4ac7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.506627ms
    Jan 28 01:55:09.568: INFO: Pod "pod-projected-configmaps-7e1561fd-4f84-4f85-9821-6061de4ac7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019280729s
    Jan 28 01:55:11.568: INFO: Pod "pod-projected-configmaps-7e1561fd-4f84-4f85-9821-6061de4ac7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018881077s
    Jan 28 01:55:13.579: INFO: Pod "pod-projected-configmaps-7e1561fd-4f84-4f85-9821-6061de4ac7c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029689441s
    STEP: Saw pod success 01/28/23 01:55:13.579
    Jan 28 01:55:13.579: INFO: Pod "pod-projected-configmaps-7e1561fd-4f84-4f85-9821-6061de4ac7c1" satisfied condition "Succeeded or Failed"
    Jan 28 01:55:13.589: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-configmaps-7e1561fd-4f84-4f85-9821-6061de4ac7c1 container agnhost-container: <nil>
    STEP: delete the pod 01/28/23 01:55:13.616
    Jan 28 01:55:13.649: INFO: Waiting for pod pod-projected-configmaps-7e1561fd-4f84-4f85-9821-6061de4ac7c1 to disappear
    Jan 28 01:55:13.659: INFO: Pod pod-projected-configmaps-7e1561fd-4f84-4f85-9821-6061de4ac7c1 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 28 01:55:13.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2760" for this suite. 01/28/23 01:55:13.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:55:13.694
Jan 28 01:55:13.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename crd-publish-openapi 01/28/23 01:55:13.697
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:55:13.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:55:13.745
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/28/23 01:55:13.752
Jan 28 01:55:13.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/28/23 01:55:28.146
Jan 28 01:55:28.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
Jan 28 01:55:32.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 01:55:44.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2656" for this suite. 01/28/23 01:55:44.743
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":337,"skipped":6279,"failed":0}
------------------------------
â€¢ [SLOW TEST] [31.068 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:55:13.694
    Jan 28 01:55:13.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename crd-publish-openapi 01/28/23 01:55:13.697
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:55:13.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:55:13.745
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/28/23 01:55:13.752
    Jan 28 01:55:13.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/28/23 01:55:28.146
    Jan 28 01:55:28.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    Jan 28 01:55:32.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 01:55:44.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2656" for this suite. 01/28/23 01:55:44.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:55:44.764
Jan 28 01:55:44.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename downward-api 01/28/23 01:55:44.766
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:55:44.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:55:44.804
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 01/28/23 01:55:44.819
Jan 28 01:55:44.839: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3aec78d-a406-4a48-9886-f1fe65747d85" in namespace "downward-api-5577" to be "Succeeded or Failed"
Jan 28 01:55:44.850: INFO: Pod "downwardapi-volume-d3aec78d-a406-4a48-9886-f1fe65747d85": Phase="Pending", Reason="", readiness=false. Elapsed: 11.173763ms
Jan 28 01:55:46.862: INFO: Pod "downwardapi-volume-d3aec78d-a406-4a48-9886-f1fe65747d85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022875517s
Jan 28 01:55:48.862: INFO: Pod "downwardapi-volume-d3aec78d-a406-4a48-9886-f1fe65747d85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022750803s
Jan 28 01:55:50.861: INFO: Pod "downwardapi-volume-d3aec78d-a406-4a48-9886-f1fe65747d85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022347568s
STEP: Saw pod success 01/28/23 01:55:50.861
Jan 28 01:55:50.862: INFO: Pod "downwardapi-volume-d3aec78d-a406-4a48-9886-f1fe65747d85" satisfied condition "Succeeded or Failed"
Jan 28 01:55:50.870: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-d3aec78d-a406-4a48-9886-f1fe65747d85 container client-container: <nil>
STEP: delete the pod 01/28/23 01:55:50.969
Jan 28 01:55:50.998: INFO: Waiting for pod downwardapi-volume-d3aec78d-a406-4a48-9886-f1fe65747d85 to disappear
Jan 28 01:55:51.007: INFO: Pod downwardapi-volume-d3aec78d-a406-4a48-9886-f1fe65747d85 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 28 01:55:51.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5577" for this suite. 01/28/23 01:55:51.024
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":338,"skipped":6285,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.277 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:55:44.764
    Jan 28 01:55:44.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename downward-api 01/28/23 01:55:44.766
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:55:44.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:55:44.804
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 01/28/23 01:55:44.819
    Jan 28 01:55:44.839: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3aec78d-a406-4a48-9886-f1fe65747d85" in namespace "downward-api-5577" to be "Succeeded or Failed"
    Jan 28 01:55:44.850: INFO: Pod "downwardapi-volume-d3aec78d-a406-4a48-9886-f1fe65747d85": Phase="Pending", Reason="", readiness=false. Elapsed: 11.173763ms
    Jan 28 01:55:46.862: INFO: Pod "downwardapi-volume-d3aec78d-a406-4a48-9886-f1fe65747d85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022875517s
    Jan 28 01:55:48.862: INFO: Pod "downwardapi-volume-d3aec78d-a406-4a48-9886-f1fe65747d85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022750803s
    Jan 28 01:55:50.861: INFO: Pod "downwardapi-volume-d3aec78d-a406-4a48-9886-f1fe65747d85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022347568s
    STEP: Saw pod success 01/28/23 01:55:50.861
    Jan 28 01:55:50.862: INFO: Pod "downwardapi-volume-d3aec78d-a406-4a48-9886-f1fe65747d85" satisfied condition "Succeeded or Failed"
    Jan 28 01:55:50.870: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-d3aec78d-a406-4a48-9886-f1fe65747d85 container client-container: <nil>
    STEP: delete the pod 01/28/23 01:55:50.969
    Jan 28 01:55:50.998: INFO: Waiting for pod downwardapi-volume-d3aec78d-a406-4a48-9886-f1fe65747d85 to disappear
    Jan 28 01:55:51.007: INFO: Pod downwardapi-volume-d3aec78d-a406-4a48-9886-f1fe65747d85 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 28 01:55:51.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5577" for this suite. 01/28/23 01:55:51.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:55:51.047
Jan 28 01:55:51.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename services 01/28/23 01:55:51.049
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:55:51.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:55:51.102
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 01/28/23 01:55:51.111
Jan 28 01:55:51.111: INFO: Creating e2e-svc-a-tn6c6
Jan 28 01:55:51.151: INFO: Creating e2e-svc-b-hw492
Jan 28 01:55:51.190: INFO: Creating e2e-svc-c-5nmdv
STEP: deleting service collection 01/28/23 01:55:51.244
Jan 28 01:55:51.391: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 28 01:55:51.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-843" for this suite. 01/28/23 01:55:51.408
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":339,"skipped":6294,"failed":0}
------------------------------
â€¢ [0.380 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:55:51.047
    Jan 28 01:55:51.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename services 01/28/23 01:55:51.049
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:55:51.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:55:51.102
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 01/28/23 01:55:51.111
    Jan 28 01:55:51.111: INFO: Creating e2e-svc-a-tn6c6
    Jan 28 01:55:51.151: INFO: Creating e2e-svc-b-hw492
    Jan 28 01:55:51.190: INFO: Creating e2e-svc-c-5nmdv
    STEP: deleting service collection 01/28/23 01:55:51.244
    Jan 28 01:55:51.391: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 28 01:55:51.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-843" for this suite. 01/28/23 01:55:51.408
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:55:51.431
Jan 28 01:55:51.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 01:55:51.433
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:55:51.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:55:51.476
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-30b75053-1298-43aa-b6ef-d9eb5c2611a7 01/28/23 01:55:51.502
STEP: Creating configMap with name cm-test-opt-upd-24ef6805-568b-45bb-9517-79f718afd826 01/28/23 01:55:51.513
STEP: Creating the pod 01/28/23 01:55:51.525
Jan 28 01:55:51.545: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bd5e7ec7-ef12-4c2c-8056-567855d78982" in namespace "projected-1683" to be "running and ready"
Jan 28 01:55:51.568: INFO: Pod "pod-projected-configmaps-bd5e7ec7-ef12-4c2c-8056-567855d78982": Phase="Pending", Reason="", readiness=false. Elapsed: 22.50388ms
Jan 28 01:55:51.568: INFO: The phase of Pod pod-projected-configmaps-bd5e7ec7-ef12-4c2c-8056-567855d78982 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:55:53.581: INFO: Pod "pod-projected-configmaps-bd5e7ec7-ef12-4c2c-8056-567855d78982": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036100204s
Jan 28 01:55:53.582: INFO: The phase of Pod pod-projected-configmaps-bd5e7ec7-ef12-4c2c-8056-567855d78982 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:55:55.579: INFO: Pod "pod-projected-configmaps-bd5e7ec7-ef12-4c2c-8056-567855d78982": Phase="Running", Reason="", readiness=true. Elapsed: 4.033407383s
Jan 28 01:55:55.579: INFO: The phase of Pod pod-projected-configmaps-bd5e7ec7-ef12-4c2c-8056-567855d78982 is Running (Ready = true)
Jan 28 01:55:55.579: INFO: Pod "pod-projected-configmaps-bd5e7ec7-ef12-4c2c-8056-567855d78982" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-30b75053-1298-43aa-b6ef-d9eb5c2611a7 01/28/23 01:55:55.665
STEP: Updating configmap cm-test-opt-upd-24ef6805-568b-45bb-9517-79f718afd826 01/28/23 01:55:55.682
STEP: Creating configMap with name cm-test-opt-create-dc6fc86c-2081-42f4-b34b-bba6c9fea938 01/28/23 01:55:55.694
STEP: waiting to observe update in volume 01/28/23 01:55:55.709
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 28 01:55:57.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1683" for this suite. 01/28/23 01:55:57.837
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":340,"skipped":6316,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.422 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:55:51.431
    Jan 28 01:55:51.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 01:55:51.433
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:55:51.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:55:51.476
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-30b75053-1298-43aa-b6ef-d9eb5c2611a7 01/28/23 01:55:51.502
    STEP: Creating configMap with name cm-test-opt-upd-24ef6805-568b-45bb-9517-79f718afd826 01/28/23 01:55:51.513
    STEP: Creating the pod 01/28/23 01:55:51.525
    Jan 28 01:55:51.545: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bd5e7ec7-ef12-4c2c-8056-567855d78982" in namespace "projected-1683" to be "running and ready"
    Jan 28 01:55:51.568: INFO: Pod "pod-projected-configmaps-bd5e7ec7-ef12-4c2c-8056-567855d78982": Phase="Pending", Reason="", readiness=false. Elapsed: 22.50388ms
    Jan 28 01:55:51.568: INFO: The phase of Pod pod-projected-configmaps-bd5e7ec7-ef12-4c2c-8056-567855d78982 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:55:53.581: INFO: Pod "pod-projected-configmaps-bd5e7ec7-ef12-4c2c-8056-567855d78982": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036100204s
    Jan 28 01:55:53.582: INFO: The phase of Pod pod-projected-configmaps-bd5e7ec7-ef12-4c2c-8056-567855d78982 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:55:55.579: INFO: Pod "pod-projected-configmaps-bd5e7ec7-ef12-4c2c-8056-567855d78982": Phase="Running", Reason="", readiness=true. Elapsed: 4.033407383s
    Jan 28 01:55:55.579: INFO: The phase of Pod pod-projected-configmaps-bd5e7ec7-ef12-4c2c-8056-567855d78982 is Running (Ready = true)
    Jan 28 01:55:55.579: INFO: Pod "pod-projected-configmaps-bd5e7ec7-ef12-4c2c-8056-567855d78982" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-30b75053-1298-43aa-b6ef-d9eb5c2611a7 01/28/23 01:55:55.665
    STEP: Updating configmap cm-test-opt-upd-24ef6805-568b-45bb-9517-79f718afd826 01/28/23 01:55:55.682
    STEP: Creating configMap with name cm-test-opt-create-dc6fc86c-2081-42f4-b34b-bba6c9fea938 01/28/23 01:55:55.694
    STEP: waiting to observe update in volume 01/28/23 01:55:55.709
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 28 01:55:57.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1683" for this suite. 01/28/23 01:55:57.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:55:57.858
Jan 28 01:55:57.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename deployment 01/28/23 01:55:57.86
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:55:57.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:55:57.897
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan 28 01:55:57.985: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 28 01:56:02.996: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/28/23 01:56:02.996
Jan 28 01:56:02.997: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/28/23 01:56:03.023
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 28 01:56:07.080: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1481  e7c0b4c0-0d4f-4944-bf9e-88a28fda9031 47890 1 2023-01-28 01:56:03 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-28 01:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:56:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e89ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-28 01:56:03 +0000 UTC,LastTransitionTime:2023-01-28 01:56:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-01-28 01:56:05 +0000 UTC,LastTransitionTime:2023-01-28 01:56:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 28 01:56:07.092: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-1481  d483c796-bf6e-4b3d-aeb3-cf282a3036a8 47880 1 2023-01-28 01:56:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment e7c0b4c0-0d4f-4944-bf9e-88a28fda9031 0xc0055c19d7 0xc0055c19d8}] [] [{kube-controller-manager Update apps/v1 2023-01-28 01:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7c0b4c0-0d4f-4944-bf9e-88a28fda9031\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:56:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055c1a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 28 01:56:07.102: INFO: Pod "test-cleanup-deployment-69cb9c5497-scvs6" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-scvs6 test-cleanup-deployment-69cb9c5497- deployment-1481  e3f856dd-3d58-4a8e-881a-4f39d3591306 47879 0 2023-01-28 01:56:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:de98da4088665627b559a382797ebc4fc088d10b86ee00143ad8e52e55808371 cni.projectcalico.org/podIP:172.30.12.247/32 cni.projectcalico.org/podIPs:172.30.12.247/32] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 d483c796-bf6e-4b3d-aeb3-cf282a3036a8 0xc0055c1e37 0xc0055c1e38}] [] [{kube-controller-manager Update v1 2023-01-28 01:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d483c796-bf6e-4b3d-aeb3-cf282a3036a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:56:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:56:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-422ch,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-422ch,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:56:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:56:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:172.30.12.247,StartTime:2023-01-28 01:56:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:56:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://285bf841897b4b00c3c1a603e40b9bf1ba8948a8d633170889cf7eeaed3cde52,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.12.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 28 01:56:07.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1481" for this suite. 01/28/23 01:56:07.116
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":341,"skipped":6334,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.306 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:55:57.858
    Jan 28 01:55:57.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename deployment 01/28/23 01:55:57.86
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:55:57.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:55:57.897
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan 28 01:55:57.985: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan 28 01:56:02.996: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/28/23 01:56:02.996
    Jan 28 01:56:02.997: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/28/23 01:56:03.023
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 28 01:56:07.080: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1481  e7c0b4c0-0d4f-4944-bf9e-88a28fda9031 47890 1 2023-01-28 01:56:03 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-28 01:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:56:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e89ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-28 01:56:03 +0000 UTC,LastTransitionTime:2023-01-28 01:56:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-01-28 01:56:05 +0000 UTC,LastTransitionTime:2023-01-28 01:56:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 28 01:56:07.092: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-1481  d483c796-bf6e-4b3d-aeb3-cf282a3036a8 47880 1 2023-01-28 01:56:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment e7c0b4c0-0d4f-4944-bf9e-88a28fda9031 0xc0055c19d7 0xc0055c19d8}] [] [{kube-controller-manager Update apps/v1 2023-01-28 01:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7c0b4c0-0d4f-4944-bf9e-88a28fda9031\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:56:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055c1a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 28 01:56:07.102: INFO: Pod "test-cleanup-deployment-69cb9c5497-scvs6" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-scvs6 test-cleanup-deployment-69cb9c5497- deployment-1481  e3f856dd-3d58-4a8e-881a-4f39d3591306 47879 0 2023-01-28 01:56:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:de98da4088665627b559a382797ebc4fc088d10b86ee00143ad8e52e55808371 cni.projectcalico.org/podIP:172.30.12.247/32 cni.projectcalico.org/podIPs:172.30.12.247/32] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 d483c796-bf6e-4b3d-aeb3-cf282a3036a8 0xc0055c1e37 0xc0055c1e38}] [] [{kube-controller-manager Update v1 2023-01-28 01:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d483c796-bf6e-4b3d-aeb3-cf282a3036a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:56:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:56:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-422ch,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-422ch,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:56:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:56:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:172.30.12.247,StartTime:2023-01-28 01:56:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:56:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://285bf841897b4b00c3c1a603e40b9bf1ba8948a8d633170889cf7eeaed3cde52,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.12.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 28 01:56:07.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1481" for this suite. 01/28/23 01:56:07.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:56:07.173
Jan 28 01:56:07.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename downward-api 01/28/23 01:56:07.174
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:56:07.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:56:07.224
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 01/28/23 01:56:07.233
Jan 28 01:56:07.256: INFO: Waiting up to 5m0s for pod "annotationupdatee675bad5-825b-489a-b17a-0832fa158346" in namespace "downward-api-5009" to be "running and ready"
Jan 28 01:56:07.266: INFO: Pod "annotationupdatee675bad5-825b-489a-b17a-0832fa158346": Phase="Pending", Reason="", readiness=false. Elapsed: 10.056572ms
Jan 28 01:56:07.267: INFO: The phase of Pod annotationupdatee675bad5-825b-489a-b17a-0832fa158346 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:56:09.309: INFO: Pod "annotationupdatee675bad5-825b-489a-b17a-0832fa158346": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052252888s
Jan 28 01:56:09.309: INFO: The phase of Pod annotationupdatee675bad5-825b-489a-b17a-0832fa158346 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:56:11.277: INFO: Pod "annotationupdatee675bad5-825b-489a-b17a-0832fa158346": Phase="Running", Reason="", readiness=true. Elapsed: 4.020584337s
Jan 28 01:56:11.277: INFO: The phase of Pod annotationupdatee675bad5-825b-489a-b17a-0832fa158346 is Running (Ready = true)
Jan 28 01:56:11.277: INFO: Pod "annotationupdatee675bad5-825b-489a-b17a-0832fa158346" satisfied condition "running and ready"
Jan 28 01:56:11.838: INFO: Successfully updated pod "annotationupdatee675bad5-825b-489a-b17a-0832fa158346"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 28 01:56:13.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5009" for this suite. 01/28/23 01:56:13.935
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":342,"skipped":6341,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.836 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:56:07.173
    Jan 28 01:56:07.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename downward-api 01/28/23 01:56:07.174
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:56:07.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:56:07.224
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 01/28/23 01:56:07.233
    Jan 28 01:56:07.256: INFO: Waiting up to 5m0s for pod "annotationupdatee675bad5-825b-489a-b17a-0832fa158346" in namespace "downward-api-5009" to be "running and ready"
    Jan 28 01:56:07.266: INFO: Pod "annotationupdatee675bad5-825b-489a-b17a-0832fa158346": Phase="Pending", Reason="", readiness=false. Elapsed: 10.056572ms
    Jan 28 01:56:07.267: INFO: The phase of Pod annotationupdatee675bad5-825b-489a-b17a-0832fa158346 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:56:09.309: INFO: Pod "annotationupdatee675bad5-825b-489a-b17a-0832fa158346": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052252888s
    Jan 28 01:56:09.309: INFO: The phase of Pod annotationupdatee675bad5-825b-489a-b17a-0832fa158346 is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:56:11.277: INFO: Pod "annotationupdatee675bad5-825b-489a-b17a-0832fa158346": Phase="Running", Reason="", readiness=true. Elapsed: 4.020584337s
    Jan 28 01:56:11.277: INFO: The phase of Pod annotationupdatee675bad5-825b-489a-b17a-0832fa158346 is Running (Ready = true)
    Jan 28 01:56:11.277: INFO: Pod "annotationupdatee675bad5-825b-489a-b17a-0832fa158346" satisfied condition "running and ready"
    Jan 28 01:56:11.838: INFO: Successfully updated pod "annotationupdatee675bad5-825b-489a-b17a-0832fa158346"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 28 01:56:13.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5009" for this suite. 01/28/23 01:56:13.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:56:14.014
Jan 28 01:56:14.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename resourcequota 01/28/23 01:56:14.016
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:56:14.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:56:14.056
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 01/28/23 01:56:14.065
STEP: Counting existing ResourceQuota 01/28/23 01:56:19.074
STEP: Creating a ResourceQuota 01/28/23 01:56:24.097
STEP: Ensuring resource quota status is calculated 01/28/23 01:56:24.137
STEP: Creating a Secret 01/28/23 01:56:26.162
STEP: Ensuring resource quota status captures secret creation 01/28/23 01:56:26.211
STEP: Deleting a secret 01/28/23 01:56:28.251
STEP: Ensuring resource quota status released usage 01/28/23 01:56:28.287
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 28 01:56:30.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4048" for this suite. 01/28/23 01:56:30.313
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":343,"skipped":6358,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.328 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:56:14.014
    Jan 28 01:56:14.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename resourcequota 01/28/23 01:56:14.016
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:56:14.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:56:14.056
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 01/28/23 01:56:14.065
    STEP: Counting existing ResourceQuota 01/28/23 01:56:19.074
    STEP: Creating a ResourceQuota 01/28/23 01:56:24.097
    STEP: Ensuring resource quota status is calculated 01/28/23 01:56:24.137
    STEP: Creating a Secret 01/28/23 01:56:26.162
    STEP: Ensuring resource quota status captures secret creation 01/28/23 01:56:26.211
    STEP: Deleting a secret 01/28/23 01:56:28.251
    STEP: Ensuring resource quota status released usage 01/28/23 01:56:28.287
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 28 01:56:30.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4048" for this suite. 01/28/23 01:56:30.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:56:30.354
Jan 28 01:56:30.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename proxy 01/28/23 01:56:30.355
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:56:30.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:56:30.394
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan 28 01:56:30.403: INFO: Creating pod...
Jan 28 01:56:30.428: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9699" to be "running"
Jan 28 01:56:30.464: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 36.413562ms
Jan 28 01:56:32.478: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050371619s
Jan 28 01:56:34.476: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.048765522s
Jan 28 01:56:34.476: INFO: Pod "agnhost" satisfied condition "running"
Jan 28 01:56:34.477: INFO: Creating service...
Jan 28 01:56:34.514: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/pods/agnhost/proxy?method=DELETE
Jan 28 01:56:34.564: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 28 01:56:34.564: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/pods/agnhost/proxy?method=OPTIONS
Jan 28 01:56:34.607: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 28 01:56:34.607: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/pods/agnhost/proxy?method=PATCH
Jan 28 01:56:34.622: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 28 01:56:34.622: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/pods/agnhost/proxy?method=POST
Jan 28 01:56:34.666: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 28 01:56:34.666: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/pods/agnhost/proxy?method=PUT
Jan 28 01:56:34.681: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 28 01:56:34.681: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 28 01:56:34.701: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 28 01:56:34.701: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 28 01:56:34.752: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 28 01:56:34.752: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 28 01:56:34.775: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 28 01:56:34.775: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/services/e2e-proxy-test-service/proxy?method=POST
Jan 28 01:56:34.849: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 28 01:56:34.849: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/services/e2e-proxy-test-service/proxy?method=PUT
Jan 28 01:56:34.869: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 28 01:56:34.870: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/pods/agnhost/proxy?method=GET
Jan 28 01:56:34.879: INFO: http.Client request:GET StatusCode:301
Jan 28 01:56:34.879: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/services/e2e-proxy-test-service/proxy?method=GET
Jan 28 01:56:34.894: INFO: http.Client request:GET StatusCode:301
Jan 28 01:56:34.894: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/pods/agnhost/proxy?method=HEAD
Jan 28 01:56:34.903: INFO: http.Client request:HEAD StatusCode:301
Jan 28 01:56:34.904: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 28 01:56:34.919: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 28 01:56:34.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9699" for this suite. 01/28/23 01:56:34.935
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":344,"skipped":6410,"failed":0}
------------------------------
â€¢ [4.600 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:56:30.354
    Jan 28 01:56:30.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename proxy 01/28/23 01:56:30.355
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:56:30.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:56:30.394
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan 28 01:56:30.403: INFO: Creating pod...
    Jan 28 01:56:30.428: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9699" to be "running"
    Jan 28 01:56:30.464: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 36.413562ms
    Jan 28 01:56:32.478: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050371619s
    Jan 28 01:56:34.476: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.048765522s
    Jan 28 01:56:34.476: INFO: Pod "agnhost" satisfied condition "running"
    Jan 28 01:56:34.477: INFO: Creating service...
    Jan 28 01:56:34.514: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/pods/agnhost/proxy?method=DELETE
    Jan 28 01:56:34.564: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 28 01:56:34.564: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/pods/agnhost/proxy?method=OPTIONS
    Jan 28 01:56:34.607: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 28 01:56:34.607: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/pods/agnhost/proxy?method=PATCH
    Jan 28 01:56:34.622: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 28 01:56:34.622: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/pods/agnhost/proxy?method=POST
    Jan 28 01:56:34.666: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 28 01:56:34.666: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/pods/agnhost/proxy?method=PUT
    Jan 28 01:56:34.681: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 28 01:56:34.681: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan 28 01:56:34.701: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 28 01:56:34.701: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan 28 01:56:34.752: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 28 01:56:34.752: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan 28 01:56:34.775: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 28 01:56:34.775: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/services/e2e-proxy-test-service/proxy?method=POST
    Jan 28 01:56:34.849: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 28 01:56:34.849: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/services/e2e-proxy-test-service/proxy?method=PUT
    Jan 28 01:56:34.869: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 28 01:56:34.870: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/pods/agnhost/proxy?method=GET
    Jan 28 01:56:34.879: INFO: http.Client request:GET StatusCode:301
    Jan 28 01:56:34.879: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/services/e2e-proxy-test-service/proxy?method=GET
    Jan 28 01:56:34.894: INFO: http.Client request:GET StatusCode:301
    Jan 28 01:56:34.894: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/pods/agnhost/proxy?method=HEAD
    Jan 28 01:56:34.903: INFO: http.Client request:HEAD StatusCode:301
    Jan 28 01:56:34.904: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9699/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan 28 01:56:34.919: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 28 01:56:34.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-9699" for this suite. 01/28/23 01:56:34.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:56:34.962
Jan 28 01:56:34.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename sched-preemption 01/28/23 01:56:34.964
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:56:34.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:56:35.011
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 28 01:56:35.079: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 28 01:57:35.169: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 01/28/23 01:57:35.18
Jan 28 01:57:35.237: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 28 01:57:35.250: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 28 01:57:35.282: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 28 01:57:35.294: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 28 01:57:35.331: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 28 01:57:35.341: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/28/23 01:57:35.341
Jan 28 01:57:35.342: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9654" to be "running"
Jan 28 01:57:35.357: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 15.421451ms
Jan 28 01:57:37.369: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027493179s
Jan 28 01:57:39.370: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.027883876s
Jan 28 01:57:39.370: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 28 01:57:39.370: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9654" to be "running"
Jan 28 01:57:39.379: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.173834ms
Jan 28 01:57:39.379: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 28 01:57:39.380: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9654" to be "running"
Jan 28 01:57:39.389: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.63472ms
Jan 28 01:57:39.389: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 28 01:57:39.389: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9654" to be "running"
Jan 28 01:57:39.398: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.579985ms
Jan 28 01:57:39.398: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 28 01:57:39.398: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9654" to be "running"
Jan 28 01:57:39.407: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.221595ms
Jan 28 01:57:39.407: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 28 01:57:39.407: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9654" to be "running"
Jan 28 01:57:39.418: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.851679ms
Jan 28 01:57:39.418: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/28/23 01:57:39.418
Jan 28 01:57:39.432: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9654" to be "running"
Jan 28 01:57:39.443: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.461638ms
Jan 28 01:57:41.453: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020521593s
Jan 28 01:57:43.452: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019543118s
Jan 28 01:57:45.454: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022000837s
Jan 28 01:57:47.453: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.02112351s
Jan 28 01:57:47.453: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 28 01:57:47.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9654" for this suite. 01/28/23 01:57:47.54
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":345,"skipped":6420,"failed":0}
------------------------------
â€¢ [SLOW TEST] [72.741 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:56:34.962
    Jan 28 01:56:34.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename sched-preemption 01/28/23 01:56:34.964
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:56:34.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:56:35.011
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 28 01:56:35.079: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 28 01:57:35.169: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 01/28/23 01:57:35.18
    Jan 28 01:57:35.237: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 28 01:57:35.250: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 28 01:57:35.282: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 28 01:57:35.294: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan 28 01:57:35.331: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan 28 01:57:35.341: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/28/23 01:57:35.341
    Jan 28 01:57:35.342: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9654" to be "running"
    Jan 28 01:57:35.357: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 15.421451ms
    Jan 28 01:57:37.369: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027493179s
    Jan 28 01:57:39.370: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.027883876s
    Jan 28 01:57:39.370: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 28 01:57:39.370: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9654" to be "running"
    Jan 28 01:57:39.379: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.173834ms
    Jan 28 01:57:39.379: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 28 01:57:39.380: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9654" to be "running"
    Jan 28 01:57:39.389: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.63472ms
    Jan 28 01:57:39.389: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 28 01:57:39.389: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9654" to be "running"
    Jan 28 01:57:39.398: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.579985ms
    Jan 28 01:57:39.398: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 28 01:57:39.398: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9654" to be "running"
    Jan 28 01:57:39.407: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.221595ms
    Jan 28 01:57:39.407: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 28 01:57:39.407: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9654" to be "running"
    Jan 28 01:57:39.418: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.851679ms
    Jan 28 01:57:39.418: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/28/23 01:57:39.418
    Jan 28 01:57:39.432: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9654" to be "running"
    Jan 28 01:57:39.443: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.461638ms
    Jan 28 01:57:41.453: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020521593s
    Jan 28 01:57:43.452: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019543118s
    Jan 28 01:57:45.454: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022000837s
    Jan 28 01:57:47.453: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.02112351s
    Jan 28 01:57:47.453: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 01:57:47.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-9654" for this suite. 01/28/23 01:57:47.54
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:57:47.707
Jan 28 01:57:47.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 01:57:47.71
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:57:47.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:57:47.753
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 01/28/23 01:57:47.765
Jan 28 01:57:47.787: INFO: Waiting up to 5m0s for pod "annotationupdate1d7089aa-2352-4043-b3ba-5d4c5cc78c3b" in namespace "projected-3607" to be "running and ready"
Jan 28 01:57:47.801: INFO: Pod "annotationupdate1d7089aa-2352-4043-b3ba-5d4c5cc78c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.963287ms
Jan 28 01:57:47.801: INFO: The phase of Pod annotationupdate1d7089aa-2352-4043-b3ba-5d4c5cc78c3b is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:57:49.813: INFO: Pod "annotationupdate1d7089aa-2352-4043-b3ba-5d4c5cc78c3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.025984613s
Jan 28 01:57:49.813: INFO: The phase of Pod annotationupdate1d7089aa-2352-4043-b3ba-5d4c5cc78c3b is Running (Ready = true)
Jan 28 01:57:49.813: INFO: Pod "annotationupdate1d7089aa-2352-4043-b3ba-5d4c5cc78c3b" satisfied condition "running and ready"
Jan 28 01:57:50.435: INFO: Successfully updated pod "annotationupdate1d7089aa-2352-4043-b3ba-5d4c5cc78c3b"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 28 01:57:52.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3607" for this suite. 01/28/23 01:57:52.508
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":346,"skipped":6430,"failed":0}
------------------------------
â€¢ [4.818 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:57:47.707
    Jan 28 01:57:47.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 01:57:47.71
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:57:47.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:57:47.753
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 01/28/23 01:57:47.765
    Jan 28 01:57:47.787: INFO: Waiting up to 5m0s for pod "annotationupdate1d7089aa-2352-4043-b3ba-5d4c5cc78c3b" in namespace "projected-3607" to be "running and ready"
    Jan 28 01:57:47.801: INFO: Pod "annotationupdate1d7089aa-2352-4043-b3ba-5d4c5cc78c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.963287ms
    Jan 28 01:57:47.801: INFO: The phase of Pod annotationupdate1d7089aa-2352-4043-b3ba-5d4c5cc78c3b is Pending, waiting for it to be Running (with Ready = true)
    Jan 28 01:57:49.813: INFO: Pod "annotationupdate1d7089aa-2352-4043-b3ba-5d4c5cc78c3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.025984613s
    Jan 28 01:57:49.813: INFO: The phase of Pod annotationupdate1d7089aa-2352-4043-b3ba-5d4c5cc78c3b is Running (Ready = true)
    Jan 28 01:57:49.813: INFO: Pod "annotationupdate1d7089aa-2352-4043-b3ba-5d4c5cc78c3b" satisfied condition "running and ready"
    Jan 28 01:57:50.435: INFO: Successfully updated pod "annotationupdate1d7089aa-2352-4043-b3ba-5d4c5cc78c3b"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 28 01:57:52.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3607" for this suite. 01/28/23 01:57:52.508
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:57:52.529
Jan 28 01:57:52.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename job 01/28/23 01:57:52.53
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:57:52.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:57:52.581
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 01/28/23 01:57:52.601
STEP: Patching the Job 01/28/23 01:57:52.615
STEP: Watching for Job to be patched 01/28/23 01:57:52.643
Jan 28 01:57:52.648: INFO: Event ADDED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 28 01:57:52.648: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 28 01:57:52.648: INFO: Event MODIFIED found for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/28/23 01:57:52.649
STEP: Watching for Job to be updated 01/28/23 01:57:52.676
Jan 28 01:57:52.681: INFO: Event MODIFIED found for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 28 01:57:52.681: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/28/23 01:57:52.681
Jan 28 01:57:52.693: INFO: Job: e2e-drwj8 as labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8]
STEP: Waiting for job to complete 01/28/23 01:57:52.693
STEP: Delete a job collection with a labelselector 01/28/23 01:58:04.705
STEP: Watching for Job to be deleted 01/28/23 01:58:04.732
Jan 28 01:58:04.737: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 28 01:58:04.737: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 28 01:58:04.738: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 28 01:58:04.738: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 28 01:58:04.739: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 28 01:58:04.739: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 28 01:58:04.739: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 28 01:58:04.739: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 28 01:58:04.739: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 28 01:58:04.740: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 28 01:58:04.740: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 28 01:58:04.740: INFO: Event DELETED found for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/28/23 01:58:04.74
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 28 01:58:04.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5378" for this suite. 01/28/23 01:58:04.77
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":347,"skipped":6461,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.268 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:57:52.529
    Jan 28 01:57:52.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename job 01/28/23 01:57:52.53
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:57:52.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:57:52.581
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 01/28/23 01:57:52.601
    STEP: Patching the Job 01/28/23 01:57:52.615
    STEP: Watching for Job to be patched 01/28/23 01:57:52.643
    Jan 28 01:57:52.648: INFO: Event ADDED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 28 01:57:52.648: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 28 01:57:52.648: INFO: Event MODIFIED found for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/28/23 01:57:52.649
    STEP: Watching for Job to be updated 01/28/23 01:57:52.676
    Jan 28 01:57:52.681: INFO: Event MODIFIED found for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 28 01:57:52.681: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/28/23 01:57:52.681
    Jan 28 01:57:52.693: INFO: Job: e2e-drwj8 as labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8]
    STEP: Waiting for job to complete 01/28/23 01:57:52.693
    STEP: Delete a job collection with a labelselector 01/28/23 01:58:04.705
    STEP: Watching for Job to be deleted 01/28/23 01:58:04.732
    Jan 28 01:58:04.737: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 28 01:58:04.737: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 28 01:58:04.738: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 28 01:58:04.738: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 28 01:58:04.739: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 28 01:58:04.739: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 28 01:58:04.739: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 28 01:58:04.739: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 28 01:58:04.739: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 28 01:58:04.740: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 28 01:58:04.740: INFO: Event MODIFIED observed for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 28 01:58:04.740: INFO: Event DELETED found for Job e2e-drwj8 in namespace job-5378 with labels: map[e2e-drwj8:patched e2e-job-label:e2e-drwj8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/28/23 01:58:04.74
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 28 01:58:04.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5378" for this suite. 01/28/23 01:58:04.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:58:04.802
Jan 28 01:58:04.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename runtimeclass 01/28/23 01:58:04.804
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:58:04.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:58:04.838
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/28/23 01:58:04.848
STEP: getting /apis/node.k8s.io 01/28/23 01:58:04.855
STEP: getting /apis/node.k8s.io/v1 01/28/23 01:58:04.86
STEP: creating 01/28/23 01:58:04.864
STEP: watching 01/28/23 01:58:04.909
Jan 28 01:58:04.910: INFO: starting watch
STEP: getting 01/28/23 01:58:04.924
STEP: listing 01/28/23 01:58:04.935
STEP: patching 01/28/23 01:58:04.946
STEP: updating 01/28/23 01:58:04.958
Jan 28 01:58:04.971: INFO: waiting for watch events with expected annotations
STEP: deleting 01/28/23 01:58:04.972
STEP: deleting a collection 01/28/23 01:58:05.011
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 28 01:58:05.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2259" for this suite. 01/28/23 01:58:05.077
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":348,"skipped":6501,"failed":0}
------------------------------
â€¢ [0.291 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:58:04.802
    Jan 28 01:58:04.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename runtimeclass 01/28/23 01:58:04.804
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:58:04.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:58:04.838
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/28/23 01:58:04.848
    STEP: getting /apis/node.k8s.io 01/28/23 01:58:04.855
    STEP: getting /apis/node.k8s.io/v1 01/28/23 01:58:04.86
    STEP: creating 01/28/23 01:58:04.864
    STEP: watching 01/28/23 01:58:04.909
    Jan 28 01:58:04.910: INFO: starting watch
    STEP: getting 01/28/23 01:58:04.924
    STEP: listing 01/28/23 01:58:04.935
    STEP: patching 01/28/23 01:58:04.946
    STEP: updating 01/28/23 01:58:04.958
    Jan 28 01:58:04.971: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/28/23 01:58:04.972
    STEP: deleting a collection 01/28/23 01:58:05.011
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 28 01:58:05.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2259" for this suite. 01/28/23 01:58:05.077
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:58:05.093
Jan 28 01:58:05.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename downward-api 01/28/23 01:58:05.096
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:58:05.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:58:05.133
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 01/28/23 01:58:05.142
Jan 28 01:58:05.162: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3d18b08-7bca-4331-b060-f5396e1cffdd" in namespace "downward-api-9811" to be "Succeeded or Failed"
Jan 28 01:58:05.172: INFO: Pod "downwardapi-volume-d3d18b08-7bca-4331-b060-f5396e1cffdd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.8919ms
Jan 28 01:58:07.184: INFO: Pod "downwardapi-volume-d3d18b08-7bca-4331-b060-f5396e1cffdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022317098s
Jan 28 01:58:09.184: INFO: Pod "downwardapi-volume-d3d18b08-7bca-4331-b060-f5396e1cffdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022080863s
Jan 28 01:58:11.183: INFO: Pod "downwardapi-volume-d3d18b08-7bca-4331-b060-f5396e1cffdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021476679s
STEP: Saw pod success 01/28/23 01:58:11.183
Jan 28 01:58:11.184: INFO: Pod "downwardapi-volume-d3d18b08-7bca-4331-b060-f5396e1cffdd" satisfied condition "Succeeded or Failed"
Jan 28 01:58:11.195: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-d3d18b08-7bca-4331-b060-f5396e1cffdd container client-container: <nil>
STEP: delete the pod 01/28/23 01:58:11.231
Jan 28 01:58:11.257: INFO: Waiting for pod downwardapi-volume-d3d18b08-7bca-4331-b060-f5396e1cffdd to disappear
Jan 28 01:58:11.265: INFO: Pod downwardapi-volume-d3d18b08-7bca-4331-b060-f5396e1cffdd no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 28 01:58:11.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9811" for this suite. 01/28/23 01:58:11.281
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":349,"skipped":6503,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.204 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:58:05.093
    Jan 28 01:58:05.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename downward-api 01/28/23 01:58:05.096
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:58:05.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:58:05.133
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 01/28/23 01:58:05.142
    Jan 28 01:58:05.162: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3d18b08-7bca-4331-b060-f5396e1cffdd" in namespace "downward-api-9811" to be "Succeeded or Failed"
    Jan 28 01:58:05.172: INFO: Pod "downwardapi-volume-d3d18b08-7bca-4331-b060-f5396e1cffdd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.8919ms
    Jan 28 01:58:07.184: INFO: Pod "downwardapi-volume-d3d18b08-7bca-4331-b060-f5396e1cffdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022317098s
    Jan 28 01:58:09.184: INFO: Pod "downwardapi-volume-d3d18b08-7bca-4331-b060-f5396e1cffdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022080863s
    Jan 28 01:58:11.183: INFO: Pod "downwardapi-volume-d3d18b08-7bca-4331-b060-f5396e1cffdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021476679s
    STEP: Saw pod success 01/28/23 01:58:11.183
    Jan 28 01:58:11.184: INFO: Pod "downwardapi-volume-d3d18b08-7bca-4331-b060-f5396e1cffdd" satisfied condition "Succeeded or Failed"
    Jan 28 01:58:11.195: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-d3d18b08-7bca-4331-b060-f5396e1cffdd container client-container: <nil>
    STEP: delete the pod 01/28/23 01:58:11.231
    Jan 28 01:58:11.257: INFO: Waiting for pod downwardapi-volume-d3d18b08-7bca-4331-b060-f5396e1cffdd to disappear
    Jan 28 01:58:11.265: INFO: Pod downwardapi-volume-d3d18b08-7bca-4331-b060-f5396e1cffdd no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 28 01:58:11.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9811" for this suite. 01/28/23 01:58:11.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:58:11.319
Jan 28 01:58:11.319: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename watch 01/28/23 01:58:11.32
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:58:11.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:58:11.355
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/28/23 01:58:11.364
STEP: creating a watch on configmaps with label B 01/28/23 01:58:11.368
STEP: creating a watch on configmaps with label A or B 01/28/23 01:58:11.372
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/28/23 01:58:11.376
Jan 28 01:58:11.389: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8069  2f533a1b-f3b1-4e37-aeab-b762c87b0e18 48533 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 01:58:11.389: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8069  2f533a1b-f3b1-4e37-aeab-b762c87b0e18 48533 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/28/23 01:58:11.39
Jan 28 01:58:11.410: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8069  2f533a1b-f3b1-4e37-aeab-b762c87b0e18 48534 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 01:58:11.410: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8069  2f533a1b-f3b1-4e37-aeab-b762c87b0e18 48534 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/28/23 01:58:11.411
Jan 28 01:58:11.434: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8069  2f533a1b-f3b1-4e37-aeab-b762c87b0e18 48535 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 01:58:11.435: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8069  2f533a1b-f3b1-4e37-aeab-b762c87b0e18 48535 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/28/23 01:58:11.435
Jan 28 01:58:11.452: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8069  2f533a1b-f3b1-4e37-aeab-b762c87b0e18 48536 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 01:58:11.452: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8069  2f533a1b-f3b1-4e37-aeab-b762c87b0e18 48536 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/28/23 01:58:11.453
Jan 28 01:58:11.469: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8069  39cad42f-1634-4a81-8d49-0fd3f7009558 48537 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 01:58:11.469: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8069  39cad42f-1634-4a81-8d49-0fd3f7009558 48537 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/28/23 01:58:21.47
Jan 28 01:58:21.489: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8069  39cad42f-1634-4a81-8d49-0fd3f7009558 48557 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 01:58:21.490: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8069  39cad42f-1634-4a81-8d49-0fd3f7009558 48557 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 28 01:58:31.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8069" for this suite. 01/28/23 01:58:31.51
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":350,"skipped":6559,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.208 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:58:11.319
    Jan 28 01:58:11.319: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename watch 01/28/23 01:58:11.32
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:58:11.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:58:11.355
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/28/23 01:58:11.364
    STEP: creating a watch on configmaps with label B 01/28/23 01:58:11.368
    STEP: creating a watch on configmaps with label A or B 01/28/23 01:58:11.372
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/28/23 01:58:11.376
    Jan 28 01:58:11.389: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8069  2f533a1b-f3b1-4e37-aeab-b762c87b0e18 48533 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 28 01:58:11.389: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8069  2f533a1b-f3b1-4e37-aeab-b762c87b0e18 48533 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/28/23 01:58:11.39
    Jan 28 01:58:11.410: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8069  2f533a1b-f3b1-4e37-aeab-b762c87b0e18 48534 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 28 01:58:11.410: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8069  2f533a1b-f3b1-4e37-aeab-b762c87b0e18 48534 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/28/23 01:58:11.411
    Jan 28 01:58:11.434: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8069  2f533a1b-f3b1-4e37-aeab-b762c87b0e18 48535 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 28 01:58:11.435: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8069  2f533a1b-f3b1-4e37-aeab-b762c87b0e18 48535 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/28/23 01:58:11.435
    Jan 28 01:58:11.452: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8069  2f533a1b-f3b1-4e37-aeab-b762c87b0e18 48536 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 28 01:58:11.452: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8069  2f533a1b-f3b1-4e37-aeab-b762c87b0e18 48536 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/28/23 01:58:11.453
    Jan 28 01:58:11.469: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8069  39cad42f-1634-4a81-8d49-0fd3f7009558 48537 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 28 01:58:11.469: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8069  39cad42f-1634-4a81-8d49-0fd3f7009558 48537 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/28/23 01:58:21.47
    Jan 28 01:58:21.489: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8069  39cad42f-1634-4a81-8d49-0fd3f7009558 48557 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 28 01:58:21.490: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8069  39cad42f-1634-4a81-8d49-0fd3f7009558 48557 0 2023-01-28 01:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-28 01:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 28 01:58:31.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-8069" for this suite. 01/28/23 01:58:31.51
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:58:31.536
Jan 28 01:58:31.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename sched-preemption 01/28/23 01:58:31.539
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:58:31.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:58:31.58
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 28 01:58:31.623: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 28 01:59:31.733: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 01:59:31.745
Jan 28 01:59:31.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename sched-preemption-path 01/28/23 01:59:31.747
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:59:31.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:59:31.807
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 01/28/23 01:59:31.813
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/28/23 01:59:31.813
Jan 28 01:59:31.833: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-994" to be "running"
Jan 28 01:59:31.843: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.82035ms
Jan 28 01:59:33.854: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021603818s
Jan 28 01:59:35.853: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.020154186s
Jan 28 01:59:35.853: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/28/23 01:59:35.862
Jan 28 01:59:35.914: INFO: found a healthy node: 10.9.20.126
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Jan 28 01:59:52.085: INFO: pods created so far: [1 1 1]
Jan 28 01:59:52.085: INFO: length of pods created so far: 3
Jan 28 01:59:56.109: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Jan 28 02:00:03.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-994" for this suite. 01/28/23 02:00:03.128
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 28 02:00:03.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3852" for this suite. 01/28/23 02:00:03.257
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":351,"skipped":6561,"failed":0}
------------------------------
â€¢ [SLOW TEST] [91.869 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:58:31.536
    Jan 28 01:58:31.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename sched-preemption 01/28/23 01:58:31.539
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:58:31.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:58:31.58
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 28 01:58:31.623: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 28 01:59:31.733: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 01:59:31.745
    Jan 28 01:59:31.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename sched-preemption-path 01/28/23 01:59:31.747
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 01:59:31.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 01:59:31.807
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 01/28/23 01:59:31.813
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/28/23 01:59:31.813
    Jan 28 01:59:31.833: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-994" to be "running"
    Jan 28 01:59:31.843: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.82035ms
    Jan 28 01:59:33.854: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021603818s
    Jan 28 01:59:35.853: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.020154186s
    Jan 28 01:59:35.853: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/28/23 01:59:35.862
    Jan 28 01:59:35.914: INFO: found a healthy node: 10.9.20.126
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Jan 28 01:59:52.085: INFO: pods created so far: [1 1 1]
    Jan 28 01:59:52.085: INFO: length of pods created so far: 3
    Jan 28 01:59:56.109: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Jan 28 02:00:03.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-994" for this suite. 01/28/23 02:00:03.128
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 28 02:00:03.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-3852" for this suite. 01/28/23 02:00:03.257
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 02:00:03.405
Jan 28 02:00:03.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename container-runtime 01/28/23 02:00:03.408
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:03.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:03.462
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 01/28/23 02:00:03.468
STEP: wait for the container to reach Succeeded 01/28/23 02:00:03.488
STEP: get the container status 01/28/23 02:00:07.536
STEP: the container should be terminated 01/28/23 02:00:07.544
STEP: the termination message should be set 01/28/23 02:00:07.545
Jan 28 02:00:07.545: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/28/23 02:00:07.545
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 28 02:00:07.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5927" for this suite. 01/28/23 02:00:07.599
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":352,"skipped":6572,"failed":0}
------------------------------
â€¢ [4.210 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 02:00:03.405
    Jan 28 02:00:03.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename container-runtime 01/28/23 02:00:03.408
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:03.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:03.462
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 01/28/23 02:00:03.468
    STEP: wait for the container to reach Succeeded 01/28/23 02:00:03.488
    STEP: get the container status 01/28/23 02:00:07.536
    STEP: the container should be terminated 01/28/23 02:00:07.544
    STEP: the termination message should be set 01/28/23 02:00:07.545
    Jan 28 02:00:07.545: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/28/23 02:00:07.545
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 28 02:00:07.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-5927" for this suite. 01/28/23 02:00:07.599
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 02:00:07.617
Jan 28 02:00:07.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 02:00:07.619
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:07.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:07.671
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 01/28/23 02:00:07.679
Jan 28 02:00:07.699: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e7b75d32-4d5d-4d06-a8e6-c2a49ba49f39" in namespace "projected-3009" to be "Succeeded or Failed"
Jan 28 02:00:07.719: INFO: Pod "downwardapi-volume-e7b75d32-4d5d-4d06-a8e6-c2a49ba49f39": Phase="Pending", Reason="", readiness=false. Elapsed: 19.71244ms
Jan 28 02:00:09.732: INFO: Pod "downwardapi-volume-e7b75d32-4d5d-4d06-a8e6-c2a49ba49f39": Phase="Running", Reason="", readiness=true. Elapsed: 2.032785141s
Jan 28 02:00:11.729: INFO: Pod "downwardapi-volume-e7b75d32-4d5d-4d06-a8e6-c2a49ba49f39": Phase="Running", Reason="", readiness=false. Elapsed: 4.030136316s
Jan 28 02:00:13.729: INFO: Pod "downwardapi-volume-e7b75d32-4d5d-4d06-a8e6-c2a49ba49f39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029747584s
STEP: Saw pod success 01/28/23 02:00:13.729
Jan 28 02:00:13.729: INFO: Pod "downwardapi-volume-e7b75d32-4d5d-4d06-a8e6-c2a49ba49f39" satisfied condition "Succeeded or Failed"
Jan 28 02:00:13.739: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-e7b75d32-4d5d-4d06-a8e6-c2a49ba49f39 container client-container: <nil>
STEP: delete the pod 01/28/23 02:00:13.807
Jan 28 02:00:13.847: INFO: Waiting for pod downwardapi-volume-e7b75d32-4d5d-4d06-a8e6-c2a49ba49f39 to disappear
Jan 28 02:00:13.856: INFO: Pod downwardapi-volume-e7b75d32-4d5d-4d06-a8e6-c2a49ba49f39 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 28 02:00:13.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3009" for this suite. 01/28/23 02:00:13.87
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":353,"skipped":6574,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.270 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 02:00:07.617
    Jan 28 02:00:07.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 02:00:07.619
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:07.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:07.671
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 01/28/23 02:00:07.679
    Jan 28 02:00:07.699: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e7b75d32-4d5d-4d06-a8e6-c2a49ba49f39" in namespace "projected-3009" to be "Succeeded or Failed"
    Jan 28 02:00:07.719: INFO: Pod "downwardapi-volume-e7b75d32-4d5d-4d06-a8e6-c2a49ba49f39": Phase="Pending", Reason="", readiness=false. Elapsed: 19.71244ms
    Jan 28 02:00:09.732: INFO: Pod "downwardapi-volume-e7b75d32-4d5d-4d06-a8e6-c2a49ba49f39": Phase="Running", Reason="", readiness=true. Elapsed: 2.032785141s
    Jan 28 02:00:11.729: INFO: Pod "downwardapi-volume-e7b75d32-4d5d-4d06-a8e6-c2a49ba49f39": Phase="Running", Reason="", readiness=false. Elapsed: 4.030136316s
    Jan 28 02:00:13.729: INFO: Pod "downwardapi-volume-e7b75d32-4d5d-4d06-a8e6-c2a49ba49f39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029747584s
    STEP: Saw pod success 01/28/23 02:00:13.729
    Jan 28 02:00:13.729: INFO: Pod "downwardapi-volume-e7b75d32-4d5d-4d06-a8e6-c2a49ba49f39" satisfied condition "Succeeded or Failed"
    Jan 28 02:00:13.739: INFO: Trying to get logs from node 10.9.20.126 pod downwardapi-volume-e7b75d32-4d5d-4d06-a8e6-c2a49ba49f39 container client-container: <nil>
    STEP: delete the pod 01/28/23 02:00:13.807
    Jan 28 02:00:13.847: INFO: Waiting for pod downwardapi-volume-e7b75d32-4d5d-4d06-a8e6-c2a49ba49f39 to disappear
    Jan 28 02:00:13.856: INFO: Pod downwardapi-volume-e7b75d32-4d5d-4d06-a8e6-c2a49ba49f39 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 28 02:00:13.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3009" for this suite. 01/28/23 02:00:13.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 02:00:13.891
Jan 28 02:00:13.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename configmap 01/28/23 02:00:13.893
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:13.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:13.945
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-8393ff7d-cb1c-44cb-a705-0ba2ad81cfef 01/28/23 02:00:13.952
STEP: Creating a pod to test consume configMaps 01/28/23 02:00:13.972
Jan 28 02:00:13.992: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a268bce-1193-461b-afd8-5e4de5d6b4c5" in namespace "configmap-1492" to be "Succeeded or Failed"
Jan 28 02:00:14.004: INFO: Pod "pod-configmaps-5a268bce-1193-461b-afd8-5e4de5d6b4c5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.130252ms
Jan 28 02:00:16.014: INFO: Pod "pod-configmaps-5a268bce-1193-461b-afd8-5e4de5d6b4c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02183162s
Jan 28 02:00:18.015: INFO: Pod "pod-configmaps-5a268bce-1193-461b-afd8-5e4de5d6b4c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022619038s
Jan 28 02:00:20.014: INFO: Pod "pod-configmaps-5a268bce-1193-461b-afd8-5e4de5d6b4c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021831431s
STEP: Saw pod success 01/28/23 02:00:20.014
Jan 28 02:00:20.015: INFO: Pod "pod-configmaps-5a268bce-1193-461b-afd8-5e4de5d6b4c5" satisfied condition "Succeeded or Failed"
Jan 28 02:00:20.024: INFO: Trying to get logs from node 10.9.20.126 pod pod-configmaps-5a268bce-1193-461b-afd8-5e4de5d6b4c5 container agnhost-container: <nil>
STEP: delete the pod 01/28/23 02:00:20.049
Jan 28 02:00:20.081: INFO: Waiting for pod pod-configmaps-5a268bce-1193-461b-afd8-5e4de5d6b4c5 to disappear
Jan 28 02:00:20.090: INFO: Pod pod-configmaps-5a268bce-1193-461b-afd8-5e4de5d6b4c5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 28 02:00:20.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1492" for this suite. 01/28/23 02:00:20.102
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":354,"skipped":6584,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.226 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 02:00:13.891
    Jan 28 02:00:13.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename configmap 01/28/23 02:00:13.893
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:13.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:13.945
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-8393ff7d-cb1c-44cb-a705-0ba2ad81cfef 01/28/23 02:00:13.952
    STEP: Creating a pod to test consume configMaps 01/28/23 02:00:13.972
    Jan 28 02:00:13.992: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a268bce-1193-461b-afd8-5e4de5d6b4c5" in namespace "configmap-1492" to be "Succeeded or Failed"
    Jan 28 02:00:14.004: INFO: Pod "pod-configmaps-5a268bce-1193-461b-afd8-5e4de5d6b4c5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.130252ms
    Jan 28 02:00:16.014: INFO: Pod "pod-configmaps-5a268bce-1193-461b-afd8-5e4de5d6b4c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02183162s
    Jan 28 02:00:18.015: INFO: Pod "pod-configmaps-5a268bce-1193-461b-afd8-5e4de5d6b4c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022619038s
    Jan 28 02:00:20.014: INFO: Pod "pod-configmaps-5a268bce-1193-461b-afd8-5e4de5d6b4c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021831431s
    STEP: Saw pod success 01/28/23 02:00:20.014
    Jan 28 02:00:20.015: INFO: Pod "pod-configmaps-5a268bce-1193-461b-afd8-5e4de5d6b4c5" satisfied condition "Succeeded or Failed"
    Jan 28 02:00:20.024: INFO: Trying to get logs from node 10.9.20.126 pod pod-configmaps-5a268bce-1193-461b-afd8-5e4de5d6b4c5 container agnhost-container: <nil>
    STEP: delete the pod 01/28/23 02:00:20.049
    Jan 28 02:00:20.081: INFO: Waiting for pod pod-configmaps-5a268bce-1193-461b-afd8-5e4de5d6b4c5 to disappear
    Jan 28 02:00:20.090: INFO: Pod pod-configmaps-5a268bce-1193-461b-afd8-5e4de5d6b4c5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 28 02:00:20.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1492" for this suite. 01/28/23 02:00:20.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 02:00:20.127
Jan 28 02:00:20.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename projected 01/28/23 02:00:20.129
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:20.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:20.172
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-45a516cd-8573-4617-9bbb-d86443c5e65c 01/28/23 02:00:20.208
STEP: Creating a pod to test consume configMaps 01/28/23 02:00:20.22
Jan 28 02:00:20.240: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a6bbada9-8496-484d-af74-2c5a6801cfbf" in namespace "projected-7499" to be "Succeeded or Failed"
Jan 28 02:00:20.251: INFO: Pod "pod-projected-configmaps-a6bbada9-8496-484d-af74-2c5a6801cfbf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.933957ms
Jan 28 02:00:22.262: INFO: Pod "pod-projected-configmaps-a6bbada9-8496-484d-af74-2c5a6801cfbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021595001s
Jan 28 02:00:24.262: INFO: Pod "pod-projected-configmaps-a6bbada9-8496-484d-af74-2c5a6801cfbf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021229849s
Jan 28 02:00:26.261: INFO: Pod "pod-projected-configmaps-a6bbada9-8496-484d-af74-2c5a6801cfbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019968942s
STEP: Saw pod success 01/28/23 02:00:26.261
Jan 28 02:00:26.262: INFO: Pod "pod-projected-configmaps-a6bbada9-8496-484d-af74-2c5a6801cfbf" satisfied condition "Succeeded or Failed"
Jan 28 02:00:26.271: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-configmaps-a6bbada9-8496-484d-af74-2c5a6801cfbf container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/28/23 02:00:26.294
Jan 28 02:00:26.320: INFO: Waiting for pod pod-projected-configmaps-a6bbada9-8496-484d-af74-2c5a6801cfbf to disappear
Jan 28 02:00:26.330: INFO: Pod pod-projected-configmaps-a6bbada9-8496-484d-af74-2c5a6801cfbf no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 28 02:00:26.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7499" for this suite. 01/28/23 02:00:26.344
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":355,"skipped":6616,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.238 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 02:00:20.127
    Jan 28 02:00:20.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename projected 01/28/23 02:00:20.129
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:20.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:20.172
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-45a516cd-8573-4617-9bbb-d86443c5e65c 01/28/23 02:00:20.208
    STEP: Creating a pod to test consume configMaps 01/28/23 02:00:20.22
    Jan 28 02:00:20.240: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a6bbada9-8496-484d-af74-2c5a6801cfbf" in namespace "projected-7499" to be "Succeeded or Failed"
    Jan 28 02:00:20.251: INFO: Pod "pod-projected-configmaps-a6bbada9-8496-484d-af74-2c5a6801cfbf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.933957ms
    Jan 28 02:00:22.262: INFO: Pod "pod-projected-configmaps-a6bbada9-8496-484d-af74-2c5a6801cfbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021595001s
    Jan 28 02:00:24.262: INFO: Pod "pod-projected-configmaps-a6bbada9-8496-484d-af74-2c5a6801cfbf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021229849s
    Jan 28 02:00:26.261: INFO: Pod "pod-projected-configmaps-a6bbada9-8496-484d-af74-2c5a6801cfbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019968942s
    STEP: Saw pod success 01/28/23 02:00:26.261
    Jan 28 02:00:26.262: INFO: Pod "pod-projected-configmaps-a6bbada9-8496-484d-af74-2c5a6801cfbf" satisfied condition "Succeeded or Failed"
    Jan 28 02:00:26.271: INFO: Trying to get logs from node 10.9.20.126 pod pod-projected-configmaps-a6bbada9-8496-484d-af74-2c5a6801cfbf container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/28/23 02:00:26.294
    Jan 28 02:00:26.320: INFO: Waiting for pod pod-projected-configmaps-a6bbada9-8496-484d-af74-2c5a6801cfbf to disappear
    Jan 28 02:00:26.330: INFO: Pod pod-projected-configmaps-a6bbada9-8496-484d-af74-2c5a6801cfbf no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 28 02:00:26.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7499" for this suite. 01/28/23 02:00:26.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 02:00:26.368
Jan 28 02:00:26.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename disruption 01/28/23 02:00:26.371
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:26.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:26.415
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 01/28/23 02:00:26.433
STEP: Updating PodDisruptionBudget status 01/28/23 02:00:28.452
STEP: Waiting for all pods to be running 01/28/23 02:00:28.476
Jan 28 02:00:28.489: INFO: running pods: 0 < 1
STEP: locating a running pod 01/28/23 02:00:30.499
STEP: Waiting for the pdb to be processed 01/28/23 02:00:30.554
STEP: Patching PodDisruptionBudget status 01/28/23 02:00:30.574
STEP: Waiting for the pdb to be processed 01/28/23 02:00:30.597
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 28 02:00:30.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1172" for this suite. 01/28/23 02:00:30.619
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":356,"skipped":6629,"failed":0}
------------------------------
â€¢ [4.267 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 02:00:26.368
    Jan 28 02:00:26.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename disruption 01/28/23 02:00:26.371
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:26.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:26.415
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 01/28/23 02:00:26.433
    STEP: Updating PodDisruptionBudget status 01/28/23 02:00:28.452
    STEP: Waiting for all pods to be running 01/28/23 02:00:28.476
    Jan 28 02:00:28.489: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/28/23 02:00:30.499
    STEP: Waiting for the pdb to be processed 01/28/23 02:00:30.554
    STEP: Patching PodDisruptionBudget status 01/28/23 02:00:30.574
    STEP: Waiting for the pdb to be processed 01/28/23 02:00:30.597
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 28 02:00:30.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1172" for this suite. 01/28/23 02:00:30.619
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 02:00:30.636
Jan 28 02:00:30.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename services 01/28/23 02:00:30.638
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:30.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:30.683
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-4603 01/28/23 02:00:30.691
STEP: creating service affinity-nodeport in namespace services-4603 01/28/23 02:00:30.692
STEP: creating replication controller affinity-nodeport in namespace services-4603 01/28/23 02:00:30.754
I0128 02:00:30.771463      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4603, replica count: 3
I0128 02:00:33.823323      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 02:00:33.857: INFO: Creating new exec pod
Jan 28 02:00:33.876: INFO: Waiting up to 5m0s for pod "execpod-affinityswtng" in namespace "services-4603" to be "running"
Jan 28 02:00:33.885: INFO: Pod "execpod-affinityswtng": Phase="Pending", Reason="", readiness=false. Elapsed: 9.157847ms
Jan 28 02:00:35.896: INFO: Pod "execpod-affinityswtng": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019859645s
Jan 28 02:00:37.895: INFO: Pod "execpod-affinityswtng": Phase="Running", Reason="", readiness=true. Elapsed: 4.019064616s
Jan 28 02:00:37.896: INFO: Pod "execpod-affinityswtng" satisfied condition "running"
Jan 28 02:00:38.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-4603 exec execpod-affinityswtng -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan 28 02:00:39.268: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 28 02:00:39.268: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 02:00:39.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-4603 exec execpod-affinityswtng -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.236.160 80'
Jan 28 02:00:39.593: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.236.160 80\nConnection to 172.21.236.160 80 port [tcp/http] succeeded!\n"
Jan 28 02:00:39.593: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 02:00:39.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-4603 exec execpod-affinityswtng -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.75 30004'
Jan 28 02:00:39.875: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.9.20.75 30004\nConnection to 10.9.20.75 30004 port [tcp/*] succeeded!\n"
Jan 28 02:00:39.875: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 02:00:39.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-4603 exec execpod-affinityswtng -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.126 30004'
Jan 28 02:00:40.189: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.9.20.126 30004\nConnection to 10.9.20.126 30004 port [tcp/*] succeeded!\n"
Jan 28 02:00:40.189: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 02:00:40.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-4603 exec execpod-affinityswtng -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.9.20.126:30004/ ; done'
Jan 28 02:00:40.639: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n"
Jan 28 02:00:40.639: INFO: stdout: "\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v"
Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
Jan 28 02:00:40.639: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4603, will wait for the garbage collector to delete the pods 01/28/23 02:00:40.673
Jan 28 02:00:40.767: INFO: Deleting ReplicationController affinity-nodeport took: 19.352292ms
Jan 28 02:00:40.868: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.00992ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 28 02:00:43.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4603" for this suite. 01/28/23 02:00:43.642
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":357,"skipped":6629,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.025 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 02:00:30.636
    Jan 28 02:00:30.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename services 01/28/23 02:00:30.638
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:30.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:30.683
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-4603 01/28/23 02:00:30.691
    STEP: creating service affinity-nodeport in namespace services-4603 01/28/23 02:00:30.692
    STEP: creating replication controller affinity-nodeport in namespace services-4603 01/28/23 02:00:30.754
    I0128 02:00:30.771463      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4603, replica count: 3
    I0128 02:00:33.823323      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 28 02:00:33.857: INFO: Creating new exec pod
    Jan 28 02:00:33.876: INFO: Waiting up to 5m0s for pod "execpod-affinityswtng" in namespace "services-4603" to be "running"
    Jan 28 02:00:33.885: INFO: Pod "execpod-affinityswtng": Phase="Pending", Reason="", readiness=false. Elapsed: 9.157847ms
    Jan 28 02:00:35.896: INFO: Pod "execpod-affinityswtng": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019859645s
    Jan 28 02:00:37.895: INFO: Pod "execpod-affinityswtng": Phase="Running", Reason="", readiness=true. Elapsed: 4.019064616s
    Jan 28 02:00:37.896: INFO: Pod "execpod-affinityswtng" satisfied condition "running"
    Jan 28 02:00:38.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-4603 exec execpod-affinityswtng -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Jan 28 02:00:39.268: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan 28 02:00:39.268: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 02:00:39.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-4603 exec execpod-affinityswtng -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.236.160 80'
    Jan 28 02:00:39.593: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.236.160 80\nConnection to 172.21.236.160 80 port [tcp/http] succeeded!\n"
    Jan 28 02:00:39.593: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 02:00:39.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-4603 exec execpod-affinityswtng -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.75 30004'
    Jan 28 02:00:39.875: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.9.20.75 30004\nConnection to 10.9.20.75 30004 port [tcp/*] succeeded!\n"
    Jan 28 02:00:39.875: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 02:00:39.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-4603 exec execpod-affinityswtng -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.9.20.126 30004'
    Jan 28 02:00:40.189: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.9.20.126 30004\nConnection to 10.9.20.126 30004 port [tcp/*] succeeded!\n"
    Jan 28 02:00:40.189: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 28 02:00:40.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=services-4603 exec execpod-affinityswtng -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.9.20.126:30004/ ; done'
    Jan 28 02:00:40.639: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.20.126:30004/\n"
    Jan 28 02:00:40.639: INFO: stdout: "\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v\naffinity-nodeport-bcw4v"
    Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
    Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
    Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
    Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
    Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
    Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
    Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
    Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
    Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
    Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
    Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
    Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
    Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
    Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
    Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
    Jan 28 02:00:40.639: INFO: Received response from host: affinity-nodeport-bcw4v
    Jan 28 02:00:40.639: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-4603, will wait for the garbage collector to delete the pods 01/28/23 02:00:40.673
    Jan 28 02:00:40.767: INFO: Deleting ReplicationController affinity-nodeport took: 19.352292ms
    Jan 28 02:00:40.868: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.00992ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 28 02:00:43.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4603" for this suite. 01/28/23 02:00:43.642
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 02:00:43.663
Jan 28 02:00:43.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename container-runtime 01/28/23 02:00:43.666
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:43.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:43.717
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 01/28/23 02:00:43.73
STEP: wait for the container to reach Succeeded 01/28/23 02:00:43.753
STEP: get the container status 01/28/23 02:00:47.819
STEP: the container should be terminated 01/28/23 02:00:47.831
STEP: the termination message should be set 01/28/23 02:00:47.831
Jan 28 02:00:47.832: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/28/23 02:00:47.832
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 28 02:00:47.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4400" for this suite. 01/28/23 02:00:47.91
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":358,"skipped":6643,"failed":0}
------------------------------
â€¢ [4.266 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 02:00:43.663
    Jan 28 02:00:43.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename container-runtime 01/28/23 02:00:43.666
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:43.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:43.717
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 01/28/23 02:00:43.73
    STEP: wait for the container to reach Succeeded 01/28/23 02:00:43.753
    STEP: get the container status 01/28/23 02:00:47.819
    STEP: the container should be terminated 01/28/23 02:00:47.831
    STEP: the termination message should be set 01/28/23 02:00:47.831
    Jan 28 02:00:47.832: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/28/23 02:00:47.832
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 28 02:00:47.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-4400" for this suite. 01/28/23 02:00:47.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 02:00:47.945
Jan 28 02:00:47.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename custom-resource-definition 01/28/23 02:00:47.946
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:47.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:48
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan 28 02:00:48.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 28 02:00:48.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7022" for this suite. 01/28/23 02:00:48.646
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":359,"skipped":6672,"failed":0}
------------------------------
â€¢ [0.720 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 02:00:47.945
    Jan 28 02:00:47.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename custom-resource-definition 01/28/23 02:00:47.946
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:47.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:48
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan 28 02:00:48.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 28 02:00:48.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7022" for this suite. 01/28/23 02:00:48.646
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 02:00:48.665
Jan 28 02:00:48.665: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename kubectl 01/28/23 02:00:48.667
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:48.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:48.72
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Jan 28 02:00:48.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-7590 version'
Jan 28 02:00:48.832: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 28 02:00:48.832: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:22:09Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6+IKS\", GitCommit:\"86cd3c4e19ac0b0bc4f7f6bae0e9df6f5517942f\", GitTreeState:\"clean\", BuildDate:\"2023-01-19T21:31:32Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 28 02:00:48.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7590" for this suite. 01/28/23 02:00:48.848
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":360,"skipped":6674,"failed":0}
------------------------------
â€¢ [0.200 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 02:00:48.665
    Jan 28 02:00:48.665: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename kubectl 01/28/23 02:00:48.667
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:48.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:48.72
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Jan 28 02:00:48.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3522974414 --namespace=kubectl-7590 version'
    Jan 28 02:00:48.832: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan 28 02:00:48.832: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:22:09Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6+IKS\", GitCommit:\"86cd3c4e19ac0b0bc4f7f6bae0e9df6f5517942f\", GitTreeState:\"clean\", BuildDate:\"2023-01-19T21:31:32Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 28 02:00:48.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7590" for this suite. 01/28/23 02:00:48.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 02:00:48.881
Jan 28 02:00:48.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename deployment 01/28/23 02:00:48.882
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:48.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:48.94
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan 28 02:00:48.953: INFO: Creating deployment "webserver-deployment"
Jan 28 02:00:48.972: INFO: Waiting for observed generation 1
Jan 28 02:00:50.998: INFO: Waiting for all required pods to come up
Jan 28 02:00:51.011: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/28/23 02:00:51.011
Jan 28 02:00:51.012: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hv5fn" in namespace "deployment-2853" to be "running"
Jan 28 02:00:51.012: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-ttxjm" in namespace "deployment-2853" to be "running"
Jan 28 02:00:51.012: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-pxwbt" in namespace "deployment-2853" to be "running"
Jan 28 02:00:51.012: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-b69hb" in namespace "deployment-2853" to be "running"
Jan 28 02:00:51.012: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-fh8c6" in namespace "deployment-2853" to be "running"
Jan 28 02:00:51.012: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-jcphv" in namespace "deployment-2853" to be "running"
Jan 28 02:00:51.012: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-5zk9t" in namespace "deployment-2853" to be "running"
Jan 28 02:00:51.012: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-lmd5z" in namespace "deployment-2853" to be "running"
Jan 28 02:00:51.024: INFO: Pod "webserver-deployment-845c8977d9-hv5fn": Phase="Pending", Reason="", readiness=false. Elapsed: 12.769053ms
Jan 28 02:00:51.030: INFO: Pod "webserver-deployment-845c8977d9-b69hb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.518797ms
Jan 28 02:00:51.031: INFO: Pod "webserver-deployment-845c8977d9-pxwbt": Phase="Pending", Reason="", readiness=false. Elapsed: 19.008812ms
Jan 28 02:00:51.031: INFO: Pod "webserver-deployment-845c8977d9-lmd5z": Phase="Pending", Reason="", readiness=false. Elapsed: 18.608492ms
Jan 28 02:00:51.031: INFO: Pod "webserver-deployment-845c8977d9-jcphv": Phase="Pending", Reason="", readiness=false. Elapsed: 18.990519ms
Jan 28 02:00:51.031: INFO: Pod "webserver-deployment-845c8977d9-ttxjm": Phase="Pending", Reason="", readiness=false. Elapsed: 19.374003ms
Jan 28 02:00:51.031: INFO: Pod "webserver-deployment-845c8977d9-fh8c6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.344986ms
Jan 28 02:00:51.031: INFO: Pod "webserver-deployment-845c8977d9-5zk9t": Phase="Pending", Reason="", readiness=false. Elapsed: 19.074521ms
Jan 28 02:00:53.039: INFO: Pod "webserver-deployment-845c8977d9-hv5fn": Phase="Running", Reason="", readiness=true. Elapsed: 2.02759564s
Jan 28 02:00:53.039: INFO: Pod "webserver-deployment-845c8977d9-hv5fn" satisfied condition "running"
Jan 28 02:00:53.044: INFO: Pod "webserver-deployment-845c8977d9-pxwbt": Phase="Running", Reason="", readiness=true. Elapsed: 2.032750151s
Jan 28 02:00:53.044: INFO: Pod "webserver-deployment-845c8977d9-pxwbt" satisfied condition "running"
Jan 28 02:00:53.049: INFO: Pod "webserver-deployment-845c8977d9-5zk9t": Phase="Running", Reason="", readiness=true. Elapsed: 2.036768633s
Jan 28 02:00:53.049: INFO: Pod "webserver-deployment-845c8977d9-5zk9t" satisfied condition "running"
Jan 28 02:00:53.049: INFO: Pod "webserver-deployment-845c8977d9-ttxjm": Phase="Running", Reason="", readiness=true. Elapsed: 2.037543062s
Jan 28 02:00:53.049: INFO: Pod "webserver-deployment-845c8977d9-ttxjm" satisfied condition "running"
Jan 28 02:00:53.051: INFO: Pod "webserver-deployment-845c8977d9-b69hb": Phase="Running", Reason="", readiness=true. Elapsed: 2.039337033s
Jan 28 02:00:53.051: INFO: Pod "webserver-deployment-845c8977d9-b69hb" satisfied condition "running"
Jan 28 02:00:53.052: INFO: Pod "webserver-deployment-845c8977d9-lmd5z": Phase="Running", Reason="", readiness=true. Elapsed: 2.03941275s
Jan 28 02:00:53.052: INFO: Pod "webserver-deployment-845c8977d9-lmd5z" satisfied condition "running"
Jan 28 02:00:53.052: INFO: Pod "webserver-deployment-845c8977d9-fh8c6": Phase="Running", Reason="", readiness=true. Elapsed: 2.040312104s
Jan 28 02:00:53.053: INFO: Pod "webserver-deployment-845c8977d9-fh8c6" satisfied condition "running"
Jan 28 02:00:53.052: INFO: Pod "webserver-deployment-845c8977d9-jcphv": Phase="Running", Reason="", readiness=true. Elapsed: 2.04026185s
Jan 28 02:00:53.053: INFO: Pod "webserver-deployment-845c8977d9-jcphv" satisfied condition "running"
Jan 28 02:00:53.053: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 28 02:00:53.077: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 28 02:00:53.105: INFO: Updating deployment webserver-deployment
Jan 28 02:00:53.105: INFO: Waiting for observed generation 2
Jan 28 02:00:55.132: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 28 02:00:55.144: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 28 02:00:55.157: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 28 02:00:55.193: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 28 02:00:55.193: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 28 02:00:55.205: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 28 02:00:55.228: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 28 02:00:55.228: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 28 02:00:55.257: INFO: Updating deployment webserver-deployment
Jan 28 02:00:55.257: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 28 02:00:55.280: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 28 02:00:57.316: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 28 02:00:57.340: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2853  29629f5d-d8b5-4d2e-9749-5d43122c029d 49580 3 2023-01-28 02:00:48 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00346d1d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-28 02:00:55 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-28 02:00:55 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 28 02:00:57.358: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-2853  bf0935ff-cff0-42e8-8b6d-8324252eb994 49577 3 2023-01-28 02:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 29629f5d-d8b5-4d2e-9749-5d43122c029d 0xc003967087 0xc003967088}] [] [{kube-controller-manager Update apps/v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"29629f5d-d8b5-4d2e-9749-5d43122c029d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003967128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 28 02:00:57.359: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 28 02:00:57.360: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-2853  1758a602-a36b-4331-9c71-3e141c2d1e4b 49568 3 2023-01-28 02:00:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 29629f5d-d8b5-4d2e-9749-5d43122c029d 0xc003967187 0xc003967188}] [] [{kube-controller-manager Update apps/v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"29629f5d-d8b5-4d2e-9749-5d43122c029d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003967218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 28 02:00:57.387: INFO: Pod "webserver-deployment-69b7448995-6sl8j" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-6sl8j webserver-deployment-69b7448995- deployment-2853  c2fb5886-6bd9-499a-ae87-cd6076fa4623 49485 0 2023-01-28 02:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:86c2eac7d62ceb41e6eef301b9f8d8001301176b6954972f0dee81ade1f96e14 cni.projectcalico.org/podIP:172.30.12.218/32 cni.projectcalico.org/podIPs:172.30.12.218/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc00346d5e7 0xc00346d5e8}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zgk8g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zgk8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.387: INFO: Pod "webserver-deployment-69b7448995-7lx6s" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-7lx6s webserver-deployment-69b7448995- deployment-2853  56ab1342-a4db-4425-91b5-a6ec66a7bb42 49506 0 2023-01-28 02:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:699977199930ae2af22ae27da720e50ae906d126352dd77f3b1bba7e41287430 cni.projectcalico.org/podIP:172.30.185.59/32 cni.projectcalico.org/podIPs:172.30.185.59/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc00346d830 0xc00346d831}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8msc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8msc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:,StartTime:2023-01-28 02:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.387: INFO: Pod "webserver-deployment-69b7448995-7w7bj" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-7w7bj webserver-deployment-69b7448995- deployment-2853  c6b58307-29e4-4a6c-8be0-65f26cd68734 49613 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc00346da40 0xc00346da41}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l87vd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l87vd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.388: INFO: Pod "webserver-deployment-69b7448995-8rtgg" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-8rtgg webserver-deployment-69b7448995- deployment-2853  ee2c9339-727f-432c-9062-e8f6c09626d0 49677 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:0f1b7b024c8d015f75d0d28d5750c8b3ab6a8d3618dd1bd83afbf8a23268db21 cni.projectcalico.org/podIP:172.30.12.219/32 cni.projectcalico.org/podIPs:172.30.12.219/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc00346dc40 0xc00346dc41}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7nnpd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7nnpd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.388: INFO: Pod "webserver-deployment-69b7448995-8sqrx" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-8sqrx webserver-deployment-69b7448995- deployment-2853  7986908f-a299-4026-9abf-3700a4f365e8 49634 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:b6d4e88c33b2cd7bcca358339ee390363e719c71e4d215c190fc05c4279a1d20 cni.projectcalico.org/podIP:172.30.84.58/32 cni.projectcalico.org/podIPs:172.30.84.58/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc00346de60 0xc00346de61}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4w6mq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4w6mq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.388: INFO: Pod "webserver-deployment-69b7448995-bqpd2" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-bqpd2 webserver-deployment-69b7448995- deployment-2853  c595634b-bc29-4565-9f74-0fa8da1b6d16 49607 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc000c1c570 0xc000c1c571}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s4tfm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s4tfm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.388: INFO: Pod "webserver-deployment-69b7448995-crmnz" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-crmnz webserver-deployment-69b7448995- deployment-2853  dfaef315-ea00-433c-9f8b-265d052e11fb 49495 0 2023-01-28 02:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:bc50f2240c80ea9c2c18632e825738146155fb734de0eea582e186f05067e951 cni.projectcalico.org/podIP:172.30.84.60/32 cni.projectcalico.org/podIPs:172.30.84.60/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc000c1de30 0xc000c1de31}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ddp8f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ddp8f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:,StartTime:2023-01-28 02:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.389: INFO: Pod "webserver-deployment-69b7448995-dpmfs" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-dpmfs webserver-deployment-69b7448995- deployment-2853  7de4c0a7-d62f-4149-9217-3767d194624e 49684 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:be14b3d2850c5bf287cbcddedd9bfed12fd1f997e96f0d2420b9d15b97602542 cni.projectcalico.org/podIP:172.30.185.38/32 cni.projectcalico.org/podIPs:172.30.185.38/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc003a2e210 0xc003a2e211}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b2vnf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b2vnf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.390: INFO: Pod "webserver-deployment-69b7448995-h7rfr" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-h7rfr webserver-deployment-69b7448995- deployment-2853  4033f90e-94a4-48da-b25d-fd3711889a47 49637 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:65651799da49632a88c833e61cac9c28190efbe7abd99ee231b2ae86929d5cc6 cni.projectcalico.org/podIP:172.30.185.58/32 cni.projectcalico.org/podIPs:172.30.185.58/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc003a2e440 0xc003a2e441}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q5xms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q5xms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.391: INFO: Pod "webserver-deployment-69b7448995-h9c58" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-h9c58 webserver-deployment-69b7448995- deployment-2853  14d5a167-366c-4342-856a-68992c58b6dc 49487 0 2023-01-28 02:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:70b2c151de3ec8169f7ba2d818b9ef7c692a683a7936f6cce52e91d63a84c585 cni.projectcalico.org/podIP:172.30.185.60/32 cni.projectcalico.org/podIPs:172.30.185.60/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc003a2e670 0xc003a2e671}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h5rkb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h5rkb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:,StartTime:2023-01-28 02:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.392: INFO: Pod "webserver-deployment-69b7448995-nwtc9" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-nwtc9 webserver-deployment-69b7448995- deployment-2853  1d3d5653-e12c-4a90-82b7-535284ba2eba 49697 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:09bc76ecfa9e06ee2dc88ed3d37e1f2cccf7c24aecfc58415e430955cf47bfc6 cni.projectcalico.org/podIP:172.30.84.21/32 cni.projectcalico.org/podIPs:172.30.84.21/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc003a2e890 0xc003a2e891}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qcxrh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcxrh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.393: INFO: Pod "webserver-deployment-69b7448995-pnmpz" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-pnmpz webserver-deployment-69b7448995- deployment-2853  c0cf51ee-508c-40d8-8f14-b76fccae6407 49511 0 2023-01-28 02:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:37e61a1d09e4da02b84050c2a43ceb2c04e50988d661c51884fc078329b0eb8f cni.projectcalico.org/podIP:172.30.12.211/32 cni.projectcalico.org/podIPs:172.30.12.211/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc003a2eab0 0xc003a2eab1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tn59c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tn59c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.393: INFO: Pod "webserver-deployment-69b7448995-w6l4w" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-w6l4w webserver-deployment-69b7448995- deployment-2853  3da88e36-8eb4-43d0-9d4c-52b09168c29f 49620 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc003a2ecb0 0xc003a2ecb1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2nnbq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2nnbq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.394: INFO: Pod "webserver-deployment-845c8977d9-5rlfv" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-5rlfv webserver-deployment-845c8977d9- deployment-2853  de3346c3-6715-40c1-a00a-db464c95adae 49693 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1038cb703acb46b0d85fffe3bcb45bc4bdf1e017b9b67509f50ec171113bdee2 cni.projectcalico.org/podIP:172.30.12.239/32 cni.projectcalico.org/podIPs:172.30.12.239/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003a2eeb0 0xc003a2eeb1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t6df9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t6df9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.394: INFO: Pod "webserver-deployment-845c8977d9-5v56n" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-5v56n webserver-deployment-845c8977d9- deployment-2853  d3ba4e6f-c8d5-41ba-b74a-d8830c0aaf38 49679 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:43b4d3140136eb76fe868870b62a4f560789905623a7425ecc2e6a719fe48d96 cni.projectcalico.org/podIP:172.30.84.62/32 cni.projectcalico.org/podIPs:172.30.84.62/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003a2f0b7 0xc003a2f0b8}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ndcht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ndcht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.395: INFO: Pod "webserver-deployment-845c8977d9-5zk9t" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-5zk9t webserver-deployment-845c8977d9- deployment-2853  3d1063f9-e51d-4337-9a58-95fbbadecd7a 49412 0 2023-01-28 02:00:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:43250a5633ace2fa62fa24ea71e2b15d784984456819a65807119b8a089dfafe cni.projectcalico.org/podIP:172.30.84.44/32 cni.projectcalico.org/podIPs:172.30.84.44/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003a2f2c0 0xc003a2f2c1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 02:00:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.84.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-97sqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-97sqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:172.30.84.44,StartTime:2023-01-28 02:00:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 02:00:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4822dc4b24901e9a1e98bc0b7b3c524f02283f35b3d65f5652e025514c491a2f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.84.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.395: INFO: Pod "webserver-deployment-845c8977d9-6v9j7" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6v9j7 webserver-deployment-845c8977d9- deployment-2853  fff05d7e-72ea-4127-be20-b5afcfec930c 49641 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e4dc9411651afde27ac2c5f4ff1c57d049f71fb30c68a4eb61d2656d3b6e7721 cni.projectcalico.org/podIP:172.30.12.206/32 cni.projectcalico.org/podIPs:172.30.12.206/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003a2f4f0 0xc003a2f4f1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lb7qm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lb7qm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.395: INFO: Pod "webserver-deployment-845c8977d9-8k6rp" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-8k6rp webserver-deployment-845c8977d9- deployment-2853  59f7d167-4ea7-442b-896b-b4b99fd037cf 49386 0 2023-01-28 02:00:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8d7f72aa2f4c05881ef1b6ec76900c2aa337ce42280ab4770f9937271454a434 cni.projectcalico.org/podIP:172.30.185.57/32 cni.projectcalico.org/podIPs:172.30.185.57/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003a2f6f7 0xc003a2f6f8}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.185.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hfzqb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hfzqb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:172.30.185.57,StartTime:2023-01-28 02:00:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 02:00:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e4b34dc6355f28f211b82616488a7caff6e61dc8c6f03c86a21ed5a873782421,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.185.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.395: INFO: Pod "webserver-deployment-845c8977d9-9cxpn" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-9cxpn webserver-deployment-845c8977d9- deployment-2853  c19fc08e-0067-4a03-bdbd-fc72cf66693e 49630 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:f935eecb14f26c3fad845e660f6b7973ca52744394586ad214bb837d8664ed77 cni.projectcalico.org/podIP:172.30.12.207/32 cni.projectcalico.org/podIPs:172.30.12.207/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003a2f920 0xc003a2f921}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8ccdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8ccdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.398: INFO: Pod "webserver-deployment-845c8977d9-b65h9" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-b65h9 webserver-deployment-845c8977d9- deployment-2853  2209b1d9-3999-4469-ae51-da256640266e 49652 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:c4d1f53b334cbf4583d33aa4dc1dcc4665edd7ff2e7466ecdc9982e2a38f7490 cni.projectcalico.org/podIP:172.30.12.194/32 cni.projectcalico.org/podIPs:172.30.12.194/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003a2fc27 0xc003a2fc28}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jx8wl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jx8wl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.399: INFO: Pod "webserver-deployment-845c8977d9-bbfhz" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-bbfhz webserver-deployment-845c8977d9- deployment-2853  4c48290a-f3bb-4e34-90bb-57ea9d2a691a 49664 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:200aff52bd58702e24254d903a2a45de83401cc58dc932bb64d09346e80703f1 cni.projectcalico.org/podIP:172.30.12.249/32 cni.projectcalico.org/podIPs:172.30.12.249/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c340f7 0xc003c340f8}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g8vwd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g8vwd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.408: INFO: Pod "webserver-deployment-845c8977d9-csspm" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-csspm webserver-deployment-845c8977d9- deployment-2853  d09d02f1-dfbb-44fc-a01c-b87854eac8a5 49648 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7af3a73d6da3610b676c4bd9fe7c308a4d9300e0c93e6168ae11fa002f69bb44 cni.projectcalico.org/podIP:172.30.84.56/32 cni.projectcalico.org/podIPs:172.30.84.56/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c34307 0xc003c34308}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wv5nb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wv5nb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.408: INFO: Pod "webserver-deployment-845c8977d9-dr86f" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-dr86f webserver-deployment-845c8977d9- deployment-2853  e9211ba1-b466-488b-bbde-cc0135729fe7 49602 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c344f0 0xc003c344f1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-54tqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-54tqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.409: INFO: Pod "webserver-deployment-845c8977d9-dtfjr" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-dtfjr webserver-deployment-845c8977d9- deployment-2853  88c6d58c-ad3d-43a4-8e86-b072ca3c3b82 49698 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d8b311a84844bff67cf99db7e92cd3ca8602652a98ee4a6135f5d33274940e0d cni.projectcalico.org/podIP:172.30.185.29/32 cni.projectcalico.org/podIPs:172.30.185.29/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c346d7 0xc003c346d8}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kt6r8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kt6r8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.409: INFO: Pod "webserver-deployment-845c8977d9-hv5fn" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hv5fn webserver-deployment-845c8977d9- deployment-2853  e0e4f236-8288-4ac8-934f-6f704078b3e3 49400 0 2023-01-28 02:00:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:49f3a23fb0c0858ae021cdec01f2a5fd20ef089eeb85ab9d6d0c65440975b419 cni.projectcalico.org/podIP:172.30.84.57/32 cni.projectcalico.org/podIPs:172.30.84.57/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c348e0 0xc003c348e1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 02:00:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.84.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hnmpx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hnmpx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:172.30.84.57,StartTime:2023-01-28 02:00:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 02:00:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://eb210381d9a3d39b4380c0e436417a8bed11a9c67bf4b894bbea5279c16195fb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.84.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.410: INFO: Pod "webserver-deployment-845c8977d9-jcphv" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jcphv webserver-deployment-845c8977d9- deployment-2853  65f81b91-a50e-4861-b4aa-330c45633247 49402 0 2023-01-28 02:00:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2073ac403d2748ce268bc049c587586fb5752172eb4fec61197e42ba2469231d cni.projectcalico.org/podIP:172.30.12.198/32 cni.projectcalico.org/podIPs:172.30.12.198/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c34b00 0xc003c34b01}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 02:00:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f4b57,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f4b57,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:172.30.12.198,StartTime:2023-01-28 02:00:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 02:00:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://69dd913b6e290af70cc0ef1f6841dff3fdae33a4e8d90d3b338e3b55c5c4d844,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.12.198,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.410: INFO: Pod "webserver-deployment-845c8977d9-lmd5z" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-lmd5z webserver-deployment-845c8977d9- deployment-2853  b94e0ace-66b2-46a7-8544-88a8e8cd8314 49398 0 2023-01-28 02:00:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:66062c7efb6e8afcf3dea27ca601538dd8bb140adf16c54efcd27b79269cb8b5 cni.projectcalico.org/podIP:172.30.12.196/32 cni.projectcalico.org/podIPs:172.30.12.196/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c34d27 0xc003c34d28}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 02:00:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kfx85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kfx85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:172.30.12.196,StartTime:2023-01-28 02:00:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 02:00:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9110b5c1c11a2323df8231ce4ef33a4c1506ae5f53a01df87c64fafce43c57c1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.12.196,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.410: INFO: Pod "webserver-deployment-845c8977d9-npb8q" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-npb8q webserver-deployment-845c8977d9- deployment-2853  0fe682dc-bb15-4a8b-ad7d-c1abc275cbe7 49646 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:dc49088c2da46b4d7df09a11ed7bd9612fb89f3bf40fd953ac241dba957445f9 cni.projectcalico.org/podIP:172.30.185.62/32 cni.projectcalico.org/podIPs:172.30.185.62/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c34f87 0xc003c34f88}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p5nrd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p5nrd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.410: INFO: Pod "webserver-deployment-845c8977d9-pxwbt" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-pxwbt webserver-deployment-845c8977d9- deployment-2853  ea5ac6ba-7578-40cf-9a0c-3f7c375f20b2 49406 0 2023-01-28 02:00:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6bf746fa96149978f60fcd51c0e46331545a5accc493d47ea2cc427651e6d60a cni.projectcalico.org/podIP:172.30.84.23/32 cni.projectcalico.org/podIPs:172.30.84.23/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c35190 0xc003c35191}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 02:00:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.84.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-92lf2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-92lf2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:172.30.84.23,StartTime:2023-01-28 02:00:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 02:00:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e97faafb96c703067cb18966fe83cd35e03ba7ca9b7563cca65e96bda2416e78,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.84.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.411: INFO: Pod "webserver-deployment-845c8977d9-tl57d" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-tl57d webserver-deployment-845c8977d9- deployment-2853  e306c17b-fd46-4b2e-9afc-68f6b3dce3e6 49668 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:474e098b2f82045dbd0cf9917ebfd5d6ba4e1cd30c9728e6575f0164e0f15758 cni.projectcalico.org/podIP:172.30.185.61/32 cni.projectcalico.org/podIPs:172.30.185.61/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c353b0 0xc003c353b1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sq8p8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sq8p8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.411: INFO: Pod "webserver-deployment-845c8977d9-ttxjm" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-ttxjm webserver-deployment-845c8977d9- deployment-2853  781c1eb1-2655-4c60-998d-3a18e9d9d6f1 49418 0 2023-01-28 02:00:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:b7f09007ab582ab5a61a0d14ba057c7fb873533228e64304c209f5391caf6eae cni.projectcalico.org/podIP:172.30.185.56/32 cni.projectcalico.org/podIPs:172.30.185.56/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c355b0 0xc003c355b1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 02:00:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.185.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zb92m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zb92m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:172.30.185.56,StartTime:2023-01-28 02:00:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 02:00:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://581ce70a6319cde2b34b46a34a21adb03890aeb3313a5ba64c184f600d7bf1f4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.185.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.411: INFO: Pod "webserver-deployment-845c8977d9-xf6g4" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xf6g4 webserver-deployment-845c8977d9- deployment-2853  ad895d5a-c245-4b44-a55c-c2b0bab860d2 49656 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e08116fc2da7e996f89ada3883800aaccbf9e52f490fc48c6b21051852b3e562 cni.projectcalico.org/podIP:172.30.84.15/32 cni.projectcalico.org/podIPs:172.30.84.15/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c357d0 0xc003c357d1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tssh2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tssh2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 02:00:57.411: INFO: Pod "webserver-deployment-845c8977d9-z672b" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-z672b webserver-deployment-845c8977d9- deployment-2853  982f11c6-c479-449c-b879-abb4d6c32e1b 49382 0 2023-01-28 02:00:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9627fd66f8313060cf3069fec972bdc59f7e6a24c129964c85f2fd99d532b004 cni.projectcalico.org/podIP:172.30.185.53/32 cni.projectcalico.org/podIPs:172.30.185.53/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c359d0 0xc003c359d1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.185.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d5hd7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d5hd7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:172.30.185.53,StartTime:2023-01-28 02:00:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 02:00:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cc32bc43d600ec25821acafb8068a5802f945cee13cde2d97ceca2ee746d9f4b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.185.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 28 02:00:57.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2853" for this suite. 01/28/23 02:00:57.427
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":361,"skipped":6693,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.566 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 02:00:48.881
    Jan 28 02:00:48.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename deployment 01/28/23 02:00:48.882
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:48.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:48.94
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan 28 02:00:48.953: INFO: Creating deployment "webserver-deployment"
    Jan 28 02:00:48.972: INFO: Waiting for observed generation 1
    Jan 28 02:00:50.998: INFO: Waiting for all required pods to come up
    Jan 28 02:00:51.011: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/28/23 02:00:51.011
    Jan 28 02:00:51.012: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hv5fn" in namespace "deployment-2853" to be "running"
    Jan 28 02:00:51.012: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-ttxjm" in namespace "deployment-2853" to be "running"
    Jan 28 02:00:51.012: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-pxwbt" in namespace "deployment-2853" to be "running"
    Jan 28 02:00:51.012: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-b69hb" in namespace "deployment-2853" to be "running"
    Jan 28 02:00:51.012: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-fh8c6" in namespace "deployment-2853" to be "running"
    Jan 28 02:00:51.012: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-jcphv" in namespace "deployment-2853" to be "running"
    Jan 28 02:00:51.012: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-5zk9t" in namespace "deployment-2853" to be "running"
    Jan 28 02:00:51.012: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-lmd5z" in namespace "deployment-2853" to be "running"
    Jan 28 02:00:51.024: INFO: Pod "webserver-deployment-845c8977d9-hv5fn": Phase="Pending", Reason="", readiness=false. Elapsed: 12.769053ms
    Jan 28 02:00:51.030: INFO: Pod "webserver-deployment-845c8977d9-b69hb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.518797ms
    Jan 28 02:00:51.031: INFO: Pod "webserver-deployment-845c8977d9-pxwbt": Phase="Pending", Reason="", readiness=false. Elapsed: 19.008812ms
    Jan 28 02:00:51.031: INFO: Pod "webserver-deployment-845c8977d9-lmd5z": Phase="Pending", Reason="", readiness=false. Elapsed: 18.608492ms
    Jan 28 02:00:51.031: INFO: Pod "webserver-deployment-845c8977d9-jcphv": Phase="Pending", Reason="", readiness=false. Elapsed: 18.990519ms
    Jan 28 02:00:51.031: INFO: Pod "webserver-deployment-845c8977d9-ttxjm": Phase="Pending", Reason="", readiness=false. Elapsed: 19.374003ms
    Jan 28 02:00:51.031: INFO: Pod "webserver-deployment-845c8977d9-fh8c6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.344986ms
    Jan 28 02:00:51.031: INFO: Pod "webserver-deployment-845c8977d9-5zk9t": Phase="Pending", Reason="", readiness=false. Elapsed: 19.074521ms
    Jan 28 02:00:53.039: INFO: Pod "webserver-deployment-845c8977d9-hv5fn": Phase="Running", Reason="", readiness=true. Elapsed: 2.02759564s
    Jan 28 02:00:53.039: INFO: Pod "webserver-deployment-845c8977d9-hv5fn" satisfied condition "running"
    Jan 28 02:00:53.044: INFO: Pod "webserver-deployment-845c8977d9-pxwbt": Phase="Running", Reason="", readiness=true. Elapsed: 2.032750151s
    Jan 28 02:00:53.044: INFO: Pod "webserver-deployment-845c8977d9-pxwbt" satisfied condition "running"
    Jan 28 02:00:53.049: INFO: Pod "webserver-deployment-845c8977d9-5zk9t": Phase="Running", Reason="", readiness=true. Elapsed: 2.036768633s
    Jan 28 02:00:53.049: INFO: Pod "webserver-deployment-845c8977d9-5zk9t" satisfied condition "running"
    Jan 28 02:00:53.049: INFO: Pod "webserver-deployment-845c8977d9-ttxjm": Phase="Running", Reason="", readiness=true. Elapsed: 2.037543062s
    Jan 28 02:00:53.049: INFO: Pod "webserver-deployment-845c8977d9-ttxjm" satisfied condition "running"
    Jan 28 02:00:53.051: INFO: Pod "webserver-deployment-845c8977d9-b69hb": Phase="Running", Reason="", readiness=true. Elapsed: 2.039337033s
    Jan 28 02:00:53.051: INFO: Pod "webserver-deployment-845c8977d9-b69hb" satisfied condition "running"
    Jan 28 02:00:53.052: INFO: Pod "webserver-deployment-845c8977d9-lmd5z": Phase="Running", Reason="", readiness=true. Elapsed: 2.03941275s
    Jan 28 02:00:53.052: INFO: Pod "webserver-deployment-845c8977d9-lmd5z" satisfied condition "running"
    Jan 28 02:00:53.052: INFO: Pod "webserver-deployment-845c8977d9-fh8c6": Phase="Running", Reason="", readiness=true. Elapsed: 2.040312104s
    Jan 28 02:00:53.053: INFO: Pod "webserver-deployment-845c8977d9-fh8c6" satisfied condition "running"
    Jan 28 02:00:53.052: INFO: Pod "webserver-deployment-845c8977d9-jcphv": Phase="Running", Reason="", readiness=true. Elapsed: 2.04026185s
    Jan 28 02:00:53.053: INFO: Pod "webserver-deployment-845c8977d9-jcphv" satisfied condition "running"
    Jan 28 02:00:53.053: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan 28 02:00:53.077: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan 28 02:00:53.105: INFO: Updating deployment webserver-deployment
    Jan 28 02:00:53.105: INFO: Waiting for observed generation 2
    Jan 28 02:00:55.132: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan 28 02:00:55.144: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan 28 02:00:55.157: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 28 02:00:55.193: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan 28 02:00:55.193: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan 28 02:00:55.205: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 28 02:00:55.228: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan 28 02:00:55.228: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan 28 02:00:55.257: INFO: Updating deployment webserver-deployment
    Jan 28 02:00:55.257: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan 28 02:00:55.280: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan 28 02:00:57.316: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 28 02:00:57.340: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-2853  29629f5d-d8b5-4d2e-9749-5d43122c029d 49580 3 2023-01-28 02:00:48 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00346d1d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-28 02:00:55 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-28 02:00:55 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan 28 02:00:57.358: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-2853  bf0935ff-cff0-42e8-8b6d-8324252eb994 49577 3 2023-01-28 02:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 29629f5d-d8b5-4d2e-9749-5d43122c029d 0xc003967087 0xc003967088}] [] [{kube-controller-manager Update apps/v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"29629f5d-d8b5-4d2e-9749-5d43122c029d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003967128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 28 02:00:57.359: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan 28 02:00:57.360: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-2853  1758a602-a36b-4331-9c71-3e141c2d1e4b 49568 3 2023-01-28 02:00:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 29629f5d-d8b5-4d2e-9749-5d43122c029d 0xc003967187 0xc003967188}] [] [{kube-controller-manager Update apps/v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"29629f5d-d8b5-4d2e-9749-5d43122c029d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003967218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan 28 02:00:57.387: INFO: Pod "webserver-deployment-69b7448995-6sl8j" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-6sl8j webserver-deployment-69b7448995- deployment-2853  c2fb5886-6bd9-499a-ae87-cd6076fa4623 49485 0 2023-01-28 02:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:86c2eac7d62ceb41e6eef301b9f8d8001301176b6954972f0dee81ade1f96e14 cni.projectcalico.org/podIP:172.30.12.218/32 cni.projectcalico.org/podIPs:172.30.12.218/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc00346d5e7 0xc00346d5e8}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zgk8g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zgk8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.387: INFO: Pod "webserver-deployment-69b7448995-7lx6s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-7lx6s webserver-deployment-69b7448995- deployment-2853  56ab1342-a4db-4425-91b5-a6ec66a7bb42 49506 0 2023-01-28 02:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:699977199930ae2af22ae27da720e50ae906d126352dd77f3b1bba7e41287430 cni.projectcalico.org/podIP:172.30.185.59/32 cni.projectcalico.org/podIPs:172.30.185.59/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc00346d830 0xc00346d831}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8msc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8msc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:,StartTime:2023-01-28 02:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.387: INFO: Pod "webserver-deployment-69b7448995-7w7bj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-7w7bj webserver-deployment-69b7448995- deployment-2853  c6b58307-29e4-4a6c-8be0-65f26cd68734 49613 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc00346da40 0xc00346da41}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l87vd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l87vd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.388: INFO: Pod "webserver-deployment-69b7448995-8rtgg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-8rtgg webserver-deployment-69b7448995- deployment-2853  ee2c9339-727f-432c-9062-e8f6c09626d0 49677 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:0f1b7b024c8d015f75d0d28d5750c8b3ab6a8d3618dd1bd83afbf8a23268db21 cni.projectcalico.org/podIP:172.30.12.219/32 cni.projectcalico.org/podIPs:172.30.12.219/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc00346dc40 0xc00346dc41}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7nnpd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7nnpd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.388: INFO: Pod "webserver-deployment-69b7448995-8sqrx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-8sqrx webserver-deployment-69b7448995- deployment-2853  7986908f-a299-4026-9abf-3700a4f365e8 49634 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:b6d4e88c33b2cd7bcca358339ee390363e719c71e4d215c190fc05c4279a1d20 cni.projectcalico.org/podIP:172.30.84.58/32 cni.projectcalico.org/podIPs:172.30.84.58/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc00346de60 0xc00346de61}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4w6mq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4w6mq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.388: INFO: Pod "webserver-deployment-69b7448995-bqpd2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-bqpd2 webserver-deployment-69b7448995- deployment-2853  c595634b-bc29-4565-9f74-0fa8da1b6d16 49607 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc000c1c570 0xc000c1c571}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s4tfm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s4tfm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.388: INFO: Pod "webserver-deployment-69b7448995-crmnz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-crmnz webserver-deployment-69b7448995- deployment-2853  dfaef315-ea00-433c-9f8b-265d052e11fb 49495 0 2023-01-28 02:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:bc50f2240c80ea9c2c18632e825738146155fb734de0eea582e186f05067e951 cni.projectcalico.org/podIP:172.30.84.60/32 cni.projectcalico.org/podIPs:172.30.84.60/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc000c1de30 0xc000c1de31}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ddp8f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ddp8f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:,StartTime:2023-01-28 02:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.389: INFO: Pod "webserver-deployment-69b7448995-dpmfs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-dpmfs webserver-deployment-69b7448995- deployment-2853  7de4c0a7-d62f-4149-9217-3767d194624e 49684 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:be14b3d2850c5bf287cbcddedd9bfed12fd1f997e96f0d2420b9d15b97602542 cni.projectcalico.org/podIP:172.30.185.38/32 cni.projectcalico.org/podIPs:172.30.185.38/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc003a2e210 0xc003a2e211}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b2vnf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b2vnf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.390: INFO: Pod "webserver-deployment-69b7448995-h7rfr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-h7rfr webserver-deployment-69b7448995- deployment-2853  4033f90e-94a4-48da-b25d-fd3711889a47 49637 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:65651799da49632a88c833e61cac9c28190efbe7abd99ee231b2ae86929d5cc6 cni.projectcalico.org/podIP:172.30.185.58/32 cni.projectcalico.org/podIPs:172.30.185.58/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc003a2e440 0xc003a2e441}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q5xms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q5xms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.391: INFO: Pod "webserver-deployment-69b7448995-h9c58" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-h9c58 webserver-deployment-69b7448995- deployment-2853  14d5a167-366c-4342-856a-68992c58b6dc 49487 0 2023-01-28 02:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:70b2c151de3ec8169f7ba2d818b9ef7c692a683a7936f6cce52e91d63a84c585 cni.projectcalico.org/podIP:172.30.185.60/32 cni.projectcalico.org/podIPs:172.30.185.60/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc003a2e670 0xc003a2e671}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h5rkb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h5rkb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:,StartTime:2023-01-28 02:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.392: INFO: Pod "webserver-deployment-69b7448995-nwtc9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-nwtc9 webserver-deployment-69b7448995- deployment-2853  1d3d5653-e12c-4a90-82b7-535284ba2eba 49697 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:09bc76ecfa9e06ee2dc88ed3d37e1f2cccf7c24aecfc58415e430955cf47bfc6 cni.projectcalico.org/podIP:172.30.84.21/32 cni.projectcalico.org/podIPs:172.30.84.21/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc003a2e890 0xc003a2e891}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qcxrh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcxrh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.393: INFO: Pod "webserver-deployment-69b7448995-pnmpz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-pnmpz webserver-deployment-69b7448995- deployment-2853  c0cf51ee-508c-40d8-8f14-b76fccae6407 49511 0 2023-01-28 02:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:37e61a1d09e4da02b84050c2a43ceb2c04e50988d661c51884fc078329b0eb8f cni.projectcalico.org/podIP:172.30.12.211/32 cni.projectcalico.org/podIPs:172.30.12.211/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc003a2eab0 0xc003a2eab1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tn59c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tn59c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.393: INFO: Pod "webserver-deployment-69b7448995-w6l4w" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-w6l4w webserver-deployment-69b7448995- deployment-2853  3da88e36-8eb4-43d0-9d4c-52b09168c29f 49620 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf0935ff-cff0-42e8-8b6d-8324252eb994 0xc003a2ecb0 0xc003a2ecb1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf0935ff-cff0-42e8-8b6d-8324252eb994\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2nnbq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2nnbq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.394: INFO: Pod "webserver-deployment-845c8977d9-5rlfv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-5rlfv webserver-deployment-845c8977d9- deployment-2853  de3346c3-6715-40c1-a00a-db464c95adae 49693 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1038cb703acb46b0d85fffe3bcb45bc4bdf1e017b9b67509f50ec171113bdee2 cni.projectcalico.org/podIP:172.30.12.239/32 cni.projectcalico.org/podIPs:172.30.12.239/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003a2eeb0 0xc003a2eeb1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t6df9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t6df9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.394: INFO: Pod "webserver-deployment-845c8977d9-5v56n" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-5v56n webserver-deployment-845c8977d9- deployment-2853  d3ba4e6f-c8d5-41ba-b74a-d8830c0aaf38 49679 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:43b4d3140136eb76fe868870b62a4f560789905623a7425ecc2e6a719fe48d96 cni.projectcalico.org/podIP:172.30.84.62/32 cni.projectcalico.org/podIPs:172.30.84.62/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003a2f0b7 0xc003a2f0b8}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ndcht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ndcht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.395: INFO: Pod "webserver-deployment-845c8977d9-5zk9t" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-5zk9t webserver-deployment-845c8977d9- deployment-2853  3d1063f9-e51d-4337-9a58-95fbbadecd7a 49412 0 2023-01-28 02:00:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:43250a5633ace2fa62fa24ea71e2b15d784984456819a65807119b8a089dfafe cni.projectcalico.org/podIP:172.30.84.44/32 cni.projectcalico.org/podIPs:172.30.84.44/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003a2f2c0 0xc003a2f2c1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 02:00:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.84.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-97sqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-97sqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:172.30.84.44,StartTime:2023-01-28 02:00:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 02:00:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4822dc4b24901e9a1e98bc0b7b3c524f02283f35b3d65f5652e025514c491a2f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.84.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.395: INFO: Pod "webserver-deployment-845c8977d9-6v9j7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6v9j7 webserver-deployment-845c8977d9- deployment-2853  fff05d7e-72ea-4127-be20-b5afcfec930c 49641 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e4dc9411651afde27ac2c5f4ff1c57d049f71fb30c68a4eb61d2656d3b6e7721 cni.projectcalico.org/podIP:172.30.12.206/32 cni.projectcalico.org/podIPs:172.30.12.206/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003a2f4f0 0xc003a2f4f1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lb7qm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lb7qm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.395: INFO: Pod "webserver-deployment-845c8977d9-8k6rp" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-8k6rp webserver-deployment-845c8977d9- deployment-2853  59f7d167-4ea7-442b-896b-b4b99fd037cf 49386 0 2023-01-28 02:00:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8d7f72aa2f4c05881ef1b6ec76900c2aa337ce42280ab4770f9937271454a434 cni.projectcalico.org/podIP:172.30.185.57/32 cni.projectcalico.org/podIPs:172.30.185.57/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003a2f6f7 0xc003a2f6f8}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.185.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hfzqb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hfzqb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:172.30.185.57,StartTime:2023-01-28 02:00:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 02:00:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e4b34dc6355f28f211b82616488a7caff6e61dc8c6f03c86a21ed5a873782421,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.185.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.395: INFO: Pod "webserver-deployment-845c8977d9-9cxpn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-9cxpn webserver-deployment-845c8977d9- deployment-2853  c19fc08e-0067-4a03-bdbd-fc72cf66693e 49630 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:f935eecb14f26c3fad845e660f6b7973ca52744394586ad214bb837d8664ed77 cni.projectcalico.org/podIP:172.30.12.207/32 cni.projectcalico.org/podIPs:172.30.12.207/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003a2f920 0xc003a2f921}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8ccdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8ccdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.398: INFO: Pod "webserver-deployment-845c8977d9-b65h9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-b65h9 webserver-deployment-845c8977d9- deployment-2853  2209b1d9-3999-4469-ae51-da256640266e 49652 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:c4d1f53b334cbf4583d33aa4dc1dcc4665edd7ff2e7466ecdc9982e2a38f7490 cni.projectcalico.org/podIP:172.30.12.194/32 cni.projectcalico.org/podIPs:172.30.12.194/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003a2fc27 0xc003a2fc28}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jx8wl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jx8wl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.399: INFO: Pod "webserver-deployment-845c8977d9-bbfhz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-bbfhz webserver-deployment-845c8977d9- deployment-2853  4c48290a-f3bb-4e34-90bb-57ea9d2a691a 49664 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:200aff52bd58702e24254d903a2a45de83401cc58dc932bb64d09346e80703f1 cni.projectcalico.org/podIP:172.30.12.249/32 cni.projectcalico.org/podIPs:172.30.12.249/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c340f7 0xc003c340f8}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g8vwd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g8vwd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.408: INFO: Pod "webserver-deployment-845c8977d9-csspm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-csspm webserver-deployment-845c8977d9- deployment-2853  d09d02f1-dfbb-44fc-a01c-b87854eac8a5 49648 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7af3a73d6da3610b676c4bd9fe7c308a4d9300e0c93e6168ae11fa002f69bb44 cni.projectcalico.org/podIP:172.30.84.56/32 cni.projectcalico.org/podIPs:172.30.84.56/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c34307 0xc003c34308}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wv5nb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wv5nb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.408: INFO: Pod "webserver-deployment-845c8977d9-dr86f" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-dr86f webserver-deployment-845c8977d9- deployment-2853  e9211ba1-b466-488b-bbde-cc0135729fe7 49602 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c344f0 0xc003c344f1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-54tqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-54tqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.409: INFO: Pod "webserver-deployment-845c8977d9-dtfjr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-dtfjr webserver-deployment-845c8977d9- deployment-2853  88c6d58c-ad3d-43a4-8e86-b072ca3c3b82 49698 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d8b311a84844bff67cf99db7e92cd3ca8602652a98ee4a6135f5d33274940e0d cni.projectcalico.org/podIP:172.30.185.29/32 cni.projectcalico.org/podIPs:172.30.185.29/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c346d7 0xc003c346d8}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kt6r8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kt6r8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.409: INFO: Pod "webserver-deployment-845c8977d9-hv5fn" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hv5fn webserver-deployment-845c8977d9- deployment-2853  e0e4f236-8288-4ac8-934f-6f704078b3e3 49400 0 2023-01-28 02:00:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:49f3a23fb0c0858ae021cdec01f2a5fd20ef089eeb85ab9d6d0c65440975b419 cni.projectcalico.org/podIP:172.30.84.57/32 cni.projectcalico.org/podIPs:172.30.84.57/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c348e0 0xc003c348e1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 02:00:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.84.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hnmpx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hnmpx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:172.30.84.57,StartTime:2023-01-28 02:00:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 02:00:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://eb210381d9a3d39b4380c0e436417a8bed11a9c67bf4b894bbea5279c16195fb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.84.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.410: INFO: Pod "webserver-deployment-845c8977d9-jcphv" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jcphv webserver-deployment-845c8977d9- deployment-2853  65f81b91-a50e-4861-b4aa-330c45633247 49402 0 2023-01-28 02:00:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2073ac403d2748ce268bc049c587586fb5752172eb4fec61197e42ba2469231d cni.projectcalico.org/podIP:172.30.12.198/32 cni.projectcalico.org/podIPs:172.30.12.198/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c34b00 0xc003c34b01}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 02:00:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f4b57,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f4b57,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:172.30.12.198,StartTime:2023-01-28 02:00:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 02:00:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://69dd913b6e290af70cc0ef1f6841dff3fdae33a4e8d90d3b338e3b55c5c4d844,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.12.198,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.410: INFO: Pod "webserver-deployment-845c8977d9-lmd5z" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-lmd5z webserver-deployment-845c8977d9- deployment-2853  b94e0ace-66b2-46a7-8544-88a8e8cd8314 49398 0 2023-01-28 02:00:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:66062c7efb6e8afcf3dea27ca601538dd8bb140adf16c54efcd27b79269cb8b5 cni.projectcalico.org/podIP:172.30.12.196/32 cni.projectcalico.org/podIPs:172.30.12.196/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c34d27 0xc003c34d28}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 02:00:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.12.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kfx85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kfx85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.126,PodIP:172.30.12.196,StartTime:2023-01-28 02:00:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 02:00:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9110b5c1c11a2323df8231ce4ef33a4c1506ae5f53a01df87c64fafce43c57c1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.12.196,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.410: INFO: Pod "webserver-deployment-845c8977d9-npb8q" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-npb8q webserver-deployment-845c8977d9- deployment-2853  0fe682dc-bb15-4a8b-ad7d-c1abc275cbe7 49646 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:dc49088c2da46b4d7df09a11ed7bd9612fb89f3bf40fd953ac241dba957445f9 cni.projectcalico.org/podIP:172.30.185.62/32 cni.projectcalico.org/podIPs:172.30.185.62/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c34f87 0xc003c34f88}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p5nrd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p5nrd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.410: INFO: Pod "webserver-deployment-845c8977d9-pxwbt" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-pxwbt webserver-deployment-845c8977d9- deployment-2853  ea5ac6ba-7578-40cf-9a0c-3f7c375f20b2 49406 0 2023-01-28 02:00:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6bf746fa96149978f60fcd51c0e46331545a5accc493d47ea2cc427651e6d60a cni.projectcalico.org/podIP:172.30.84.23/32 cni.projectcalico.org/podIPs:172.30.84.23/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c35190 0xc003c35191}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 02:00:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.84.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-92lf2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-92lf2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:172.30.84.23,StartTime:2023-01-28 02:00:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 02:00:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e97faafb96c703067cb18966fe83cd35e03ba7ca9b7563cca65e96bda2416e78,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.84.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.411: INFO: Pod "webserver-deployment-845c8977d9-tl57d" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-tl57d webserver-deployment-845c8977d9- deployment-2853  e306c17b-fd46-4b2e-9afc-68f6b3dce3e6 49668 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:474e098b2f82045dbd0cf9917ebfd5d6ba4e1cd30c9728e6575f0164e0f15758 cni.projectcalico.org/podIP:172.30.185.61/32 cni.projectcalico.org/podIPs:172.30.185.61/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c353b0 0xc003c353b1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sq8p8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sq8p8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.411: INFO: Pod "webserver-deployment-845c8977d9-ttxjm" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-ttxjm webserver-deployment-845c8977d9- deployment-2853  781c1eb1-2655-4c60-998d-3a18e9d9d6f1 49418 0 2023-01-28 02:00:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:b7f09007ab582ab5a61a0d14ba057c7fb873533228e64304c209f5391caf6eae cni.projectcalico.org/podIP:172.30.185.56/32 cni.projectcalico.org/podIPs:172.30.185.56/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c355b0 0xc003c355b1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 02:00:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.185.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zb92m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zb92m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:172.30.185.56,StartTime:2023-01-28 02:00:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 02:00:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://581ce70a6319cde2b34b46a34a21adb03890aeb3313a5ba64c184f600d7bf1f4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.185.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.411: INFO: Pod "webserver-deployment-845c8977d9-xf6g4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xf6g4 webserver-deployment-845c8977d9- deployment-2853  ad895d5a-c245-4b44-a55c-c2b0bab860d2 49656 0 2023-01-28 02:00:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e08116fc2da7e996f89ada3883800aaccbf9e52f490fc48c6b21051852b3e562 cni.projectcalico.org/podIP:172.30.84.15/32 cni.projectcalico.org/podIPs:172.30.84.15/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c357d0 0xc003c357d1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 02:00:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 02:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tssh2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tssh2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.75,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.75,PodIP:,StartTime:2023-01-28 02:00:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 28 02:00:57.411: INFO: Pod "webserver-deployment-845c8977d9-z672b" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-z672b webserver-deployment-845c8977d9- deployment-2853  982f11c6-c479-449c-b879-abb4d6c32e1b 49382 0 2023-01-28 02:00:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9627fd66f8313060cf3069fec972bdc59f7e6a24c129964c85f2fd99d532b004 cni.projectcalico.org/podIP:172.30.185.53/32 cni.projectcalico.org/podIPs:172.30.185.53/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 1758a602-a36b-4331-9c71-3e141c2d1e4b 0xc003c359d0 0xc003c359d1}] [] [{kube-controller-manager Update v1 2023-01-28 02:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1758a602-a36b-4331-9c71-3e141c2d1e4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 02:00:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.185.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d5hd7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d5hd7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.9.20.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 02:00:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.20.72,PodIP:172.30.185.53,StartTime:2023-01-28 02:00:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 02:00:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cc32bc43d600ec25821acafb8068a5802f945cee13cde2d97ceca2ee746d9f4b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.185.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 28 02:00:57.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2853" for this suite. 01/28/23 02:00:57.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/28/23 02:00:57.46
Jan 28 02:00:57.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
STEP: Building a namespace api object, basename endpointslice 01/28/23 02:00:57.461
STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:57.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:57.528
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 01/28/23 02:01:02.696
STEP: referencing matching pods with named port 01/28/23 02:01:07.722
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/28/23 02:01:12.746
STEP: recreating EndpointSlices after they've been deleted 01/28/23 02:01:17.773
Jan 28 02:01:17.838: INFO: EndpointSlice for Service endpointslice-2107/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 28 02:01:27.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2107" for this suite. 01/28/23 02:01:27.884
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":362,"skipped":6699,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.449 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/28/23 02:00:57.46
    Jan 28 02:00:57.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3522974414
    STEP: Building a namespace api object, basename endpointslice 01/28/23 02:00:57.461
    STEP: Waiting for a default service account to be provisioned in namespace 01/28/23 02:00:57.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/28/23 02:00:57.528
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 01/28/23 02:01:02.696
    STEP: referencing matching pods with named port 01/28/23 02:01:07.722
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/28/23 02:01:12.746
    STEP: recreating EndpointSlices after they've been deleted 01/28/23 02:01:17.773
    Jan 28 02:01:17.838: INFO: EndpointSlice for Service endpointslice-2107/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 28 02:01:27.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-2107" for this suite. 01/28/23 02:01:27.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
Jan 28 02:01:27.916: INFO: Running AfterSuite actions on all nodes
Jan 28 02:01:27.916: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Jan 28 02:01:27.916: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Jan 28 02:01:27.917: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jan 28 02:01:27.917: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jan 28 02:01:27.917: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jan 28 02:01:27.917: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jan 28 02:01:27.917: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Jan 28 02:01:27.917: INFO: Running AfterSuite actions on node 1
Jan 28 02:01:27.917: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.002 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan 28 02:01:27.916: INFO: Running AfterSuite actions on all nodes
    Jan 28 02:01:27.916: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Jan 28 02:01:27.916: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Jan 28 02:01:27.917: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Jan 28 02:01:27.917: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Jan 28 02:01:27.917: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Jan 28 02:01:27.917: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Jan 28 02:01:27.917: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan 28 02:01:27.917: INFO: Running AfterSuite actions on node 1
    Jan 28 02:01:27.917: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.087 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 6439.550 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h47m19.929868242s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

