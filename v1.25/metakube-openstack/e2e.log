I0327 14:06:04.353980      24 e2e.go:116] Starting e2e run "623b7062-a94d-4373-aa13-325cae39bd8b" on Ginkgo node 1
Mar 27 14:06:04.367: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1679925964 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Mar 27 14:06:04.459: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
E0327 14:06:04.462396      24 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0327 14:06:04.462396      24 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Mar 27 14:06:04.462: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar 27 14:06:04.483: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 27 14:06:04.520: INFO: 30 / 30 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 27 14:06:04.520: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Mar 27 14:06:04.520: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 27 14:06:04.530: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Mar 27 14:06:04.530: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
Mar 27 14:06:04.530: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar 27 14:06:04.530: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Mar 27 14:06:04.530: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Mar 27 14:06:04.530: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'syseleven-node-problem-detector' (0 seconds elapsed)
Mar 27 14:06:04.530: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'user-ssh-keys-agent' (0 seconds elapsed)
Mar 27 14:06:04.530: INFO: e2e test version: v1.25.7
Mar 27 14:06:04.532: INFO: kube-apiserver version: v1.25.7
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Mar 27 14:06:04.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:06:04.537: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.078 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Mar 27 14:06:04.459: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    E0327 14:06:04.462396      24 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Mar 27 14:06:04.462: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Mar 27 14:06:04.483: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Mar 27 14:06:04.520: INFO: 30 / 30 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Mar 27 14:06:04.520: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
    Mar 27 14:06:04.520: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Mar 27 14:06:04.530: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
    Mar 27 14:06:04.530: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
    Mar 27 14:06:04.530: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Mar 27 14:06:04.530: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
    Mar 27 14:06:04.530: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
    Mar 27 14:06:04.530: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'syseleven-node-problem-detector' (0 seconds elapsed)
    Mar 27 14:06:04.530: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'user-ssh-keys-agent' (0 seconds elapsed)
    Mar 27 14:06:04.530: INFO: e2e test version: v1.25.7
    Mar 27 14:06:04.532: INFO: kube-apiserver version: v1.25.7
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Mar 27 14:06:04.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:06:04.537: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:06:04.563
Mar 27 14:06:04.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename configmap 03/27/23 14:06:04.564
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:06:04.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:06:04.597
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 27 14:06:04.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2073" for this suite. 03/27/23 14:06:04.678
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":1,"skipped":33,"failed":0}
------------------------------
• [0.122 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:06:04.563
    Mar 27 14:06:04.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename configmap 03/27/23 14:06:04.564
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:06:04.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:06:04.597
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 27 14:06:04.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2073" for this suite. 03/27/23 14:06:04.678
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:06:04.686
Mar 27 14:06:04.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
E0327 14:06:04.687218      24 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0327 14:06:04.687218      24 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
STEP: Building a namespace api object, basename aggregator 03/27/23 14:06:04.687
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:06:04.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:06:04.729
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Mar 27 14:06:04.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 03/27/23 14:06:04.734
Mar 27 14:06:05.031: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar 27 14:06:07.086: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:06:09.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:06:11.091: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:06:13.091: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:06:15.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:06:17.090: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:06:19.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:06:21.091: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:06:23.091: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:06:25.093: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:06:27.093: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:06:29.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:06:31.238: INFO: Waited 137.972408ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 03/27/23 14:06:32.515
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/27/23 14:06:32.519
STEP: List APIServices 03/27/23 14:06:32.527
Mar 27 14:06:32.534: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Mar 27 14:06:32.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4256" for this suite. 03/27/23 14:06:32.728
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":2,"skipped":37,"failed":0}
------------------------------
• [SLOW TEST] [28.048 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:06:04.686
    Mar 27 14:06:04.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    E0327 14:06:04.687218      24 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    STEP: Building a namespace api object, basename aggregator 03/27/23 14:06:04.687
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:06:04.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:06:04.729
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Mar 27 14:06:04.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 03/27/23 14:06:04.734
    Mar 27 14:06:05.031: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Mar 27 14:06:07.086: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:06:09.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:06:11.091: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:06:13.091: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:06:15.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:06:17.090: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:06:19.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:06:21.091: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:06:23.091: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:06:25.093: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:06:27.093: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:06:29.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 6, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:06:31.238: INFO: Waited 137.972408ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 03/27/23 14:06:32.515
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/27/23 14:06:32.519
    STEP: List APIServices 03/27/23 14:06:32.527
    Mar 27 14:06:32.534: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Mar 27 14:06:32.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-4256" for this suite. 03/27/23 14:06:32.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:06:32.735
Mar 27 14:06:32.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename resourcequota 03/27/23 14:06:32.736
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:06:32.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:06:32.76
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 03/27/23 14:06:32.764
STEP: Ensuring ResourceQuota status is calculated 03/27/23 14:06:32.773
STEP: Creating a ResourceQuota with not best effort scope 03/27/23 14:06:34.78
STEP: Ensuring ResourceQuota status is calculated 03/27/23 14:06:34.874
STEP: Creating a best-effort pod 03/27/23 14:06:36.878
STEP: Ensuring resource quota with best effort scope captures the pod usage 03/27/23 14:06:37.142
STEP: Ensuring resource quota with not best effort ignored the pod usage 03/27/23 14:06:39.156
STEP: Deleting the pod 03/27/23 14:06:41.161
STEP: Ensuring resource quota status released the pod usage 03/27/23 14:06:41.173
STEP: Creating a not best-effort pod 03/27/23 14:06:43.18
STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/27/23 14:06:43.194
STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/27/23 14:06:45.2
STEP: Deleting the pod 03/27/23 14:06:47.205
STEP: Ensuring resource quota status released the pod usage 03/27/23 14:06:47.221
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 27 14:06:49.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4142" for this suite. 03/27/23 14:06:49.232
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":3,"skipped":44,"failed":0}
------------------------------
• [SLOW TEST] [16.505 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:06:32.735
    Mar 27 14:06:32.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename resourcequota 03/27/23 14:06:32.736
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:06:32.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:06:32.76
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 03/27/23 14:06:32.764
    STEP: Ensuring ResourceQuota status is calculated 03/27/23 14:06:32.773
    STEP: Creating a ResourceQuota with not best effort scope 03/27/23 14:06:34.78
    STEP: Ensuring ResourceQuota status is calculated 03/27/23 14:06:34.874
    STEP: Creating a best-effort pod 03/27/23 14:06:36.878
    STEP: Ensuring resource quota with best effort scope captures the pod usage 03/27/23 14:06:37.142
    STEP: Ensuring resource quota with not best effort ignored the pod usage 03/27/23 14:06:39.156
    STEP: Deleting the pod 03/27/23 14:06:41.161
    STEP: Ensuring resource quota status released the pod usage 03/27/23 14:06:41.173
    STEP: Creating a not best-effort pod 03/27/23 14:06:43.18
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/27/23 14:06:43.194
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/27/23 14:06:45.2
    STEP: Deleting the pod 03/27/23 14:06:47.205
    STEP: Ensuring resource quota status released the pod usage 03/27/23 14:06:47.221
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 27 14:06:49.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4142" for this suite. 03/27/23 14:06:49.232
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:06:49.241
Mar 27 14:06:49.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename namespaces 03/27/23 14:06:49.242
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:06:49.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:06:49.262
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 03/27/23 14:06:49.265
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:06:49.282
STEP: Creating a service in the namespace 03/27/23 14:06:49.285
STEP: Deleting the namespace 03/27/23 14:06:49.302
STEP: Waiting for the namespace to be removed. 03/27/23 14:06:49.316
STEP: Recreating the namespace 03/27/23 14:06:55.32
STEP: Verifying there is no service in the namespace 03/27/23 14:06:55.341
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar 27 14:06:55.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8604" for this suite. 03/27/23 14:06:55.35
STEP: Destroying namespace "nsdeletetest-4902" for this suite. 03/27/23 14:06:55.357
Mar 27 14:06:55.360: INFO: Namespace nsdeletetest-4902 was already deleted
STEP: Destroying namespace "nsdeletetest-2023" for this suite. 03/27/23 14:06:55.36
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":4,"skipped":46,"failed":0}
------------------------------
• [SLOW TEST] [6.126 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:06:49.241
    Mar 27 14:06:49.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename namespaces 03/27/23 14:06:49.242
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:06:49.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:06:49.262
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 03/27/23 14:06:49.265
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:06:49.282
    STEP: Creating a service in the namespace 03/27/23 14:06:49.285
    STEP: Deleting the namespace 03/27/23 14:06:49.302
    STEP: Waiting for the namespace to be removed. 03/27/23 14:06:49.316
    STEP: Recreating the namespace 03/27/23 14:06:55.32
    STEP: Verifying there is no service in the namespace 03/27/23 14:06:55.341
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 14:06:55.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-8604" for this suite. 03/27/23 14:06:55.35
    STEP: Destroying namespace "nsdeletetest-4902" for this suite. 03/27/23 14:06:55.357
    Mar 27 14:06:55.360: INFO: Namespace nsdeletetest-4902 was already deleted
    STEP: Destroying namespace "nsdeletetest-2023" for this suite. 03/27/23 14:06:55.36
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:06:55.367
Mar 27 14:06:55.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename secrets 03/27/23 14:06:55.368
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:06:55.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:06:55.391
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-ef3f5ffc-3b53-4a23-91a2-bf97aa7d38a2 03/27/23 14:06:55.394
STEP: Creating a pod to test consume secrets 03/27/23 14:06:55.402
Mar 27 14:06:55.410: INFO: Waiting up to 5m0s for pod "pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0" in namespace "secrets-3761" to be "Succeeded or Failed"
Mar 27 14:06:55.413: INFO: Pod "pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.490342ms
Mar 27 14:06:57.418: INFO: Pod "pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007699627s
Mar 27 14:06:59.419: INFO: Pod "pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008723058s
Mar 27 14:07:01.419: INFO: Pod "pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008327084s
Mar 27 14:07:03.420: INFO: Pod "pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009150624s
Mar 27 14:07:05.420: INFO: Pod "pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009683996s
Mar 27 14:07:07.418: INFO: Pod "pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.007585515s
STEP: Saw pod success 03/27/23 14:07:07.418
Mar 27 14:07:07.418: INFO: Pod "pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0" satisfied condition "Succeeded or Failed"
Mar 27 14:07:07.421: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0 container secret-volume-test: <nil>
STEP: delete the pod 03/27/23 14:07:07.458
Mar 27 14:07:07.476: INFO: Waiting for pod pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0 to disappear
Mar 27 14:07:07.488: INFO: Pod pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 27 14:07:07.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3761" for this suite. 03/27/23 14:07:07.494
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":5,"skipped":49,"failed":0}
------------------------------
• [SLOW TEST] [12.135 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:06:55.367
    Mar 27 14:06:55.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename secrets 03/27/23 14:06:55.368
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:06:55.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:06:55.391
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-ef3f5ffc-3b53-4a23-91a2-bf97aa7d38a2 03/27/23 14:06:55.394
    STEP: Creating a pod to test consume secrets 03/27/23 14:06:55.402
    Mar 27 14:06:55.410: INFO: Waiting up to 5m0s for pod "pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0" in namespace "secrets-3761" to be "Succeeded or Failed"
    Mar 27 14:06:55.413: INFO: Pod "pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.490342ms
    Mar 27 14:06:57.418: INFO: Pod "pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007699627s
    Mar 27 14:06:59.419: INFO: Pod "pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008723058s
    Mar 27 14:07:01.419: INFO: Pod "pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008327084s
    Mar 27 14:07:03.420: INFO: Pod "pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009150624s
    Mar 27 14:07:05.420: INFO: Pod "pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009683996s
    Mar 27 14:07:07.418: INFO: Pod "pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.007585515s
    STEP: Saw pod success 03/27/23 14:07:07.418
    Mar 27 14:07:07.418: INFO: Pod "pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0" satisfied condition "Succeeded or Failed"
    Mar 27 14:07:07.421: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0 container secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 14:07:07.458
    Mar 27 14:07:07.476: INFO: Waiting for pod pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0 to disappear
    Mar 27 14:07:07.488: INFO: Pod pod-secrets-7177cf3a-709e-4a85-bda6-713950fea3f0 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 27 14:07:07.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3761" for this suite. 03/27/23 14:07:07.494
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:07:07.502
Mar 27 14:07:07.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename configmap 03/27/23 14:07:07.503
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:07:07.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:07:07.543
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-fd2e31e3-05de-42eb-b7a1-23241c36bd41 03/27/23 14:07:07.546
STEP: Creating a pod to test consume configMaps 03/27/23 14:07:07.552
Mar 27 14:07:07.562: INFO: Waiting up to 5m0s for pod "pod-configmaps-0bcb50bb-6913-430c-b2b2-55dd3efa213a" in namespace "configmap-7722" to be "Succeeded or Failed"
Mar 27 14:07:07.565: INFO: Pod "pod-configmaps-0bcb50bb-6913-430c-b2b2-55dd3efa213a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.223874ms
Mar 27 14:07:09.570: INFO: Pod "pod-configmaps-0bcb50bb-6913-430c-b2b2-55dd3efa213a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007763145s
Mar 27 14:07:11.572: INFO: Pod "pod-configmaps-0bcb50bb-6913-430c-b2b2-55dd3efa213a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009364125s
Mar 27 14:07:13.570: INFO: Pod "pod-configmaps-0bcb50bb-6913-430c-b2b2-55dd3efa213a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0073224s
STEP: Saw pod success 03/27/23 14:07:13.57
Mar 27 14:07:13.570: INFO: Pod "pod-configmaps-0bcb50bb-6913-430c-b2b2-55dd3efa213a" satisfied condition "Succeeded or Failed"
Mar 27 14:07:13.573: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-configmaps-0bcb50bb-6913-430c-b2b2-55dd3efa213a container agnhost-container: <nil>
STEP: delete the pod 03/27/23 14:07:13.585
Mar 27 14:07:13.599: INFO: Waiting for pod pod-configmaps-0bcb50bb-6913-430c-b2b2-55dd3efa213a to disappear
Mar 27 14:07:13.603: INFO: Pod pod-configmaps-0bcb50bb-6913-430c-b2b2-55dd3efa213a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 27 14:07:13.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7722" for this suite. 03/27/23 14:07:13.617
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":6,"skipped":49,"failed":0}
------------------------------
• [SLOW TEST] [6.122 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:07:07.502
    Mar 27 14:07:07.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename configmap 03/27/23 14:07:07.503
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:07:07.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:07:07.543
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-fd2e31e3-05de-42eb-b7a1-23241c36bd41 03/27/23 14:07:07.546
    STEP: Creating a pod to test consume configMaps 03/27/23 14:07:07.552
    Mar 27 14:07:07.562: INFO: Waiting up to 5m0s for pod "pod-configmaps-0bcb50bb-6913-430c-b2b2-55dd3efa213a" in namespace "configmap-7722" to be "Succeeded or Failed"
    Mar 27 14:07:07.565: INFO: Pod "pod-configmaps-0bcb50bb-6913-430c-b2b2-55dd3efa213a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.223874ms
    Mar 27 14:07:09.570: INFO: Pod "pod-configmaps-0bcb50bb-6913-430c-b2b2-55dd3efa213a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007763145s
    Mar 27 14:07:11.572: INFO: Pod "pod-configmaps-0bcb50bb-6913-430c-b2b2-55dd3efa213a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009364125s
    Mar 27 14:07:13.570: INFO: Pod "pod-configmaps-0bcb50bb-6913-430c-b2b2-55dd3efa213a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0073224s
    STEP: Saw pod success 03/27/23 14:07:13.57
    Mar 27 14:07:13.570: INFO: Pod "pod-configmaps-0bcb50bb-6913-430c-b2b2-55dd3efa213a" satisfied condition "Succeeded or Failed"
    Mar 27 14:07:13.573: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-configmaps-0bcb50bb-6913-430c-b2b2-55dd3efa213a container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 14:07:13.585
    Mar 27 14:07:13.599: INFO: Waiting for pod pod-configmaps-0bcb50bb-6913-430c-b2b2-55dd3efa213a to disappear
    Mar 27 14:07:13.603: INFO: Pod pod-configmaps-0bcb50bb-6913-430c-b2b2-55dd3efa213a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 27 14:07:13.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7722" for this suite. 03/27/23 14:07:13.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:07:13.626
Mar 27 14:07:13.626: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename dns 03/27/23 14:07:13.627
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:07:13.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:07:13.648
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 03/27/23 14:07:13.654
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3099.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3099.svc.cluster.local; sleep 1; done
 03/27/23 14:07:13.66
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3099.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3099.svc.cluster.local; sleep 1; done
 03/27/23 14:07:13.66
STEP: creating a pod to probe DNS 03/27/23 14:07:13.66
STEP: submitting the pod to kubernetes 03/27/23 14:07:13.66
Mar 27 14:07:13.672: INFO: Waiting up to 15m0s for pod "dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63" in namespace "dns-3099" to be "running"
Mar 27 14:07:13.677: INFO: Pod "dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63": Phase="Pending", Reason="", readiness=false. Elapsed: 4.933924ms
Mar 27 14:07:15.686: INFO: Pod "dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014065955s
Mar 27 14:07:17.683: INFO: Pod "dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011158602s
Mar 27 14:07:19.684: INFO: Pod "dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012235701s
Mar 27 14:07:21.682: INFO: Pod "dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009949077s
Mar 27 14:07:23.682: INFO: Pod "dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01011072s
Mar 27 14:07:25.841: INFO: Pod "dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63": Phase="Running", Reason="", readiness=true. Elapsed: 12.169208875s
Mar 27 14:07:25.841: INFO: Pod "dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63" satisfied condition "running"
STEP: retrieving the pod 03/27/23 14:07:25.841
STEP: looking for the results for each expected name from probers 03/27/23 14:07:25.845
Mar 27 14:07:26.085: INFO: DNS probes using dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63 succeeded

STEP: deleting the pod 03/27/23 14:07:26.085
STEP: changing the externalName to bar.example.com 03/27/23 14:07:26.268
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3099.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3099.svc.cluster.local; sleep 1; done
 03/27/23 14:07:26.28
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3099.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3099.svc.cluster.local; sleep 1; done
 03/27/23 14:07:26.28
STEP: creating a second pod to probe DNS 03/27/23 14:07:26.281
STEP: submitting the pod to kubernetes 03/27/23 14:07:26.281
Mar 27 14:07:26.288: INFO: Waiting up to 15m0s for pod "dns-test-864418c9-f7ee-45dd-bd36-68abcef269c3" in namespace "dns-3099" to be "running"
Mar 27 14:07:26.537: INFO: Pod "dns-test-864418c9-f7ee-45dd-bd36-68abcef269c3": Phase="Pending", Reason="", readiness=false. Elapsed: 248.4793ms
Mar 27 14:07:28.545: INFO: Pod "dns-test-864418c9-f7ee-45dd-bd36-68abcef269c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.256459884s
Mar 27 14:07:30.542: INFO: Pod "dns-test-864418c9-f7ee-45dd-bd36-68abcef269c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.253888601s
Mar 27 14:07:32.543: INFO: Pod "dns-test-864418c9-f7ee-45dd-bd36-68abcef269c3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.254196688s
Mar 27 14:07:34.543: INFO: Pod "dns-test-864418c9-f7ee-45dd-bd36-68abcef269c3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.254511306s
Mar 27 14:07:36.544: INFO: Pod "dns-test-864418c9-f7ee-45dd-bd36-68abcef269c3": Phase="Running", Reason="", readiness=true. Elapsed: 10.255272386s
Mar 27 14:07:36.544: INFO: Pod "dns-test-864418c9-f7ee-45dd-bd36-68abcef269c3" satisfied condition "running"
STEP: retrieving the pod 03/27/23 14:07:36.544
STEP: looking for the results for each expected name from probers 03/27/23 14:07:36.548
Mar 27 14:07:36.568: INFO: DNS probes using dns-test-864418c9-f7ee-45dd-bd36-68abcef269c3 succeeded

STEP: deleting the pod 03/27/23 14:07:36.568
STEP: changing the service to type=ClusterIP 03/27/23 14:07:36.581
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3099.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3099.svc.cluster.local; sleep 1; done
 03/27/23 14:07:36.596
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3099.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3099.svc.cluster.local; sleep 1; done
 03/27/23 14:07:36.596
STEP: creating a third pod to probe DNS 03/27/23 14:07:36.596
STEP: submitting the pod to kubernetes 03/27/23 14:07:36.599
Mar 27 14:07:36.609: INFO: Waiting up to 15m0s for pod "dns-test-ae99ff75-ee54-4146-bfa3-de9561cadb56" in namespace "dns-3099" to be "running"
Mar 27 14:07:36.616: INFO: Pod "dns-test-ae99ff75-ee54-4146-bfa3-de9561cadb56": Phase="Pending", Reason="", readiness=false. Elapsed: 7.302658ms
Mar 27 14:07:38.621: INFO: Pod "dns-test-ae99ff75-ee54-4146-bfa3-de9561cadb56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012338459s
Mar 27 14:07:40.622: INFO: Pod "dns-test-ae99ff75-ee54-4146-bfa3-de9561cadb56": Phase="Running", Reason="", readiness=true. Elapsed: 4.013443215s
Mar 27 14:07:40.622: INFO: Pod "dns-test-ae99ff75-ee54-4146-bfa3-de9561cadb56" satisfied condition "running"
STEP: retrieving the pod 03/27/23 14:07:40.622
STEP: looking for the results for each expected name from probers 03/27/23 14:07:40.627
Mar 27 14:07:40.647: INFO: DNS probes using dns-test-ae99ff75-ee54-4146-bfa3-de9561cadb56 succeeded

STEP: deleting the pod 03/27/23 14:07:40.647
STEP: deleting the test externalName service 03/27/23 14:07:40.662
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 27 14:07:40.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3099" for this suite. 03/27/23 14:07:40.685
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":7,"skipped":78,"failed":0}
------------------------------
• [SLOW TEST] [27.068 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:07:13.626
    Mar 27 14:07:13.626: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename dns 03/27/23 14:07:13.627
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:07:13.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:07:13.648
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 03/27/23 14:07:13.654
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3099.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3099.svc.cluster.local; sleep 1; done
     03/27/23 14:07:13.66
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3099.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3099.svc.cluster.local; sleep 1; done
     03/27/23 14:07:13.66
    STEP: creating a pod to probe DNS 03/27/23 14:07:13.66
    STEP: submitting the pod to kubernetes 03/27/23 14:07:13.66
    Mar 27 14:07:13.672: INFO: Waiting up to 15m0s for pod "dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63" in namespace "dns-3099" to be "running"
    Mar 27 14:07:13.677: INFO: Pod "dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63": Phase="Pending", Reason="", readiness=false. Elapsed: 4.933924ms
    Mar 27 14:07:15.686: INFO: Pod "dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014065955s
    Mar 27 14:07:17.683: INFO: Pod "dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011158602s
    Mar 27 14:07:19.684: INFO: Pod "dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012235701s
    Mar 27 14:07:21.682: INFO: Pod "dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009949077s
    Mar 27 14:07:23.682: INFO: Pod "dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01011072s
    Mar 27 14:07:25.841: INFO: Pod "dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63": Phase="Running", Reason="", readiness=true. Elapsed: 12.169208875s
    Mar 27 14:07:25.841: INFO: Pod "dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63" satisfied condition "running"
    STEP: retrieving the pod 03/27/23 14:07:25.841
    STEP: looking for the results for each expected name from probers 03/27/23 14:07:25.845
    Mar 27 14:07:26.085: INFO: DNS probes using dns-test-b26c933f-43d5-40eb-a944-fe6e7d0ccc63 succeeded

    STEP: deleting the pod 03/27/23 14:07:26.085
    STEP: changing the externalName to bar.example.com 03/27/23 14:07:26.268
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3099.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3099.svc.cluster.local; sleep 1; done
     03/27/23 14:07:26.28
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3099.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3099.svc.cluster.local; sleep 1; done
     03/27/23 14:07:26.28
    STEP: creating a second pod to probe DNS 03/27/23 14:07:26.281
    STEP: submitting the pod to kubernetes 03/27/23 14:07:26.281
    Mar 27 14:07:26.288: INFO: Waiting up to 15m0s for pod "dns-test-864418c9-f7ee-45dd-bd36-68abcef269c3" in namespace "dns-3099" to be "running"
    Mar 27 14:07:26.537: INFO: Pod "dns-test-864418c9-f7ee-45dd-bd36-68abcef269c3": Phase="Pending", Reason="", readiness=false. Elapsed: 248.4793ms
    Mar 27 14:07:28.545: INFO: Pod "dns-test-864418c9-f7ee-45dd-bd36-68abcef269c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.256459884s
    Mar 27 14:07:30.542: INFO: Pod "dns-test-864418c9-f7ee-45dd-bd36-68abcef269c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.253888601s
    Mar 27 14:07:32.543: INFO: Pod "dns-test-864418c9-f7ee-45dd-bd36-68abcef269c3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.254196688s
    Mar 27 14:07:34.543: INFO: Pod "dns-test-864418c9-f7ee-45dd-bd36-68abcef269c3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.254511306s
    Mar 27 14:07:36.544: INFO: Pod "dns-test-864418c9-f7ee-45dd-bd36-68abcef269c3": Phase="Running", Reason="", readiness=true. Elapsed: 10.255272386s
    Mar 27 14:07:36.544: INFO: Pod "dns-test-864418c9-f7ee-45dd-bd36-68abcef269c3" satisfied condition "running"
    STEP: retrieving the pod 03/27/23 14:07:36.544
    STEP: looking for the results for each expected name from probers 03/27/23 14:07:36.548
    Mar 27 14:07:36.568: INFO: DNS probes using dns-test-864418c9-f7ee-45dd-bd36-68abcef269c3 succeeded

    STEP: deleting the pod 03/27/23 14:07:36.568
    STEP: changing the service to type=ClusterIP 03/27/23 14:07:36.581
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3099.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3099.svc.cluster.local; sleep 1; done
     03/27/23 14:07:36.596
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3099.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3099.svc.cluster.local; sleep 1; done
     03/27/23 14:07:36.596
    STEP: creating a third pod to probe DNS 03/27/23 14:07:36.596
    STEP: submitting the pod to kubernetes 03/27/23 14:07:36.599
    Mar 27 14:07:36.609: INFO: Waiting up to 15m0s for pod "dns-test-ae99ff75-ee54-4146-bfa3-de9561cadb56" in namespace "dns-3099" to be "running"
    Mar 27 14:07:36.616: INFO: Pod "dns-test-ae99ff75-ee54-4146-bfa3-de9561cadb56": Phase="Pending", Reason="", readiness=false. Elapsed: 7.302658ms
    Mar 27 14:07:38.621: INFO: Pod "dns-test-ae99ff75-ee54-4146-bfa3-de9561cadb56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012338459s
    Mar 27 14:07:40.622: INFO: Pod "dns-test-ae99ff75-ee54-4146-bfa3-de9561cadb56": Phase="Running", Reason="", readiness=true. Elapsed: 4.013443215s
    Mar 27 14:07:40.622: INFO: Pod "dns-test-ae99ff75-ee54-4146-bfa3-de9561cadb56" satisfied condition "running"
    STEP: retrieving the pod 03/27/23 14:07:40.622
    STEP: looking for the results for each expected name from probers 03/27/23 14:07:40.627
    Mar 27 14:07:40.647: INFO: DNS probes using dns-test-ae99ff75-ee54-4146-bfa3-de9561cadb56 succeeded

    STEP: deleting the pod 03/27/23 14:07:40.647
    STEP: deleting the test externalName service 03/27/23 14:07:40.662
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 27 14:07:40.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3099" for this suite. 03/27/23 14:07:40.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:07:40.694
Mar 27 14:07:40.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename daemonsets 03/27/23 14:07:40.696
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:07:40.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:07:40.721
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 03/27/23 14:07:40.747
STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 14:07:40.753
Mar 27 14:07:40.762: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 14:07:40.762: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 14:07:41.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 14:07:41.773: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 14:07:42.772: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 14:07:42.772: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 14:07:43.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 14:07:43.773: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 14:07:44.774: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 14:07:44.774: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 14:07:45.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 14:07:45.773: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 14:07:46.776: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 27 14:07:46.776: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 14:07:47.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 27 14:07:47.773: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 14:07:48.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 14:07:48.773: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc is running 0 daemon pod, expected 1
Mar 27 14:07:49.772: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 27 14:07:49.772: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/27/23 14:07:49.783
Mar 27 14:07:49.815: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 14:07:49.815: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv is running 0 daemon pod, expected 1
Mar 27 14:07:50.829: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 14:07:50.829: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv is running 0 daemon pod, expected 1
Mar 27 14:07:51.827: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 14:07:51.827: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv is running 0 daemon pod, expected 1
Mar 27 14:07:52.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 14:07:52.826: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv is running 0 daemon pod, expected 1
Mar 27 14:07:53.827: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 14:07:53.827: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv is running 0 daemon pod, expected 1
Mar 27 14:07:54.827: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 27 14:07:54.827: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 03/27/23 14:07:54.827
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/27/23 14:07:54.834
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1326, will wait for the garbage collector to delete the pods 03/27/23 14:07:54.834
Mar 27 14:07:54.899: INFO: Deleting DaemonSet.extensions daemon-set took: 8.36515ms
Mar 27 14:07:55.000: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.106147ms
Mar 27 14:07:58.907: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 14:07:58.907: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 27 14:07:58.914: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5505"},"items":null}

Mar 27 14:07:58.918: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5505"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 27 14:07:58.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1326" for this suite. 03/27/23 14:07:58.94
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":8,"skipped":93,"failed":0}
------------------------------
• [SLOW TEST] [18.253 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:07:40.694
    Mar 27 14:07:40.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename daemonsets 03/27/23 14:07:40.696
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:07:40.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:07:40.721
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 03/27/23 14:07:40.747
    STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 14:07:40.753
    Mar 27 14:07:40.762: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 14:07:40.762: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 14:07:41.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 14:07:41.773: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 14:07:42.772: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 14:07:42.772: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 14:07:43.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 14:07:43.773: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 14:07:44.774: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 14:07:44.774: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 14:07:45.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 14:07:45.773: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 14:07:46.776: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 27 14:07:46.776: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 14:07:47.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 27 14:07:47.773: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 14:07:48.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 14:07:48.773: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc is running 0 daemon pod, expected 1
    Mar 27 14:07:49.772: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 27 14:07:49.772: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/27/23 14:07:49.783
    Mar 27 14:07:49.815: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 14:07:49.815: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv is running 0 daemon pod, expected 1
    Mar 27 14:07:50.829: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 14:07:50.829: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv is running 0 daemon pod, expected 1
    Mar 27 14:07:51.827: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 14:07:51.827: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv is running 0 daemon pod, expected 1
    Mar 27 14:07:52.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 14:07:52.826: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv is running 0 daemon pod, expected 1
    Mar 27 14:07:53.827: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 14:07:53.827: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv is running 0 daemon pod, expected 1
    Mar 27 14:07:54.827: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 27 14:07:54.827: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 03/27/23 14:07:54.827
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/27/23 14:07:54.834
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1326, will wait for the garbage collector to delete the pods 03/27/23 14:07:54.834
    Mar 27 14:07:54.899: INFO: Deleting DaemonSet.extensions daemon-set took: 8.36515ms
    Mar 27 14:07:55.000: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.106147ms
    Mar 27 14:07:58.907: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 14:07:58.907: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 27 14:07:58.914: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5505"},"items":null}

    Mar 27 14:07:58.918: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5505"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 14:07:58.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1326" for this suite. 03/27/23 14:07:58.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:07:58.949
Mar 27 14:07:58.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename services 03/27/23 14:07:58.95
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:07:58.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:07:58.97
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-2346 03/27/23 14:07:58.973
Mar 27 14:07:58.985: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-2346" to be "running and ready"
Mar 27 14:07:58.994: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 8.574296ms
Mar 27 14:07:58.994: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:08:01.000: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.014276238s
Mar 27 14:08:01.000: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar 27 14:08:01.000: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Mar 27 14:08:01.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2346 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar 27 14:08:01.216: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar 27 14:08:01.216: INFO: stdout: "ipvs"
Mar 27 14:08:01.216: INFO: proxyMode: ipvs
Mar 27 14:08:01.231: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 27 14:08:01.233: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-2346 03/27/23 14:08:01.233
STEP: creating replication controller affinity-nodeport-timeout in namespace services-2346 03/27/23 14:08:01.25
I0327 14:08:01.260145      24 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2346, replica count: 3
I0327 14:08:04.311598      24 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0327 14:08:07.314376      24 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0327 14:08:10.314794      24 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 14:08:10.328: INFO: Creating new exec pod
Mar 27 14:08:10.335: INFO: Waiting up to 5m0s for pod "execpod-affinityn4vjc" in namespace "services-2346" to be "running"
Mar 27 14:08:10.338: INFO: Pod "execpod-affinityn4vjc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.835936ms
Mar 27 14:08:12.343: INFO: Pod "execpod-affinityn4vjc": Phase="Running", Reason="", readiness=true. Elapsed: 2.007510589s
Mar 27 14:08:12.343: INFO: Pod "execpod-affinityn4vjc" satisfied condition "running"
Mar 27 14:08:13.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2346 exec execpod-affinityn4vjc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Mar 27 14:08:13.512: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Mar 27 14:08:13.512: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 14:08:13.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2346 exec execpod-affinityn4vjc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.22.255 80'
Mar 27 14:08:13.685: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.22.255 80\nConnection to 10.240.22.255 80 port [tcp/http] succeeded!\n"
Mar 27 14:08:13.685: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 14:08:13.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2346 exec execpod-affinityn4vjc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.4 30796'
Mar 27 14:08:13.840: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.4 30796\nConnection to 192.168.1.4 30796 port [tcp/*] succeeded!\n"
Mar 27 14:08:13.840: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 14:08:13.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2346 exec execpod-affinityn4vjc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.3 30796'
Mar 27 14:08:13.997: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.3 30796\nConnection to 192.168.1.3 30796 port [tcp/*] succeeded!\n"
Mar 27 14:08:13.997: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 14:08:13.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2346 exec execpod-affinityn4vjc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.3:30796/ ; done'
Mar 27 14:08:14.260: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n"
Mar 27 14:08:14.260: INFO: stdout: "\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n"
Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
Mar 27 14:08:14.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2346 exec execpod-affinityn4vjc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.3:30796/'
Mar 27 14:08:14.429: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n"
Mar 27 14:08:14.429: INFO: stdout: "affinity-nodeport-timeout-5gl4n"
Mar 27 14:10:24.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2346 exec execpod-affinityn4vjc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.3:30796/'
Mar 27 14:10:24.598: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n"
Mar 27 14:10:24.598: INFO: stdout: "affinity-nodeport-timeout-mzmdv"
Mar 27 14:10:24.598: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-2346, will wait for the garbage collector to delete the pods 03/27/23 14:10:31.168
Mar 27 14:10:33.041: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 1.816881254s
Mar 27 14:10:33.141: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.778827ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 27 14:10:35.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2346" for this suite. 03/27/23 14:10:35.583
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":9,"skipped":116,"failed":0}
------------------------------
• [SLOW TEST] [156.646 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:07:58.949
    Mar 27 14:07:58.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename services 03/27/23 14:07:58.95
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:07:58.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:07:58.97
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-2346 03/27/23 14:07:58.973
    Mar 27 14:07:58.985: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-2346" to be "running and ready"
    Mar 27 14:07:58.994: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 8.574296ms
    Mar 27 14:07:58.994: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:08:01.000: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.014276238s
    Mar 27 14:08:01.000: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Mar 27 14:08:01.000: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Mar 27 14:08:01.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2346 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Mar 27 14:08:01.216: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Mar 27 14:08:01.216: INFO: stdout: "ipvs"
    Mar 27 14:08:01.216: INFO: proxyMode: ipvs
    Mar 27 14:08:01.231: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Mar 27 14:08:01.233: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-2346 03/27/23 14:08:01.233
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-2346 03/27/23 14:08:01.25
    I0327 14:08:01.260145      24 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2346, replica count: 3
    I0327 14:08:04.311598      24 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0327 14:08:07.314376      24 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0327 14:08:10.314794      24 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 14:08:10.328: INFO: Creating new exec pod
    Mar 27 14:08:10.335: INFO: Waiting up to 5m0s for pod "execpod-affinityn4vjc" in namespace "services-2346" to be "running"
    Mar 27 14:08:10.338: INFO: Pod "execpod-affinityn4vjc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.835936ms
    Mar 27 14:08:12.343: INFO: Pod "execpod-affinityn4vjc": Phase="Running", Reason="", readiness=true. Elapsed: 2.007510589s
    Mar 27 14:08:12.343: INFO: Pod "execpod-affinityn4vjc" satisfied condition "running"
    Mar 27 14:08:13.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2346 exec execpod-affinityn4vjc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Mar 27 14:08:13.512: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Mar 27 14:08:13.512: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 14:08:13.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2346 exec execpod-affinityn4vjc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.22.255 80'
    Mar 27 14:08:13.685: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.22.255 80\nConnection to 10.240.22.255 80 port [tcp/http] succeeded!\n"
    Mar 27 14:08:13.685: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 14:08:13.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2346 exec execpod-affinityn4vjc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.4 30796'
    Mar 27 14:08:13.840: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.4 30796\nConnection to 192.168.1.4 30796 port [tcp/*] succeeded!\n"
    Mar 27 14:08:13.840: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 14:08:13.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2346 exec execpod-affinityn4vjc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.3 30796'
    Mar 27 14:08:13.997: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.3 30796\nConnection to 192.168.1.3 30796 port [tcp/*] succeeded!\n"
    Mar 27 14:08:13.997: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 14:08:13.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2346 exec execpod-affinityn4vjc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.3:30796/ ; done'
    Mar 27 14:08:14.260: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n"
    Mar 27 14:08:14.260: INFO: stdout: "\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n\naffinity-nodeport-timeout-5gl4n"
    Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
    Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
    Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
    Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
    Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
    Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
    Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
    Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
    Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
    Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
    Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
    Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
    Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
    Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
    Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
    Mar 27 14:08:14.260: INFO: Received response from host: affinity-nodeport-timeout-5gl4n
    Mar 27 14:08:14.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2346 exec execpod-affinityn4vjc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.3:30796/'
    Mar 27 14:08:14.429: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n"
    Mar 27 14:08:14.429: INFO: stdout: "affinity-nodeport-timeout-5gl4n"
    Mar 27 14:10:24.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2346 exec execpod-affinityn4vjc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.3:30796/'
    Mar 27 14:10:24.598: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.3:30796/\n"
    Mar 27 14:10:24.598: INFO: stdout: "affinity-nodeport-timeout-mzmdv"
    Mar 27 14:10:24.598: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-2346, will wait for the garbage collector to delete the pods 03/27/23 14:10:31.168
    Mar 27 14:10:33.041: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 1.816881254s
    Mar 27 14:10:33.141: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.778827ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 27 14:10:35.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2346" for this suite. 03/27/23 14:10:35.583
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:10:35.596
Mar 27 14:10:35.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 14:10:35.597
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:10:35.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:10:35.626
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-75b03397-6b96-455d-ba92-716d3b1dca6a 03/27/23 14:10:35.63
STEP: Creating a pod to test consume configMaps 03/27/23 14:10:35.638
Mar 27 14:10:35.652: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f56b89a9-2ede-4f87-b0ba-6e942562fe33" in namespace "projected-2978" to be "Succeeded or Failed"
Mar 27 14:10:35.658: INFO: Pod "pod-projected-configmaps-f56b89a9-2ede-4f87-b0ba-6e942562fe33": Phase="Pending", Reason="", readiness=false. Elapsed: 5.700187ms
Mar 27 14:10:37.665: INFO: Pod "pod-projected-configmaps-f56b89a9-2ede-4f87-b0ba-6e942562fe33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011999204s
Mar 27 14:10:39.664: INFO: Pod "pod-projected-configmaps-f56b89a9-2ede-4f87-b0ba-6e942562fe33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011556517s
STEP: Saw pod success 03/27/23 14:10:39.664
Mar 27 14:10:39.664: INFO: Pod "pod-projected-configmaps-f56b89a9-2ede-4f87-b0ba-6e942562fe33" satisfied condition "Succeeded or Failed"
Mar 27 14:10:39.671: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-configmaps-f56b89a9-2ede-4f87-b0ba-6e942562fe33 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 14:10:39.679
Mar 27 14:10:39.693: INFO: Waiting for pod pod-projected-configmaps-f56b89a9-2ede-4f87-b0ba-6e942562fe33 to disappear
Mar 27 14:10:39.699: INFO: Pod pod-projected-configmaps-f56b89a9-2ede-4f87-b0ba-6e942562fe33 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 27 14:10:39.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2978" for this suite. 03/27/23 14:10:39.708
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":10,"skipped":116,"failed":0}
------------------------------
• [4.126 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:10:35.596
    Mar 27 14:10:35.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 14:10:35.597
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:10:35.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:10:35.626
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-75b03397-6b96-455d-ba92-716d3b1dca6a 03/27/23 14:10:35.63
    STEP: Creating a pod to test consume configMaps 03/27/23 14:10:35.638
    Mar 27 14:10:35.652: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f56b89a9-2ede-4f87-b0ba-6e942562fe33" in namespace "projected-2978" to be "Succeeded or Failed"
    Mar 27 14:10:35.658: INFO: Pod "pod-projected-configmaps-f56b89a9-2ede-4f87-b0ba-6e942562fe33": Phase="Pending", Reason="", readiness=false. Elapsed: 5.700187ms
    Mar 27 14:10:37.665: INFO: Pod "pod-projected-configmaps-f56b89a9-2ede-4f87-b0ba-6e942562fe33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011999204s
    Mar 27 14:10:39.664: INFO: Pod "pod-projected-configmaps-f56b89a9-2ede-4f87-b0ba-6e942562fe33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011556517s
    STEP: Saw pod success 03/27/23 14:10:39.664
    Mar 27 14:10:39.664: INFO: Pod "pod-projected-configmaps-f56b89a9-2ede-4f87-b0ba-6e942562fe33" satisfied condition "Succeeded or Failed"
    Mar 27 14:10:39.671: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-configmaps-f56b89a9-2ede-4f87-b0ba-6e942562fe33 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 14:10:39.679
    Mar 27 14:10:39.693: INFO: Waiting for pod pod-projected-configmaps-f56b89a9-2ede-4f87-b0ba-6e942562fe33 to disappear
    Mar 27 14:10:39.699: INFO: Pod pod-projected-configmaps-f56b89a9-2ede-4f87-b0ba-6e942562fe33 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 27 14:10:39.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2978" for this suite. 03/27/23 14:10:39.708
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:10:39.723
Mar 27 14:10:39.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename resourcequota 03/27/23 14:10:39.724
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:10:39.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:10:39.747
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 03/27/23 14:10:39.75
STEP: Counting existing ResourceQuota 03/27/23 14:10:44.754
STEP: Creating a ResourceQuota 03/27/23 14:10:49.76
STEP: Ensuring resource quota status is calculated 03/27/23 14:10:49.767
STEP: Creating a Secret 03/27/23 14:10:51.777
STEP: Ensuring resource quota status captures secret creation 03/27/23 14:10:51.791
STEP: Deleting a secret 03/27/23 14:10:53.796
STEP: Ensuring resource quota status released usage 03/27/23 14:10:53.808
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 27 14:10:55.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1603" for this suite. 03/27/23 14:10:55.819
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":11,"skipped":156,"failed":0}
------------------------------
• [SLOW TEST] [16.103 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:10:39.723
    Mar 27 14:10:39.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename resourcequota 03/27/23 14:10:39.724
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:10:39.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:10:39.747
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 03/27/23 14:10:39.75
    STEP: Counting existing ResourceQuota 03/27/23 14:10:44.754
    STEP: Creating a ResourceQuota 03/27/23 14:10:49.76
    STEP: Ensuring resource quota status is calculated 03/27/23 14:10:49.767
    STEP: Creating a Secret 03/27/23 14:10:51.777
    STEP: Ensuring resource quota status captures secret creation 03/27/23 14:10:51.791
    STEP: Deleting a secret 03/27/23 14:10:53.796
    STEP: Ensuring resource quota status released usage 03/27/23 14:10:53.808
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 27 14:10:55.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1603" for this suite. 03/27/23 14:10:55.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:10:55.826
Mar 27 14:10:55.826: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubectl 03/27/23 14:10:55.827
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:10:55.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:10:55.849
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 03/27/23 14:10:55.852
Mar 27 14:10:55.852: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar 27 14:10:55.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 create -f -'
Mar 27 14:10:56.941: INFO: stderr: ""
Mar 27 14:10:56.941: INFO: stdout: "service/agnhost-replica created\n"
Mar 27 14:10:56.941: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar 27 14:10:56.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 create -f -'
Mar 27 14:10:57.133: INFO: stderr: ""
Mar 27 14:10:57.133: INFO: stdout: "service/agnhost-primary created\n"
Mar 27 14:10:57.133: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 27 14:10:57.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 create -f -'
Mar 27 14:10:57.327: INFO: stderr: ""
Mar 27 14:10:57.327: INFO: stdout: "service/frontend created\n"
Mar 27 14:10:57.327: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar 27 14:10:57.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 create -f -'
Mar 27 14:10:57.565: INFO: stderr: ""
Mar 27 14:10:57.565: INFO: stdout: "deployment.apps/frontend created\n"
Mar 27 14:10:57.565: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 27 14:10:57.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 create -f -'
Mar 27 14:10:57.750: INFO: stderr: ""
Mar 27 14:10:57.750: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar 27 14:10:57.750: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 27 14:10:57.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 create -f -'
Mar 27 14:10:58.006: INFO: stderr: ""
Mar 27 14:10:58.006: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 03/27/23 14:10:58.006
Mar 27 14:10:58.006: INFO: Waiting for all frontend pods to be Running.
Mar 27 14:11:03.057: INFO: Waiting for frontend to serve content.
Mar 27 14:11:03.071: INFO: Trying to add a new entry to the guestbook.
Mar 27 14:11:03.089: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 03/27/23 14:11:03.098
Mar 27 14:11:03.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 delete --grace-period=0 --force -f -'
Mar 27 14:11:03.170: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 27 14:11:03.170: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 03/27/23 14:11:03.17
Mar 27 14:11:03.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 delete --grace-period=0 --force -f -'
Mar 27 14:11:03.267: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 27 14:11:03.267: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/27/23 14:11:03.267
Mar 27 14:11:03.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 delete --grace-period=0 --force -f -'
Mar 27 14:11:03.355: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 27 14:11:03.355: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/27/23 14:11:03.355
Mar 27 14:11:03.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 delete --grace-period=0 --force -f -'
Mar 27 14:11:03.420: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 27 14:11:03.420: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/27/23 14:11:03.42
Mar 27 14:11:03.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 delete --grace-period=0 --force -f -'
Mar 27 14:11:03.489: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 27 14:11:03.489: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/27/23 14:11:03.489
Mar 27 14:11:03.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 delete --grace-period=0 --force -f -'
Mar 27 14:11:03.587: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 27 14:11:03.587: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 27 14:11:03.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5822" for this suite. 03/27/23 14:11:03.596
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":12,"skipped":163,"failed":0}
------------------------------
• [SLOW TEST] [7.781 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:10:55.826
    Mar 27 14:10:55.826: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubectl 03/27/23 14:10:55.827
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:10:55.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:10:55.849
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 03/27/23 14:10:55.852
    Mar 27 14:10:55.852: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Mar 27 14:10:55.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 create -f -'
    Mar 27 14:10:56.941: INFO: stderr: ""
    Mar 27 14:10:56.941: INFO: stdout: "service/agnhost-replica created\n"
    Mar 27 14:10:56.941: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Mar 27 14:10:56.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 create -f -'
    Mar 27 14:10:57.133: INFO: stderr: ""
    Mar 27 14:10:57.133: INFO: stdout: "service/agnhost-primary created\n"
    Mar 27 14:10:57.133: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Mar 27 14:10:57.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 create -f -'
    Mar 27 14:10:57.327: INFO: stderr: ""
    Mar 27 14:10:57.327: INFO: stdout: "service/frontend created\n"
    Mar 27 14:10:57.327: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Mar 27 14:10:57.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 create -f -'
    Mar 27 14:10:57.565: INFO: stderr: ""
    Mar 27 14:10:57.565: INFO: stdout: "deployment.apps/frontend created\n"
    Mar 27 14:10:57.565: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar 27 14:10:57.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 create -f -'
    Mar 27 14:10:57.750: INFO: stderr: ""
    Mar 27 14:10:57.750: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Mar 27 14:10:57.750: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar 27 14:10:57.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 create -f -'
    Mar 27 14:10:58.006: INFO: stderr: ""
    Mar 27 14:10:58.006: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 03/27/23 14:10:58.006
    Mar 27 14:10:58.006: INFO: Waiting for all frontend pods to be Running.
    Mar 27 14:11:03.057: INFO: Waiting for frontend to serve content.
    Mar 27 14:11:03.071: INFO: Trying to add a new entry to the guestbook.
    Mar 27 14:11:03.089: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 03/27/23 14:11:03.098
    Mar 27 14:11:03.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 delete --grace-period=0 --force -f -'
    Mar 27 14:11:03.170: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 27 14:11:03.170: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 03/27/23 14:11:03.17
    Mar 27 14:11:03.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 delete --grace-period=0 --force -f -'
    Mar 27 14:11:03.267: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 27 14:11:03.267: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/27/23 14:11:03.267
    Mar 27 14:11:03.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 delete --grace-period=0 --force -f -'
    Mar 27 14:11:03.355: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 27 14:11:03.355: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/27/23 14:11:03.355
    Mar 27 14:11:03.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 delete --grace-period=0 --force -f -'
    Mar 27 14:11:03.420: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 27 14:11:03.420: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/27/23 14:11:03.42
    Mar 27 14:11:03.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 delete --grace-period=0 --force -f -'
    Mar 27 14:11:03.489: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 27 14:11:03.489: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/27/23 14:11:03.489
    Mar 27 14:11:03.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5822 delete --grace-period=0 --force -f -'
    Mar 27 14:11:03.587: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 27 14:11:03.587: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 27 14:11:03.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5822" for this suite. 03/27/23 14:11:03.596
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:11:03.61
Mar 27 14:11:03.610: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 14:11:03.611
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:11:03.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:11:03.635
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 03/27/23 14:11:03.639
Mar 27 14:11:03.648: INFO: Waiting up to 5m0s for pod "downwardapi-volume-999be900-5c01-48e1-a9d3-1458780e8ee2" in namespace "projected-3366" to be "Succeeded or Failed"
Mar 27 14:11:03.652: INFO: Pod "downwardapi-volume-999be900-5c01-48e1-a9d3-1458780e8ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.776951ms
Mar 27 14:11:05.657: INFO: Pod "downwardapi-volume-999be900-5c01-48e1-a9d3-1458780e8ee2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008693947s
Mar 27 14:11:07.657: INFO: Pod "downwardapi-volume-999be900-5c01-48e1-a9d3-1458780e8ee2": Phase="Running", Reason="", readiness=false. Elapsed: 4.008608277s
Mar 27 14:11:09.664: INFO: Pod "downwardapi-volume-999be900-5c01-48e1-a9d3-1458780e8ee2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015892541s
STEP: Saw pod success 03/27/23 14:11:09.664
Mar 27 14:11:09.664: INFO: Pod "downwardapi-volume-999be900-5c01-48e1-a9d3-1458780e8ee2" satisfied condition "Succeeded or Failed"
Mar 27 14:11:09.670: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-999be900-5c01-48e1-a9d3-1458780e8ee2 container client-container: <nil>
STEP: delete the pod 03/27/23 14:11:09.68
Mar 27 14:11:09.700: INFO: Waiting for pod downwardapi-volume-999be900-5c01-48e1-a9d3-1458780e8ee2 to disappear
Mar 27 14:11:09.703: INFO: Pod downwardapi-volume-999be900-5c01-48e1-a9d3-1458780e8ee2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 27 14:11:09.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3366" for this suite. 03/27/23 14:11:09.708
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":13,"skipped":211,"failed":0}
------------------------------
• [SLOW TEST] [6.106 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:11:03.61
    Mar 27 14:11:03.610: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 14:11:03.611
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:11:03.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:11:03.635
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 03/27/23 14:11:03.639
    Mar 27 14:11:03.648: INFO: Waiting up to 5m0s for pod "downwardapi-volume-999be900-5c01-48e1-a9d3-1458780e8ee2" in namespace "projected-3366" to be "Succeeded or Failed"
    Mar 27 14:11:03.652: INFO: Pod "downwardapi-volume-999be900-5c01-48e1-a9d3-1458780e8ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.776951ms
    Mar 27 14:11:05.657: INFO: Pod "downwardapi-volume-999be900-5c01-48e1-a9d3-1458780e8ee2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008693947s
    Mar 27 14:11:07.657: INFO: Pod "downwardapi-volume-999be900-5c01-48e1-a9d3-1458780e8ee2": Phase="Running", Reason="", readiness=false. Elapsed: 4.008608277s
    Mar 27 14:11:09.664: INFO: Pod "downwardapi-volume-999be900-5c01-48e1-a9d3-1458780e8ee2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015892541s
    STEP: Saw pod success 03/27/23 14:11:09.664
    Mar 27 14:11:09.664: INFO: Pod "downwardapi-volume-999be900-5c01-48e1-a9d3-1458780e8ee2" satisfied condition "Succeeded or Failed"
    Mar 27 14:11:09.670: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-999be900-5c01-48e1-a9d3-1458780e8ee2 container client-container: <nil>
    STEP: delete the pod 03/27/23 14:11:09.68
    Mar 27 14:11:09.700: INFO: Waiting for pod downwardapi-volume-999be900-5c01-48e1-a9d3-1458780e8ee2 to disappear
    Mar 27 14:11:09.703: INFO: Pod downwardapi-volume-999be900-5c01-48e1-a9d3-1458780e8ee2 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 27 14:11:09.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3366" for this suite. 03/27/23 14:11:09.708
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:11:09.716
Mar 27 14:11:09.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename crd-webhook 03/27/23 14:11:09.717
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:11:09.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:11:09.738
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/27/23 14:11:09.741
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/27/23 14:11:10.58
STEP: Deploying the custom resource conversion webhook pod 03/27/23 14:11:10.592
STEP: Wait for the deployment to be ready 03/27/23 14:11:10.61
Mar 27 14:11:10.622: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 14:11:12.636
STEP: Verifying the service has paired with the endpoint 03/27/23 14:11:12.649
Mar 27 14:11:13.649: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Mar 27 14:11:13.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Creating a v1 custom resource 03/27/23 14:11:16.291
STEP: v2 custom resource should be converted 03/27/23 14:11:16.3
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:11:16.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2925" for this suite. 03/27/23 14:11:16.827
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":14,"skipped":231,"failed":0}
------------------------------
• [SLOW TEST] [7.157 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:11:09.716
    Mar 27 14:11:09.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename crd-webhook 03/27/23 14:11:09.717
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:11:09.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:11:09.738
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/27/23 14:11:09.741
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/27/23 14:11:10.58
    STEP: Deploying the custom resource conversion webhook pod 03/27/23 14:11:10.592
    STEP: Wait for the deployment to be ready 03/27/23 14:11:10.61
    Mar 27 14:11:10.622: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 14:11:12.636
    STEP: Verifying the service has paired with the endpoint 03/27/23 14:11:12.649
    Mar 27 14:11:13.649: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Mar 27 14:11:13.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Creating a v1 custom resource 03/27/23 14:11:16.291
    STEP: v2 custom resource should be converted 03/27/23 14:11:16.3
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:11:16.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-2925" for this suite. 03/27/23 14:11:16.827
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:11:16.875
Mar 27 14:11:16.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename replicaset 03/27/23 14:11:16.875
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:11:16.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:11:16.9
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Mar 27 14:11:16.915: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 27 14:11:21.921: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/27/23 14:11:21.921
STEP: Scaling up "test-rs" replicaset  03/27/23 14:11:21.922
Mar 27 14:11:21.935: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 03/27/23 14:11:21.935
W0327 14:11:21.948110      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 27 14:11:21.950: INFO: observed ReplicaSet test-rs in namespace replicaset-1903 with ReadyReplicas 1, AvailableReplicas 1
Mar 27 14:11:21.967: INFO: observed ReplicaSet test-rs in namespace replicaset-1903 with ReadyReplicas 1, AvailableReplicas 1
Mar 27 14:11:22.002: INFO: observed ReplicaSet test-rs in namespace replicaset-1903 with ReadyReplicas 1, AvailableReplicas 1
Mar 27 14:11:22.018: INFO: observed ReplicaSet test-rs in namespace replicaset-1903 with ReadyReplicas 1, AvailableReplicas 1
Mar 27 14:11:23.853: INFO: observed ReplicaSet test-rs in namespace replicaset-1903 with ReadyReplicas 2, AvailableReplicas 2
Mar 27 14:11:25.875: INFO: observed Replicaset test-rs in namespace replicaset-1903 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar 27 14:11:25.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1903" for this suite. 03/27/23 14:11:25.881
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":15,"skipped":234,"failed":0}
------------------------------
• [SLOW TEST] [9.019 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:11:16.875
    Mar 27 14:11:16.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename replicaset 03/27/23 14:11:16.875
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:11:16.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:11:16.9
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Mar 27 14:11:16.915: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 27 14:11:21.921: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/27/23 14:11:21.921
    STEP: Scaling up "test-rs" replicaset  03/27/23 14:11:21.922
    Mar 27 14:11:21.935: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 03/27/23 14:11:21.935
    W0327 14:11:21.948110      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 27 14:11:21.950: INFO: observed ReplicaSet test-rs in namespace replicaset-1903 with ReadyReplicas 1, AvailableReplicas 1
    Mar 27 14:11:21.967: INFO: observed ReplicaSet test-rs in namespace replicaset-1903 with ReadyReplicas 1, AvailableReplicas 1
    Mar 27 14:11:22.002: INFO: observed ReplicaSet test-rs in namespace replicaset-1903 with ReadyReplicas 1, AvailableReplicas 1
    Mar 27 14:11:22.018: INFO: observed ReplicaSet test-rs in namespace replicaset-1903 with ReadyReplicas 1, AvailableReplicas 1
    Mar 27 14:11:23.853: INFO: observed ReplicaSet test-rs in namespace replicaset-1903 with ReadyReplicas 2, AvailableReplicas 2
    Mar 27 14:11:25.875: INFO: observed Replicaset test-rs in namespace replicaset-1903 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar 27 14:11:25.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1903" for this suite. 03/27/23 14:11:25.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:11:25.898
Mar 27 14:11:25.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename services 03/27/23 14:11:25.899
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:11:25.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:11:25.927
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8835 03/27/23 14:11:25.932
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/27/23 14:11:25.944
STEP: creating service externalsvc in namespace services-8835 03/27/23 14:11:25.944
STEP: creating replication controller externalsvc in namespace services-8835 03/27/23 14:11:25.961
I0327 14:11:25.971696      24 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8835, replica count: 2
I0327 14:11:29.023058      24 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 03/27/23 14:11:29.029
Mar 27 14:11:29.045: INFO: Creating new exec pod
Mar 27 14:11:29.050: INFO: Waiting up to 5m0s for pod "execpodfchwb" in namespace "services-8835" to be "running"
Mar 27 14:11:29.058: INFO: Pod "execpodfchwb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.602249ms
Mar 27 14:11:31.072: INFO: Pod "execpodfchwb": Phase="Running", Reason="", readiness=true. Elapsed: 2.021519925s
Mar 27 14:11:31.072: INFO: Pod "execpodfchwb" satisfied condition "running"
Mar 27 14:11:31.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-8835 exec execpodfchwb -- /bin/sh -x -c nslookup clusterip-service.services-8835.svc.cluster.local'
Mar 27 14:11:31.287: INFO: stderr: "+ nslookup clusterip-service.services-8835.svc.cluster.local\n"
Mar 27 14:11:31.287: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-8835.svc.cluster.local\tcanonical name = externalsvc.services-8835.svc.cluster.local.\nName:\texternalsvc.services-8835.svc.cluster.local\nAddress: 10.240.31.98\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8835, will wait for the garbage collector to delete the pods 03/27/23 14:11:31.287
Mar 27 14:11:31.357: INFO: Deleting ReplicationController externalsvc took: 6.142395ms
Mar 27 14:11:31.458: INFO: Terminating ReplicationController externalsvc pods took: 100.877179ms
Mar 27 14:11:46.078: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 27 14:11:46.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8835" for this suite. 03/27/23 14:11:46.166
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":16,"skipped":267,"failed":0}
------------------------------
• [SLOW TEST] [20.274 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:11:25.898
    Mar 27 14:11:25.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename services 03/27/23 14:11:25.899
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:11:25.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:11:25.927
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8835 03/27/23 14:11:25.932
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/27/23 14:11:25.944
    STEP: creating service externalsvc in namespace services-8835 03/27/23 14:11:25.944
    STEP: creating replication controller externalsvc in namespace services-8835 03/27/23 14:11:25.961
    I0327 14:11:25.971696      24 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8835, replica count: 2
    I0327 14:11:29.023058      24 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 03/27/23 14:11:29.029
    Mar 27 14:11:29.045: INFO: Creating new exec pod
    Mar 27 14:11:29.050: INFO: Waiting up to 5m0s for pod "execpodfchwb" in namespace "services-8835" to be "running"
    Mar 27 14:11:29.058: INFO: Pod "execpodfchwb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.602249ms
    Mar 27 14:11:31.072: INFO: Pod "execpodfchwb": Phase="Running", Reason="", readiness=true. Elapsed: 2.021519925s
    Mar 27 14:11:31.072: INFO: Pod "execpodfchwb" satisfied condition "running"
    Mar 27 14:11:31.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-8835 exec execpodfchwb -- /bin/sh -x -c nslookup clusterip-service.services-8835.svc.cluster.local'
    Mar 27 14:11:31.287: INFO: stderr: "+ nslookup clusterip-service.services-8835.svc.cluster.local\n"
    Mar 27 14:11:31.287: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-8835.svc.cluster.local\tcanonical name = externalsvc.services-8835.svc.cluster.local.\nName:\texternalsvc.services-8835.svc.cluster.local\nAddress: 10.240.31.98\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8835, will wait for the garbage collector to delete the pods 03/27/23 14:11:31.287
    Mar 27 14:11:31.357: INFO: Deleting ReplicationController externalsvc took: 6.142395ms
    Mar 27 14:11:31.458: INFO: Terminating ReplicationController externalsvc pods took: 100.877179ms
    Mar 27 14:11:46.078: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 27 14:11:46.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8835" for this suite. 03/27/23 14:11:46.166
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:11:46.177
Mar 27 14:11:46.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 14:11:46.178
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:11:46.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:11:46.259
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-a4071546-d6bd-451e-8f6d-4ef8e9dbb51e 03/27/23 14:11:46.262
STEP: Creating a pod to test consume secrets 03/27/23 14:11:46.268
Mar 27 14:11:46.336: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e" in namespace "projected-9514" to be "Succeeded or Failed"
Mar 27 14:11:46.340: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.663439ms
Mar 27 14:11:48.345: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008558198s
Mar 27 14:11:50.344: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008209044s
Mar 27 14:11:52.378: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041667719s
Mar 27 14:11:54.345: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00895589s
Mar 27 14:11:56.345: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008817839s
Mar 27 14:11:58.511: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.174818331s
Mar 27 14:12:00.345: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009029786s
Mar 27 14:12:02.345: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008531608s
Mar 27 14:12:04.345: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008842285s
Mar 27 14:12:06.345: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009005569s
Mar 27 14:12:08.345: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.009372443s
STEP: Saw pod success 03/27/23 14:12:08.345
Mar 27 14:12:08.346: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e" satisfied condition "Succeeded or Failed"
Mar 27 14:12:08.350: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e container projected-secret-volume-test: <nil>
STEP: delete the pod 03/27/23 14:12:08.365
Mar 27 14:12:08.382: INFO: Waiting for pod pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e to disappear
Mar 27 14:12:08.385: INFO: Pod pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 27 14:12:08.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9514" for this suite. 03/27/23 14:12:08.391
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":17,"skipped":276,"failed":0}
------------------------------
• [SLOW TEST] [22.221 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:11:46.177
    Mar 27 14:11:46.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 14:11:46.178
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:11:46.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:11:46.259
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-a4071546-d6bd-451e-8f6d-4ef8e9dbb51e 03/27/23 14:11:46.262
    STEP: Creating a pod to test consume secrets 03/27/23 14:11:46.268
    Mar 27 14:11:46.336: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e" in namespace "projected-9514" to be "Succeeded or Failed"
    Mar 27 14:11:46.340: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.663439ms
    Mar 27 14:11:48.345: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008558198s
    Mar 27 14:11:50.344: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008209044s
    Mar 27 14:11:52.378: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041667719s
    Mar 27 14:11:54.345: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00895589s
    Mar 27 14:11:56.345: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008817839s
    Mar 27 14:11:58.511: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.174818331s
    Mar 27 14:12:00.345: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009029786s
    Mar 27 14:12:02.345: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008531608s
    Mar 27 14:12:04.345: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008842285s
    Mar 27 14:12:06.345: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009005569s
    Mar 27 14:12:08.345: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.009372443s
    STEP: Saw pod success 03/27/23 14:12:08.345
    Mar 27 14:12:08.346: INFO: Pod "pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e" satisfied condition "Succeeded or Failed"
    Mar 27 14:12:08.350: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 14:12:08.365
    Mar 27 14:12:08.382: INFO: Waiting for pod pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e to disappear
    Mar 27 14:12:08.385: INFO: Pod pod-projected-secrets-f076be9d-4a27-4c8b-b639-b9372cc8b23e no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 27 14:12:08.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9514" for this suite. 03/27/23 14:12:08.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:12:08.4
Mar 27 14:12:08.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename webhook 03/27/23 14:12:08.401
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:12:08.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:12:08.419
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/27/23 14:12:08.444
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 14:12:08.685
STEP: Deploying the webhook pod 03/27/23 14:12:08.696
STEP: Wait for the deployment to be ready 03/27/23 14:12:08.712
Mar 27 14:12:08.725: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 14:12:10.737
STEP: Verifying the service has paired with the endpoint 03/27/23 14:12:10.75
Mar 27 14:12:11.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 03/27/23 14:12:11.755
STEP: Creating a custom resource definition that should be denied by the webhook 03/27/23 14:12:11.782
Mar 27 14:12:11.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:12:11.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2569" for this suite. 03/27/23 14:12:11.809
STEP: Destroying namespace "webhook-2569-markers" for this suite. 03/27/23 14:12:11.818
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":18,"skipped":317,"failed":0}
------------------------------
• [3.472 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:12:08.4
    Mar 27 14:12:08.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename webhook 03/27/23 14:12:08.401
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:12:08.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:12:08.419
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/27/23 14:12:08.444
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 14:12:08.685
    STEP: Deploying the webhook pod 03/27/23 14:12:08.696
    STEP: Wait for the deployment to be ready 03/27/23 14:12:08.712
    Mar 27 14:12:08.725: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 14:12:10.737
    STEP: Verifying the service has paired with the endpoint 03/27/23 14:12:10.75
    Mar 27 14:12:11.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 03/27/23 14:12:11.755
    STEP: Creating a custom resource definition that should be denied by the webhook 03/27/23 14:12:11.782
    Mar 27 14:12:11.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:12:11.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2569" for this suite. 03/27/23 14:12:11.809
    STEP: Destroying namespace "webhook-2569-markers" for this suite. 03/27/23 14:12:11.818
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:12:11.873
Mar 27 14:12:11.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename endpointslice 03/27/23 14:12:11.873
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:12:11.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:12:11.895
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 03/27/23 14:12:21.983
STEP: referencing matching pods with named port 03/27/23 14:12:26.991
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/27/23 14:12:32.001
STEP: recreating EndpointSlices after they've been deleted 03/27/23 14:12:37.016
Mar 27 14:12:37.036: INFO: EndpointSlice for Service endpointslice-6429/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar 27 14:12:47.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6429" for this suite. 03/27/23 14:12:47.056
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":19,"skipped":330,"failed":0}
------------------------------
• [SLOW TEST] [35.192 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:12:11.873
    Mar 27 14:12:11.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename endpointslice 03/27/23 14:12:11.873
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:12:11.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:12:11.895
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 03/27/23 14:12:21.983
    STEP: referencing matching pods with named port 03/27/23 14:12:26.991
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/27/23 14:12:32.001
    STEP: recreating EndpointSlices after they've been deleted 03/27/23 14:12:37.016
    Mar 27 14:12:37.036: INFO: EndpointSlice for Service endpointslice-6429/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar 27 14:12:47.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6429" for this suite. 03/27/23 14:12:47.056
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:12:47.067
Mar 27 14:12:47.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubectl 03/27/23 14:12:47.069
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:12:47.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:12:47.087
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/27/23 14:12:47.09
Mar 27 14:12:47.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-568 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 27 14:12:47.157: INFO: stderr: ""
Mar 27 14:12:47.157: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 03/27/23 14:12:47.157
STEP: verifying the pod e2e-test-httpd-pod was created 03/27/23 14:12:52.21
Mar 27 14:12:52.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-568 get pod e2e-test-httpd-pod -o json'
Mar 27 14:12:52.279: INFO: stderr: ""
Mar 27 14:12:52.279: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"ae7f37ef33e2ab0a7efebcbbe1fef674916c739fa04248ef0af06fdceb93ea1d\",\n            \"cni.projectcalico.org/podIP\": \"172.25.2.27/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.25.2.27/32\"\n        },\n        \"creationTimestamp\": \"2023-03-27T14:12:47Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-568\",\n        \"resourceVersion\": \"8066\",\n        \"uid\": \"8d017220-9c53-4233-8238-2613da0560bc\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-5lt4b\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-5lt4b\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-27T14:12:47Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-27T14:12:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-27T14:12:48Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-27T14:12:47Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://387a4e54cc8f2f4900def557d55e08fe0c3db2926e2f28154bb2282e0dec23a1\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-27T14:12:48Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.1.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.2.27\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.25.2.27\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-27T14:12:47Z\"\n    }\n}\n"
STEP: replace the image in the pod 03/27/23 14:12:52.279
Mar 27 14:12:52.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-568 replace -f -'
Mar 27 14:12:53.318: INFO: stderr: ""
Mar 27 14:12:53.319: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 03/27/23 14:12:53.319
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Mar 27 14:12:53.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-568 delete pods e2e-test-httpd-pod'
Mar 27 14:12:58.864: INFO: stderr: ""
Mar 27 14:12:58.864: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 27 14:12:58.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-568" for this suite. 03/27/23 14:12:58.874
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":20,"skipped":379,"failed":0}
------------------------------
• [SLOW TEST] [11.815 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:12:47.067
    Mar 27 14:12:47.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubectl 03/27/23 14:12:47.069
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:12:47.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:12:47.087
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/27/23 14:12:47.09
    Mar 27 14:12:47.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-568 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar 27 14:12:47.157: INFO: stderr: ""
    Mar 27 14:12:47.157: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 03/27/23 14:12:47.157
    STEP: verifying the pod e2e-test-httpd-pod was created 03/27/23 14:12:52.21
    Mar 27 14:12:52.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-568 get pod e2e-test-httpd-pod -o json'
    Mar 27 14:12:52.279: INFO: stderr: ""
    Mar 27 14:12:52.279: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"ae7f37ef33e2ab0a7efebcbbe1fef674916c739fa04248ef0af06fdceb93ea1d\",\n            \"cni.projectcalico.org/podIP\": \"172.25.2.27/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.25.2.27/32\"\n        },\n        \"creationTimestamp\": \"2023-03-27T14:12:47Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-568\",\n        \"resourceVersion\": \"8066\",\n        \"uid\": \"8d017220-9c53-4233-8238-2613da0560bc\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-5lt4b\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-5lt4b\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-27T14:12:47Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-27T14:12:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-27T14:12:48Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-27T14:12:47Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://387a4e54cc8f2f4900def557d55e08fe0c3db2926e2f28154bb2282e0dec23a1\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-27T14:12:48Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.1.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.2.27\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.25.2.27\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-27T14:12:47Z\"\n    }\n}\n"
    STEP: replace the image in the pod 03/27/23 14:12:52.279
    Mar 27 14:12:52.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-568 replace -f -'
    Mar 27 14:12:53.318: INFO: stderr: ""
    Mar 27 14:12:53.319: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 03/27/23 14:12:53.319
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Mar 27 14:12:53.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-568 delete pods e2e-test-httpd-pod'
    Mar 27 14:12:58.864: INFO: stderr: ""
    Mar 27 14:12:58.864: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 27 14:12:58.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-568" for this suite. 03/27/23 14:12:58.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:12:58.883
Mar 27 14:12:58.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename gc 03/27/23 14:12:58.884
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:12:58.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:12:58.906
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 03/27/23 14:12:58.911
STEP: Wait for the Deployment to create new ReplicaSet 03/27/23 14:12:58.918
STEP: delete the deployment 03/27/23 14:12:59.437
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/27/23 14:12:59.444
STEP: Gathering metrics 03/27/23 14:12:59.97
W0327 14:12:59.982731      24 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 27 14:12:59.982: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 27 14:12:59.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1854" for this suite. 03/27/23 14:12:59.987
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":21,"skipped":387,"failed":0}
------------------------------
• [1.111 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:12:58.883
    Mar 27 14:12:58.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename gc 03/27/23 14:12:58.884
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:12:58.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:12:58.906
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 03/27/23 14:12:58.911
    STEP: Wait for the Deployment to create new ReplicaSet 03/27/23 14:12:58.918
    STEP: delete the deployment 03/27/23 14:12:59.437
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/27/23 14:12:59.444
    STEP: Gathering metrics 03/27/23 14:12:59.97
    W0327 14:12:59.982731      24 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 27 14:12:59.982: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 27 14:12:59.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1854" for this suite. 03/27/23 14:12:59.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:12:59.996
Mar 27 14:12:59.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename secrets 03/27/23 14:12:59.997
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:13:00.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:13:00.021
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-fa3fd57f-782b-40ed-84e6-6efebb0c0358 03/27/23 14:13:00.025
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar 27 14:13:00.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3010" for this suite. 03/27/23 14:13:00.036
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":22,"skipped":465,"failed":0}
------------------------------
• [0.051 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:12:59.996
    Mar 27 14:12:59.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename secrets 03/27/23 14:12:59.997
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:13:00.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:13:00.021
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-fa3fd57f-782b-40ed-84e6-6efebb0c0358 03/27/23 14:13:00.025
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar 27 14:13:00.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3010" for this suite. 03/27/23 14:13:00.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:13:00.047
Mar 27 14:13:00.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubectl 03/27/23 14:13:00.048
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:13:00.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:13:00.071
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Mar 27 14:13:00.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-6876 create -f -'
Mar 27 14:13:00.274: INFO: stderr: ""
Mar 27 14:13:00.274: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar 27 14:13:00.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-6876 create -f -'
Mar 27 14:13:00.462: INFO: stderr: ""
Mar 27 14:13:00.462: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/27/23 14:13:00.462
Mar 27 14:13:01.468: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 14:13:01.469: INFO: Found 0 / 1
Mar 27 14:13:02.468: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 14:13:02.468: INFO: Found 1 / 1
Mar 27 14:13:02.468: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 27 14:13:02.472: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 14:13:02.472: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 27 14:13:02.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-6876 describe pod agnhost-primary-rc8hl'
Mar 27 14:13:02.544: INFO: stderr: ""
Mar 27 14:13:02.544: INFO: stdout: "Name:             agnhost-primary-rc8hl\nNamespace:        kubectl-6876\nPriority:         0\nService Account:  default\nNode:             k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc/192.168.1.4\nStart Time:       Mon, 27 Mar 2023 14:13:00 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: b85977c571176b88c3f5cd1ca92405cf874e6111985bf901ed5ad850e7622b67\n                  cni.projectcalico.org/podIP: 172.25.2.29/32\n                  cni.projectcalico.org/podIPs: 172.25.2.29/32\nStatus:           Running\nIP:               172.25.2.29\nIPs:\n  IP:           172.25.2.29\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://5a7747327667736dd6f3d2fefdc0e5d4ab175c8160b43ad84ecbfbc5c7ca7dce\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 27 Mar 2023 14:13:01 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kwc9g (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-kwc9g:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-6876/agnhost-primary-rc8hl to k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Mar 27 14:13:02.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-6876 describe rc agnhost-primary'
Mar 27 14:13:02.625: INFO: stderr: ""
Mar 27 14:13:02.625: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6876\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-rc8hl\n"
Mar 27 14:13:02.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-6876 describe service agnhost-primary'
Mar 27 14:13:02.692: INFO: stderr: ""
Mar 27 14:13:02.692: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6876\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.240.24.20\nIPs:               10.240.24.20\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.25.2.29:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 27 14:13:02.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-6876 describe node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq'
Mar 27 14:13:02.818: INFO: stderr: ""
Mar 27 14:13:02.818: INFO: stdout: "Name:               k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m1c.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=dbl\n                    failure-domain.beta.kubernetes.io/zone=dbl1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq\n                    kubernetes.io/os=linux\n                    machine-controller/host-id=50e8bca9a16e886123fd56b2e42c585a939b43cbf13cac6f6b22b12e\n                    machine-controller/owned-by=27f725d1-b12e-484a-aa81-719b0df5b3b3\n                    machine-controller/physical-host-id=50e8bca9a16e886123fd56b2e42c585a939b43cbf13cac6f6b22b12e\n                    node.kubernetes.io/instance-type=m1c.medium\n                    system/cluster=n6d2clvr86\n                    system/project=tdf8d8k44n\n                    topology.cinder.csi.openstack.org/zone=dbl1\n                    topology.kubernetes.io/region=dbl\n                    topology.kubernetes.io/zone=dbl1\n                    v1.machine-controller.kubermatic.io/operating-system=ubuntu\n                    x-kubernetes.io/distribution=ubuntu\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 192.168.1.3\n                    cluster.k8s.io/machine: kube-system/k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"ef3bc7ca-b1bd-41f7-bbe6-c39e9acbc1cf\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"1a:79:74:ba:33:62\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.1.3\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.1.3/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.25.1.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 27 Mar 2023 14:03:24 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 27 Mar 2023 14:13:02 +0000\nConditions:\n  Type                Status  LastHeartbeatTime                 LastTransitionTime                Reason           Message\n  ----                ------  -----------------                 ------------------                ------           -------\n  MetakubeNodeReady   True    Mon, 27 Mar 2023 14:10:08 +0000   Mon, 27 Mar 2023 14:05:07 +0000   MetakubeNodeUp   Server:    169.254.20.10\nAddress:              169.254.20.10#53\n\nName:                  kubernetes.default.svc.c\n  KernelDeadlock       False   Mon, 27 Mar 2023 14:10:08 +0000   Mon, 27 Mar 2023 14:05:03 +0000   KernelHasNoDeadlock          kernel has no deadlock\n  ReadonlyFilesystem   False   Mon, 27 Mar 2023 14:10:08 +0000   Mon, 27 Mar 2023 14:05:03 +0000   FilesystemIsNotReadOnly      Filesystem is not read-only\n  NetworkUnavailable   False   Mon, 27 Mar 2023 14:04:47 +0000   Mon, 27 Mar 2023 14:04:47 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Mon, 27 Mar 2023 14:12:46 +0000   Mon, 27 Mar 2023 14:03:24 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 27 Mar 2023 14:12:46 +0000   Mon, 27 Mar 2023 14:03:24 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 27 Mar 2023 14:12:46 +0000   Mon, 27 Mar 2023 14:03:24 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 27 Mar 2023 14:12:46 +0000   Mon, 27 Mar 2023 14:04:26 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.1.3\n  ExternalIP:  195.192.129.205\n  Hostname:    k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq\nCapacity:\n  cpu:                4\n  ephemeral-storage:  50620216Ki\n  hugepages-2Mi:      0\n  memory:             8148212Ki\n  pods:               110\nAllocatable:\n  cpu:                3600m\n  ephemeral-storage:  44504107341\n  hugepages-2Mi:      0\n  memory:             7226612Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 ef3bc7cab1bd41f7bbe6c39e9acbc1cf\n  System UUID:                ef3bc7ca-b1bd-41f7-bbe6-c39e9acbc1cf\n  Boot ID:                    11a5e076-f4ed-4036-b6fb-3e44772036a3\n  Kernel Version:             5.4.0-139-generic\n  OS Image:                   Ubuntu 20.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.5.11\n  Kubelet Version:            v1.25.7\n  Kube-Proxy Version:         v1.25.7\nPodCIDR:                      172.25.1.0/24\nPodCIDRs:                     172.25.1.0/24\nProviderID:                   openstack:///ef3bc7ca-b1bd-41f7-bbe6-c39e9acbc1cf\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  gc-1854                     simpletest.deployment-558d4bc797-h2g57                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  kube-system                 canal-vhmnn                                                250m (6%)     0 (0%)      0 (0%)           0 (0%)         9m38s\n  kube-system                 csi-cinder-nodeplugin-lhmqp                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m38s\n  kube-system                 kube-proxy-4njm5                                           75m (2%)      250m (6%)   50Mi (0%)        250Mi (3%)     9m38s\n  kube-system                 node-exporter-x8df2                                        3m (0%)       300m (8%)   16Mi (0%)        50Mi (0%)      9m38s\n  kube-system                 node-local-dns-zvl8z                                       50m (1%)      0 (0%)      20Mi (0%)        100Mi (1%)     9m38s\n  kube-system                 syseleven-node-problem-detector-2x2wt                      10m (0%)      20m (0%)    80Mi (1%)        80Mi (1%)      9m38s\n  kube-system                 user-ssh-keys-agent-65hx9                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m38s\n  sonobuoy                    sonobuoy-e2e-job-267544c30cdc4a58                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m17s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-bg2pk    0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m17s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                388m (10%)  570m (15%)\n  memory             166Mi (2%)  480Mi (6%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age                    From                     Message\n  ----    ------                   ----                   ----                     -------\n  Normal  Starting                 8m53s                  kube-proxy               \n  Normal  NodeHasSufficientMemory  9m38s (x8 over 9m51s)  kubelet                  Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq status is now: NodeHasSufficientMemory\n  Normal  RegisteredNode           9m36s                  node-controller          Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq event: Registered Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq in Controller\n  Normal  Synced                   9m33s                  cloud-node-controller    Node synced successfully\n  Normal  RegisteredNode           8m24s                  node-controller          Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq event: Registered Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq in Controller\n  Normal  MetakubeNodeUp           7m55s                  metakube-plugin-monitor  Node condition MetakubeNodeReady is now: True, reason: MetakubeNodeUp\n"
Mar 27 14:13:02.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-6876 describe namespace kubectl-6876'
Mar 27 14:13:02.910: INFO: stderr: ""
Mar 27 14:13:02.910: INFO: stdout: "Name:         kubectl-6876\nLabels:       e2e-framework=kubectl\n              e2e-run=623b7062-a94d-4373-aa13-325cae39bd8b\n              kubernetes.io/metadata.name=kubectl-6876\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 27 14:13:02.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6876" for this suite. 03/27/23 14:13:02.914
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":23,"skipped":478,"failed":0}
------------------------------
• [2.874 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:13:00.047
    Mar 27 14:13:00.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubectl 03/27/23 14:13:00.048
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:13:00.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:13:00.071
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Mar 27 14:13:00.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-6876 create -f -'
    Mar 27 14:13:00.274: INFO: stderr: ""
    Mar 27 14:13:00.274: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Mar 27 14:13:00.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-6876 create -f -'
    Mar 27 14:13:00.462: INFO: stderr: ""
    Mar 27 14:13:00.462: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/27/23 14:13:00.462
    Mar 27 14:13:01.468: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 14:13:01.469: INFO: Found 0 / 1
    Mar 27 14:13:02.468: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 14:13:02.468: INFO: Found 1 / 1
    Mar 27 14:13:02.468: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar 27 14:13:02.472: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 14:13:02.472: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 27 14:13:02.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-6876 describe pod agnhost-primary-rc8hl'
    Mar 27 14:13:02.544: INFO: stderr: ""
    Mar 27 14:13:02.544: INFO: stdout: "Name:             agnhost-primary-rc8hl\nNamespace:        kubectl-6876\nPriority:         0\nService Account:  default\nNode:             k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc/192.168.1.4\nStart Time:       Mon, 27 Mar 2023 14:13:00 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: b85977c571176b88c3f5cd1ca92405cf874e6111985bf901ed5ad850e7622b67\n                  cni.projectcalico.org/podIP: 172.25.2.29/32\n                  cni.projectcalico.org/podIPs: 172.25.2.29/32\nStatus:           Running\nIP:               172.25.2.29\nIPs:\n  IP:           172.25.2.29\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://5a7747327667736dd6f3d2fefdc0e5d4ab175c8160b43ad84ecbfbc5c7ca7dce\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 27 Mar 2023 14:13:01 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kwc9g (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-kwc9g:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-6876/agnhost-primary-rc8hl to k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Mar 27 14:13:02.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-6876 describe rc agnhost-primary'
    Mar 27 14:13:02.625: INFO: stderr: ""
    Mar 27 14:13:02.625: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6876\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-rc8hl\n"
    Mar 27 14:13:02.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-6876 describe service agnhost-primary'
    Mar 27 14:13:02.692: INFO: stderr: ""
    Mar 27 14:13:02.692: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6876\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.240.24.20\nIPs:               10.240.24.20\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.25.2.29:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Mar 27 14:13:02.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-6876 describe node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq'
    Mar 27 14:13:02.818: INFO: stderr: ""
    Mar 27 14:13:02.818: INFO: stdout: "Name:               k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m1c.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=dbl\n                    failure-domain.beta.kubernetes.io/zone=dbl1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq\n                    kubernetes.io/os=linux\n                    machine-controller/host-id=50e8bca9a16e886123fd56b2e42c585a939b43cbf13cac6f6b22b12e\n                    machine-controller/owned-by=27f725d1-b12e-484a-aa81-719b0df5b3b3\n                    machine-controller/physical-host-id=50e8bca9a16e886123fd56b2e42c585a939b43cbf13cac6f6b22b12e\n                    node.kubernetes.io/instance-type=m1c.medium\n                    system/cluster=n6d2clvr86\n                    system/project=tdf8d8k44n\n                    topology.cinder.csi.openstack.org/zone=dbl1\n                    topology.kubernetes.io/region=dbl\n                    topology.kubernetes.io/zone=dbl1\n                    v1.machine-controller.kubermatic.io/operating-system=ubuntu\n                    x-kubernetes.io/distribution=ubuntu\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 192.168.1.3\n                    cluster.k8s.io/machine: kube-system/k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"ef3bc7ca-b1bd-41f7-bbe6-c39e9acbc1cf\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"1a:79:74:ba:33:62\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.1.3\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.1.3/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.25.1.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 27 Mar 2023 14:03:24 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 27 Mar 2023 14:13:02 +0000\nConditions:\n  Type                Status  LastHeartbeatTime                 LastTransitionTime                Reason           Message\n  ----                ------  -----------------                 ------------------                ------           -------\n  MetakubeNodeReady   True    Mon, 27 Mar 2023 14:10:08 +0000   Mon, 27 Mar 2023 14:05:07 +0000   MetakubeNodeUp   Server:    169.254.20.10\nAddress:              169.254.20.10#53\n\nName:                  kubernetes.default.svc.c\n  KernelDeadlock       False   Mon, 27 Mar 2023 14:10:08 +0000   Mon, 27 Mar 2023 14:05:03 +0000   KernelHasNoDeadlock          kernel has no deadlock\n  ReadonlyFilesystem   False   Mon, 27 Mar 2023 14:10:08 +0000   Mon, 27 Mar 2023 14:05:03 +0000   FilesystemIsNotReadOnly      Filesystem is not read-only\n  NetworkUnavailable   False   Mon, 27 Mar 2023 14:04:47 +0000   Mon, 27 Mar 2023 14:04:47 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Mon, 27 Mar 2023 14:12:46 +0000   Mon, 27 Mar 2023 14:03:24 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 27 Mar 2023 14:12:46 +0000   Mon, 27 Mar 2023 14:03:24 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 27 Mar 2023 14:12:46 +0000   Mon, 27 Mar 2023 14:03:24 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 27 Mar 2023 14:12:46 +0000   Mon, 27 Mar 2023 14:04:26 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.1.3\n  ExternalIP:  195.192.129.205\n  Hostname:    k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq\nCapacity:\n  cpu:                4\n  ephemeral-storage:  50620216Ki\n  hugepages-2Mi:      0\n  memory:             8148212Ki\n  pods:               110\nAllocatable:\n  cpu:                3600m\n  ephemeral-storage:  44504107341\n  hugepages-2Mi:      0\n  memory:             7226612Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 ef3bc7cab1bd41f7bbe6c39e9acbc1cf\n  System UUID:                ef3bc7ca-b1bd-41f7-bbe6-c39e9acbc1cf\n  Boot ID:                    11a5e076-f4ed-4036-b6fb-3e44772036a3\n  Kernel Version:             5.4.0-139-generic\n  OS Image:                   Ubuntu 20.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.5.11\n  Kubelet Version:            v1.25.7\n  Kube-Proxy Version:         v1.25.7\nPodCIDR:                      172.25.1.0/24\nPodCIDRs:                     172.25.1.0/24\nProviderID:                   openstack:///ef3bc7ca-b1bd-41f7-bbe6-c39e9acbc1cf\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  gc-1854                     simpletest.deployment-558d4bc797-h2g57                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  kube-system                 canal-vhmnn                                                250m (6%)     0 (0%)      0 (0%)           0 (0%)         9m38s\n  kube-system                 csi-cinder-nodeplugin-lhmqp                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m38s\n  kube-system                 kube-proxy-4njm5                                           75m (2%)      250m (6%)   50Mi (0%)        250Mi (3%)     9m38s\n  kube-system                 node-exporter-x8df2                                        3m (0%)       300m (8%)   16Mi (0%)        50Mi (0%)      9m38s\n  kube-system                 node-local-dns-zvl8z                                       50m (1%)      0 (0%)      20Mi (0%)        100Mi (1%)     9m38s\n  kube-system                 syseleven-node-problem-detector-2x2wt                      10m (0%)      20m (0%)    80Mi (1%)        80Mi (1%)      9m38s\n  kube-system                 user-ssh-keys-agent-65hx9                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m38s\n  sonobuoy                    sonobuoy-e2e-job-267544c30cdc4a58                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m17s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-bg2pk    0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m17s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                388m (10%)  570m (15%)\n  memory             166Mi (2%)  480Mi (6%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age                    From                     Message\n  ----    ------                   ----                   ----                     -------\n  Normal  Starting                 8m53s                  kube-proxy               \n  Normal  NodeHasSufficientMemory  9m38s (x8 over 9m51s)  kubelet                  Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq status is now: NodeHasSufficientMemory\n  Normal  RegisteredNode           9m36s                  node-controller          Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq event: Registered Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq in Controller\n  Normal  Synced                   9m33s                  cloud-node-controller    Node synced successfully\n  Normal  RegisteredNode           8m24s                  node-controller          Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq event: Registered Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq in Controller\n  Normal  MetakubeNodeUp           7m55s                  metakube-plugin-monitor  Node condition MetakubeNodeReady is now: True, reason: MetakubeNodeUp\n"
    Mar 27 14:13:02.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-6876 describe namespace kubectl-6876'
    Mar 27 14:13:02.910: INFO: stderr: ""
    Mar 27 14:13:02.910: INFO: stdout: "Name:         kubectl-6876\nLabels:       e2e-framework=kubectl\n              e2e-run=623b7062-a94d-4373-aa13-325cae39bd8b\n              kubernetes.io/metadata.name=kubectl-6876\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 27 14:13:02.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6876" for this suite. 03/27/23 14:13:02.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:13:02.922
Mar 27 14:13:02.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename watch 03/27/23 14:13:02.923
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:13:02.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:13:02.944
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 03/27/23 14:13:02.948
STEP: starting a background goroutine to produce watch events 03/27/23 14:13:02.953
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/27/23 14:13:02.953
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar 27 14:13:05.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9442" for this suite. 03/27/23 14:13:05.784
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":24,"skipped":494,"failed":0}
------------------------------
• [2.909 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:13:02.922
    Mar 27 14:13:02.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename watch 03/27/23 14:13:02.923
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:13:02.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:13:02.944
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 03/27/23 14:13:02.948
    STEP: starting a background goroutine to produce watch events 03/27/23 14:13:02.953
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/27/23 14:13:02.953
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar 27 14:13:05.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9442" for this suite. 03/27/23 14:13:05.784
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:13:05.833
Mar 27 14:13:05.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename sched-preemption 03/27/23 14:13:05.834
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:13:05.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:13:05.858
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar 27 14:13:05.881: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 27 14:14:05.922: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:14:05.931
Mar 27 14:14:05.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename sched-preemption-path 03/27/23 14:14:05.932
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:14:05.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:14:05.95
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Mar 27 14:14:05.981: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Mar 27 14:14:05.985: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Mar 27 14:14:06.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2608" for this suite. 03/27/23 14:14:06.034
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar 27 14:14:06.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6605" for this suite. 03/27/23 14:14:06.088
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":25,"skipped":552,"failed":0}
------------------------------
• [SLOW TEST] [60.327 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:13:05.833
    Mar 27 14:13:05.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename sched-preemption 03/27/23 14:13:05.834
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:13:05.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:13:05.858
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar 27 14:13:05.881: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 27 14:14:05.922: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:14:05.931
    Mar 27 14:14:05.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename sched-preemption-path 03/27/23 14:14:05.932
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:14:05.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:14:05.95
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Mar 27 14:14:05.981: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Mar 27 14:14:05.985: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Mar 27 14:14:06.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-2608" for this suite. 03/27/23 14:14:06.034
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 14:14:06.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-6605" for this suite. 03/27/23 14:14:06.088
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:14:06.162
Mar 27 14:14:06.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 14:14:06.163
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:14:06.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:14:06.184
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Mar 27 14:14:06.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:14:07.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8307" for this suite. 03/27/23 14:14:07.223
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":26,"skipped":601,"failed":0}
------------------------------
• [1.076 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:14:06.162
    Mar 27 14:14:06.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 14:14:06.163
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:14:06.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:14:06.184
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Mar 27 14:14:06.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:14:07.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-8307" for this suite. 03/27/23 14:14:07.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:14:07.239
Mar 27 14:14:07.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename replication-controller 03/27/23 14:14:07.24
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:14:07.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:14:07.267
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 03/27/23 14:14:07.273
Mar 27 14:14:07.291: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6504" to be "running and ready"
Mar 27 14:14:07.300: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 9.03766ms
Mar 27 14:14:07.300: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:14:09.304: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.013151554s
Mar 27 14:14:09.304: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Mar 27 14:14:09.304: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 03/27/23 14:14:09.308
STEP: Then the orphan pod is adopted 03/27/23 14:14:09.314
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar 27 14:14:10.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6504" for this suite. 03/27/23 14:14:10.329
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":27,"skipped":621,"failed":0}
------------------------------
• [3.097 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:14:07.239
    Mar 27 14:14:07.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename replication-controller 03/27/23 14:14:07.24
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:14:07.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:14:07.267
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 03/27/23 14:14:07.273
    Mar 27 14:14:07.291: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6504" to be "running and ready"
    Mar 27 14:14:07.300: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 9.03766ms
    Mar 27 14:14:07.300: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:14:09.304: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.013151554s
    Mar 27 14:14:09.304: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Mar 27 14:14:09.304: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 03/27/23 14:14:09.308
    STEP: Then the orphan pod is adopted 03/27/23 14:14:09.314
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar 27 14:14:10.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6504" for this suite. 03/27/23 14:14:10.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:14:10.337
Mar 27 14:14:10.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename taint-multiple-pods 03/27/23 14:14:10.338
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:14:10.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:14:10.364
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Mar 27 14:14:10.368: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 27 14:15:10.400: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Mar 27 14:15:10.405: INFO: Starting informer...
STEP: Starting pods... 03/27/23 14:15:10.405
Mar 27 14:15:10.631: INFO: Pod1 is running on k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc. Tainting Node
Mar 27 14:15:10.845: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5756" to be "running"
Mar 27 14:15:10.848: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.899983ms
Mar 27 14:15:12.853: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008165045s
Mar 27 14:15:14.853: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.008282876s
Mar 27 14:15:14.853: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Mar 27 14:15:14.853: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5756" to be "running"
Mar 27 14:15:14.857: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.078665ms
Mar 27 14:15:14.858: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Mar 27 14:15:14.858: INFO: Pod2 is running on k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc. Tainting Node
STEP: Trying to apply a taint on the Node 03/27/23 14:15:14.858
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/27/23 14:15:14.887
STEP: Waiting for Pod1 and Pod2 to be deleted 03/27/23 14:15:14.892
Mar 27 14:15:21.220: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar 27 14:15:41.268: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/27/23 14:15:41.287
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Mar 27 14:15:41.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5756" for this suite. 03/27/23 14:15:41.297
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":28,"skipped":628,"failed":0}
------------------------------
• [SLOW TEST] [90.969 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:14:10.337
    Mar 27 14:14:10.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename taint-multiple-pods 03/27/23 14:14:10.338
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:14:10.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:14:10.364
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Mar 27 14:14:10.368: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 27 14:15:10.400: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Mar 27 14:15:10.405: INFO: Starting informer...
    STEP: Starting pods... 03/27/23 14:15:10.405
    Mar 27 14:15:10.631: INFO: Pod1 is running on k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc. Tainting Node
    Mar 27 14:15:10.845: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5756" to be "running"
    Mar 27 14:15:10.848: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.899983ms
    Mar 27 14:15:12.853: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008165045s
    Mar 27 14:15:14.853: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.008282876s
    Mar 27 14:15:14.853: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Mar 27 14:15:14.853: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5756" to be "running"
    Mar 27 14:15:14.857: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.078665ms
    Mar 27 14:15:14.858: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Mar 27 14:15:14.858: INFO: Pod2 is running on k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc. Tainting Node
    STEP: Trying to apply a taint on the Node 03/27/23 14:15:14.858
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/27/23 14:15:14.887
    STEP: Waiting for Pod1 and Pod2 to be deleted 03/27/23 14:15:14.892
    Mar 27 14:15:21.220: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Mar 27 14:15:41.268: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/27/23 14:15:41.287
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 14:15:41.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-5756" for this suite. 03/27/23 14:15:41.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:15:41.309
Mar 27 14:15:41.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename emptydir 03/27/23 14:15:41.31
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:15:41.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:15:41.332
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 03/27/23 14:15:41.335
Mar 27 14:15:41.345: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-8876b49a-7b1b-4eb6-a41e-d504b2ba8ac8" in namespace "emptydir-4318" to be "running"
Mar 27 14:15:41.353: INFO: Pod "pod-sharedvolume-8876b49a-7b1b-4eb6-a41e-d504b2ba8ac8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.469502ms
Mar 27 14:15:43.357: INFO: Pod "pod-sharedvolume-8876b49a-7b1b-4eb6-a41e-d504b2ba8ac8": Phase="Running", Reason="", readiness=false. Elapsed: 2.012295411s
Mar 27 14:15:43.357: INFO: Pod "pod-sharedvolume-8876b49a-7b1b-4eb6-a41e-d504b2ba8ac8" satisfied condition "running"
STEP: Reading file content from the nginx-container 03/27/23 14:15:43.357
Mar 27 14:15:43.358: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4318 PodName:pod-sharedvolume-8876b49a-7b1b-4eb6-a41e-d504b2ba8ac8 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:15:43.358: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:15:43.358: INFO: ExecWithOptions: Clientset creation
Mar 27 14:15:43.358: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/emptydir-4318/pods/pod-sharedvolume-8876b49a-7b1b-4eb6-a41e-d504b2ba8ac8/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Mar 27 14:15:43.465: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 27 14:15:43.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4318" for this suite. 03/27/23 14:15:43.47
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":29,"skipped":685,"failed":0}
------------------------------
• [2.172 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:15:41.309
    Mar 27 14:15:41.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename emptydir 03/27/23 14:15:41.31
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:15:41.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:15:41.332
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 03/27/23 14:15:41.335
    Mar 27 14:15:41.345: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-8876b49a-7b1b-4eb6-a41e-d504b2ba8ac8" in namespace "emptydir-4318" to be "running"
    Mar 27 14:15:41.353: INFO: Pod "pod-sharedvolume-8876b49a-7b1b-4eb6-a41e-d504b2ba8ac8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.469502ms
    Mar 27 14:15:43.357: INFO: Pod "pod-sharedvolume-8876b49a-7b1b-4eb6-a41e-d504b2ba8ac8": Phase="Running", Reason="", readiness=false. Elapsed: 2.012295411s
    Mar 27 14:15:43.357: INFO: Pod "pod-sharedvolume-8876b49a-7b1b-4eb6-a41e-d504b2ba8ac8" satisfied condition "running"
    STEP: Reading file content from the nginx-container 03/27/23 14:15:43.357
    Mar 27 14:15:43.358: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4318 PodName:pod-sharedvolume-8876b49a-7b1b-4eb6-a41e-d504b2ba8ac8 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:15:43.358: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:15:43.358: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:15:43.358: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/emptydir-4318/pods/pod-sharedvolume-8876b49a-7b1b-4eb6-a41e-d504b2ba8ac8/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Mar 27 14:15:43.465: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 27 14:15:43.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4318" for this suite. 03/27/23 14:15:43.47
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:15:43.48
Mar 27 14:15:43.481: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 14:15:43.481
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:15:43.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:15:43.5
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Mar 27 14:15:43.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/27/23 14:15:51.222
Mar 27 14:15:51.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-3809 --namespace=crd-publish-openapi-3809 create -f -'
Mar 27 14:15:52.258: INFO: stderr: ""
Mar 27 14:15:52.258: INFO: stdout: "e2e-test-crd-publish-openapi-2284-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 27 14:15:52.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-3809 --namespace=crd-publish-openapi-3809 delete e2e-test-crd-publish-openapi-2284-crds test-cr'
Mar 27 14:15:52.326: INFO: stderr: ""
Mar 27 14:15:52.326: INFO: stdout: "e2e-test-crd-publish-openapi-2284-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 27 14:15:52.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-3809 --namespace=crd-publish-openapi-3809 apply -f -'
Mar 27 14:15:53.093: INFO: stderr: ""
Mar 27 14:15:53.093: INFO: stdout: "e2e-test-crd-publish-openapi-2284-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 27 14:15:53.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-3809 --namespace=crd-publish-openapi-3809 delete e2e-test-crd-publish-openapi-2284-crds test-cr'
Mar 27 14:15:53.161: INFO: stderr: ""
Mar 27 14:15:53.161: INFO: stdout: "e2e-test-crd-publish-openapi-2284-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/27/23 14:15:53.161
Mar 27 14:15:53.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-3809 explain e2e-test-crd-publish-openapi-2284-crds'
Mar 27 14:15:53.333: INFO: stderr: ""
Mar 27 14:15:53.333: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2284-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:16:02.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3809" for this suite. 03/27/23 14:16:02.561
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":30,"skipped":687,"failed":0}
------------------------------
• [SLOW TEST] [19.089 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:15:43.48
    Mar 27 14:15:43.481: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 14:15:43.481
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:15:43.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:15:43.5
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Mar 27 14:15:43.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/27/23 14:15:51.222
    Mar 27 14:15:51.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-3809 --namespace=crd-publish-openapi-3809 create -f -'
    Mar 27 14:15:52.258: INFO: stderr: ""
    Mar 27 14:15:52.258: INFO: stdout: "e2e-test-crd-publish-openapi-2284-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar 27 14:15:52.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-3809 --namespace=crd-publish-openapi-3809 delete e2e-test-crd-publish-openapi-2284-crds test-cr'
    Mar 27 14:15:52.326: INFO: stderr: ""
    Mar 27 14:15:52.326: INFO: stdout: "e2e-test-crd-publish-openapi-2284-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Mar 27 14:15:52.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-3809 --namespace=crd-publish-openapi-3809 apply -f -'
    Mar 27 14:15:53.093: INFO: stderr: ""
    Mar 27 14:15:53.093: INFO: stdout: "e2e-test-crd-publish-openapi-2284-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar 27 14:15:53.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-3809 --namespace=crd-publish-openapi-3809 delete e2e-test-crd-publish-openapi-2284-crds test-cr'
    Mar 27 14:15:53.161: INFO: stderr: ""
    Mar 27 14:15:53.161: INFO: stdout: "e2e-test-crd-publish-openapi-2284-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/27/23 14:15:53.161
    Mar 27 14:15:53.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-3809 explain e2e-test-crd-publish-openapi-2284-crds'
    Mar 27 14:15:53.333: INFO: stderr: ""
    Mar 27 14:15:53.333: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2284-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:16:02.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3809" for this suite. 03/27/23 14:16:02.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:16:02.57
Mar 27 14:16:02.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename pods 03/27/23 14:16:02.571
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:16:02.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:16:02.594
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 03/27/23 14:16:02.597
Mar 27 14:16:02.609: INFO: created test-pod-1
Mar 27 14:16:02.616: INFO: created test-pod-2
Mar 27 14:16:02.628: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 03/27/23 14:16:02.628
Mar 27 14:16:02.628: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2191' to be running and ready
Mar 27 14:16:02.653: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 27 14:16:02.653: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 27 14:16:02.653: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 27 14:16:02.653: INFO: 0 / 3 pods in namespace 'pods-2191' are running and ready (0 seconds elapsed)
Mar 27 14:16:02.653: INFO: expected 0 pod replicas in namespace 'pods-2191', 0 are Running and Ready.
Mar 27 14:16:02.653: INFO: POD         NODE                                                         PHASE    GRACE  CONDITIONS
Mar 27 14:16:02.653: INFO: test-pod-1  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:16:02 +0000 UTC  }]
Mar 27 14:16:02.653: INFO: test-pod-2  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:16:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:16:02 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:16:02 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:16:02 +0000 UTC  }]
Mar 27 14:16:02.653: INFO: test-pod-3  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:16:02 +0000 UTC  }]
Mar 27 14:16:02.653: INFO: 
Mar 27 14:16:04.668: INFO: 3 / 3 pods in namespace 'pods-2191' are running and ready (2 seconds elapsed)
Mar 27 14:16:04.668: INFO: expected 0 pod replicas in namespace 'pods-2191', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 03/27/23 14:16:04.692
Mar 27 14:16:04.697: INFO: Pod quantity 3 is different from expected quantity 0
Mar 27 14:16:05.704: INFO: Pod quantity 3 is different from expected quantity 0
Mar 27 14:16:06.703: INFO: Pod quantity 3 is different from expected quantity 0
Mar 27 14:16:07.705: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 27 14:16:08.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2191" for this suite. 03/27/23 14:16:08.709
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":31,"skipped":692,"failed":0}
------------------------------
• [SLOW TEST] [6.148 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:16:02.57
    Mar 27 14:16:02.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename pods 03/27/23 14:16:02.571
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:16:02.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:16:02.594
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 03/27/23 14:16:02.597
    Mar 27 14:16:02.609: INFO: created test-pod-1
    Mar 27 14:16:02.616: INFO: created test-pod-2
    Mar 27 14:16:02.628: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 03/27/23 14:16:02.628
    Mar 27 14:16:02.628: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2191' to be running and ready
    Mar 27 14:16:02.653: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 27 14:16:02.653: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 27 14:16:02.653: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 27 14:16:02.653: INFO: 0 / 3 pods in namespace 'pods-2191' are running and ready (0 seconds elapsed)
    Mar 27 14:16:02.653: INFO: expected 0 pod replicas in namespace 'pods-2191', 0 are Running and Ready.
    Mar 27 14:16:02.653: INFO: POD         NODE                                                         PHASE    GRACE  CONDITIONS
    Mar 27 14:16:02.653: INFO: test-pod-1  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:16:02 +0000 UTC  }]
    Mar 27 14:16:02.653: INFO: test-pod-2  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:16:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:16:02 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:16:02 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:16:02 +0000 UTC  }]
    Mar 27 14:16:02.653: INFO: test-pod-3  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:16:02 +0000 UTC  }]
    Mar 27 14:16:02.653: INFO: 
    Mar 27 14:16:04.668: INFO: 3 / 3 pods in namespace 'pods-2191' are running and ready (2 seconds elapsed)
    Mar 27 14:16:04.668: INFO: expected 0 pod replicas in namespace 'pods-2191', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 03/27/23 14:16:04.692
    Mar 27 14:16:04.697: INFO: Pod quantity 3 is different from expected quantity 0
    Mar 27 14:16:05.704: INFO: Pod quantity 3 is different from expected quantity 0
    Mar 27 14:16:06.703: INFO: Pod quantity 3 is different from expected quantity 0
    Mar 27 14:16:07.705: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 27 14:16:08.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2191" for this suite. 03/27/23 14:16:08.709
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:16:08.719
Mar 27 14:16:08.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename gc 03/27/23 14:16:08.72
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:16:08.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:16:08.741
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 03/27/23 14:16:08.75
STEP: delete the rc 03/27/23 14:16:13.767
STEP: wait for the rc to be deleted 03/27/23 14:16:13.778
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/27/23 14:16:18.781
STEP: Gathering metrics 03/27/23 14:16:48.799
W0327 14:16:48.812450      24 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 27 14:16:48.812: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 27 14:16:48.812: INFO: Deleting pod "simpletest.rc-26pr4" in namespace "gc-7939"
Mar 27 14:16:48.828: INFO: Deleting pod "simpletest.rc-29tvb" in namespace "gc-7939"
Mar 27 14:16:48.848: INFO: Deleting pod "simpletest.rc-2wwwm" in namespace "gc-7939"
Mar 27 14:16:48.864: INFO: Deleting pod "simpletest.rc-42k9z" in namespace "gc-7939"
Mar 27 14:16:48.883: INFO: Deleting pod "simpletest.rc-485pn" in namespace "gc-7939"
Mar 27 14:16:48.904: INFO: Deleting pod "simpletest.rc-4cm7f" in namespace "gc-7939"
Mar 27 14:16:48.920: INFO: Deleting pod "simpletest.rc-4km7z" in namespace "gc-7939"
Mar 27 14:16:48.936: INFO: Deleting pod "simpletest.rc-4lxpn" in namespace "gc-7939"
Mar 27 14:16:48.952: INFO: Deleting pod "simpletest.rc-4r7h7" in namespace "gc-7939"
Mar 27 14:16:48.966: INFO: Deleting pod "simpletest.rc-4rtkp" in namespace "gc-7939"
Mar 27 14:16:48.985: INFO: Deleting pod "simpletest.rc-52k5f" in namespace "gc-7939"
Mar 27 14:16:49.003: INFO: Deleting pod "simpletest.rc-5686p" in namespace "gc-7939"
Mar 27 14:16:49.021: INFO: Deleting pod "simpletest.rc-577rs" in namespace "gc-7939"
Mar 27 14:16:49.037: INFO: Deleting pod "simpletest.rc-5gldg" in namespace "gc-7939"
Mar 27 14:16:49.051: INFO: Deleting pod "simpletest.rc-5ktwl" in namespace "gc-7939"
Mar 27 14:16:49.066: INFO: Deleting pod "simpletest.rc-5lg9x" in namespace "gc-7939"
Mar 27 14:16:49.084: INFO: Deleting pod "simpletest.rc-5pzrn" in namespace "gc-7939"
Mar 27 14:16:49.095: INFO: Deleting pod "simpletest.rc-65gxp" in namespace "gc-7939"
Mar 27 14:16:49.108: INFO: Deleting pod "simpletest.rc-6wzgn" in namespace "gc-7939"
Mar 27 14:16:49.121: INFO: Deleting pod "simpletest.rc-6zmzf" in namespace "gc-7939"
Mar 27 14:16:49.135: INFO: Deleting pod "simpletest.rc-778tk" in namespace "gc-7939"
Mar 27 14:16:49.153: INFO: Deleting pod "simpletest.rc-7c8kt" in namespace "gc-7939"
Mar 27 14:16:49.171: INFO: Deleting pod "simpletest.rc-7hhd7" in namespace "gc-7939"
Mar 27 14:16:49.190: INFO: Deleting pod "simpletest.rc-7nt2p" in namespace "gc-7939"
Mar 27 14:16:49.201: INFO: Deleting pod "simpletest.rc-7tt2l" in namespace "gc-7939"
Mar 27 14:16:49.216: INFO: Deleting pod "simpletest.rc-86tpq" in namespace "gc-7939"
Mar 27 14:16:49.230: INFO: Deleting pod "simpletest.rc-8kzdj" in namespace "gc-7939"
Mar 27 14:16:49.249: INFO: Deleting pod "simpletest.rc-8n9fv" in namespace "gc-7939"
Mar 27 14:16:49.263: INFO: Deleting pod "simpletest.rc-8tc88" in namespace "gc-7939"
Mar 27 14:16:49.275: INFO: Deleting pod "simpletest.rc-8trpn" in namespace "gc-7939"
Mar 27 14:16:49.291: INFO: Deleting pod "simpletest.rc-8wnht" in namespace "gc-7939"
Mar 27 14:16:49.307: INFO: Deleting pod "simpletest.rc-9759d" in namespace "gc-7939"
Mar 27 14:16:49.325: INFO: Deleting pod "simpletest.rc-97fgm" in namespace "gc-7939"
Mar 27 14:16:49.350: INFO: Deleting pod "simpletest.rc-9gchl" in namespace "gc-7939"
Mar 27 14:16:49.371: INFO: Deleting pod "simpletest.rc-9wmgf" in namespace "gc-7939"
Mar 27 14:16:49.396: INFO: Deleting pod "simpletest.rc-b5hr5" in namespace "gc-7939"
Mar 27 14:16:49.416: INFO: Deleting pod "simpletest.rc-b6rmt" in namespace "gc-7939"
Mar 27 14:16:49.433: INFO: Deleting pod "simpletest.rc-bpg5j" in namespace "gc-7939"
Mar 27 14:16:49.449: INFO: Deleting pod "simpletest.rc-bv8hw" in namespace "gc-7939"
Mar 27 14:16:49.464: INFO: Deleting pod "simpletest.rc-bzg7g" in namespace "gc-7939"
Mar 27 14:16:49.474: INFO: Deleting pod "simpletest.rc-c6jgw" in namespace "gc-7939"
Mar 27 14:16:49.486: INFO: Deleting pod "simpletest.rc-c7r6f" in namespace "gc-7939"
Mar 27 14:16:49.503: INFO: Deleting pod "simpletest.rc-d4bcx" in namespace "gc-7939"
Mar 27 14:16:49.518: INFO: Deleting pod "simpletest.rc-dfrt5" in namespace "gc-7939"
Mar 27 14:16:49.545: INFO: Deleting pod "simpletest.rc-dqd5t" in namespace "gc-7939"
Mar 27 14:16:49.568: INFO: Deleting pod "simpletest.rc-flkw4" in namespace "gc-7939"
Mar 27 14:16:49.593: INFO: Deleting pod "simpletest.rc-fv4gs" in namespace "gc-7939"
Mar 27 14:16:49.612: INFO: Deleting pod "simpletest.rc-fxj4x" in namespace "gc-7939"
Mar 27 14:16:49.624: INFO: Deleting pod "simpletest.rc-gdbhl" in namespace "gc-7939"
Mar 27 14:16:49.640: INFO: Deleting pod "simpletest.rc-gdfvw" in namespace "gc-7939"
Mar 27 14:16:49.658: INFO: Deleting pod "simpletest.rc-gjmml" in namespace "gc-7939"
Mar 27 14:16:49.669: INFO: Deleting pod "simpletest.rc-gmgsd" in namespace "gc-7939"
Mar 27 14:16:49.685: INFO: Deleting pod "simpletest.rc-gv4qg" in namespace "gc-7939"
Mar 27 14:16:49.700: INFO: Deleting pod "simpletest.rc-h7gwr" in namespace "gc-7939"
Mar 27 14:16:49.720: INFO: Deleting pod "simpletest.rc-hhpn4" in namespace "gc-7939"
Mar 27 14:16:49.734: INFO: Deleting pod "simpletest.rc-hpfbj" in namespace "gc-7939"
Mar 27 14:16:49.756: INFO: Deleting pod "simpletest.rc-hzwx2" in namespace "gc-7939"
Mar 27 14:16:49.772: INFO: Deleting pod "simpletest.rc-jtpvt" in namespace "gc-7939"
Mar 27 14:16:49.794: INFO: Deleting pod "simpletest.rc-jvm6h" in namespace "gc-7939"
Mar 27 14:16:49.811: INFO: Deleting pod "simpletest.rc-kdgmm" in namespace "gc-7939"
Mar 27 14:16:49.827: INFO: Deleting pod "simpletest.rc-kxljf" in namespace "gc-7939"
Mar 27 14:16:49.850: INFO: Deleting pod "simpletest.rc-lspsz" in namespace "gc-7939"
Mar 27 14:16:49.873: INFO: Deleting pod "simpletest.rc-m7vvf" in namespace "gc-7939"
Mar 27 14:16:49.890: INFO: Deleting pod "simpletest.rc-mtsz7" in namespace "gc-7939"
Mar 27 14:16:49.909: INFO: Deleting pod "simpletest.rc-n58p5" in namespace "gc-7939"
Mar 27 14:16:49.937: INFO: Deleting pod "simpletest.rc-n58zx" in namespace "gc-7939"
Mar 27 14:16:49.955: INFO: Deleting pod "simpletest.rc-ngh86" in namespace "gc-7939"
Mar 27 14:16:49.973: INFO: Deleting pod "simpletest.rc-njchg" in namespace "gc-7939"
Mar 27 14:16:49.990: INFO: Deleting pod "simpletest.rc-nl2zp" in namespace "gc-7939"
Mar 27 14:16:50.003: INFO: Deleting pod "simpletest.rc-nl7jg" in namespace "gc-7939"
Mar 27 14:16:50.018: INFO: Deleting pod "simpletest.rc-ns8x4" in namespace "gc-7939"
Mar 27 14:16:50.035: INFO: Deleting pod "simpletest.rc-nvmzw" in namespace "gc-7939"
Mar 27 14:16:50.049: INFO: Deleting pod "simpletest.rc-nz9jc" in namespace "gc-7939"
Mar 27 14:16:50.067: INFO: Deleting pod "simpletest.rc-pbt6m" in namespace "gc-7939"
Mar 27 14:16:50.103: INFO: Deleting pod "simpletest.rc-pdc92" in namespace "gc-7939"
Mar 27 14:16:50.161: INFO: Deleting pod "simpletest.rc-pqs7w" in namespace "gc-7939"
Mar 27 14:16:50.198: INFO: Deleting pod "simpletest.rc-q62rt" in namespace "gc-7939"
Mar 27 14:16:50.257: INFO: Deleting pod "simpletest.rc-qj447" in namespace "gc-7939"
Mar 27 14:16:50.311: INFO: Deleting pod "simpletest.rc-qpmnw" in namespace "gc-7939"
Mar 27 14:16:50.364: INFO: Deleting pod "simpletest.rc-rbp8g" in namespace "gc-7939"
Mar 27 14:16:50.405: INFO: Deleting pod "simpletest.rc-rrrc7" in namespace "gc-7939"
Mar 27 14:16:50.460: INFO: Deleting pod "simpletest.rc-rwn7b" in namespace "gc-7939"
Mar 27 14:16:50.501: INFO: Deleting pod "simpletest.rc-s7f4m" in namespace "gc-7939"
Mar 27 14:16:50.566: INFO: Deleting pod "simpletest.rc-sd6tx" in namespace "gc-7939"
Mar 27 14:16:50.603: INFO: Deleting pod "simpletest.rc-shxw7" in namespace "gc-7939"
Mar 27 14:16:50.656: INFO: Deleting pod "simpletest.rc-spxbj" in namespace "gc-7939"
Mar 27 14:16:50.706: INFO: Deleting pod "simpletest.rc-stvg9" in namespace "gc-7939"
Mar 27 14:16:50.756: INFO: Deleting pod "simpletest.rc-sz2t5" in namespace "gc-7939"
Mar 27 14:16:50.805: INFO: Deleting pod "simpletest.rc-tfscx" in namespace "gc-7939"
Mar 27 14:16:50.857: INFO: Deleting pod "simpletest.rc-trx88" in namespace "gc-7939"
Mar 27 14:16:50.898: INFO: Deleting pod "simpletest.rc-twdql" in namespace "gc-7939"
Mar 27 14:16:50.953: INFO: Deleting pod "simpletest.rc-tzxc2" in namespace "gc-7939"
Mar 27 14:16:51.003: INFO: Deleting pod "simpletest.rc-vhzrn" in namespace "gc-7939"
Mar 27 14:16:51.051: INFO: Deleting pod "simpletest.rc-wn828" in namespace "gc-7939"
Mar 27 14:16:51.102: INFO: Deleting pod "simpletest.rc-xfc2c" in namespace "gc-7939"
Mar 27 14:16:51.151: INFO: Deleting pod "simpletest.rc-xnbtp" in namespace "gc-7939"
Mar 27 14:16:51.200: INFO: Deleting pod "simpletest.rc-xxthm" in namespace "gc-7939"
Mar 27 14:16:51.250: INFO: Deleting pod "simpletest.rc-zdh9p" in namespace "gc-7939"
Mar 27 14:16:51.301: INFO: Deleting pod "simpletest.rc-znn7h" in namespace "gc-7939"
Mar 27 14:16:51.357: INFO: Deleting pod "simpletest.rc-zt7x5" in namespace "gc-7939"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 27 14:16:51.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7939" for this suite. 03/27/23 14:16:51.441
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":32,"skipped":695,"failed":0}
------------------------------
• [SLOW TEST] [42.775 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:16:08.719
    Mar 27 14:16:08.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename gc 03/27/23 14:16:08.72
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:16:08.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:16:08.741
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 03/27/23 14:16:08.75
    STEP: delete the rc 03/27/23 14:16:13.767
    STEP: wait for the rc to be deleted 03/27/23 14:16:13.778
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/27/23 14:16:18.781
    STEP: Gathering metrics 03/27/23 14:16:48.799
    W0327 14:16:48.812450      24 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 27 14:16:48.812: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar 27 14:16:48.812: INFO: Deleting pod "simpletest.rc-26pr4" in namespace "gc-7939"
    Mar 27 14:16:48.828: INFO: Deleting pod "simpletest.rc-29tvb" in namespace "gc-7939"
    Mar 27 14:16:48.848: INFO: Deleting pod "simpletest.rc-2wwwm" in namespace "gc-7939"
    Mar 27 14:16:48.864: INFO: Deleting pod "simpletest.rc-42k9z" in namespace "gc-7939"
    Mar 27 14:16:48.883: INFO: Deleting pod "simpletest.rc-485pn" in namespace "gc-7939"
    Mar 27 14:16:48.904: INFO: Deleting pod "simpletest.rc-4cm7f" in namespace "gc-7939"
    Mar 27 14:16:48.920: INFO: Deleting pod "simpletest.rc-4km7z" in namespace "gc-7939"
    Mar 27 14:16:48.936: INFO: Deleting pod "simpletest.rc-4lxpn" in namespace "gc-7939"
    Mar 27 14:16:48.952: INFO: Deleting pod "simpletest.rc-4r7h7" in namespace "gc-7939"
    Mar 27 14:16:48.966: INFO: Deleting pod "simpletest.rc-4rtkp" in namespace "gc-7939"
    Mar 27 14:16:48.985: INFO: Deleting pod "simpletest.rc-52k5f" in namespace "gc-7939"
    Mar 27 14:16:49.003: INFO: Deleting pod "simpletest.rc-5686p" in namespace "gc-7939"
    Mar 27 14:16:49.021: INFO: Deleting pod "simpletest.rc-577rs" in namespace "gc-7939"
    Mar 27 14:16:49.037: INFO: Deleting pod "simpletest.rc-5gldg" in namespace "gc-7939"
    Mar 27 14:16:49.051: INFO: Deleting pod "simpletest.rc-5ktwl" in namespace "gc-7939"
    Mar 27 14:16:49.066: INFO: Deleting pod "simpletest.rc-5lg9x" in namespace "gc-7939"
    Mar 27 14:16:49.084: INFO: Deleting pod "simpletest.rc-5pzrn" in namespace "gc-7939"
    Mar 27 14:16:49.095: INFO: Deleting pod "simpletest.rc-65gxp" in namespace "gc-7939"
    Mar 27 14:16:49.108: INFO: Deleting pod "simpletest.rc-6wzgn" in namespace "gc-7939"
    Mar 27 14:16:49.121: INFO: Deleting pod "simpletest.rc-6zmzf" in namespace "gc-7939"
    Mar 27 14:16:49.135: INFO: Deleting pod "simpletest.rc-778tk" in namespace "gc-7939"
    Mar 27 14:16:49.153: INFO: Deleting pod "simpletest.rc-7c8kt" in namespace "gc-7939"
    Mar 27 14:16:49.171: INFO: Deleting pod "simpletest.rc-7hhd7" in namespace "gc-7939"
    Mar 27 14:16:49.190: INFO: Deleting pod "simpletest.rc-7nt2p" in namespace "gc-7939"
    Mar 27 14:16:49.201: INFO: Deleting pod "simpletest.rc-7tt2l" in namespace "gc-7939"
    Mar 27 14:16:49.216: INFO: Deleting pod "simpletest.rc-86tpq" in namespace "gc-7939"
    Mar 27 14:16:49.230: INFO: Deleting pod "simpletest.rc-8kzdj" in namespace "gc-7939"
    Mar 27 14:16:49.249: INFO: Deleting pod "simpletest.rc-8n9fv" in namespace "gc-7939"
    Mar 27 14:16:49.263: INFO: Deleting pod "simpletest.rc-8tc88" in namespace "gc-7939"
    Mar 27 14:16:49.275: INFO: Deleting pod "simpletest.rc-8trpn" in namespace "gc-7939"
    Mar 27 14:16:49.291: INFO: Deleting pod "simpletest.rc-8wnht" in namespace "gc-7939"
    Mar 27 14:16:49.307: INFO: Deleting pod "simpletest.rc-9759d" in namespace "gc-7939"
    Mar 27 14:16:49.325: INFO: Deleting pod "simpletest.rc-97fgm" in namespace "gc-7939"
    Mar 27 14:16:49.350: INFO: Deleting pod "simpletest.rc-9gchl" in namespace "gc-7939"
    Mar 27 14:16:49.371: INFO: Deleting pod "simpletest.rc-9wmgf" in namespace "gc-7939"
    Mar 27 14:16:49.396: INFO: Deleting pod "simpletest.rc-b5hr5" in namespace "gc-7939"
    Mar 27 14:16:49.416: INFO: Deleting pod "simpletest.rc-b6rmt" in namespace "gc-7939"
    Mar 27 14:16:49.433: INFO: Deleting pod "simpletest.rc-bpg5j" in namespace "gc-7939"
    Mar 27 14:16:49.449: INFO: Deleting pod "simpletest.rc-bv8hw" in namespace "gc-7939"
    Mar 27 14:16:49.464: INFO: Deleting pod "simpletest.rc-bzg7g" in namespace "gc-7939"
    Mar 27 14:16:49.474: INFO: Deleting pod "simpletest.rc-c6jgw" in namespace "gc-7939"
    Mar 27 14:16:49.486: INFO: Deleting pod "simpletest.rc-c7r6f" in namespace "gc-7939"
    Mar 27 14:16:49.503: INFO: Deleting pod "simpletest.rc-d4bcx" in namespace "gc-7939"
    Mar 27 14:16:49.518: INFO: Deleting pod "simpletest.rc-dfrt5" in namespace "gc-7939"
    Mar 27 14:16:49.545: INFO: Deleting pod "simpletest.rc-dqd5t" in namespace "gc-7939"
    Mar 27 14:16:49.568: INFO: Deleting pod "simpletest.rc-flkw4" in namespace "gc-7939"
    Mar 27 14:16:49.593: INFO: Deleting pod "simpletest.rc-fv4gs" in namespace "gc-7939"
    Mar 27 14:16:49.612: INFO: Deleting pod "simpletest.rc-fxj4x" in namespace "gc-7939"
    Mar 27 14:16:49.624: INFO: Deleting pod "simpletest.rc-gdbhl" in namespace "gc-7939"
    Mar 27 14:16:49.640: INFO: Deleting pod "simpletest.rc-gdfvw" in namespace "gc-7939"
    Mar 27 14:16:49.658: INFO: Deleting pod "simpletest.rc-gjmml" in namespace "gc-7939"
    Mar 27 14:16:49.669: INFO: Deleting pod "simpletest.rc-gmgsd" in namespace "gc-7939"
    Mar 27 14:16:49.685: INFO: Deleting pod "simpletest.rc-gv4qg" in namespace "gc-7939"
    Mar 27 14:16:49.700: INFO: Deleting pod "simpletest.rc-h7gwr" in namespace "gc-7939"
    Mar 27 14:16:49.720: INFO: Deleting pod "simpletest.rc-hhpn4" in namespace "gc-7939"
    Mar 27 14:16:49.734: INFO: Deleting pod "simpletest.rc-hpfbj" in namespace "gc-7939"
    Mar 27 14:16:49.756: INFO: Deleting pod "simpletest.rc-hzwx2" in namespace "gc-7939"
    Mar 27 14:16:49.772: INFO: Deleting pod "simpletest.rc-jtpvt" in namespace "gc-7939"
    Mar 27 14:16:49.794: INFO: Deleting pod "simpletest.rc-jvm6h" in namespace "gc-7939"
    Mar 27 14:16:49.811: INFO: Deleting pod "simpletest.rc-kdgmm" in namespace "gc-7939"
    Mar 27 14:16:49.827: INFO: Deleting pod "simpletest.rc-kxljf" in namespace "gc-7939"
    Mar 27 14:16:49.850: INFO: Deleting pod "simpletest.rc-lspsz" in namespace "gc-7939"
    Mar 27 14:16:49.873: INFO: Deleting pod "simpletest.rc-m7vvf" in namespace "gc-7939"
    Mar 27 14:16:49.890: INFO: Deleting pod "simpletest.rc-mtsz7" in namespace "gc-7939"
    Mar 27 14:16:49.909: INFO: Deleting pod "simpletest.rc-n58p5" in namespace "gc-7939"
    Mar 27 14:16:49.937: INFO: Deleting pod "simpletest.rc-n58zx" in namespace "gc-7939"
    Mar 27 14:16:49.955: INFO: Deleting pod "simpletest.rc-ngh86" in namespace "gc-7939"
    Mar 27 14:16:49.973: INFO: Deleting pod "simpletest.rc-njchg" in namespace "gc-7939"
    Mar 27 14:16:49.990: INFO: Deleting pod "simpletest.rc-nl2zp" in namespace "gc-7939"
    Mar 27 14:16:50.003: INFO: Deleting pod "simpletest.rc-nl7jg" in namespace "gc-7939"
    Mar 27 14:16:50.018: INFO: Deleting pod "simpletest.rc-ns8x4" in namespace "gc-7939"
    Mar 27 14:16:50.035: INFO: Deleting pod "simpletest.rc-nvmzw" in namespace "gc-7939"
    Mar 27 14:16:50.049: INFO: Deleting pod "simpletest.rc-nz9jc" in namespace "gc-7939"
    Mar 27 14:16:50.067: INFO: Deleting pod "simpletest.rc-pbt6m" in namespace "gc-7939"
    Mar 27 14:16:50.103: INFO: Deleting pod "simpletest.rc-pdc92" in namespace "gc-7939"
    Mar 27 14:16:50.161: INFO: Deleting pod "simpletest.rc-pqs7w" in namespace "gc-7939"
    Mar 27 14:16:50.198: INFO: Deleting pod "simpletest.rc-q62rt" in namespace "gc-7939"
    Mar 27 14:16:50.257: INFO: Deleting pod "simpletest.rc-qj447" in namespace "gc-7939"
    Mar 27 14:16:50.311: INFO: Deleting pod "simpletest.rc-qpmnw" in namespace "gc-7939"
    Mar 27 14:16:50.364: INFO: Deleting pod "simpletest.rc-rbp8g" in namespace "gc-7939"
    Mar 27 14:16:50.405: INFO: Deleting pod "simpletest.rc-rrrc7" in namespace "gc-7939"
    Mar 27 14:16:50.460: INFO: Deleting pod "simpletest.rc-rwn7b" in namespace "gc-7939"
    Mar 27 14:16:50.501: INFO: Deleting pod "simpletest.rc-s7f4m" in namespace "gc-7939"
    Mar 27 14:16:50.566: INFO: Deleting pod "simpletest.rc-sd6tx" in namespace "gc-7939"
    Mar 27 14:16:50.603: INFO: Deleting pod "simpletest.rc-shxw7" in namespace "gc-7939"
    Mar 27 14:16:50.656: INFO: Deleting pod "simpletest.rc-spxbj" in namespace "gc-7939"
    Mar 27 14:16:50.706: INFO: Deleting pod "simpletest.rc-stvg9" in namespace "gc-7939"
    Mar 27 14:16:50.756: INFO: Deleting pod "simpletest.rc-sz2t5" in namespace "gc-7939"
    Mar 27 14:16:50.805: INFO: Deleting pod "simpletest.rc-tfscx" in namespace "gc-7939"
    Mar 27 14:16:50.857: INFO: Deleting pod "simpletest.rc-trx88" in namespace "gc-7939"
    Mar 27 14:16:50.898: INFO: Deleting pod "simpletest.rc-twdql" in namespace "gc-7939"
    Mar 27 14:16:50.953: INFO: Deleting pod "simpletest.rc-tzxc2" in namespace "gc-7939"
    Mar 27 14:16:51.003: INFO: Deleting pod "simpletest.rc-vhzrn" in namespace "gc-7939"
    Mar 27 14:16:51.051: INFO: Deleting pod "simpletest.rc-wn828" in namespace "gc-7939"
    Mar 27 14:16:51.102: INFO: Deleting pod "simpletest.rc-xfc2c" in namespace "gc-7939"
    Mar 27 14:16:51.151: INFO: Deleting pod "simpletest.rc-xnbtp" in namespace "gc-7939"
    Mar 27 14:16:51.200: INFO: Deleting pod "simpletest.rc-xxthm" in namespace "gc-7939"
    Mar 27 14:16:51.250: INFO: Deleting pod "simpletest.rc-zdh9p" in namespace "gc-7939"
    Mar 27 14:16:51.301: INFO: Deleting pod "simpletest.rc-znn7h" in namespace "gc-7939"
    Mar 27 14:16:51.357: INFO: Deleting pod "simpletest.rc-zt7x5" in namespace "gc-7939"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 27 14:16:51.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7939" for this suite. 03/27/23 14:16:51.441
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:16:51.494
Mar 27 14:16:51.495: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename deployment 03/27/23 14:16:51.495
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:16:51.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:16:51.516
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Mar 27 14:16:51.518: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 27 14:16:51.527: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 27 14:16:56.533: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/27/23 14:16:56.533
Mar 27 14:16:56.533: INFO: Creating deployment "test-rolling-update-deployment"
Mar 27 14:16:56.541: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 27 14:16:56.549: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 27 14:16:58.559: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 27 14:16:58.562: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:17:06.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:17:08.567: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:17:10.568: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:17:12.568: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 27 14:17:12.582: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4169  b47a6203-7187-438d-be59-32ad29cc0bf9 11551 1 2023-03-27 14:16:56 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-27 14:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00463ee18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-27 14:16:56 +0000 UTC,LastTransitionTime:2023-03-27 14:16:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-03-27 14:17:10 +0000 UTC,LastTransitionTime:2023-03-27 14:16:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 27 14:17:12.585: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-4169  7a6ce70b-15c7-4ec8-b588-9d7a7014481f 11522 1 2023-03-27 14:16:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment b47a6203-7187-438d-be59-32ad29cc0bf9 0xc0045bd2f7 0xc0045bd2f8}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b47a6203-7187-438d-be59-32ad29cc0bf9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045bd508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 27 14:17:12.585: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 27 14:17:12.585: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4169  05a8c31d-a1d6-411d-9779-9ab75c76e3d1 11548 2 2023-03-27 14:16:51 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment b47a6203-7187-438d-be59-32ad29cc0bf9 0xc0045bd047 0xc0045bd048}] [] [{e2e.test Update apps/v1 2023-03-27 14:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b47a6203-7187-438d-be59-32ad29cc0bf9\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0045bd1f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 27 14:17:12.592: INFO: Pod "test-rolling-update-deployment-78f575d8ff-nw9d5" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-nw9d5 test-rolling-update-deployment-78f575d8ff- deployment-4169  4f90d10e-e67b-4f87-afa2-c2be42da7512 11519 0 2023-03-27 14:16:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:596cac401f49ecee242d797b77028207d89d18bab4d21db15f3c556b83e42500 cni.projectcalico.org/podIP:172.25.1.49/32 cni.projectcalico.org/podIPs:172.25.1.49/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 7a6ce70b-15c7-4ec8-b588-9d7a7014481f 0xc0045bdd17 0xc0045bdd18}] [] [{kube-controller-manager Update v1 2023-03-27 14:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a6ce70b-15c7-4ec8-b588-9d7a7014481f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:16:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:17:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dzg4m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dzg4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:16:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:16:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:16:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:16:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.49,StartTime:2023-03-27 14:16:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:16:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://fbd647c650d718d9151867a630a0454d76e6baaf9b156b37e32b8b97e4c1236f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 27 14:17:12.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4169" for this suite. 03/27/23 14:17:12.597
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":33,"skipped":701,"failed":0}
------------------------------
• [SLOW TEST] [21.112 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:16:51.494
    Mar 27 14:16:51.495: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename deployment 03/27/23 14:16:51.495
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:16:51.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:16:51.516
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Mar 27 14:16:51.518: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Mar 27 14:16:51.527: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 27 14:16:56.533: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/27/23 14:16:56.533
    Mar 27 14:16:56.533: INFO: Creating deployment "test-rolling-update-deployment"
    Mar 27 14:16:56.541: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Mar 27 14:16:56.549: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Mar 27 14:16:58.559: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Mar 27 14:16:58.562: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:17:06.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:17:08.567: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:17:10.568: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 16, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:17:12.568: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 27 14:17:12.582: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4169  b47a6203-7187-438d-be59-32ad29cc0bf9 11551 1 2023-03-27 14:16:56 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-27 14:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00463ee18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-27 14:16:56 +0000 UTC,LastTransitionTime:2023-03-27 14:16:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-03-27 14:17:10 +0000 UTC,LastTransitionTime:2023-03-27 14:16:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 27 14:17:12.585: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-4169  7a6ce70b-15c7-4ec8-b588-9d7a7014481f 11522 1 2023-03-27 14:16:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment b47a6203-7187-438d-be59-32ad29cc0bf9 0xc0045bd2f7 0xc0045bd2f8}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b47a6203-7187-438d-be59-32ad29cc0bf9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045bd508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 14:17:12.585: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Mar 27 14:17:12.585: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4169  05a8c31d-a1d6-411d-9779-9ab75c76e3d1 11548 2 2023-03-27 14:16:51 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment b47a6203-7187-438d-be59-32ad29cc0bf9 0xc0045bd047 0xc0045bd048}] [] [{e2e.test Update apps/v1 2023-03-27 14:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b47a6203-7187-438d-be59-32ad29cc0bf9\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0045bd1f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 14:17:12.592: INFO: Pod "test-rolling-update-deployment-78f575d8ff-nw9d5" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-nw9d5 test-rolling-update-deployment-78f575d8ff- deployment-4169  4f90d10e-e67b-4f87-afa2-c2be42da7512 11519 0 2023-03-27 14:16:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:596cac401f49ecee242d797b77028207d89d18bab4d21db15f3c556b83e42500 cni.projectcalico.org/podIP:172.25.1.49/32 cni.projectcalico.org/podIPs:172.25.1.49/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 7a6ce70b-15c7-4ec8-b588-9d7a7014481f 0xc0045bdd17 0xc0045bdd18}] [] [{kube-controller-manager Update v1 2023-03-27 14:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a6ce70b-15c7-4ec8-b588-9d7a7014481f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:16:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:17:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dzg4m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dzg4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:16:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:16:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:16:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:16:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.49,StartTime:2023-03-27 14:16:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:16:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://fbd647c650d718d9151867a630a0454d76e6baaf9b156b37e32b8b97e4c1236f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 27 14:17:12.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4169" for this suite. 03/27/23 14:17:12.597
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:17:12.607
Mar 27 14:17:12.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename deployment 03/27/23 14:17:12.608
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:17:12.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:17:12.631
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Mar 27 14:17:12.646: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 27 14:17:17.651: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/27/23 14:17:17.651
Mar 27 14:17:17.651: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 27 14:17:19.657: INFO: Creating deployment "test-rollover-deployment"
Mar 27 14:17:19.667: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 27 14:17:21.675: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 27 14:17:21.681: INFO: Ensure that both replica sets have 1 created replica
Mar 27 14:17:21.687: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 27 14:17:21.701: INFO: Updating deployment test-rollover-deployment
Mar 27 14:17:21.701: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 27 14:17:23.712: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 27 14:17:23.719: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 27 14:17:23.725: INFO: all replica sets need to contain the pod-template-hash label
Mar 27 14:17:23.725: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:17:25.734: INFO: all replica sets need to contain the pod-template-hash label
Mar 27 14:17:25.734: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:17:27.733: INFO: all replica sets need to contain the pod-template-hash label
Mar 27 14:17:27.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:17:29.734: INFO: all replica sets need to contain the pod-template-hash label
Mar 27 14:17:29.734: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:17:31.737: INFO: all replica sets need to contain the pod-template-hash label
Mar 27 14:17:31.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 14:17:33.738: INFO: 
Mar 27 14:17:33.738: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 27 14:17:33.751: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7389  44c8e627-a6dc-4dc1-bf51-306cbc740730 12030 2 2023-03-27 14:17:19 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-27 14:17:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e55908 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-27 14:17:19 +0000 UTC,LastTransitionTime:2023-03-27 14:17:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-03-27 14:17:33 +0000 UTC,LastTransitionTime:2023-03-27 14:17:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 27 14:17:33.754: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-7389  ec544fcc-5915-4c06-8b29-5a68aaa8a62f 12020 2 2023-03-27 14:17:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 44c8e627-a6dc-4dc1-bf51-306cbc740730 0xc00509a467 0xc00509a468}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:17:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44c8e627-a6dc-4dc1-bf51-306cbc740730\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00509a588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 27 14:17:33.754: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 27 14:17:33.754: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7389  15f7611a-ff56-4ec0-b41e-01efd9d97a43 12029 2 2023-03-27 14:17:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 44c8e627-a6dc-4dc1-bf51-306cbc740730 0xc004e55f57 0xc004e55f58}] [] [{e2e.test Update apps/v1 2023-03-27 14:17:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44c8e627-a6dc-4dc1-bf51-306cbc740730\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00509a108 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 27 14:17:33.755: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-7389  74bf970a-4090-4dd3-a46b-872b39011aeb 11950 2 2023-03-27 14:17:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 44c8e627-a6dc-4dc1-bf51-306cbc740730 0xc00509a207 0xc00509a208}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:17:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44c8e627-a6dc-4dc1-bf51-306cbc740730\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00509a398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 27 14:17:33.758: INFO: Pod "test-rollover-deployment-6d45fd857b-k6888" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-k6888 test-rollover-deployment-6d45fd857b- deployment-7389  e62f3287-d30e-4f79-a108-6c0dd1aee537 11969 0 2023-03-27 14:17:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:eb6ff8a9fef4faf1b8e0e9da85914195ec648f7248c9b40f8c1e60631844de83 cni.projectcalico.org/podIP:172.25.1.50/32 cni.projectcalico.org/podIPs:172.25.1.50/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b ec544fcc-5915-4c06-8b29-5a68aaa8a62f 0xc004cd9967 0xc004cd9968}] [] [{kube-controller-manager Update v1 2023-03-27 14:17:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec544fcc-5915-4c06-8b29-5a68aaa8a62f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:17:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:17:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87sz9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87sz9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.50,StartTime:2023-03-27 14:17:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:17:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://9b5d3bdbf1f46b5392d99a148c9e1c6bc64efcf79f8ca199d86616224038de65,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 27 14:17:33.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7389" for this suite. 03/27/23 14:17:33.764
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":34,"skipped":702,"failed":0}
------------------------------
• [SLOW TEST] [21.168 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:17:12.607
    Mar 27 14:17:12.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename deployment 03/27/23 14:17:12.608
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:17:12.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:17:12.631
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Mar 27 14:17:12.646: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Mar 27 14:17:17.651: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/27/23 14:17:17.651
    Mar 27 14:17:17.651: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Mar 27 14:17:19.657: INFO: Creating deployment "test-rollover-deployment"
    Mar 27 14:17:19.667: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Mar 27 14:17:21.675: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Mar 27 14:17:21.681: INFO: Ensure that both replica sets have 1 created replica
    Mar 27 14:17:21.687: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Mar 27 14:17:21.701: INFO: Updating deployment test-rollover-deployment
    Mar 27 14:17:21.701: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Mar 27 14:17:23.712: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Mar 27 14:17:23.719: INFO: Make sure deployment "test-rollover-deployment" is complete
    Mar 27 14:17:23.725: INFO: all replica sets need to contain the pod-template-hash label
    Mar 27 14:17:23.725: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:17:25.734: INFO: all replica sets need to contain the pod-template-hash label
    Mar 27 14:17:25.734: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:17:27.733: INFO: all replica sets need to contain the pod-template-hash label
    Mar 27 14:17:27.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:17:29.734: INFO: all replica sets need to contain the pod-template-hash label
    Mar 27 14:17:29.734: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:17:31.737: INFO: all replica sets need to contain the pod-template-hash label
    Mar 27 14:17:31.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 17, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 14:17:33.738: INFO: 
    Mar 27 14:17:33.738: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 27 14:17:33.751: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-7389  44c8e627-a6dc-4dc1-bf51-306cbc740730 12030 2 2023-03-27 14:17:19 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-27 14:17:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e55908 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-27 14:17:19 +0000 UTC,LastTransitionTime:2023-03-27 14:17:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-03-27 14:17:33 +0000 UTC,LastTransitionTime:2023-03-27 14:17:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 27 14:17:33.754: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-7389  ec544fcc-5915-4c06-8b29-5a68aaa8a62f 12020 2 2023-03-27 14:17:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 44c8e627-a6dc-4dc1-bf51-306cbc740730 0xc00509a467 0xc00509a468}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:17:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44c8e627-a6dc-4dc1-bf51-306cbc740730\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00509a588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 14:17:33.754: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Mar 27 14:17:33.754: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7389  15f7611a-ff56-4ec0-b41e-01efd9d97a43 12029 2 2023-03-27 14:17:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 44c8e627-a6dc-4dc1-bf51-306cbc740730 0xc004e55f57 0xc004e55f58}] [] [{e2e.test Update apps/v1 2023-03-27 14:17:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44c8e627-a6dc-4dc1-bf51-306cbc740730\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00509a108 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 14:17:33.755: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-7389  74bf970a-4090-4dd3-a46b-872b39011aeb 11950 2 2023-03-27 14:17:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 44c8e627-a6dc-4dc1-bf51-306cbc740730 0xc00509a207 0xc00509a208}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:17:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44c8e627-a6dc-4dc1-bf51-306cbc740730\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00509a398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 14:17:33.758: INFO: Pod "test-rollover-deployment-6d45fd857b-k6888" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-k6888 test-rollover-deployment-6d45fd857b- deployment-7389  e62f3287-d30e-4f79-a108-6c0dd1aee537 11969 0 2023-03-27 14:17:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:eb6ff8a9fef4faf1b8e0e9da85914195ec648f7248c9b40f8c1e60631844de83 cni.projectcalico.org/podIP:172.25.1.50/32 cni.projectcalico.org/podIPs:172.25.1.50/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b ec544fcc-5915-4c06-8b29-5a68aaa8a62f 0xc004cd9967 0xc004cd9968}] [] [{kube-controller-manager Update v1 2023-03-27 14:17:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec544fcc-5915-4c06-8b29-5a68aaa8a62f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:17:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:17:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87sz9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87sz9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.50,StartTime:2023-03-27 14:17:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:17:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://9b5d3bdbf1f46b5392d99a148c9e1c6bc64efcf79f8ca199d86616224038de65,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 27 14:17:33.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7389" for this suite. 03/27/23 14:17:33.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:17:33.776
Mar 27 14:17:33.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename deployment 03/27/23 14:17:33.777
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:17:33.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:17:33.8
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Mar 27 14:17:33.805: INFO: Creating simple deployment test-new-deployment
Mar 27 14:17:33.823: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 03/27/23 14:17:35.963
STEP: updating a scale subresource 03/27/23 14:17:35.967
STEP: verifying the deployment Spec.Replicas was modified 03/27/23 14:17:35.978
STEP: Patch a scale subresource 03/27/23 14:17:35.985
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 27 14:17:36.452: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-3642  780c0e49-4ccd-44cd-9c5f-e0dfd8242415 12077 3 2023-03-27 14:17:33 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-27 14:17:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f42ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-03-27 14:17:35 +0000 UTC,LastTransitionTime:2023-03-27 14:17:33 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-27 14:17:36 +0000 UTC,LastTransitionTime:2023-03-27 14:17:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 27 14:17:36.460: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-3642  f6c98854-9255-4ac0-98a2-8a1c2ecd8653 12086 2 2023-03-27 14:17:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 780c0e49-4ccd-44cd-9c5f-e0dfd8242415 0xc005f43937 0xc005f43938}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:17:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"780c0e49-4ccd-44cd-9c5f-e0dfd8242415\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f43ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 27 14:17:36.465: INFO: Pod "test-new-deployment-845c8977d9-6t2t9" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-6t2t9 test-new-deployment-845c8977d9- deployment-3642  ac0588e4-c08c-469b-ac6e-c547c28a94ae 12063 0 2023-03-27 14:17:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:66fc6140c9b6d120598c6af5bb4e6914a1d79d9210d705c906ae33d9869f5a1d cni.projectcalico.org/podIP:172.25.2.72/32 cni.projectcalico.org/podIPs:172.25.2.72/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 f6c98854-9255-4ac0-98a2-8a1c2ecd8653 0xc005f1ca77 0xc005f1ca78}] [] [{kube-controller-manager Update v1 2023-03-27 14:17:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6c98854-9255-4ac0-98a2-8a1c2ecd8653\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:17:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:17:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68fv9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68fv9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:172.25.2.72,StartTime:2023-03-27 14:17:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:17:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f8af3ca460d69b55d9d38a83fc99ac4b565b419a234220d44467beaabc432d19,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:17:36.466: INFO: Pod "test-new-deployment-845c8977d9-jptwd" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-jptwd test-new-deployment-845c8977d9- deployment-3642  a0bf77fe-27f8-423f-9b0d-5c16c32392e7 12081 0 2023-03-27 14:17:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 f6c98854-9255-4ac0-98a2-8a1c2ecd8653 0xc005f1ce40 0xc005f1ce41}] [] [{kube-controller-manager Update v1 2023-03-27 14:17:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6c98854-9255-4ac0-98a2-8a1c2ecd8653\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:17:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n6kld,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n6kld,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:17:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 27 14:17:36.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3642" for this suite. 03/27/23 14:17:36.47
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":35,"skipped":718,"failed":0}
------------------------------
• [2.700 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:17:33.776
    Mar 27 14:17:33.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename deployment 03/27/23 14:17:33.777
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:17:33.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:17:33.8
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Mar 27 14:17:33.805: INFO: Creating simple deployment test-new-deployment
    Mar 27 14:17:33.823: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 03/27/23 14:17:35.963
    STEP: updating a scale subresource 03/27/23 14:17:35.967
    STEP: verifying the deployment Spec.Replicas was modified 03/27/23 14:17:35.978
    STEP: Patch a scale subresource 03/27/23 14:17:35.985
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 27 14:17:36.452: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-3642  780c0e49-4ccd-44cd-9c5f-e0dfd8242415 12077 3 2023-03-27 14:17:33 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-27 14:17:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f42ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-03-27 14:17:35 +0000 UTC,LastTransitionTime:2023-03-27 14:17:33 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-27 14:17:36 +0000 UTC,LastTransitionTime:2023-03-27 14:17:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 27 14:17:36.460: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-3642  f6c98854-9255-4ac0-98a2-8a1c2ecd8653 12086 2 2023-03-27 14:17:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 780c0e49-4ccd-44cd-9c5f-e0dfd8242415 0xc005f43937 0xc005f43938}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:17:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"780c0e49-4ccd-44cd-9c5f-e0dfd8242415\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:17:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f43ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 14:17:36.465: INFO: Pod "test-new-deployment-845c8977d9-6t2t9" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-6t2t9 test-new-deployment-845c8977d9- deployment-3642  ac0588e4-c08c-469b-ac6e-c547c28a94ae 12063 0 2023-03-27 14:17:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:66fc6140c9b6d120598c6af5bb4e6914a1d79d9210d705c906ae33d9869f5a1d cni.projectcalico.org/podIP:172.25.2.72/32 cni.projectcalico.org/podIPs:172.25.2.72/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 f6c98854-9255-4ac0-98a2-8a1c2ecd8653 0xc005f1ca77 0xc005f1ca78}] [] [{kube-controller-manager Update v1 2023-03-27 14:17:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6c98854-9255-4ac0-98a2-8a1c2ecd8653\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:17:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:17:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68fv9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68fv9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:172.25.2.72,StartTime:2023-03-27 14:17:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:17:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f8af3ca460d69b55d9d38a83fc99ac4b565b419a234220d44467beaabc432d19,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:17:36.466: INFO: Pod "test-new-deployment-845c8977d9-jptwd" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-jptwd test-new-deployment-845c8977d9- deployment-3642  a0bf77fe-27f8-423f-9b0d-5c16c32392e7 12081 0 2023-03-27 14:17:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 f6c98854-9255-4ac0-98a2-8a1c2ecd8653 0xc005f1ce40 0xc005f1ce41}] [] [{kube-controller-manager Update v1 2023-03-27 14:17:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6c98854-9255-4ac0-98a2-8a1c2ecd8653\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:17:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n6kld,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n6kld,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:17:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:17:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 27 14:17:36.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3642" for this suite. 03/27/23 14:17:36.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:17:36.477
Mar 27 14:17:36.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename server-version 03/27/23 14:17:36.478
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:17:36.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:17:36.865
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 03/27/23 14:17:36.872
STEP: Confirm major version 03/27/23 14:17:36.873
Mar 27 14:17:36.874: INFO: Major version: 1
STEP: Confirm minor version 03/27/23 14:17:36.874
Mar 27 14:17:36.874: INFO: cleanMinorVersion: 25
Mar 27 14:17:36.874: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Mar 27 14:17:36.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-5894" for this suite. 03/27/23 14:17:36.878
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":36,"skipped":728,"failed":0}
------------------------------
• [0.723 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:17:36.477
    Mar 27 14:17:36.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename server-version 03/27/23 14:17:36.478
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:17:36.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:17:36.865
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 03/27/23 14:17:36.872
    STEP: Confirm major version 03/27/23 14:17:36.873
    Mar 27 14:17:36.874: INFO: Major version: 1
    STEP: Confirm minor version 03/27/23 14:17:36.874
    Mar 27 14:17:36.874: INFO: cleanMinorVersion: 25
    Mar 27 14:17:36.874: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Mar 27 14:17:36.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-5894" for this suite. 03/27/23 14:17:36.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:17:37.202
Mar 27 14:17:37.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename init-container 03/27/23 14:17:37.203
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:17:37.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:17:37.228
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 03/27/23 14:17:37.231
Mar 27 14:17:37.232: INFO: PodSpec: initContainers in spec.initContainers
Mar 27 14:18:19.264: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-fc62e9d8-ed74-43a0-88f6-2a058d9b32b4", GenerateName:"", Namespace:"init-container-3315", SelfLink:"", UID:"41027578-9a7a-4b45-9cd1-418455c63216", ResourceVersion:"12407", Generation:0, CreationTimestamp:time.Date(2023, time.March, 27, 14, 17, 37, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"232060656"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"a9ca8396c04a621779d5c90773e5f868ada0e12b1f33ecb6a259129c45a25387", "cni.projectcalico.org/podIP":"172.25.2.73/32", "cni.projectcalico.org/podIPs":"172.25.2.73/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 27, 14, 17, 37, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006b3a0d8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 27, 14, 17, 38, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006b3a108), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 27, 14, 18, 19, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006b3a138), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-8lzrf", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc005214020), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8lzrf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8lzrf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8lzrf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc006212120), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0003da000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0062121a0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0062121c0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0062121c8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0062121cc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc003246060), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 37, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 37, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 37, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 37, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.1.4", PodIP:"172.25.2.73", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.25.2.73"}}, StartTime:time.Date(2023, time.March, 27, 14, 17, 37, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc006b3a180), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0003da0e0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://a414dba40342d3af27795ad80b7631c4cf9cc341d0bb5fcedb71007e0b54ce37", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0052140a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005214080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc0062122cf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 27 14:18:19.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3315" for this suite. 03/27/23 14:18:19.277
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":37,"skipped":736,"failed":0}
------------------------------
• [SLOW TEST] [42.106 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:17:37.202
    Mar 27 14:17:37.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename init-container 03/27/23 14:17:37.203
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:17:37.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:17:37.228
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 03/27/23 14:17:37.231
    Mar 27 14:17:37.232: INFO: PodSpec: initContainers in spec.initContainers
    Mar 27 14:18:19.264: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-fc62e9d8-ed74-43a0-88f6-2a058d9b32b4", GenerateName:"", Namespace:"init-container-3315", SelfLink:"", UID:"41027578-9a7a-4b45-9cd1-418455c63216", ResourceVersion:"12407", Generation:0, CreationTimestamp:time.Date(2023, time.March, 27, 14, 17, 37, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"232060656"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"a9ca8396c04a621779d5c90773e5f868ada0e12b1f33ecb6a259129c45a25387", "cni.projectcalico.org/podIP":"172.25.2.73/32", "cni.projectcalico.org/podIPs":"172.25.2.73/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 27, 14, 17, 37, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006b3a0d8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 27, 14, 17, 38, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006b3a108), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 27, 14, 18, 19, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006b3a138), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-8lzrf", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc005214020), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8lzrf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8lzrf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8lzrf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc006212120), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0003da000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0062121a0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0062121c0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0062121c8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0062121cc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc003246060), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 37, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 37, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 37, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 27, 14, 17, 37, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.1.4", PodIP:"172.25.2.73", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.25.2.73"}}, StartTime:time.Date(2023, time.March, 27, 14, 17, 37, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc006b3a180), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0003da0e0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://a414dba40342d3af27795ad80b7631c4cf9cc341d0bb5fcedb71007e0b54ce37", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0052140a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005214080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc0062122cf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 27 14:18:19.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-3315" for this suite. 03/27/23 14:18:19.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:18:19.313
Mar 27 14:18:19.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename cronjob 03/27/23 14:18:19.314
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:18:19.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:18:19.336
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 03/27/23 14:18:19.419
STEP: Ensuring a job is scheduled 03/27/23 14:18:19.426
STEP: Ensuring exactly one is scheduled 03/27/23 14:19:01.434
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/27/23 14:19:01.437
STEP: Ensuring no more jobs are scheduled 03/27/23 14:19:01.44
STEP: Removing cronjob 03/27/23 14:24:01.452
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar 27 14:24:01.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5065" for this suite. 03/27/23 14:24:01.467
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":38,"skipped":764,"failed":0}
------------------------------
• [SLOW TEST] [342.161 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:18:19.313
    Mar 27 14:18:19.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename cronjob 03/27/23 14:18:19.314
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:18:19.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:18:19.336
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 03/27/23 14:18:19.419
    STEP: Ensuring a job is scheduled 03/27/23 14:18:19.426
    STEP: Ensuring exactly one is scheduled 03/27/23 14:19:01.434
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/27/23 14:19:01.437
    STEP: Ensuring no more jobs are scheduled 03/27/23 14:19:01.44
    STEP: Removing cronjob 03/27/23 14:24:01.452
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar 27 14:24:01.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5065" for this suite. 03/27/23 14:24:01.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:24:01.475
Mar 27 14:24:01.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename emptydir 03/27/23 14:24:01.477
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:01.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:01.501
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/27/23 14:24:01.504
Mar 27 14:24:01.520: INFO: Waiting up to 5m0s for pod "pod-dff22116-7e3b-4144-ae9d-32069ba00fa7" in namespace "emptydir-1369" to be "Succeeded or Failed"
Mar 27 14:24:01.524: INFO: Pod "pod-dff22116-7e3b-4144-ae9d-32069ba00fa7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.493003ms
Mar 27 14:24:03.530: INFO: Pod "pod-dff22116-7e3b-4144-ae9d-32069ba00fa7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009600949s
Mar 27 14:24:05.530: INFO: Pod "pod-dff22116-7e3b-4144-ae9d-32069ba00fa7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010183777s
STEP: Saw pod success 03/27/23 14:24:05.53
Mar 27 14:24:05.530: INFO: Pod "pod-dff22116-7e3b-4144-ae9d-32069ba00fa7" satisfied condition "Succeeded or Failed"
Mar 27 14:24:05.534: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-dff22116-7e3b-4144-ae9d-32069ba00fa7 container test-container: <nil>
STEP: delete the pod 03/27/23 14:24:05.55
Mar 27 14:24:05.578: INFO: Waiting for pod pod-dff22116-7e3b-4144-ae9d-32069ba00fa7 to disappear
Mar 27 14:24:05.581: INFO: Pod pod-dff22116-7e3b-4144-ae9d-32069ba00fa7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 27 14:24:05.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1369" for this suite. 03/27/23 14:24:05.587
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":39,"skipped":775,"failed":0}
------------------------------
• [4.119 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:24:01.475
    Mar 27 14:24:01.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename emptydir 03/27/23 14:24:01.477
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:01.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:01.501
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/27/23 14:24:01.504
    Mar 27 14:24:01.520: INFO: Waiting up to 5m0s for pod "pod-dff22116-7e3b-4144-ae9d-32069ba00fa7" in namespace "emptydir-1369" to be "Succeeded or Failed"
    Mar 27 14:24:01.524: INFO: Pod "pod-dff22116-7e3b-4144-ae9d-32069ba00fa7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.493003ms
    Mar 27 14:24:03.530: INFO: Pod "pod-dff22116-7e3b-4144-ae9d-32069ba00fa7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009600949s
    Mar 27 14:24:05.530: INFO: Pod "pod-dff22116-7e3b-4144-ae9d-32069ba00fa7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010183777s
    STEP: Saw pod success 03/27/23 14:24:05.53
    Mar 27 14:24:05.530: INFO: Pod "pod-dff22116-7e3b-4144-ae9d-32069ba00fa7" satisfied condition "Succeeded or Failed"
    Mar 27 14:24:05.534: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-dff22116-7e3b-4144-ae9d-32069ba00fa7 container test-container: <nil>
    STEP: delete the pod 03/27/23 14:24:05.55
    Mar 27 14:24:05.578: INFO: Waiting for pod pod-dff22116-7e3b-4144-ae9d-32069ba00fa7 to disappear
    Mar 27 14:24:05.581: INFO: Pod pod-dff22116-7e3b-4144-ae9d-32069ba00fa7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 27 14:24:05.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1369" for this suite. 03/27/23 14:24:05.587
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:24:05.596
Mar 27 14:24:05.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubectl 03/27/23 14:24:05.597
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:05.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:05.622
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 03/27/23 14:24:05.625
Mar 27 14:24:05.625: INFO: namespace kubectl-8889
Mar 27 14:24:05.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-8889 create -f -'
Mar 27 14:24:05.925: INFO: stderr: ""
Mar 27 14:24:05.925: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/27/23 14:24:05.925
Mar 27 14:24:06.931: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 14:24:06.931: INFO: Found 0 / 1
Mar 27 14:24:07.932: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 14:24:07.932: INFO: Found 0 / 1
Mar 27 14:24:08.929: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 14:24:08.929: INFO: Found 1 / 1
Mar 27 14:24:08.929: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 27 14:24:08.933: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 14:24:08.933: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 27 14:24:08.933: INFO: wait on agnhost-primary startup in kubectl-8889 
Mar 27 14:24:08.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-8889 logs agnhost-primary-hvtn4 agnhost-primary'
Mar 27 14:24:09.009: INFO: stderr: ""
Mar 27 14:24:09.009: INFO: stdout: "Paused\n"
STEP: exposing RC 03/27/23 14:24:09.009
Mar 27 14:24:09.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-8889 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar 27 14:24:09.301: INFO: stderr: ""
Mar 27 14:24:09.301: INFO: stdout: "service/rm2 exposed\n"
Mar 27 14:24:09.307: INFO: Service rm2 in namespace kubectl-8889 found.
STEP: exposing service 03/27/23 14:24:11.316
Mar 27 14:24:11.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-8889 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar 27 14:24:11.396: INFO: stderr: ""
Mar 27 14:24:11.396: INFO: stdout: "service/rm3 exposed\n"
Mar 27 14:24:11.403: INFO: Service rm3 in namespace kubectl-8889 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 27 14:24:13.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8889" for this suite. 03/27/23 14:24:13.417
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":40,"skipped":776,"failed":0}
------------------------------
• [SLOW TEST] [7.832 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:24:05.596
    Mar 27 14:24:05.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubectl 03/27/23 14:24:05.597
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:05.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:05.622
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 03/27/23 14:24:05.625
    Mar 27 14:24:05.625: INFO: namespace kubectl-8889
    Mar 27 14:24:05.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-8889 create -f -'
    Mar 27 14:24:05.925: INFO: stderr: ""
    Mar 27 14:24:05.925: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/27/23 14:24:05.925
    Mar 27 14:24:06.931: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 14:24:06.931: INFO: Found 0 / 1
    Mar 27 14:24:07.932: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 14:24:07.932: INFO: Found 0 / 1
    Mar 27 14:24:08.929: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 14:24:08.929: INFO: Found 1 / 1
    Mar 27 14:24:08.929: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar 27 14:24:08.933: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 14:24:08.933: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 27 14:24:08.933: INFO: wait on agnhost-primary startup in kubectl-8889 
    Mar 27 14:24:08.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-8889 logs agnhost-primary-hvtn4 agnhost-primary'
    Mar 27 14:24:09.009: INFO: stderr: ""
    Mar 27 14:24:09.009: INFO: stdout: "Paused\n"
    STEP: exposing RC 03/27/23 14:24:09.009
    Mar 27 14:24:09.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-8889 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Mar 27 14:24:09.301: INFO: stderr: ""
    Mar 27 14:24:09.301: INFO: stdout: "service/rm2 exposed\n"
    Mar 27 14:24:09.307: INFO: Service rm2 in namespace kubectl-8889 found.
    STEP: exposing service 03/27/23 14:24:11.316
    Mar 27 14:24:11.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-8889 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Mar 27 14:24:11.396: INFO: stderr: ""
    Mar 27 14:24:11.396: INFO: stdout: "service/rm3 exposed\n"
    Mar 27 14:24:11.403: INFO: Service rm3 in namespace kubectl-8889 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 27 14:24:13.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8889" for this suite. 03/27/23 14:24:13.417
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:24:13.428
Mar 27 14:24:13.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubelet-test 03/27/23 14:24:13.429
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:13.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:13.451
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Mar 27 14:24:13.463: INFO: Waiting up to 5m0s for pod "busybox-scheduling-ba32e40e-3d01-485c-84ad-a2da738d9fff" in namespace "kubelet-test-9497" to be "running and ready"
Mar 27 14:24:13.467: INFO: Pod "busybox-scheduling-ba32e40e-3d01-485c-84ad-a2da738d9fff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.892578ms
Mar 27 14:24:13.467: INFO: The phase of Pod busybox-scheduling-ba32e40e-3d01-485c-84ad-a2da738d9fff is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:24:15.472: INFO: Pod "busybox-scheduling-ba32e40e-3d01-485c-84ad-a2da738d9fff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008600673s
Mar 27 14:24:15.472: INFO: The phase of Pod busybox-scheduling-ba32e40e-3d01-485c-84ad-a2da738d9fff is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:24:17.474: INFO: Pod "busybox-scheduling-ba32e40e-3d01-485c-84ad-a2da738d9fff": Phase="Running", Reason="", readiness=true. Elapsed: 4.010115657s
Mar 27 14:24:17.474: INFO: The phase of Pod busybox-scheduling-ba32e40e-3d01-485c-84ad-a2da738d9fff is Running (Ready = true)
Mar 27 14:24:17.474: INFO: Pod "busybox-scheduling-ba32e40e-3d01-485c-84ad-a2da738d9fff" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar 27 14:24:17.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9497" for this suite. 03/27/23 14:24:17.492
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":41,"skipped":778,"failed":0}
------------------------------
• [4.071 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:24:13.428
    Mar 27 14:24:13.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubelet-test 03/27/23 14:24:13.429
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:13.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:13.451
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Mar 27 14:24:13.463: INFO: Waiting up to 5m0s for pod "busybox-scheduling-ba32e40e-3d01-485c-84ad-a2da738d9fff" in namespace "kubelet-test-9497" to be "running and ready"
    Mar 27 14:24:13.467: INFO: Pod "busybox-scheduling-ba32e40e-3d01-485c-84ad-a2da738d9fff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.892578ms
    Mar 27 14:24:13.467: INFO: The phase of Pod busybox-scheduling-ba32e40e-3d01-485c-84ad-a2da738d9fff is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:24:15.472: INFO: Pod "busybox-scheduling-ba32e40e-3d01-485c-84ad-a2da738d9fff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008600673s
    Mar 27 14:24:15.472: INFO: The phase of Pod busybox-scheduling-ba32e40e-3d01-485c-84ad-a2da738d9fff is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:24:17.474: INFO: Pod "busybox-scheduling-ba32e40e-3d01-485c-84ad-a2da738d9fff": Phase="Running", Reason="", readiness=true. Elapsed: 4.010115657s
    Mar 27 14:24:17.474: INFO: The phase of Pod busybox-scheduling-ba32e40e-3d01-485c-84ad-a2da738d9fff is Running (Ready = true)
    Mar 27 14:24:17.474: INFO: Pod "busybox-scheduling-ba32e40e-3d01-485c-84ad-a2da738d9fff" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar 27 14:24:17.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-9497" for this suite. 03/27/23 14:24:17.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:24:17.499
Mar 27 14:24:17.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename replication-controller 03/27/23 14:24:17.5
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:17.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:17.528
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Mar 27 14:24:17.532: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/27/23 14:24:18.56
STEP: Checking rc "condition-test" has the desired failure condition set 03/27/23 14:24:18.567
STEP: Scaling down rc "condition-test" to satisfy pod quota 03/27/23 14:24:19.576
Mar 27 14:24:19.590: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 03/27/23 14:24:19.59
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar 27 14:24:20.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5672" for this suite. 03/27/23 14:24:20.604
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":42,"skipped":786,"failed":0}
------------------------------
• [3.116 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:24:17.499
    Mar 27 14:24:17.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename replication-controller 03/27/23 14:24:17.5
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:17.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:17.528
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Mar 27 14:24:17.532: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/27/23 14:24:18.56
    STEP: Checking rc "condition-test" has the desired failure condition set 03/27/23 14:24:18.567
    STEP: Scaling down rc "condition-test" to satisfy pod quota 03/27/23 14:24:19.576
    Mar 27 14:24:19.590: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 03/27/23 14:24:19.59
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar 27 14:24:20.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-5672" for this suite. 03/27/23 14:24:20.604
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:24:20.616
Mar 27 14:24:20.616: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename services 03/27/23 14:24:20.617
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:20.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:20.636
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 03/27/23 14:24:20.64
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 27 14:24:20.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7098" for this suite. 03/27/23 14:24:20.65
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":43,"skipped":788,"failed":0}
------------------------------
• [0.045 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:24:20.616
    Mar 27 14:24:20.616: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename services 03/27/23 14:24:20.617
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:20.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:20.636
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 03/27/23 14:24:20.64
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 27 14:24:20.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7098" for this suite. 03/27/23 14:24:20.65
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:24:20.663
Mar 27 14:24:20.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename containers 03/27/23 14:24:20.663
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:20.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:20.683
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Mar 27 14:24:20.697: INFO: Waiting up to 5m0s for pod "client-containers-660e9685-84b9-489b-b684-47fe5612e7fb" in namespace "containers-1575" to be "running"
Mar 27 14:24:20.714: INFO: Pod "client-containers-660e9685-84b9-489b-b684-47fe5612e7fb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.405512ms
Mar 27 14:24:22.719: INFO: Pod "client-containers-660e9685-84b9-489b-b684-47fe5612e7fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.021511555s
Mar 27 14:24:22.719: INFO: Pod "client-containers-660e9685-84b9-489b-b684-47fe5612e7fb" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar 27 14:24:22.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1575" for this suite. 03/27/23 14:24:22.732
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":44,"skipped":804,"failed":0}
------------------------------
• [2.080 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:24:20.663
    Mar 27 14:24:20.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename containers 03/27/23 14:24:20.663
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:20.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:20.683
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Mar 27 14:24:20.697: INFO: Waiting up to 5m0s for pod "client-containers-660e9685-84b9-489b-b684-47fe5612e7fb" in namespace "containers-1575" to be "running"
    Mar 27 14:24:20.714: INFO: Pod "client-containers-660e9685-84b9-489b-b684-47fe5612e7fb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.405512ms
    Mar 27 14:24:22.719: INFO: Pod "client-containers-660e9685-84b9-489b-b684-47fe5612e7fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.021511555s
    Mar 27 14:24:22.719: INFO: Pod "client-containers-660e9685-84b9-489b-b684-47fe5612e7fb" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar 27 14:24:22.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-1575" for this suite. 03/27/23 14:24:22.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:24:22.747
Mar 27 14:24:22.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename webhook 03/27/23 14:24:22.748
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:22.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:22.771
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/27/23 14:24:22.792
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 14:24:23.239
STEP: Deploying the webhook pod 03/27/23 14:24:23.247
STEP: Wait for the deployment to be ready 03/27/23 14:24:23.266
Mar 27 14:24:23.276: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 14:24:25.29
STEP: Verifying the service has paired with the endpoint 03/27/23 14:24:25.306
Mar 27 14:24:26.307: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/27/23 14:24:26.319
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/27/23 14:24:26.343
STEP: Creating a dummy validating-webhook-configuration object 03/27/23 14:24:26.368
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/27/23 14:24:26.379
STEP: Creating a dummy mutating-webhook-configuration object 03/27/23 14:24:26.386
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/27/23 14:24:26.401
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:24:26.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7668" for this suite. 03/27/23 14:24:26.424
STEP: Destroying namespace "webhook-7668-markers" for this suite. 03/27/23 14:24:26.431
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":45,"skipped":871,"failed":0}
------------------------------
• [3.732 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:24:22.747
    Mar 27 14:24:22.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename webhook 03/27/23 14:24:22.748
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:22.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:22.771
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/27/23 14:24:22.792
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 14:24:23.239
    STEP: Deploying the webhook pod 03/27/23 14:24:23.247
    STEP: Wait for the deployment to be ready 03/27/23 14:24:23.266
    Mar 27 14:24:23.276: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 14:24:25.29
    STEP: Verifying the service has paired with the endpoint 03/27/23 14:24:25.306
    Mar 27 14:24:26.307: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/27/23 14:24:26.319
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/27/23 14:24:26.343
    STEP: Creating a dummy validating-webhook-configuration object 03/27/23 14:24:26.368
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/27/23 14:24:26.379
    STEP: Creating a dummy mutating-webhook-configuration object 03/27/23 14:24:26.386
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/27/23 14:24:26.401
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:24:26.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7668" for this suite. 03/27/23 14:24:26.424
    STEP: Destroying namespace "webhook-7668-markers" for this suite. 03/27/23 14:24:26.431
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:24:26.481
Mar 27 14:24:26.481: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename downward-api 03/27/23 14:24:26.482
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:26.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:26.511
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 03/27/23 14:24:26.517
Mar 27 14:24:26.528: INFO: Waiting up to 5m0s for pod "downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4" in namespace "downward-api-5261" to be "Succeeded or Failed"
Mar 27 14:24:26.531: INFO: Pod "downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.140059ms
Mar 27 14:24:28.536: INFO: Pod "downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008201452s
Mar 27 14:24:30.536: INFO: Pod "downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4": Phase="Running", Reason="", readiness=true. Elapsed: 4.008292788s
Mar 27 14:24:32.537: INFO: Pod "downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4": Phase="Running", Reason="", readiness=false. Elapsed: 6.009233959s
Mar 27 14:24:34.536: INFO: Pod "downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008299374s
STEP: Saw pod success 03/27/23 14:24:34.536
Mar 27 14:24:34.536: INFO: Pod "downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4" satisfied condition "Succeeded or Failed"
Mar 27 14:24:34.544: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4 container dapi-container: <nil>
STEP: delete the pod 03/27/23 14:24:34.553
Mar 27 14:24:34.570: INFO: Waiting for pod downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4 to disappear
Mar 27 14:24:34.577: INFO: Pod downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar 27 14:24:34.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5261" for this suite. 03/27/23 14:24:34.582
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":46,"skipped":918,"failed":0}
------------------------------
• [SLOW TEST] [8.108 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:24:26.481
    Mar 27 14:24:26.481: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename downward-api 03/27/23 14:24:26.482
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:26.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:26.511
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 03/27/23 14:24:26.517
    Mar 27 14:24:26.528: INFO: Waiting up to 5m0s for pod "downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4" in namespace "downward-api-5261" to be "Succeeded or Failed"
    Mar 27 14:24:26.531: INFO: Pod "downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.140059ms
    Mar 27 14:24:28.536: INFO: Pod "downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008201452s
    Mar 27 14:24:30.536: INFO: Pod "downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4": Phase="Running", Reason="", readiness=true. Elapsed: 4.008292788s
    Mar 27 14:24:32.537: INFO: Pod "downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4": Phase="Running", Reason="", readiness=false. Elapsed: 6.009233959s
    Mar 27 14:24:34.536: INFO: Pod "downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008299374s
    STEP: Saw pod success 03/27/23 14:24:34.536
    Mar 27 14:24:34.536: INFO: Pod "downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4" satisfied condition "Succeeded or Failed"
    Mar 27 14:24:34.544: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4 container dapi-container: <nil>
    STEP: delete the pod 03/27/23 14:24:34.553
    Mar 27 14:24:34.570: INFO: Waiting for pod downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4 to disappear
    Mar 27 14:24:34.577: INFO: Pod downward-api-d984d968-3d43-4acf-bc7c-026ce28e50e4 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar 27 14:24:34.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5261" for this suite. 03/27/23 14:24:34.582
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:24:34.589
Mar 27 14:24:34.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename statefulset 03/27/23 14:24:34.59
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:34.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:34.612
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6053 03/27/23 14:24:34.615
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-6053 03/27/23 14:24:34.624
Mar 27 14:24:34.639: INFO: Found 0 stateful pods, waiting for 1
Mar 27 14:24:44.645: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 03/27/23 14:24:44.651
STEP: updating a scale subresource 03/27/23 14:24:44.655
STEP: verifying the statefulset Spec.Replicas was modified 03/27/23 14:24:44.664
STEP: Patch a scale subresource 03/27/23 14:24:44.667
STEP: verifying the statefulset Spec.Replicas was modified 03/27/23 14:24:44.678
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 27 14:24:44.685: INFO: Deleting all statefulset in ns statefulset-6053
Mar 27 14:24:44.692: INFO: Scaling statefulset ss to 0
Mar 27 14:24:54.733: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 14:24:54.738: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 27 14:24:54.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6053" for this suite. 03/27/23 14:24:54.755
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":47,"skipped":922,"failed":0}
------------------------------
• [SLOW TEST] [20.177 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:24:34.589
    Mar 27 14:24:34.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename statefulset 03/27/23 14:24:34.59
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:34.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:34.612
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6053 03/27/23 14:24:34.615
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-6053 03/27/23 14:24:34.624
    Mar 27 14:24:34.639: INFO: Found 0 stateful pods, waiting for 1
    Mar 27 14:24:44.645: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 03/27/23 14:24:44.651
    STEP: updating a scale subresource 03/27/23 14:24:44.655
    STEP: verifying the statefulset Spec.Replicas was modified 03/27/23 14:24:44.664
    STEP: Patch a scale subresource 03/27/23 14:24:44.667
    STEP: verifying the statefulset Spec.Replicas was modified 03/27/23 14:24:44.678
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 27 14:24:44.685: INFO: Deleting all statefulset in ns statefulset-6053
    Mar 27 14:24:44.692: INFO: Scaling statefulset ss to 0
    Mar 27 14:24:54.733: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 14:24:54.738: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 27 14:24:54.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6053" for this suite. 03/27/23 14:24:54.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:24:54.767
Mar 27 14:24:54.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename downward-api 03/27/23 14:24:54.768
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:54.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:54.789
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 03/27/23 14:24:54.792
Mar 27 14:24:54.803: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ba978db-3dd1-4576-bc05-53dc91a9ce8a" in namespace "downward-api-3217" to be "Succeeded or Failed"
Mar 27 14:24:54.807: INFO: Pod "downwardapi-volume-1ba978db-3dd1-4576-bc05-53dc91a9ce8a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034804ms
Mar 27 14:24:56.813: INFO: Pod "downwardapi-volume-1ba978db-3dd1-4576-bc05-53dc91a9ce8a": Phase="Running", Reason="", readiness=true. Elapsed: 2.010873374s
Mar 27 14:24:58.814: INFO: Pod "downwardapi-volume-1ba978db-3dd1-4576-bc05-53dc91a9ce8a": Phase="Running", Reason="", readiness=false. Elapsed: 4.011855533s
Mar 27 14:25:00.813: INFO: Pod "downwardapi-volume-1ba978db-3dd1-4576-bc05-53dc91a9ce8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010004997s
STEP: Saw pod success 03/27/23 14:25:00.813
Mar 27 14:25:00.813: INFO: Pod "downwardapi-volume-1ba978db-3dd1-4576-bc05-53dc91a9ce8a" satisfied condition "Succeeded or Failed"
Mar 27 14:25:00.817: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-1ba978db-3dd1-4576-bc05-53dc91a9ce8a container client-container: <nil>
STEP: delete the pod 03/27/23 14:25:00.828
Mar 27 14:25:00.847: INFO: Waiting for pod downwardapi-volume-1ba978db-3dd1-4576-bc05-53dc91a9ce8a to disappear
Mar 27 14:25:00.850: INFO: Pod downwardapi-volume-1ba978db-3dd1-4576-bc05-53dc91a9ce8a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 27 14:25:00.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3217" for this suite. 03/27/23 14:25:00.855
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":48,"skipped":935,"failed":0}
------------------------------
• [SLOW TEST] [6.096 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:24:54.767
    Mar 27 14:24:54.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename downward-api 03/27/23 14:24:54.768
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:24:54.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:24:54.789
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 03/27/23 14:24:54.792
    Mar 27 14:24:54.803: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ba978db-3dd1-4576-bc05-53dc91a9ce8a" in namespace "downward-api-3217" to be "Succeeded or Failed"
    Mar 27 14:24:54.807: INFO: Pod "downwardapi-volume-1ba978db-3dd1-4576-bc05-53dc91a9ce8a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034804ms
    Mar 27 14:24:56.813: INFO: Pod "downwardapi-volume-1ba978db-3dd1-4576-bc05-53dc91a9ce8a": Phase="Running", Reason="", readiness=true. Elapsed: 2.010873374s
    Mar 27 14:24:58.814: INFO: Pod "downwardapi-volume-1ba978db-3dd1-4576-bc05-53dc91a9ce8a": Phase="Running", Reason="", readiness=false. Elapsed: 4.011855533s
    Mar 27 14:25:00.813: INFO: Pod "downwardapi-volume-1ba978db-3dd1-4576-bc05-53dc91a9ce8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010004997s
    STEP: Saw pod success 03/27/23 14:25:00.813
    Mar 27 14:25:00.813: INFO: Pod "downwardapi-volume-1ba978db-3dd1-4576-bc05-53dc91a9ce8a" satisfied condition "Succeeded or Failed"
    Mar 27 14:25:00.817: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-1ba978db-3dd1-4576-bc05-53dc91a9ce8a container client-container: <nil>
    STEP: delete the pod 03/27/23 14:25:00.828
    Mar 27 14:25:00.847: INFO: Waiting for pod downwardapi-volume-1ba978db-3dd1-4576-bc05-53dc91a9ce8a to disappear
    Mar 27 14:25:00.850: INFO: Pod downwardapi-volume-1ba978db-3dd1-4576-bc05-53dc91a9ce8a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 27 14:25:00.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3217" for this suite. 03/27/23 14:25:00.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:25:00.864
Mar 27 14:25:00.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename daemonsets 03/27/23 14:25:00.865
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:25:00.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:25:00.887
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 03/27/23 14:25:00.925
STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 14:25:00.936
Mar 27 14:25:00.945: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 14:25:00.945: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 14:25:01.960: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 14:25:01.960: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 14:25:02.955: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 27 14:25:02.955: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 03/27/23 14:25:02.958
Mar 27 14:25:02.962: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 03/27/23 14:25:02.962
Mar 27 14:25:02.975: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 03/27/23 14:25:02.975
Mar 27 14:25:02.978: INFO: Observed &DaemonSet event: ADDED
Mar 27 14:25:02.978: INFO: Observed &DaemonSet event: MODIFIED
Mar 27 14:25:02.978: INFO: Observed &DaemonSet event: MODIFIED
Mar 27 14:25:02.978: INFO: Observed &DaemonSet event: MODIFIED
Mar 27 14:25:02.978: INFO: Observed &DaemonSet event: MODIFIED
Mar 27 14:25:02.979: INFO: Found daemon set daemon-set in namespace daemonsets-7107 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 27 14:25:02.979: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 03/27/23 14:25:02.979
STEP: watching for the daemon set status to be patched 03/27/23 14:25:02.988
Mar 27 14:25:02.990: INFO: Observed &DaemonSet event: ADDED
Mar 27 14:25:02.991: INFO: Observed &DaemonSet event: MODIFIED
Mar 27 14:25:02.991: INFO: Observed &DaemonSet event: MODIFIED
Mar 27 14:25:02.991: INFO: Observed &DaemonSet event: MODIFIED
Mar 27 14:25:02.991: INFO: Observed &DaemonSet event: MODIFIED
Mar 27 14:25:02.991: INFO: Observed daemon set daemon-set in namespace daemonsets-7107 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 27 14:25:02.991: INFO: Observed &DaemonSet event: MODIFIED
Mar 27 14:25:02.991: INFO: Found daemon set daemon-set in namespace daemonsets-7107 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Mar 27 14:25:02.991: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/27/23 14:25:02.996
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7107, will wait for the garbage collector to delete the pods 03/27/23 14:25:02.996
Mar 27 14:25:03.059: INFO: Deleting DaemonSet.extensions daemon-set took: 9.074143ms
Mar 27 14:25:03.160: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.674308ms
Mar 27 14:25:06.665: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 14:25:06.665: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 27 14:25:06.668: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15003"},"items":null}

Mar 27 14:25:06.670: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15003"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 27 14:25:06.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7107" for this suite. 03/27/23 14:25:06.693
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":49,"skipped":960,"failed":0}
------------------------------
• [SLOW TEST] [5.836 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:25:00.864
    Mar 27 14:25:00.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename daemonsets 03/27/23 14:25:00.865
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:25:00.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:25:00.887
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 03/27/23 14:25:00.925
    STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 14:25:00.936
    Mar 27 14:25:00.945: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 14:25:00.945: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 14:25:01.960: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 14:25:01.960: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 14:25:02.955: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 27 14:25:02.955: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 03/27/23 14:25:02.958
    Mar 27 14:25:02.962: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 03/27/23 14:25:02.962
    Mar 27 14:25:02.975: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 03/27/23 14:25:02.975
    Mar 27 14:25:02.978: INFO: Observed &DaemonSet event: ADDED
    Mar 27 14:25:02.978: INFO: Observed &DaemonSet event: MODIFIED
    Mar 27 14:25:02.978: INFO: Observed &DaemonSet event: MODIFIED
    Mar 27 14:25:02.978: INFO: Observed &DaemonSet event: MODIFIED
    Mar 27 14:25:02.978: INFO: Observed &DaemonSet event: MODIFIED
    Mar 27 14:25:02.979: INFO: Found daemon set daemon-set in namespace daemonsets-7107 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 27 14:25:02.979: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 03/27/23 14:25:02.979
    STEP: watching for the daemon set status to be patched 03/27/23 14:25:02.988
    Mar 27 14:25:02.990: INFO: Observed &DaemonSet event: ADDED
    Mar 27 14:25:02.991: INFO: Observed &DaemonSet event: MODIFIED
    Mar 27 14:25:02.991: INFO: Observed &DaemonSet event: MODIFIED
    Mar 27 14:25:02.991: INFO: Observed &DaemonSet event: MODIFIED
    Mar 27 14:25:02.991: INFO: Observed &DaemonSet event: MODIFIED
    Mar 27 14:25:02.991: INFO: Observed daemon set daemon-set in namespace daemonsets-7107 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 27 14:25:02.991: INFO: Observed &DaemonSet event: MODIFIED
    Mar 27 14:25:02.991: INFO: Found daemon set daemon-set in namespace daemonsets-7107 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Mar 27 14:25:02.991: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/27/23 14:25:02.996
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7107, will wait for the garbage collector to delete the pods 03/27/23 14:25:02.996
    Mar 27 14:25:03.059: INFO: Deleting DaemonSet.extensions daemon-set took: 9.074143ms
    Mar 27 14:25:03.160: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.674308ms
    Mar 27 14:25:06.665: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 14:25:06.665: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 27 14:25:06.668: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15003"},"items":null}

    Mar 27 14:25:06.670: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15003"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 14:25:06.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7107" for this suite. 03/27/23 14:25:06.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:25:06.706
Mar 27 14:25:06.706: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename runtimeclass 03/27/23 14:25:06.707
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:25:06.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:25:06.729
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Mar 27 14:25:06.750: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5996 to be scheduled
Mar 27 14:25:06.754: INFO: 1 pods are not scheduled: [runtimeclass-5996/test-runtimeclass-runtimeclass-5996-preconfigured-handler-vq4qj(6d955bf0-4253-46fd-b312-28d1b6603523)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar 27 14:25:08.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5996" for this suite. 03/27/23 14:25:08.878
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":50,"skipped":1018,"failed":0}
------------------------------
• [2.177 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:25:06.706
    Mar 27 14:25:06.706: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename runtimeclass 03/27/23 14:25:06.707
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:25:06.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:25:06.729
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Mar 27 14:25:06.750: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5996 to be scheduled
    Mar 27 14:25:06.754: INFO: 1 pods are not scheduled: [runtimeclass-5996/test-runtimeclass-runtimeclass-5996-preconfigured-handler-vq4qj(6d955bf0-4253-46fd-b312-28d1b6603523)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar 27 14:25:08.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5996" for this suite. 03/27/23 14:25:08.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:25:08.887
Mar 27 14:25:08.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename statefulset 03/27/23 14:25:08.888
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:25:08.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:25:08.98
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7981 03/27/23 14:25:08.983
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-7981 03/27/23 14:25:08.989
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7981 03/27/23 14:25:09.319
Mar 27 14:25:09.834: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Mar 27 14:25:19.840: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/27/23 14:25:19.84
Mar 27 14:25:19.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-7981 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 14:25:20.018: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 14:25:20.018: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 14:25:20.018: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 27 14:25:20.027: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 27 14:25:30.032: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 27 14:25:30.032: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 14:25:30.055: INFO: POD   NODE                                                         PHASE    GRACE  CONDITIONS
Mar 27 14:25:30.055: INFO: ss-0  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:09 +0000 UTC  }]
Mar 27 14:25:30.055: INFO: ss-1                                                               Pending         []
Mar 27 14:25:30.055: INFO: 
Mar 27 14:25:30.055: INFO: StatefulSet ss has not reached scale 3, at 2
Mar 27 14:25:31.062: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994301425s
Mar 27 14:25:32.068: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986762103s
Mar 27 14:25:33.072: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981287168s
Mar 27 14:25:34.077: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976652604s
Mar 27 14:25:35.083: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972280983s
Mar 27 14:25:36.088: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.966362306s
Mar 27 14:25:37.096: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.960717587s
Mar 27 14:25:38.102: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.952494027s
Mar 27 14:25:39.106: INFO: Verifying statefulset ss doesn't scale past 3 for another 947.244981ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7981 03/27/23 14:25:40.107
Mar 27 14:25:40.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-7981 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 27 14:25:40.277: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 27 14:25:40.277: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 27 14:25:40.277: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 27 14:25:40.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-7981 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 27 14:25:40.545: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 27 14:25:40.545: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 27 14:25:40.545: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 27 14:25:40.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-7981 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 27 14:25:40.713: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 27 14:25:40.713: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 27 14:25:40.713: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 27 14:25:40.719: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Mar 27 14:25:50.728: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 14:25:50.728: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 14:25:50.728: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 03/27/23 14:25:50.728
Mar 27 14:25:50.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-7981 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 14:25:50.916: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 14:25:50.916: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 14:25:50.916: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 27 14:25:50.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-7981 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 14:25:51.063: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 14:25:51.063: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 14:25:51.063: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 27 14:25:51.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-7981 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 14:25:51.254: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 14:25:51.254: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 14:25:51.254: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 27 14:25:51.254: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 14:25:51.259: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 27 14:26:01.272: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 27 14:26:01.272: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 27 14:26:01.272: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 27 14:26:01.287: INFO: POD   NODE                                                         PHASE    GRACE  CONDITIONS
Mar 27 14:26:01.287: INFO: ss-0  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:09 +0000 UTC  }]
Mar 27 14:26:01.287: INFO: ss-1  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:30 +0000 UTC  }]
Mar 27 14:26:01.287: INFO: ss-2  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:30 +0000 UTC  }]
Mar 27 14:26:01.287: INFO: 
Mar 27 14:26:01.287: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 27 14:26:02.294: INFO: POD   NODE                                                         PHASE    GRACE  CONDITIONS
Mar 27 14:26:02.294: INFO: ss-0  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:09 +0000 UTC  }]
Mar 27 14:26:02.294: INFO: ss-2  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:30 +0000 UTC  }]
Mar 27 14:26:02.294: INFO: 
Mar 27 14:26:02.294: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 27 14:26:03.303: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.989395253s
Mar 27 14:26:04.307: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.979747014s
Mar 27 14:26:05.312: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.975518585s
Mar 27 14:26:06.317: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.971145595s
Mar 27 14:26:07.327: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.966242125s
Mar 27 14:26:08.331: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.955612377s
Mar 27 14:26:09.337: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.951986043s
Mar 27 14:26:10.344: INFO: Verifying statefulset ss doesn't scale past 0 for another 946.376875ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7981 03/27/23 14:26:11.344
Mar 27 14:26:11.352: INFO: Scaling statefulset ss to 0
Mar 27 14:26:11.365: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 27 14:26:11.369: INFO: Deleting all statefulset in ns statefulset-7981
Mar 27 14:26:11.373: INFO: Scaling statefulset ss to 0
Mar 27 14:26:11.387: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 14:26:11.393: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 27 14:26:11.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7981" for this suite. 03/27/23 14:26:11.415
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":51,"skipped":1044,"failed":0}
------------------------------
• [SLOW TEST] [62.535 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:25:08.887
    Mar 27 14:25:08.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename statefulset 03/27/23 14:25:08.888
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:25:08.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:25:08.98
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7981 03/27/23 14:25:08.983
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-7981 03/27/23 14:25:08.989
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7981 03/27/23 14:25:09.319
    Mar 27 14:25:09.834: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
    Mar 27 14:25:19.840: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/27/23 14:25:19.84
    Mar 27 14:25:19.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-7981 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 14:25:20.018: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 14:25:20.018: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 14:25:20.018: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 27 14:25:20.027: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar 27 14:25:30.032: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 27 14:25:30.032: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 14:25:30.055: INFO: POD   NODE                                                         PHASE    GRACE  CONDITIONS
    Mar 27 14:25:30.055: INFO: ss-0  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:09 +0000 UTC  }]
    Mar 27 14:25:30.055: INFO: ss-1                                                               Pending         []
    Mar 27 14:25:30.055: INFO: 
    Mar 27 14:25:30.055: INFO: StatefulSet ss has not reached scale 3, at 2
    Mar 27 14:25:31.062: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994301425s
    Mar 27 14:25:32.068: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986762103s
    Mar 27 14:25:33.072: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981287168s
    Mar 27 14:25:34.077: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976652604s
    Mar 27 14:25:35.083: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972280983s
    Mar 27 14:25:36.088: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.966362306s
    Mar 27 14:25:37.096: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.960717587s
    Mar 27 14:25:38.102: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.952494027s
    Mar 27 14:25:39.106: INFO: Verifying statefulset ss doesn't scale past 3 for another 947.244981ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7981 03/27/23 14:25:40.107
    Mar 27 14:25:40.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-7981 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 27 14:25:40.277: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 27 14:25:40.277: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 27 14:25:40.277: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 27 14:25:40.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-7981 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 27 14:25:40.545: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar 27 14:25:40.545: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 27 14:25:40.545: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 27 14:25:40.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-7981 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 27 14:25:40.713: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar 27 14:25:40.713: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 27 14:25:40.713: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 27 14:25:40.719: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Mar 27 14:25:50.728: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 14:25:50.728: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 14:25:50.728: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 03/27/23 14:25:50.728
    Mar 27 14:25:50.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-7981 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 14:25:50.916: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 14:25:50.916: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 14:25:50.916: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 27 14:25:50.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-7981 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 14:25:51.063: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 14:25:51.063: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 14:25:51.063: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 27 14:25:51.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-7981 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 14:25:51.254: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 14:25:51.254: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 14:25:51.254: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 27 14:25:51.254: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 14:25:51.259: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Mar 27 14:26:01.272: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 27 14:26:01.272: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar 27 14:26:01.272: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar 27 14:26:01.287: INFO: POD   NODE                                                         PHASE    GRACE  CONDITIONS
    Mar 27 14:26:01.287: INFO: ss-0  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:09 +0000 UTC  }]
    Mar 27 14:26:01.287: INFO: ss-1  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:30 +0000 UTC  }]
    Mar 27 14:26:01.287: INFO: ss-2  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:30 +0000 UTC  }]
    Mar 27 14:26:01.287: INFO: 
    Mar 27 14:26:01.287: INFO: StatefulSet ss has not reached scale 0, at 3
    Mar 27 14:26:02.294: INFO: POD   NODE                                                         PHASE    GRACE  CONDITIONS
    Mar 27 14:26:02.294: INFO: ss-0  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:09 +0000 UTC  }]
    Mar 27 14:26:02.294: INFO: ss-2  k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:25:30 +0000 UTC  }]
    Mar 27 14:26:02.294: INFO: 
    Mar 27 14:26:02.294: INFO: StatefulSet ss has not reached scale 0, at 2
    Mar 27 14:26:03.303: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.989395253s
    Mar 27 14:26:04.307: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.979747014s
    Mar 27 14:26:05.312: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.975518585s
    Mar 27 14:26:06.317: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.971145595s
    Mar 27 14:26:07.327: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.966242125s
    Mar 27 14:26:08.331: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.955612377s
    Mar 27 14:26:09.337: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.951986043s
    Mar 27 14:26:10.344: INFO: Verifying statefulset ss doesn't scale past 0 for another 946.376875ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7981 03/27/23 14:26:11.344
    Mar 27 14:26:11.352: INFO: Scaling statefulset ss to 0
    Mar 27 14:26:11.365: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 27 14:26:11.369: INFO: Deleting all statefulset in ns statefulset-7981
    Mar 27 14:26:11.373: INFO: Scaling statefulset ss to 0
    Mar 27 14:26:11.387: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 14:26:11.393: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 27 14:26:11.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7981" for this suite. 03/27/23 14:26:11.415
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:26:11.424
Mar 27 14:26:11.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 14:26:11.426
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:26:11.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:26:11.451
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/27/23 14:26:11.455
Mar 27 14:26:11.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/27/23 14:26:43.742
Mar 27 14:26:43.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:26:52.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:27:25.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7413" for this suite. 03/27/23 14:27:25.647
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":52,"skipped":1058,"failed":0}
------------------------------
• [SLOW TEST] [74.230 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:26:11.424
    Mar 27 14:26:11.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 14:26:11.426
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:26:11.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:26:11.451
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/27/23 14:26:11.455
    Mar 27 14:26:11.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/27/23 14:26:43.742
    Mar 27 14:26:43.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:26:52.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:27:25.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7413" for this suite. 03/27/23 14:27:25.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:27:25.655
Mar 27 14:27:25.656: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename resourcequota 03/27/23 14:27:25.657
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:27:25.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:27:25.682
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 03/27/23 14:27:25.685
STEP: Creating a ResourceQuota 03/27/23 14:27:30.689
STEP: Ensuring resource quota status is calculated 03/27/23 14:27:30.699
STEP: Creating a ReplicationController 03/27/23 14:27:32.705
STEP: Ensuring resource quota status captures replication controller creation 03/27/23 14:27:32.721
STEP: Deleting a ReplicationController 03/27/23 14:27:34.725
STEP: Ensuring resource quota status released usage 03/27/23 14:27:34.732
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 27 14:27:36.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3418" for this suite. 03/27/23 14:27:36.749
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":53,"skipped":1075,"failed":0}
------------------------------
• [SLOW TEST] [11.101 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:27:25.655
    Mar 27 14:27:25.656: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename resourcequota 03/27/23 14:27:25.657
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:27:25.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:27:25.682
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 03/27/23 14:27:25.685
    STEP: Creating a ResourceQuota 03/27/23 14:27:30.689
    STEP: Ensuring resource quota status is calculated 03/27/23 14:27:30.699
    STEP: Creating a ReplicationController 03/27/23 14:27:32.705
    STEP: Ensuring resource quota status captures replication controller creation 03/27/23 14:27:32.721
    STEP: Deleting a ReplicationController 03/27/23 14:27:34.725
    STEP: Ensuring resource quota status released usage 03/27/23 14:27:34.732
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 27 14:27:36.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3418" for this suite. 03/27/23 14:27:36.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:27:36.757
Mar 27 14:27:36.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename emptydir-wrapper 03/27/23 14:27:36.758
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:27:36.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:27:36.782
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Mar 27 14:27:36.823: INFO: Waiting up to 5m0s for pod "pod-secrets-357924f4-7381-41f1-9e49-9f04c855b1ba" in namespace "emptydir-wrapper-49" to be "running and ready"
Mar 27 14:27:36.830: INFO: Pod "pod-secrets-357924f4-7381-41f1-9e49-9f04c855b1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 7.149605ms
Mar 27 14:27:36.830: INFO: The phase of Pod pod-secrets-357924f4-7381-41f1-9e49-9f04c855b1ba is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:27:38.837: INFO: Pod "pod-secrets-357924f4-7381-41f1-9e49-9f04c855b1ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.014005348s
Mar 27 14:27:38.837: INFO: The phase of Pod pod-secrets-357924f4-7381-41f1-9e49-9f04c855b1ba is Running (Ready = true)
Mar 27 14:27:38.837: INFO: Pod "pod-secrets-357924f4-7381-41f1-9e49-9f04c855b1ba" satisfied condition "running and ready"
STEP: Cleaning up the secret 03/27/23 14:27:38.842
STEP: Cleaning up the configmap 03/27/23 14:27:38.849
STEP: Cleaning up the pod 03/27/23 14:27:38.856
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Mar 27 14:27:38.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-49" for this suite. 03/27/23 14:27:38.88
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":54,"skipped":1084,"failed":0}
------------------------------
• [2.133 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:27:36.757
    Mar 27 14:27:36.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename emptydir-wrapper 03/27/23 14:27:36.758
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:27:36.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:27:36.782
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Mar 27 14:27:36.823: INFO: Waiting up to 5m0s for pod "pod-secrets-357924f4-7381-41f1-9e49-9f04c855b1ba" in namespace "emptydir-wrapper-49" to be "running and ready"
    Mar 27 14:27:36.830: INFO: Pod "pod-secrets-357924f4-7381-41f1-9e49-9f04c855b1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 7.149605ms
    Mar 27 14:27:36.830: INFO: The phase of Pod pod-secrets-357924f4-7381-41f1-9e49-9f04c855b1ba is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:27:38.837: INFO: Pod "pod-secrets-357924f4-7381-41f1-9e49-9f04c855b1ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.014005348s
    Mar 27 14:27:38.837: INFO: The phase of Pod pod-secrets-357924f4-7381-41f1-9e49-9f04c855b1ba is Running (Ready = true)
    Mar 27 14:27:38.837: INFO: Pod "pod-secrets-357924f4-7381-41f1-9e49-9f04c855b1ba" satisfied condition "running and ready"
    STEP: Cleaning up the secret 03/27/23 14:27:38.842
    STEP: Cleaning up the configmap 03/27/23 14:27:38.849
    STEP: Cleaning up the pod 03/27/23 14:27:38.856
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Mar 27 14:27:38.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-49" for this suite. 03/27/23 14:27:38.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:27:38.892
Mar 27 14:27:38.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename webhook 03/27/23 14:27:38.893
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:27:38.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:27:38.916
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/27/23 14:27:38.941
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 14:27:39.42
STEP: Deploying the webhook pod 03/27/23 14:27:39.434
STEP: Wait for the deployment to be ready 03/27/23 14:27:39.455
Mar 27 14:27:39.465: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 27 14:27:41.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 27, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 27, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 27, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 27, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/27/23 14:27:43.483
STEP: Verifying the service has paired with the endpoint 03/27/23 14:27:43.493
Mar 27 14:27:44.495: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 03/27/23 14:27:44.499
STEP: create a pod 03/27/23 14:27:44.523
Mar 27 14:27:44.531: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-9309" to be "running"
Mar 27 14:27:44.539: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.275573ms
Mar 27 14:27:46.543: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012601592s
Mar 27 14:27:48.545: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014206391s
Mar 27 14:27:48.545: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 03/27/23 14:27:48.545
Mar 27 14:27:48.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=webhook-9309 attach --namespace=webhook-9309 to-be-attached-pod -i -c=container1'
Mar 27 14:27:48.638: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:27:48.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9309" for this suite. 03/27/23 14:27:48.653
STEP: Destroying namespace "webhook-9309-markers" for this suite. 03/27/23 14:27:48.661
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":55,"skipped":1096,"failed":0}
------------------------------
• [SLOW TEST] [9.816 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:27:38.892
    Mar 27 14:27:38.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename webhook 03/27/23 14:27:38.893
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:27:38.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:27:38.916
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/27/23 14:27:38.941
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 14:27:39.42
    STEP: Deploying the webhook pod 03/27/23 14:27:39.434
    STEP: Wait for the deployment to be ready 03/27/23 14:27:39.455
    Mar 27 14:27:39.465: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar 27 14:27:41.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 14, 27, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 27, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 14, 27, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 14, 27, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/27/23 14:27:43.483
    STEP: Verifying the service has paired with the endpoint 03/27/23 14:27:43.493
    Mar 27 14:27:44.495: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 03/27/23 14:27:44.499
    STEP: create a pod 03/27/23 14:27:44.523
    Mar 27 14:27:44.531: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-9309" to be "running"
    Mar 27 14:27:44.539: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.275573ms
    Mar 27 14:27:46.543: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012601592s
    Mar 27 14:27:48.545: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014206391s
    Mar 27 14:27:48.545: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 03/27/23 14:27:48.545
    Mar 27 14:27:48.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=webhook-9309 attach --namespace=webhook-9309 to-be-attached-pod -i -c=container1'
    Mar 27 14:27:48.638: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:27:48.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9309" for this suite. 03/27/23 14:27:48.653
    STEP: Destroying namespace "webhook-9309-markers" for this suite. 03/27/23 14:27:48.661
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:27:48.709
Mar 27 14:27:48.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename init-container 03/27/23 14:27:48.71
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:27:48.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:27:48.732
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 03/27/23 14:27:48.736
Mar 27 14:27:48.736: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 27 14:27:52.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6377" for this suite. 03/27/23 14:27:52.688
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":56,"skipped":1129,"failed":0}
------------------------------
• [3.987 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:27:48.709
    Mar 27 14:27:48.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename init-container 03/27/23 14:27:48.71
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:27:48.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:27:48.732
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 03/27/23 14:27:48.736
    Mar 27 14:27:48.736: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 27 14:27:52.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-6377" for this suite. 03/27/23 14:27:52.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:27:52.699
Mar 27 14:27:52.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename downward-api 03/27/23 14:27:52.7
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:27:52.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:27:52.723
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 03/27/23 14:27:52.726
Mar 27 14:27:52.740: INFO: Waiting up to 5m0s for pod "downward-api-c9fec884-6edb-4473-8e4b-1241a58be4c9" in namespace "downward-api-6390" to be "Succeeded or Failed"
Mar 27 14:27:52.746: INFO: Pod "downward-api-c9fec884-6edb-4473-8e4b-1241a58be4c9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.543358ms
Mar 27 14:27:54.753: INFO: Pod "downward-api-c9fec884-6edb-4473-8e4b-1241a58be4c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012701002s
Mar 27 14:27:56.778: INFO: Pod "downward-api-c9fec884-6edb-4473-8e4b-1241a58be4c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037775279s
STEP: Saw pod success 03/27/23 14:27:56.778
Mar 27 14:27:56.778: INFO: Pod "downward-api-c9fec884-6edb-4473-8e4b-1241a58be4c9" satisfied condition "Succeeded or Failed"
Mar 27 14:27:56.781: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downward-api-c9fec884-6edb-4473-8e4b-1241a58be4c9 container dapi-container: <nil>
STEP: delete the pod 03/27/23 14:27:56.792
Mar 27 14:27:56.806: INFO: Waiting for pod downward-api-c9fec884-6edb-4473-8e4b-1241a58be4c9 to disappear
Mar 27 14:27:56.809: INFO: Pod downward-api-c9fec884-6edb-4473-8e4b-1241a58be4c9 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar 27 14:27:56.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6390" for this suite. 03/27/23 14:27:56.816
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":57,"skipped":1152,"failed":0}
------------------------------
• [4.124 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:27:52.699
    Mar 27 14:27:52.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename downward-api 03/27/23 14:27:52.7
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:27:52.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:27:52.723
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 03/27/23 14:27:52.726
    Mar 27 14:27:52.740: INFO: Waiting up to 5m0s for pod "downward-api-c9fec884-6edb-4473-8e4b-1241a58be4c9" in namespace "downward-api-6390" to be "Succeeded or Failed"
    Mar 27 14:27:52.746: INFO: Pod "downward-api-c9fec884-6edb-4473-8e4b-1241a58be4c9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.543358ms
    Mar 27 14:27:54.753: INFO: Pod "downward-api-c9fec884-6edb-4473-8e4b-1241a58be4c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012701002s
    Mar 27 14:27:56.778: INFO: Pod "downward-api-c9fec884-6edb-4473-8e4b-1241a58be4c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037775279s
    STEP: Saw pod success 03/27/23 14:27:56.778
    Mar 27 14:27:56.778: INFO: Pod "downward-api-c9fec884-6edb-4473-8e4b-1241a58be4c9" satisfied condition "Succeeded or Failed"
    Mar 27 14:27:56.781: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downward-api-c9fec884-6edb-4473-8e4b-1241a58be4c9 container dapi-container: <nil>
    STEP: delete the pod 03/27/23 14:27:56.792
    Mar 27 14:27:56.806: INFO: Waiting for pod downward-api-c9fec884-6edb-4473-8e4b-1241a58be4c9 to disappear
    Mar 27 14:27:56.809: INFO: Pod downward-api-c9fec884-6edb-4473-8e4b-1241a58be4c9 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar 27 14:27:56.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6390" for this suite. 03/27/23 14:27:56.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:27:56.825
Mar 27 14:27:56.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename services 03/27/23 14:27:56.826
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:27:56.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:27:56.85
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-2002 03/27/23 14:27:56.854
STEP: creating service affinity-nodeport in namespace services-2002 03/27/23 14:27:56.854
STEP: creating replication controller affinity-nodeport in namespace services-2002 03/27/23 14:27:56.871
I0327 14:27:56.883536      24 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-2002, replica count: 3
I0327 14:27:59.935025      24 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 14:27:59.946: INFO: Creating new exec pod
Mar 27 14:27:59.954: INFO: Waiting up to 5m0s for pod "execpod-affinityb4r79" in namespace "services-2002" to be "running"
Mar 27 14:27:59.957: INFO: Pod "execpod-affinityb4r79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.774902ms
Mar 27 14:28:01.962: INFO: Pod "execpod-affinityb4r79": Phase="Running", Reason="", readiness=true. Elapsed: 2.007732545s
Mar 27 14:28:01.962: INFO: Pod "execpod-affinityb4r79" satisfied condition "running"
Mar 27 14:28:02.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2002 exec execpod-affinityb4r79 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Mar 27 14:28:03.149: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar 27 14:28:03.149: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 14:28:03.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2002 exec execpod-affinityb4r79 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.19.114 80'
Mar 27 14:28:03.316: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.19.114 80\nConnection to 10.240.19.114 80 port [tcp/http] succeeded!\n"
Mar 27 14:28:03.316: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 14:28:03.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2002 exec execpod-affinityb4r79 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.4 30483'
Mar 27 14:28:03.480: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.4 30483\nConnection to 192.168.1.4 30483 port [tcp/*] succeeded!\n"
Mar 27 14:28:03.480: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 14:28:03.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2002 exec execpod-affinityb4r79 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.7 30483'
Mar 27 14:28:03.655: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.7 30483\nConnection to 192.168.1.7 30483 port [tcp/*] succeeded!\n"
Mar 27 14:28:03.655: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 14:28:03.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2002 exec execpod-affinityb4r79 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.3:30483/ ; done'
Mar 27 14:28:03.969: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n"
Mar 27 14:28:03.969: INFO: stdout: "\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s"
Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
Mar 27 14:28:03.969: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2002, will wait for the garbage collector to delete the pods 03/27/23 14:28:03.987
Mar 27 14:28:04.050: INFO: Deleting ReplicationController affinity-nodeport took: 7.418745ms
Mar 27 14:28:04.151: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.972503ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 27 14:28:06.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2002" for this suite. 03/27/23 14:28:06.885
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":58,"skipped":1191,"failed":0}
------------------------------
• [SLOW TEST] [10.070 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:27:56.825
    Mar 27 14:27:56.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename services 03/27/23 14:27:56.826
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:27:56.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:27:56.85
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-2002 03/27/23 14:27:56.854
    STEP: creating service affinity-nodeport in namespace services-2002 03/27/23 14:27:56.854
    STEP: creating replication controller affinity-nodeport in namespace services-2002 03/27/23 14:27:56.871
    I0327 14:27:56.883536      24 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-2002, replica count: 3
    I0327 14:27:59.935025      24 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 14:27:59.946: INFO: Creating new exec pod
    Mar 27 14:27:59.954: INFO: Waiting up to 5m0s for pod "execpod-affinityb4r79" in namespace "services-2002" to be "running"
    Mar 27 14:27:59.957: INFO: Pod "execpod-affinityb4r79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.774902ms
    Mar 27 14:28:01.962: INFO: Pod "execpod-affinityb4r79": Phase="Running", Reason="", readiness=true. Elapsed: 2.007732545s
    Mar 27 14:28:01.962: INFO: Pod "execpod-affinityb4r79" satisfied condition "running"
    Mar 27 14:28:02.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2002 exec execpod-affinityb4r79 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Mar 27 14:28:03.149: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Mar 27 14:28:03.149: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 14:28:03.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2002 exec execpod-affinityb4r79 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.19.114 80'
    Mar 27 14:28:03.316: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.19.114 80\nConnection to 10.240.19.114 80 port [tcp/http] succeeded!\n"
    Mar 27 14:28:03.316: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 14:28:03.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2002 exec execpod-affinityb4r79 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.4 30483'
    Mar 27 14:28:03.480: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.4 30483\nConnection to 192.168.1.4 30483 port [tcp/*] succeeded!\n"
    Mar 27 14:28:03.480: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 14:28:03.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2002 exec execpod-affinityb4r79 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.7 30483'
    Mar 27 14:28:03.655: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.7 30483\nConnection to 192.168.1.7 30483 port [tcp/*] succeeded!\n"
    Mar 27 14:28:03.655: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 14:28:03.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2002 exec execpod-affinityb4r79 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.3:30483/ ; done'
    Mar 27 14:28:03.969: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:30483/\n"
    Mar 27 14:28:03.969: INFO: stdout: "\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s\naffinity-nodeport-59c9s"
    Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
    Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
    Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
    Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
    Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
    Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
    Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
    Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
    Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
    Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
    Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
    Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
    Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
    Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
    Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
    Mar 27 14:28:03.969: INFO: Received response from host: affinity-nodeport-59c9s
    Mar 27 14:28:03.969: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-2002, will wait for the garbage collector to delete the pods 03/27/23 14:28:03.987
    Mar 27 14:28:04.050: INFO: Deleting ReplicationController affinity-nodeport took: 7.418745ms
    Mar 27 14:28:04.151: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.972503ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 27 14:28:06.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2002" for this suite. 03/27/23 14:28:06.885
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:28:06.897
Mar 27 14:28:06.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/27/23 14:28:06.899
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:06.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:06.922
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 03/27/23 14:28:06.926
STEP: Creating hostNetwork=false pod 03/27/23 14:28:06.926
Mar 27 14:28:06.943: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-9659" to be "running and ready"
Mar 27 14:28:06.947: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.672333ms
Mar 27 14:28:06.947: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:28:08.952: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008825581s
Mar 27 14:28:08.952: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:28:10.953: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010452371s
Mar 27 14:28:10.953: INFO: The phase of Pod test-pod is Running (Ready = true)
Mar 27 14:28:10.953: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 03/27/23 14:28:10.957
Mar 27 14:28:10.969: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-9659" to be "running and ready"
Mar 27 14:28:10.973: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.221212ms
Mar 27 14:28:10.973: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:28:12.978: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009166987s
Mar 27 14:28:12.978: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Mar 27 14:28:12.978: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 03/27/23 14:28:12.981
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/27/23 14:28:12.982
Mar 27 14:28:12.982: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:28:12.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:28:12.982: INFO: ExecWithOptions: Clientset creation
Mar 27 14:28:12.982: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 27 14:28:13.084: INFO: Exec stderr: ""
Mar 27 14:28:13.084: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:28:13.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:28:13.084: INFO: ExecWithOptions: Clientset creation
Mar 27 14:28:13.084: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 27 14:28:13.183: INFO: Exec stderr: ""
Mar 27 14:28:13.183: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:28:13.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:28:13.184: INFO: ExecWithOptions: Clientset creation
Mar 27 14:28:13.184: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 27 14:28:13.281: INFO: Exec stderr: ""
Mar 27 14:28:13.281: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:28:13.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:28:13.281: INFO: ExecWithOptions: Clientset creation
Mar 27 14:28:13.282: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 27 14:28:13.371: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/27/23 14:28:13.371
Mar 27 14:28:13.371: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:28:13.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:28:13.372: INFO: ExecWithOptions: Clientset creation
Mar 27 14:28:13.372: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar 27 14:28:13.470: INFO: Exec stderr: ""
Mar 27 14:28:13.470: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:28:13.470: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:28:13.471: INFO: ExecWithOptions: Clientset creation
Mar 27 14:28:13.471: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar 27 14:28:13.566: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/27/23 14:28:13.566
Mar 27 14:28:13.567: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:28:13.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:28:13.567: INFO: ExecWithOptions: Clientset creation
Mar 27 14:28:13.567: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 27 14:28:13.654: INFO: Exec stderr: ""
Mar 27 14:28:13.654: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:28:13.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:28:13.655: INFO: ExecWithOptions: Clientset creation
Mar 27 14:28:13.655: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 27 14:28:13.735: INFO: Exec stderr: ""
Mar 27 14:28:13.735: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:28:13.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:28:13.736: INFO: ExecWithOptions: Clientset creation
Mar 27 14:28:13.736: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 27 14:28:13.818: INFO: Exec stderr: ""
Mar 27 14:28:13.818: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:28:13.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:28:13.818: INFO: ExecWithOptions: Clientset creation
Mar 27 14:28:13.818: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 27 14:28:13.899: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Mar 27 14:28:13.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9659" for this suite. 03/27/23 14:28:13.904
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":59,"skipped":1227,"failed":0}
------------------------------
• [SLOW TEST] [7.014 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:28:06.897
    Mar 27 14:28:06.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/27/23 14:28:06.899
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:06.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:06.922
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 03/27/23 14:28:06.926
    STEP: Creating hostNetwork=false pod 03/27/23 14:28:06.926
    Mar 27 14:28:06.943: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-9659" to be "running and ready"
    Mar 27 14:28:06.947: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.672333ms
    Mar 27 14:28:06.947: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:28:08.952: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008825581s
    Mar 27 14:28:08.952: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:28:10.953: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010452371s
    Mar 27 14:28:10.953: INFO: The phase of Pod test-pod is Running (Ready = true)
    Mar 27 14:28:10.953: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 03/27/23 14:28:10.957
    Mar 27 14:28:10.969: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-9659" to be "running and ready"
    Mar 27 14:28:10.973: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.221212ms
    Mar 27 14:28:10.973: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:28:12.978: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009166987s
    Mar 27 14:28:12.978: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Mar 27 14:28:12.978: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 03/27/23 14:28:12.981
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/27/23 14:28:12.982
    Mar 27 14:28:12.982: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:28:12.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:28:12.982: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:28:12.982: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 27 14:28:13.084: INFO: Exec stderr: ""
    Mar 27 14:28:13.084: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:28:13.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:28:13.084: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:28:13.084: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 27 14:28:13.183: INFO: Exec stderr: ""
    Mar 27 14:28:13.183: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:28:13.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:28:13.184: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:28:13.184: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 27 14:28:13.281: INFO: Exec stderr: ""
    Mar 27 14:28:13.281: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:28:13.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:28:13.281: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:28:13.282: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 27 14:28:13.371: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/27/23 14:28:13.371
    Mar 27 14:28:13.371: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:28:13.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:28:13.372: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:28:13.372: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar 27 14:28:13.470: INFO: Exec stderr: ""
    Mar 27 14:28:13.470: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:28:13.470: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:28:13.471: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:28:13.471: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar 27 14:28:13.566: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/27/23 14:28:13.566
    Mar 27 14:28:13.567: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:28:13.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:28:13.567: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:28:13.567: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 27 14:28:13.654: INFO: Exec stderr: ""
    Mar 27 14:28:13.654: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:28:13.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:28:13.655: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:28:13.655: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 27 14:28:13.735: INFO: Exec stderr: ""
    Mar 27 14:28:13.735: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:28:13.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:28:13.736: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:28:13.736: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 27 14:28:13.818: INFO: Exec stderr: ""
    Mar 27 14:28:13.818: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9659 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:28:13.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:28:13.818: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:28:13.818: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9659/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 27 14:28:13.899: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Mar 27 14:28:13.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-9659" for this suite. 03/27/23 14:28:13.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:28:13.913
Mar 27 14:28:13.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 14:28:13.914
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:13.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:13.935
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-ae111c26-621e-4742-bfd1-398c2a7415ca 03/27/23 14:28:13.94
STEP: Creating a pod to test consume secrets 03/27/23 14:28:13.946
Mar 27 14:28:13.957: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f2f125cc-bdd3-43bd-9682-d7dfbc7bbedf" in namespace "projected-4918" to be "Succeeded or Failed"
Mar 27 14:28:13.961: INFO: Pod "pod-projected-secrets-f2f125cc-bdd3-43bd-9682-d7dfbc7bbedf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.313086ms
Mar 27 14:28:15.966: INFO: Pod "pod-projected-secrets-f2f125cc-bdd3-43bd-9682-d7dfbc7bbedf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00797425s
Mar 27 14:28:17.966: INFO: Pod "pod-projected-secrets-f2f125cc-bdd3-43bd-9682-d7dfbc7bbedf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008210336s
STEP: Saw pod success 03/27/23 14:28:17.966
Mar 27 14:28:17.966: INFO: Pod "pod-projected-secrets-f2f125cc-bdd3-43bd-9682-d7dfbc7bbedf" satisfied condition "Succeeded or Failed"
Mar 27 14:28:17.969: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-secrets-f2f125cc-bdd3-43bd-9682-d7dfbc7bbedf container secret-volume-test: <nil>
STEP: delete the pod 03/27/23 14:28:17.981
Mar 27 14:28:17.997: INFO: Waiting for pod pod-projected-secrets-f2f125cc-bdd3-43bd-9682-d7dfbc7bbedf to disappear
Mar 27 14:28:18.001: INFO: Pod pod-projected-secrets-f2f125cc-bdd3-43bd-9682-d7dfbc7bbedf no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 27 14:28:18.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4918" for this suite. 03/27/23 14:28:18.006
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":60,"skipped":1254,"failed":0}
------------------------------
• [4.100 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:28:13.913
    Mar 27 14:28:13.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 14:28:13.914
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:13.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:13.935
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-ae111c26-621e-4742-bfd1-398c2a7415ca 03/27/23 14:28:13.94
    STEP: Creating a pod to test consume secrets 03/27/23 14:28:13.946
    Mar 27 14:28:13.957: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f2f125cc-bdd3-43bd-9682-d7dfbc7bbedf" in namespace "projected-4918" to be "Succeeded or Failed"
    Mar 27 14:28:13.961: INFO: Pod "pod-projected-secrets-f2f125cc-bdd3-43bd-9682-d7dfbc7bbedf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.313086ms
    Mar 27 14:28:15.966: INFO: Pod "pod-projected-secrets-f2f125cc-bdd3-43bd-9682-d7dfbc7bbedf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00797425s
    Mar 27 14:28:17.966: INFO: Pod "pod-projected-secrets-f2f125cc-bdd3-43bd-9682-d7dfbc7bbedf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008210336s
    STEP: Saw pod success 03/27/23 14:28:17.966
    Mar 27 14:28:17.966: INFO: Pod "pod-projected-secrets-f2f125cc-bdd3-43bd-9682-d7dfbc7bbedf" satisfied condition "Succeeded or Failed"
    Mar 27 14:28:17.969: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-secrets-f2f125cc-bdd3-43bd-9682-d7dfbc7bbedf container secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 14:28:17.981
    Mar 27 14:28:17.997: INFO: Waiting for pod pod-projected-secrets-f2f125cc-bdd3-43bd-9682-d7dfbc7bbedf to disappear
    Mar 27 14:28:18.001: INFO: Pod pod-projected-secrets-f2f125cc-bdd3-43bd-9682-d7dfbc7bbedf no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 27 14:28:18.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4918" for this suite. 03/27/23 14:28:18.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:28:18.013
Mar 27 14:28:18.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename disruption 03/27/23 14:28:18.014
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:18.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:18.048
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 03/27/23 14:28:18.06
STEP: Updating PodDisruptionBudget status 03/27/23 14:28:20.07
STEP: Waiting for all pods to be running 03/27/23 14:28:20.079
Mar 27 14:28:20.082: INFO: running pods: 0 < 1
STEP: locating a running pod 03/27/23 14:28:22.088
STEP: Waiting for the pdb to be processed 03/27/23 14:28:22.103
STEP: Patching PodDisruptionBudget status 03/27/23 14:28:22.113
STEP: Waiting for the pdb to be processed 03/27/23 14:28:22.132
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar 27 14:28:22.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3996" for this suite. 03/27/23 14:28:22.151
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":61,"skipped":1264,"failed":0}
------------------------------
• [4.145 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:28:18.013
    Mar 27 14:28:18.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename disruption 03/27/23 14:28:18.014
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:18.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:18.048
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 03/27/23 14:28:18.06
    STEP: Updating PodDisruptionBudget status 03/27/23 14:28:20.07
    STEP: Waiting for all pods to be running 03/27/23 14:28:20.079
    Mar 27 14:28:20.082: INFO: running pods: 0 < 1
    STEP: locating a running pod 03/27/23 14:28:22.088
    STEP: Waiting for the pdb to be processed 03/27/23 14:28:22.103
    STEP: Patching PodDisruptionBudget status 03/27/23 14:28:22.113
    STEP: Waiting for the pdb to be processed 03/27/23 14:28:22.132
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar 27 14:28:22.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-3996" for this suite. 03/27/23 14:28:22.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:28:22.159
Mar 27 14:28:22.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename proxy 03/27/23 14:28:22.159
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:22.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:22.18
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Mar 27 14:28:22.183: INFO: Creating pod...
Mar 27 14:28:22.192: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7963" to be "running"
Mar 27 14:28:22.196: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.947687ms
Mar 27 14:28:24.201: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008305379s
Mar 27 14:28:24.201: INFO: Pod "agnhost" satisfied condition "running"
Mar 27 14:28:24.201: INFO: Creating service...
Mar 27 14:28:24.215: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/pods/agnhost/proxy?method=DELETE
Mar 27 14:28:24.228: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 27 14:28:24.228: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/pods/agnhost/proxy?method=OPTIONS
Mar 27 14:28:24.233: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 27 14:28:24.233: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/pods/agnhost/proxy?method=PATCH
Mar 27 14:28:24.238: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 27 14:28:24.238: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/pods/agnhost/proxy?method=POST
Mar 27 14:28:24.246: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 27 14:28:24.246: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/pods/agnhost/proxy?method=PUT
Mar 27 14:28:24.266: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 27 14:28:24.266: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/services/e2e-proxy-test-service/proxy?method=DELETE
Mar 27 14:28:24.276: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 27 14:28:24.276: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/services/e2e-proxy-test-service/proxy?method=OPTIONS
Mar 27 14:28:24.286: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 27 14:28:24.286: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/services/e2e-proxy-test-service/proxy?method=PATCH
Mar 27 14:28:24.293: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 27 14:28:24.293: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/services/e2e-proxy-test-service/proxy?method=POST
Mar 27 14:28:24.300: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 27 14:28:24.300: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/services/e2e-proxy-test-service/proxy?method=PUT
Mar 27 14:28:24.308: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 27 14:28:24.308: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/pods/agnhost/proxy?method=GET
Mar 27 14:28:24.312: INFO: http.Client request:GET StatusCode:301
Mar 27 14:28:24.312: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/services/e2e-proxy-test-service/proxy?method=GET
Mar 27 14:28:24.316: INFO: http.Client request:GET StatusCode:301
Mar 27 14:28:24.316: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/pods/agnhost/proxy?method=HEAD
Mar 27 14:28:24.319: INFO: http.Client request:HEAD StatusCode:301
Mar 27 14:28:24.319: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/services/e2e-proxy-test-service/proxy?method=HEAD
Mar 27 14:28:24.324: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar 27 14:28:24.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7963" for this suite. 03/27/23 14:28:24.329
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":62,"skipped":1280,"failed":0}
------------------------------
• [2.178 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:28:22.159
    Mar 27 14:28:22.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename proxy 03/27/23 14:28:22.159
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:22.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:22.18
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Mar 27 14:28:22.183: INFO: Creating pod...
    Mar 27 14:28:22.192: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7963" to be "running"
    Mar 27 14:28:22.196: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.947687ms
    Mar 27 14:28:24.201: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008305379s
    Mar 27 14:28:24.201: INFO: Pod "agnhost" satisfied condition "running"
    Mar 27 14:28:24.201: INFO: Creating service...
    Mar 27 14:28:24.215: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/pods/agnhost/proxy?method=DELETE
    Mar 27 14:28:24.228: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 27 14:28:24.228: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/pods/agnhost/proxy?method=OPTIONS
    Mar 27 14:28:24.233: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 27 14:28:24.233: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/pods/agnhost/proxy?method=PATCH
    Mar 27 14:28:24.238: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 27 14:28:24.238: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/pods/agnhost/proxy?method=POST
    Mar 27 14:28:24.246: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 27 14:28:24.246: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/pods/agnhost/proxy?method=PUT
    Mar 27 14:28:24.266: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 27 14:28:24.266: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/services/e2e-proxy-test-service/proxy?method=DELETE
    Mar 27 14:28:24.276: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 27 14:28:24.276: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Mar 27 14:28:24.286: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 27 14:28:24.286: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/services/e2e-proxy-test-service/proxy?method=PATCH
    Mar 27 14:28:24.293: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 27 14:28:24.293: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/services/e2e-proxy-test-service/proxy?method=POST
    Mar 27 14:28:24.300: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 27 14:28:24.300: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/services/e2e-proxy-test-service/proxy?method=PUT
    Mar 27 14:28:24.308: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 27 14:28:24.308: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/pods/agnhost/proxy?method=GET
    Mar 27 14:28:24.312: INFO: http.Client request:GET StatusCode:301
    Mar 27 14:28:24.312: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/services/e2e-proxy-test-service/proxy?method=GET
    Mar 27 14:28:24.316: INFO: http.Client request:GET StatusCode:301
    Mar 27 14:28:24.316: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/pods/agnhost/proxy?method=HEAD
    Mar 27 14:28:24.319: INFO: http.Client request:HEAD StatusCode:301
    Mar 27 14:28:24.319: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7963/services/e2e-proxy-test-service/proxy?method=HEAD
    Mar 27 14:28:24.324: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar 27 14:28:24.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-7963" for this suite. 03/27/23 14:28:24.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:28:24.353
Mar 27 14:28:24.353: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubectl 03/27/23 14:28:24.354
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:24.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:24.374
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/27/23 14:28:24.378
Mar 27 14:28:24.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5947 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 27 14:28:24.462: INFO: stderr: ""
Mar 27 14:28:24.462: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 03/27/23 14:28:24.462
Mar 27 14:28:24.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5947 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Mar 27 14:28:25.183: INFO: stderr: ""
Mar 27 14:28:25.183: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/27/23 14:28:25.183
Mar 27 14:28:25.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5947 delete pods e2e-test-httpd-pod'
Mar 27 14:28:27.848: INFO: stderr: ""
Mar 27 14:28:27.848: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 27 14:28:27.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5947" for this suite. 03/27/23 14:28:27.859
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":63,"skipped":1327,"failed":0}
------------------------------
• [3.518 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:28:24.353
    Mar 27 14:28:24.353: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubectl 03/27/23 14:28:24.354
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:24.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:24.374
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/27/23 14:28:24.378
    Mar 27 14:28:24.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5947 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar 27 14:28:24.462: INFO: stderr: ""
    Mar 27 14:28:24.462: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 03/27/23 14:28:24.462
    Mar 27 14:28:24.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5947 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Mar 27 14:28:25.183: INFO: stderr: ""
    Mar 27 14:28:25.183: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/27/23 14:28:25.183
    Mar 27 14:28:25.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-5947 delete pods e2e-test-httpd-pod'
    Mar 27 14:28:27.848: INFO: stderr: ""
    Mar 27 14:28:27.848: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 27 14:28:27.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5947" for this suite. 03/27/23 14:28:27.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:28:27.871
Mar 27 14:28:27.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 14:28:27.872
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:27.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:27.895
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-f38c3f78-c28f-4053-b9a3-6b94229eede3 03/27/23 14:28:27.906
STEP: Creating secret with name s-test-opt-upd-368bb40a-ddd6-4ea3-8102-28d94c68cc41 03/27/23 14:28:27.914
STEP: Creating the pod 03/27/23 14:28:27.919
Mar 27 14:28:27.933: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-84479027-c4f9-48f9-a716-b49a49426702" in namespace "projected-8086" to be "running and ready"
Mar 27 14:28:27.944: INFO: Pod "pod-projected-secrets-84479027-c4f9-48f9-a716-b49a49426702": Phase="Pending", Reason="", readiness=false. Elapsed: 10.889166ms
Mar 27 14:28:27.944: INFO: The phase of Pod pod-projected-secrets-84479027-c4f9-48f9-a716-b49a49426702 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:28:29.949: INFO: Pod "pod-projected-secrets-84479027-c4f9-48f9-a716-b49a49426702": Phase="Running", Reason="", readiness=true. Elapsed: 2.015159302s
Mar 27 14:28:29.949: INFO: The phase of Pod pod-projected-secrets-84479027-c4f9-48f9-a716-b49a49426702 is Running (Ready = true)
Mar 27 14:28:29.949: INFO: Pod "pod-projected-secrets-84479027-c4f9-48f9-a716-b49a49426702" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-f38c3f78-c28f-4053-b9a3-6b94229eede3 03/27/23 14:28:30.033
STEP: Updating secret s-test-opt-upd-368bb40a-ddd6-4ea3-8102-28d94c68cc41 03/27/23 14:28:30.04
STEP: Creating secret with name s-test-opt-create-dcc921fa-2c24-4fce-a3d3-7a7f4b44d18b 03/27/23 14:28:30.047
STEP: waiting to observe update in volume 03/27/23 14:28:30.052
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 27 14:28:32.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8086" for this suite. 03/27/23 14:28:32.105
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":64,"skipped":1332,"failed":0}
------------------------------
• [4.242 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:28:27.871
    Mar 27 14:28:27.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 14:28:27.872
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:27.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:27.895
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-f38c3f78-c28f-4053-b9a3-6b94229eede3 03/27/23 14:28:27.906
    STEP: Creating secret with name s-test-opt-upd-368bb40a-ddd6-4ea3-8102-28d94c68cc41 03/27/23 14:28:27.914
    STEP: Creating the pod 03/27/23 14:28:27.919
    Mar 27 14:28:27.933: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-84479027-c4f9-48f9-a716-b49a49426702" in namespace "projected-8086" to be "running and ready"
    Mar 27 14:28:27.944: INFO: Pod "pod-projected-secrets-84479027-c4f9-48f9-a716-b49a49426702": Phase="Pending", Reason="", readiness=false. Elapsed: 10.889166ms
    Mar 27 14:28:27.944: INFO: The phase of Pod pod-projected-secrets-84479027-c4f9-48f9-a716-b49a49426702 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:28:29.949: INFO: Pod "pod-projected-secrets-84479027-c4f9-48f9-a716-b49a49426702": Phase="Running", Reason="", readiness=true. Elapsed: 2.015159302s
    Mar 27 14:28:29.949: INFO: The phase of Pod pod-projected-secrets-84479027-c4f9-48f9-a716-b49a49426702 is Running (Ready = true)
    Mar 27 14:28:29.949: INFO: Pod "pod-projected-secrets-84479027-c4f9-48f9-a716-b49a49426702" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-f38c3f78-c28f-4053-b9a3-6b94229eede3 03/27/23 14:28:30.033
    STEP: Updating secret s-test-opt-upd-368bb40a-ddd6-4ea3-8102-28d94c68cc41 03/27/23 14:28:30.04
    STEP: Creating secret with name s-test-opt-create-dcc921fa-2c24-4fce-a3d3-7a7f4b44d18b 03/27/23 14:28:30.047
    STEP: waiting to observe update in volume 03/27/23 14:28:30.052
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 27 14:28:32.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8086" for this suite. 03/27/23 14:28:32.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:28:32.116
Mar 27 14:28:32.116: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename daemonsets 03/27/23 14:28:32.117
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:32.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:32.148
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Mar 27 14:28:32.181: INFO: Create a RollingUpdate DaemonSet
Mar 27 14:28:32.187: INFO: Check that daemon pods launch on every node of the cluster
Mar 27 14:28:32.206: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 14:28:32.206: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 14:28:33.216: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 14:28:33.216: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 14:28:34.224: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 27 14:28:34.225: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 14:28:35.218: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 27 14:28:35.218: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Mar 27 14:28:35.218: INFO: Update the DaemonSet to trigger a rollout
Mar 27 14:28:35.233: INFO: Updating DaemonSet daemon-set
Mar 27 14:28:37.258: INFO: Roll back the DaemonSet before rollout is complete
Mar 27 14:28:37.269: INFO: Updating DaemonSet daemon-set
Mar 27 14:28:37.269: INFO: Make sure DaemonSet rollback is complete
Mar 27 14:28:37.279: INFO: Wrong image for pod: daemon-set-8w7g8. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Mar 27 14:28:37.279: INFO: Pod daemon-set-8w7g8 is not available
Mar 27 14:28:41.289: INFO: Pod daemon-set-vrzcv is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/27/23 14:28:41.301
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6032, will wait for the garbage collector to delete the pods 03/27/23 14:28:41.301
Mar 27 14:28:41.364: INFO: Deleting DaemonSet.extensions daemon-set took: 8.615541ms
Mar 27 14:28:41.464: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.522736ms
Mar 27 14:28:43.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 14:28:43.069: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 27 14:28:43.071: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17085"},"items":null}

Mar 27 14:28:43.074: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17085"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 27 14:28:43.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6032" for this suite. 03/27/23 14:28:43.093
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":65,"skipped":1370,"failed":0}
------------------------------
• [SLOW TEST] [10.985 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:28:32.116
    Mar 27 14:28:32.116: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename daemonsets 03/27/23 14:28:32.117
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:32.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:32.148
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Mar 27 14:28:32.181: INFO: Create a RollingUpdate DaemonSet
    Mar 27 14:28:32.187: INFO: Check that daemon pods launch on every node of the cluster
    Mar 27 14:28:32.206: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 14:28:32.206: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 14:28:33.216: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 14:28:33.216: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 14:28:34.224: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 27 14:28:34.225: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 14:28:35.218: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 27 14:28:35.218: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Mar 27 14:28:35.218: INFO: Update the DaemonSet to trigger a rollout
    Mar 27 14:28:35.233: INFO: Updating DaemonSet daemon-set
    Mar 27 14:28:37.258: INFO: Roll back the DaemonSet before rollout is complete
    Mar 27 14:28:37.269: INFO: Updating DaemonSet daemon-set
    Mar 27 14:28:37.269: INFO: Make sure DaemonSet rollback is complete
    Mar 27 14:28:37.279: INFO: Wrong image for pod: daemon-set-8w7g8. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Mar 27 14:28:37.279: INFO: Pod daemon-set-8w7g8 is not available
    Mar 27 14:28:41.289: INFO: Pod daemon-set-vrzcv is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/27/23 14:28:41.301
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6032, will wait for the garbage collector to delete the pods 03/27/23 14:28:41.301
    Mar 27 14:28:41.364: INFO: Deleting DaemonSet.extensions daemon-set took: 8.615541ms
    Mar 27 14:28:41.464: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.522736ms
    Mar 27 14:28:43.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 14:28:43.069: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 27 14:28:43.071: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17085"},"items":null}

    Mar 27 14:28:43.074: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17085"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 14:28:43.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6032" for this suite. 03/27/23 14:28:43.093
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:28:43.101
Mar 27 14:28:43.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename services 03/27/23 14:28:43.102
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:43.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:43.125
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-486 03/27/23 14:28:43.131
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-486 to expose endpoints map[] 03/27/23 14:28:43.142
Mar 27 14:28:43.151: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Mar 27 14:28:44.163: INFO: successfully validated that service multi-endpoint-test in namespace services-486 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-486 03/27/23 14:28:44.163
Mar 27 14:28:44.179: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-486" to be "running and ready"
Mar 27 14:28:44.182: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.177331ms
Mar 27 14:28:44.182: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:28:46.187: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008733744s
Mar 27 14:28:46.187: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 27 14:28:46.187: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-486 to expose endpoints map[pod1:[100]] 03/27/23 14:28:46.195
Mar 27 14:28:46.206: INFO: successfully validated that service multi-endpoint-test in namespace services-486 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-486 03/27/23 14:28:46.206
Mar 27 14:28:46.212: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-486" to be "running and ready"
Mar 27 14:28:46.215: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.545799ms
Mar 27 14:28:46.215: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:28:48.220: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008460837s
Mar 27 14:28:48.220: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 27 14:28:48.220: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-486 to expose endpoints map[pod1:[100] pod2:[101]] 03/27/23 14:28:48.223
Mar 27 14:28:48.239: INFO: successfully validated that service multi-endpoint-test in namespace services-486 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 03/27/23 14:28:48.239
Mar 27 14:28:48.239: INFO: Creating new exec pod
Mar 27 14:28:48.247: INFO: Waiting up to 5m0s for pod "execpodc2c4z" in namespace "services-486" to be "running"
Mar 27 14:28:48.257: INFO: Pod "execpodc2c4z": Phase="Pending", Reason="", readiness=false. Elapsed: 9.287721ms
Mar 27 14:28:50.264: INFO: Pod "execpodc2c4z": Phase="Running", Reason="", readiness=true. Elapsed: 2.016525238s
Mar 27 14:28:50.264: INFO: Pod "execpodc2c4z" satisfied condition "running"
Mar 27 14:28:51.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-486 exec execpodc2c4z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Mar 27 14:28:51.489: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Mar 27 14:28:51.489: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 14:28:51.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-486 exec execpodc2c4z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.20.186 80'
Mar 27 14:28:51.664: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.20.186 80\nConnection to 10.240.20.186 80 port [tcp/http] succeeded!\n"
Mar 27 14:28:51.664: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 14:28:51.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-486 exec execpodc2c4z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Mar 27 14:28:51.887: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Mar 27 14:28:51.887: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 14:28:51.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-486 exec execpodc2c4z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.20.186 81'
Mar 27 14:28:52.105: INFO: stderr: "+ nc -v -t -w 2 10.240.20.186 81\n+ echo hostName\nConnection to 10.240.20.186 81 port [tcp/*] succeeded!\n"
Mar 27 14:28:52.105: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-486 03/27/23 14:28:52.105
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-486 to expose endpoints map[pod2:[101]] 03/27/23 14:28:52.122
Mar 27 14:28:52.134: INFO: successfully validated that service multi-endpoint-test in namespace services-486 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-486 03/27/23 14:28:52.134
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-486 to expose endpoints map[] 03/27/23 14:28:52.159
Mar 27 14:28:52.180: INFO: successfully validated that service multi-endpoint-test in namespace services-486 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 27 14:28:52.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-486" for this suite. 03/27/23 14:28:52.2
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":66,"skipped":1371,"failed":0}
------------------------------
• [SLOW TEST] [9.108 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:28:43.101
    Mar 27 14:28:43.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename services 03/27/23 14:28:43.102
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:43.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:43.125
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-486 03/27/23 14:28:43.131
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-486 to expose endpoints map[] 03/27/23 14:28:43.142
    Mar 27 14:28:43.151: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Mar 27 14:28:44.163: INFO: successfully validated that service multi-endpoint-test in namespace services-486 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-486 03/27/23 14:28:44.163
    Mar 27 14:28:44.179: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-486" to be "running and ready"
    Mar 27 14:28:44.182: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.177331ms
    Mar 27 14:28:44.182: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:28:46.187: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008733744s
    Mar 27 14:28:46.187: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 27 14:28:46.187: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-486 to expose endpoints map[pod1:[100]] 03/27/23 14:28:46.195
    Mar 27 14:28:46.206: INFO: successfully validated that service multi-endpoint-test in namespace services-486 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-486 03/27/23 14:28:46.206
    Mar 27 14:28:46.212: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-486" to be "running and ready"
    Mar 27 14:28:46.215: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.545799ms
    Mar 27 14:28:46.215: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:28:48.220: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008460837s
    Mar 27 14:28:48.220: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 27 14:28:48.220: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-486 to expose endpoints map[pod1:[100] pod2:[101]] 03/27/23 14:28:48.223
    Mar 27 14:28:48.239: INFO: successfully validated that service multi-endpoint-test in namespace services-486 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 03/27/23 14:28:48.239
    Mar 27 14:28:48.239: INFO: Creating new exec pod
    Mar 27 14:28:48.247: INFO: Waiting up to 5m0s for pod "execpodc2c4z" in namespace "services-486" to be "running"
    Mar 27 14:28:48.257: INFO: Pod "execpodc2c4z": Phase="Pending", Reason="", readiness=false. Elapsed: 9.287721ms
    Mar 27 14:28:50.264: INFO: Pod "execpodc2c4z": Phase="Running", Reason="", readiness=true. Elapsed: 2.016525238s
    Mar 27 14:28:50.264: INFO: Pod "execpodc2c4z" satisfied condition "running"
    Mar 27 14:28:51.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-486 exec execpodc2c4z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Mar 27 14:28:51.489: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Mar 27 14:28:51.489: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 14:28:51.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-486 exec execpodc2c4z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.20.186 80'
    Mar 27 14:28:51.664: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.20.186 80\nConnection to 10.240.20.186 80 port [tcp/http] succeeded!\n"
    Mar 27 14:28:51.664: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 14:28:51.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-486 exec execpodc2c4z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Mar 27 14:28:51.887: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Mar 27 14:28:51.887: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 14:28:51.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-486 exec execpodc2c4z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.20.186 81'
    Mar 27 14:28:52.105: INFO: stderr: "+ nc -v -t -w 2 10.240.20.186 81\n+ echo hostName\nConnection to 10.240.20.186 81 port [tcp/*] succeeded!\n"
    Mar 27 14:28:52.105: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-486 03/27/23 14:28:52.105
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-486 to expose endpoints map[pod2:[101]] 03/27/23 14:28:52.122
    Mar 27 14:28:52.134: INFO: successfully validated that service multi-endpoint-test in namespace services-486 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-486 03/27/23 14:28:52.134
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-486 to expose endpoints map[] 03/27/23 14:28:52.159
    Mar 27 14:28:52.180: INFO: successfully validated that service multi-endpoint-test in namespace services-486 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 27 14:28:52.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-486" for this suite. 03/27/23 14:28:52.2
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:28:52.21
Mar 27 14:28:52.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename containers 03/27/23 14:28:52.211
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:52.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:52.233
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 03/27/23 14:28:52.236
Mar 27 14:28:52.243: INFO: Waiting up to 5m0s for pod "client-containers-b921651d-fe3d-4f8a-8e91-a72bef741b45" in namespace "containers-8734" to be "Succeeded or Failed"
Mar 27 14:28:52.251: INFO: Pod "client-containers-b921651d-fe3d-4f8a-8e91-a72bef741b45": Phase="Pending", Reason="", readiness=false. Elapsed: 7.638647ms
Mar 27 14:28:54.256: INFO: Pod "client-containers-b921651d-fe3d-4f8a-8e91-a72bef741b45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012298996s
Mar 27 14:28:56.258: INFO: Pod "client-containers-b921651d-fe3d-4f8a-8e91-a72bef741b45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014257501s
STEP: Saw pod success 03/27/23 14:28:56.258
Mar 27 14:28:56.258: INFO: Pod "client-containers-b921651d-fe3d-4f8a-8e91-a72bef741b45" satisfied condition "Succeeded or Failed"
Mar 27 14:28:56.262: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod client-containers-b921651d-fe3d-4f8a-8e91-a72bef741b45 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 14:28:56.27
Mar 27 14:28:56.280: INFO: Waiting for pod client-containers-b921651d-fe3d-4f8a-8e91-a72bef741b45 to disappear
Mar 27 14:28:56.283: INFO: Pod client-containers-b921651d-fe3d-4f8a-8e91-a72bef741b45 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar 27 14:28:56.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8734" for this suite. 03/27/23 14:28:56.288
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":67,"skipped":1404,"failed":0}
------------------------------
• [4.089 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:28:52.21
    Mar 27 14:28:52.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename containers 03/27/23 14:28:52.211
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:52.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:52.233
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 03/27/23 14:28:52.236
    Mar 27 14:28:52.243: INFO: Waiting up to 5m0s for pod "client-containers-b921651d-fe3d-4f8a-8e91-a72bef741b45" in namespace "containers-8734" to be "Succeeded or Failed"
    Mar 27 14:28:52.251: INFO: Pod "client-containers-b921651d-fe3d-4f8a-8e91-a72bef741b45": Phase="Pending", Reason="", readiness=false. Elapsed: 7.638647ms
    Mar 27 14:28:54.256: INFO: Pod "client-containers-b921651d-fe3d-4f8a-8e91-a72bef741b45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012298996s
    Mar 27 14:28:56.258: INFO: Pod "client-containers-b921651d-fe3d-4f8a-8e91-a72bef741b45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014257501s
    STEP: Saw pod success 03/27/23 14:28:56.258
    Mar 27 14:28:56.258: INFO: Pod "client-containers-b921651d-fe3d-4f8a-8e91-a72bef741b45" satisfied condition "Succeeded or Failed"
    Mar 27 14:28:56.262: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod client-containers-b921651d-fe3d-4f8a-8e91-a72bef741b45 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 14:28:56.27
    Mar 27 14:28:56.280: INFO: Waiting for pod client-containers-b921651d-fe3d-4f8a-8e91-a72bef741b45 to disappear
    Mar 27 14:28:56.283: INFO: Pod client-containers-b921651d-fe3d-4f8a-8e91-a72bef741b45 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar 27 14:28:56.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-8734" for this suite. 03/27/23 14:28:56.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:28:56.299
Mar 27 14:28:56.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 14:28:56.3
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:56.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:56.316
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/27/23 14:28:56.319
Mar 27 14:28:56.320: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:29:03.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:29:42.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1853" for this suite. 03/27/23 14:29:42.859
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":68,"skipped":1419,"failed":0}
------------------------------
• [SLOW TEST] [46.578 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:28:56.299
    Mar 27 14:28:56.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 14:28:56.3
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:28:56.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:28:56.316
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/27/23 14:28:56.319
    Mar 27 14:28:56.320: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:29:03.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:29:42.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1853" for this suite. 03/27/23 14:29:42.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:29:42.878
Mar 27 14:29:42.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename certificates 03/27/23 14:29:42.879
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:29:42.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:29:42.906
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 03/27/23 14:29:43.843
STEP: getting /apis/certificates.k8s.io 03/27/23 14:29:43.847
STEP: getting /apis/certificates.k8s.io/v1 03/27/23 14:29:43.848
STEP: creating 03/27/23 14:29:43.849
STEP: getting 03/27/23 14:29:43.87
STEP: listing 03/27/23 14:29:43.873
STEP: watching 03/27/23 14:29:43.878
Mar 27 14:29:43.878: INFO: starting watch
STEP: patching 03/27/23 14:29:43.879
STEP: updating 03/27/23 14:29:43.892
Mar 27 14:29:43.900: INFO: waiting for watch events with expected annotations
Mar 27 14:29:43.900: INFO: saw patched and updated annotations
STEP: getting /approval 03/27/23 14:29:43.9
STEP: patching /approval 03/27/23 14:29:43.904
STEP: updating /approval 03/27/23 14:29:43.913
STEP: getting /status 03/27/23 14:29:43.922
STEP: patching /status 03/27/23 14:29:43.926
STEP: updating /status 03/27/23 14:29:43.939
STEP: deleting 03/27/23 14:29:43.948
STEP: deleting a collection 03/27/23 14:29:43.961
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:29:43.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-2357" for this suite. 03/27/23 14:29:43.984
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":69,"skipped":1442,"failed":0}
------------------------------
• [1.113 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:29:42.878
    Mar 27 14:29:42.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename certificates 03/27/23 14:29:42.879
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:29:42.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:29:42.906
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 03/27/23 14:29:43.843
    STEP: getting /apis/certificates.k8s.io 03/27/23 14:29:43.847
    STEP: getting /apis/certificates.k8s.io/v1 03/27/23 14:29:43.848
    STEP: creating 03/27/23 14:29:43.849
    STEP: getting 03/27/23 14:29:43.87
    STEP: listing 03/27/23 14:29:43.873
    STEP: watching 03/27/23 14:29:43.878
    Mar 27 14:29:43.878: INFO: starting watch
    STEP: patching 03/27/23 14:29:43.879
    STEP: updating 03/27/23 14:29:43.892
    Mar 27 14:29:43.900: INFO: waiting for watch events with expected annotations
    Mar 27 14:29:43.900: INFO: saw patched and updated annotations
    STEP: getting /approval 03/27/23 14:29:43.9
    STEP: patching /approval 03/27/23 14:29:43.904
    STEP: updating /approval 03/27/23 14:29:43.913
    STEP: getting /status 03/27/23 14:29:43.922
    STEP: patching /status 03/27/23 14:29:43.926
    STEP: updating /status 03/27/23 14:29:43.939
    STEP: deleting 03/27/23 14:29:43.948
    STEP: deleting a collection 03/27/23 14:29:43.961
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:29:43.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-2357" for this suite. 03/27/23 14:29:43.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:29:43.992
Mar 27 14:29:43.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubectl 03/27/23 14:29:43.992
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:29:44.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:29:44.023
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 03/27/23 14:29:44.026
Mar 27 14:29:44.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-155 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 27 14:29:44.101: INFO: stderr: ""
Mar 27 14:29:44.101: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 03/27/23 14:29:44.101
Mar 27 14:29:44.101: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 27 14:29:44.101: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-155" to be "running and ready, or succeeded"
Mar 27 14:29:44.107: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.482895ms
Mar 27 14:29:44.107: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc' to be 'Running' but was 'Pending'
Mar 27 14:29:46.113: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.011477715s
Mar 27 14:29:46.113: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 27 14:29:46.113: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 03/27/23 14:29:46.113
Mar 27 14:29:46.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-155 logs logs-generator logs-generator'
Mar 27 14:29:46.191: INFO: stderr: ""
Mar 27 14:29:46.191: INFO: stdout: "I0327 14:29:45.224858       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/jb5 555\nI0327 14:29:45.425032       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/4lg 233\nI0327 14:29:45.625692       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/mkf7 545\nI0327 14:29:45.825092       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/8kh2 467\nI0327 14:29:46.025441       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/dtn2 487\n"
STEP: limiting log lines 03/27/23 14:29:46.191
Mar 27 14:29:46.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-155 logs logs-generator logs-generator --tail=1'
Mar 27 14:29:46.262: INFO: stderr: ""
Mar 27 14:29:46.262: INFO: stdout: "I0327 14:29:46.225055       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/xvs 588\n"
Mar 27 14:29:46.262: INFO: got output "I0327 14:29:46.225055       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/xvs 588\n"
STEP: limiting log bytes 03/27/23 14:29:46.262
Mar 27 14:29:46.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-155 logs logs-generator logs-generator --limit-bytes=1'
Mar 27 14:29:46.335: INFO: stderr: ""
Mar 27 14:29:46.335: INFO: stdout: "I"
Mar 27 14:29:46.335: INFO: got output "I"
STEP: exposing timestamps 03/27/23 14:29:46.335
Mar 27 14:29:46.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-155 logs logs-generator logs-generator --tail=1 --timestamps'
Mar 27 14:29:46.415: INFO: stderr: ""
Mar 27 14:29:46.415: INFO: stdout: "2023-03-27T14:29:46.425815493Z I0327 14:29:46.425544       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/mhj9 531\n"
Mar 27 14:29:46.415: INFO: got output "2023-03-27T14:29:46.425815493Z I0327 14:29:46.425544       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/mhj9 531\n"
STEP: restricting to a time range 03/27/23 14:29:46.415
Mar 27 14:29:48.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-155 logs logs-generator logs-generator --since=1s'
Mar 27 14:29:48.995: INFO: stderr: ""
Mar 27 14:29:48.995: INFO: stdout: "I0327 14:29:48.025959       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/65r 506\nI0327 14:29:48.225335       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/ldxh 351\nI0327 14:29:48.425677       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/bqg6 402\nI0327 14:29:48.624969       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/s9t 247\nI0327 14:29:48.825460       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/mlx 505\n"
Mar 27 14:29:48.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-155 logs logs-generator logs-generator --since=24h'
Mar 27 14:29:49.073: INFO: stderr: ""
Mar 27 14:29:49.073: INFO: stdout: "I0327 14:29:45.224858       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/jb5 555\nI0327 14:29:45.425032       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/4lg 233\nI0327 14:29:45.625692       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/mkf7 545\nI0327 14:29:45.825092       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/8kh2 467\nI0327 14:29:46.025441       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/dtn2 487\nI0327 14:29:46.225055       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/xvs 588\nI0327 14:29:46.425544       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/mhj9 531\nI0327 14:29:46.626001       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/jj8p 334\nI0327 14:29:46.825461       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/gvmt 278\nI0327 14:29:47.025891       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/lmtk 526\nI0327 14:29:47.225390       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/htdn 509\nI0327 14:29:47.425822       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/2xr 401\nI0327 14:29:47.625054       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/gfr 592\nI0327 14:29:47.825589       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/gnp 411\nI0327 14:29:48.025959       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/65r 506\nI0327 14:29:48.225335       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/ldxh 351\nI0327 14:29:48.425677       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/bqg6 402\nI0327 14:29:48.624969       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/s9t 247\nI0327 14:29:48.825460       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/mlx 505\nI0327 14:29:49.025887       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/rhc 423\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Mar 27 14:29:49.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-155 delete pod logs-generator'
Mar 27 14:29:50.106: INFO: stderr: ""
Mar 27 14:29:50.106: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 27 14:29:50.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-155" for this suite. 03/27/23 14:29:50.113
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":70,"skipped":1449,"failed":0}
------------------------------
• [SLOW TEST] [6.130 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:29:43.992
    Mar 27 14:29:43.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubectl 03/27/23 14:29:43.992
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:29:44.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:29:44.023
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 03/27/23 14:29:44.026
    Mar 27 14:29:44.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-155 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Mar 27 14:29:44.101: INFO: stderr: ""
    Mar 27 14:29:44.101: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 03/27/23 14:29:44.101
    Mar 27 14:29:44.101: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Mar 27 14:29:44.101: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-155" to be "running and ready, or succeeded"
    Mar 27 14:29:44.107: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.482895ms
    Mar 27 14:29:44.107: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc' to be 'Running' but was 'Pending'
    Mar 27 14:29:46.113: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.011477715s
    Mar 27 14:29:46.113: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Mar 27 14:29:46.113: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 03/27/23 14:29:46.113
    Mar 27 14:29:46.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-155 logs logs-generator logs-generator'
    Mar 27 14:29:46.191: INFO: stderr: ""
    Mar 27 14:29:46.191: INFO: stdout: "I0327 14:29:45.224858       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/jb5 555\nI0327 14:29:45.425032       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/4lg 233\nI0327 14:29:45.625692       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/mkf7 545\nI0327 14:29:45.825092       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/8kh2 467\nI0327 14:29:46.025441       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/dtn2 487\n"
    STEP: limiting log lines 03/27/23 14:29:46.191
    Mar 27 14:29:46.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-155 logs logs-generator logs-generator --tail=1'
    Mar 27 14:29:46.262: INFO: stderr: ""
    Mar 27 14:29:46.262: INFO: stdout: "I0327 14:29:46.225055       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/xvs 588\n"
    Mar 27 14:29:46.262: INFO: got output "I0327 14:29:46.225055       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/xvs 588\n"
    STEP: limiting log bytes 03/27/23 14:29:46.262
    Mar 27 14:29:46.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-155 logs logs-generator logs-generator --limit-bytes=1'
    Mar 27 14:29:46.335: INFO: stderr: ""
    Mar 27 14:29:46.335: INFO: stdout: "I"
    Mar 27 14:29:46.335: INFO: got output "I"
    STEP: exposing timestamps 03/27/23 14:29:46.335
    Mar 27 14:29:46.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-155 logs logs-generator logs-generator --tail=1 --timestamps'
    Mar 27 14:29:46.415: INFO: stderr: ""
    Mar 27 14:29:46.415: INFO: stdout: "2023-03-27T14:29:46.425815493Z I0327 14:29:46.425544       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/mhj9 531\n"
    Mar 27 14:29:46.415: INFO: got output "2023-03-27T14:29:46.425815493Z I0327 14:29:46.425544       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/mhj9 531\n"
    STEP: restricting to a time range 03/27/23 14:29:46.415
    Mar 27 14:29:48.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-155 logs logs-generator logs-generator --since=1s'
    Mar 27 14:29:48.995: INFO: stderr: ""
    Mar 27 14:29:48.995: INFO: stdout: "I0327 14:29:48.025959       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/65r 506\nI0327 14:29:48.225335       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/ldxh 351\nI0327 14:29:48.425677       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/bqg6 402\nI0327 14:29:48.624969       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/s9t 247\nI0327 14:29:48.825460       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/mlx 505\n"
    Mar 27 14:29:48.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-155 logs logs-generator logs-generator --since=24h'
    Mar 27 14:29:49.073: INFO: stderr: ""
    Mar 27 14:29:49.073: INFO: stdout: "I0327 14:29:45.224858       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/jb5 555\nI0327 14:29:45.425032       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/4lg 233\nI0327 14:29:45.625692       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/mkf7 545\nI0327 14:29:45.825092       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/8kh2 467\nI0327 14:29:46.025441       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/dtn2 487\nI0327 14:29:46.225055       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/xvs 588\nI0327 14:29:46.425544       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/mhj9 531\nI0327 14:29:46.626001       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/jj8p 334\nI0327 14:29:46.825461       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/gvmt 278\nI0327 14:29:47.025891       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/lmtk 526\nI0327 14:29:47.225390       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/htdn 509\nI0327 14:29:47.425822       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/2xr 401\nI0327 14:29:47.625054       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/gfr 592\nI0327 14:29:47.825589       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/gnp 411\nI0327 14:29:48.025959       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/65r 506\nI0327 14:29:48.225335       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/ldxh 351\nI0327 14:29:48.425677       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/bqg6 402\nI0327 14:29:48.624969       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/s9t 247\nI0327 14:29:48.825460       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/mlx 505\nI0327 14:29:49.025887       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/rhc 423\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Mar 27 14:29:49.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-155 delete pod logs-generator'
    Mar 27 14:29:50.106: INFO: stderr: ""
    Mar 27 14:29:50.106: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 27 14:29:50.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-155" for this suite. 03/27/23 14:29:50.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:29:50.124
Mar 27 14:29:50.124: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename sched-pred 03/27/23 14:29:50.125
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:29:50.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:29:50.154
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar 27 14:29:50.158: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 27 14:29:50.168: INFO: Waiting for terminating namespaces to be deleted...
Mar 27 14:29:50.171: INFO: 
Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq before test
Mar 27 14:29:50.181: INFO: canal-vhmnn from kube-system started at 2023-03-27 14:03:29 +0000 UTC (2 container statuses recorded)
Mar 27 14:29:50.181: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 14:29:50.181: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 27 14:29:50.181: INFO: csi-cinder-nodeplugin-lhmqp from kube-system started at 2023-03-27 14:03:29 +0000 UTC (3 container statuses recorded)
Mar 27 14:29:50.181: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 27 14:29:50.181: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 27 14:29:50.181: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 27 14:29:50.181: INFO: kube-proxy-4njm5 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.181: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 27 14:29:50.181: INFO: node-exporter-x8df2 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.181: INFO: 	Container node-exporter ready: true, restart count 0
Mar 27 14:29:50.181: INFO: node-local-dns-zvl8z from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.181: INFO: 	Container node-cache ready: true, restart count 0
Mar 27 14:29:50.181: INFO: syseleven-node-problem-detector-2x2wt from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.181: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 27 14:29:50.181: INFO: user-ssh-keys-agent-65hx9 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.181: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Mar 27 14:29:50.181: INFO: sonobuoy-e2e-job-267544c30cdc4a58 from sonobuoy started at 2023-03-27 14:05:45 +0000 UTC (2 container statuses recorded)
Mar 27 14:29:50.181: INFO: 	Container e2e ready: true, restart count 0
Mar 27 14:29:50.181: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 14:29:50.181: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-bg2pk from sonobuoy started at 2023-03-27 14:05:46 +0000 UTC (2 container statuses recorded)
Mar 27 14:29:50.181: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 14:29:50.181: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 27 14:29:50.181: INFO: 
Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc before test
Mar 27 14:29:50.191: INFO: canal-8sfms from kube-system started at 2023-03-27 14:03:34 +0000 UTC (2 container statuses recorded)
Mar 27 14:29:50.191: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 14:29:50.191: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 27 14:29:50.191: INFO: csi-cinder-nodeplugin-lmbcb from kube-system started at 2023-03-27 14:03:34 +0000 UTC (3 container statuses recorded)
Mar 27 14:29:50.191: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 27 14:29:50.191: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 27 14:29:50.191: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 27 14:29:50.191: INFO: kube-proxy-gj9h9 from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.191: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 27 14:29:50.191: INFO: node-exporter-fgptx from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.191: INFO: 	Container node-exporter ready: true, restart count 0
Mar 27 14:29:50.191: INFO: node-local-dns-tmg5z from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.191: INFO: 	Container node-cache ready: true, restart count 0
Mar 27 14:29:50.191: INFO: syseleven-node-problem-detector-xh9cq from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.191: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 27 14:29:50.191: INFO: user-ssh-keys-agent-x7kpb from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.191: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Mar 27 14:29:50.191: INFO: sonobuoy from sonobuoy started at 2023-03-27 14:05:39 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.191: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 27 14:29:50.191: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-4ztlm from sonobuoy started at 2023-03-27 14:05:46 +0000 UTC (2 container statuses recorded)
Mar 27 14:29:50.191: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 14:29:50.191: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 27 14:29:50.191: INFO: 
Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv before test
Mar 27 14:29:50.203: INFO: calico-kube-controllers-659979cbcb-rxz7c from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.203: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 27 14:29:50.203: INFO: canal-llxzv from kube-system started at 2023-03-27 14:02:28 +0000 UTC (2 container statuses recorded)
Mar 27 14:29:50.203: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 14:29:50.203: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 27 14:29:50.203: INFO: cluster-autoscaler-855c4bc847-b7mwh from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.203: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Mar 27 14:29:50.203: INFO: coredns-5954c46f46-k6bxj from kube-system started at 2023-03-27 14:03:06 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.203: INFO: 	Container coredns ready: true, restart count 0
Mar 27 14:29:50.203: INFO: coredns-5954c46f46-qg6wb from kube-system started at 2023-03-27 14:03:06 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.203: INFO: 	Container coredns ready: true, restart count 0
Mar 27 14:29:50.203: INFO: csi-cinder-nodeplugin-4gffp from kube-system started at 2023-03-27 14:02:28 +0000 UTC (3 container statuses recorded)
Mar 27 14:29:50.203: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 27 14:29:50.203: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 27 14:29:50.203: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 27 14:29:50.203: INFO: dns-autoscaler-69d8768b6-qg5fp from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.203: INFO: 	Container autoscaler ready: true, restart count 0
Mar 27 14:29:50.203: INFO: konnectivity-agent-7c486cb7b8-7wxj6 from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.203: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 14:29:50.203: INFO: konnectivity-agent-7c486cb7b8-ns5pt from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.204: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 14:29:50.204: INFO: kube-proxy-4w8t9 from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.204: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 27 14:29:50.204: INFO: metrics-server-67b4667644-d4vmh from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.204: INFO: 	Container metrics-server ready: true, restart count 0
Mar 27 14:29:50.204: INFO: metrics-server-67b4667644-t5ftq from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.204: INFO: 	Container metrics-server ready: true, restart count 0
Mar 27 14:29:50.204: INFO: node-exporter-8hspl from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.204: INFO: 	Container node-exporter ready: true, restart count 0
Mar 27 14:29:50.204: INFO: node-local-dns-zgn95 from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.204: INFO: 	Container node-cache ready: true, restart count 0
Mar 27 14:29:50.204: INFO: syseleven-node-problem-detector-nvvwp from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.204: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 27 14:29:50.204: INFO: user-ssh-keys-agent-svptx from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 14:29:50.204: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Mar 27 14:29:50.204: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-dgx79 from sonobuoy started at 2023-03-27 14:05:45 +0000 UTC (2 container statuses recorded)
Mar 27 14:29:50.204: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 14:29:50.205: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 03/27/23 14:29:50.205
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17504d0f905eb4ac], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 03/27/23 14:29:50.269
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar 27 14:29:51.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-583" for this suite. 03/27/23 14:29:51.271
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":71,"skipped":1455,"failed":0}
------------------------------
• [1.156 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:29:50.124
    Mar 27 14:29:50.124: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename sched-pred 03/27/23 14:29:50.125
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:29:50.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:29:50.154
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar 27 14:29:50.158: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 27 14:29:50.168: INFO: Waiting for terminating namespaces to be deleted...
    Mar 27 14:29:50.171: INFO: 
    Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq before test
    Mar 27 14:29:50.181: INFO: canal-vhmnn from kube-system started at 2023-03-27 14:03:29 +0000 UTC (2 container statuses recorded)
    Mar 27 14:29:50.181: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 14:29:50.181: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 27 14:29:50.181: INFO: csi-cinder-nodeplugin-lhmqp from kube-system started at 2023-03-27 14:03:29 +0000 UTC (3 container statuses recorded)
    Mar 27 14:29:50.181: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar 27 14:29:50.181: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 27 14:29:50.181: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 27 14:29:50.181: INFO: kube-proxy-4njm5 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.181: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 27 14:29:50.181: INFO: node-exporter-x8df2 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.181: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 27 14:29:50.181: INFO: node-local-dns-zvl8z from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.181: INFO: 	Container node-cache ready: true, restart count 0
    Mar 27 14:29:50.181: INFO: syseleven-node-problem-detector-2x2wt from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.181: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 27 14:29:50.181: INFO: user-ssh-keys-agent-65hx9 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.181: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Mar 27 14:29:50.181: INFO: sonobuoy-e2e-job-267544c30cdc4a58 from sonobuoy started at 2023-03-27 14:05:45 +0000 UTC (2 container statuses recorded)
    Mar 27 14:29:50.181: INFO: 	Container e2e ready: true, restart count 0
    Mar 27 14:29:50.181: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 14:29:50.181: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-bg2pk from sonobuoy started at 2023-03-27 14:05:46 +0000 UTC (2 container statuses recorded)
    Mar 27 14:29:50.181: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 14:29:50.181: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 27 14:29:50.181: INFO: 
    Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc before test
    Mar 27 14:29:50.191: INFO: canal-8sfms from kube-system started at 2023-03-27 14:03:34 +0000 UTC (2 container statuses recorded)
    Mar 27 14:29:50.191: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 14:29:50.191: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 27 14:29:50.191: INFO: csi-cinder-nodeplugin-lmbcb from kube-system started at 2023-03-27 14:03:34 +0000 UTC (3 container statuses recorded)
    Mar 27 14:29:50.191: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar 27 14:29:50.191: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 27 14:29:50.191: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 27 14:29:50.191: INFO: kube-proxy-gj9h9 from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.191: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 27 14:29:50.191: INFO: node-exporter-fgptx from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.191: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 27 14:29:50.191: INFO: node-local-dns-tmg5z from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.191: INFO: 	Container node-cache ready: true, restart count 0
    Mar 27 14:29:50.191: INFO: syseleven-node-problem-detector-xh9cq from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.191: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 27 14:29:50.191: INFO: user-ssh-keys-agent-x7kpb from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.191: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Mar 27 14:29:50.191: INFO: sonobuoy from sonobuoy started at 2023-03-27 14:05:39 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.191: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 27 14:29:50.191: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-4ztlm from sonobuoy started at 2023-03-27 14:05:46 +0000 UTC (2 container statuses recorded)
    Mar 27 14:29:50.191: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 14:29:50.191: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 27 14:29:50.191: INFO: 
    Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv before test
    Mar 27 14:29:50.203: INFO: calico-kube-controllers-659979cbcb-rxz7c from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.203: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar 27 14:29:50.203: INFO: canal-llxzv from kube-system started at 2023-03-27 14:02:28 +0000 UTC (2 container statuses recorded)
    Mar 27 14:29:50.203: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 14:29:50.203: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 27 14:29:50.203: INFO: cluster-autoscaler-855c4bc847-b7mwh from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.203: INFO: 	Container cluster-autoscaler ready: true, restart count 0
    Mar 27 14:29:50.203: INFO: coredns-5954c46f46-k6bxj from kube-system started at 2023-03-27 14:03:06 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.203: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 14:29:50.203: INFO: coredns-5954c46f46-qg6wb from kube-system started at 2023-03-27 14:03:06 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.203: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 14:29:50.203: INFO: csi-cinder-nodeplugin-4gffp from kube-system started at 2023-03-27 14:02:28 +0000 UTC (3 container statuses recorded)
    Mar 27 14:29:50.203: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar 27 14:29:50.203: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 27 14:29:50.203: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 27 14:29:50.203: INFO: dns-autoscaler-69d8768b6-qg5fp from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.203: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 27 14:29:50.203: INFO: konnectivity-agent-7c486cb7b8-7wxj6 from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.203: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 14:29:50.203: INFO: konnectivity-agent-7c486cb7b8-ns5pt from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.204: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 14:29:50.204: INFO: kube-proxy-4w8t9 from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.204: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 27 14:29:50.204: INFO: metrics-server-67b4667644-d4vmh from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.204: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 27 14:29:50.204: INFO: metrics-server-67b4667644-t5ftq from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.204: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 27 14:29:50.204: INFO: node-exporter-8hspl from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.204: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 27 14:29:50.204: INFO: node-local-dns-zgn95 from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.204: INFO: 	Container node-cache ready: true, restart count 0
    Mar 27 14:29:50.204: INFO: syseleven-node-problem-detector-nvvwp from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.204: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 27 14:29:50.204: INFO: user-ssh-keys-agent-svptx from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 14:29:50.204: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Mar 27 14:29:50.204: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-dgx79 from sonobuoy started at 2023-03-27 14:05:45 +0000 UTC (2 container statuses recorded)
    Mar 27 14:29:50.204: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 14:29:50.205: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 03/27/23 14:29:50.205
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17504d0f905eb4ac], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 03/27/23 14:29:50.269
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 14:29:51.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-583" for this suite. 03/27/23 14:29:51.271
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:29:51.281
Mar 27 14:29:51.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 14:29:51.282
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:29:51.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:29:51.309
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-d9ca7caa-b3af-4395-a7ce-4a8e4fccc9bf 03/27/23 14:29:51.314
STEP: Creating a pod to test consume configMaps 03/27/23 14:29:51.321
Mar 27 14:29:51.334: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3b91be17-c9b8-4195-805e-b3a351a2bf91" in namespace "projected-1108" to be "Succeeded or Failed"
Mar 27 14:29:51.343: INFO: Pod "pod-projected-configmaps-3b91be17-c9b8-4195-805e-b3a351a2bf91": Phase="Pending", Reason="", readiness=false. Elapsed: 9.128052ms
Mar 27 14:29:53.353: INFO: Pod "pod-projected-configmaps-3b91be17-c9b8-4195-805e-b3a351a2bf91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019101716s
Mar 27 14:29:55.370: INFO: Pod "pod-projected-configmaps-3b91be17-c9b8-4195-805e-b3a351a2bf91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03598594s
STEP: Saw pod success 03/27/23 14:29:55.37
Mar 27 14:29:55.370: INFO: Pod "pod-projected-configmaps-3b91be17-c9b8-4195-805e-b3a351a2bf91" satisfied condition "Succeeded or Failed"
Mar 27 14:29:55.374: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-configmaps-3b91be17-c9b8-4195-805e-b3a351a2bf91 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 14:29:55.384
Mar 27 14:29:55.573: INFO: Waiting for pod pod-projected-configmaps-3b91be17-c9b8-4195-805e-b3a351a2bf91 to disappear
Mar 27 14:29:55.579: INFO: Pod pod-projected-configmaps-3b91be17-c9b8-4195-805e-b3a351a2bf91 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 27 14:29:55.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1108" for this suite. 03/27/23 14:29:55.585
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":72,"skipped":1485,"failed":0}
------------------------------
• [4.372 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:29:51.281
    Mar 27 14:29:51.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 14:29:51.282
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:29:51.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:29:51.309
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-d9ca7caa-b3af-4395-a7ce-4a8e4fccc9bf 03/27/23 14:29:51.314
    STEP: Creating a pod to test consume configMaps 03/27/23 14:29:51.321
    Mar 27 14:29:51.334: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3b91be17-c9b8-4195-805e-b3a351a2bf91" in namespace "projected-1108" to be "Succeeded or Failed"
    Mar 27 14:29:51.343: INFO: Pod "pod-projected-configmaps-3b91be17-c9b8-4195-805e-b3a351a2bf91": Phase="Pending", Reason="", readiness=false. Elapsed: 9.128052ms
    Mar 27 14:29:53.353: INFO: Pod "pod-projected-configmaps-3b91be17-c9b8-4195-805e-b3a351a2bf91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019101716s
    Mar 27 14:29:55.370: INFO: Pod "pod-projected-configmaps-3b91be17-c9b8-4195-805e-b3a351a2bf91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03598594s
    STEP: Saw pod success 03/27/23 14:29:55.37
    Mar 27 14:29:55.370: INFO: Pod "pod-projected-configmaps-3b91be17-c9b8-4195-805e-b3a351a2bf91" satisfied condition "Succeeded or Failed"
    Mar 27 14:29:55.374: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-configmaps-3b91be17-c9b8-4195-805e-b3a351a2bf91 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 14:29:55.384
    Mar 27 14:29:55.573: INFO: Waiting for pod pod-projected-configmaps-3b91be17-c9b8-4195-805e-b3a351a2bf91 to disappear
    Mar 27 14:29:55.579: INFO: Pod pod-projected-configmaps-3b91be17-c9b8-4195-805e-b3a351a2bf91 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 27 14:29:55.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1108" for this suite. 03/27/23 14:29:55.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:29:55.657
Mar 27 14:29:55.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename downward-api 03/27/23 14:29:55.658
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:29:55.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:29:55.746
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 03/27/23 14:29:55.749
Mar 27 14:29:55.759: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b06d227c-2cde-45e4-9d57-f14e2e9d952d" in namespace "downward-api-4008" to be "Succeeded or Failed"
Mar 27 14:29:55.943: INFO: Pod "downwardapi-volume-b06d227c-2cde-45e4-9d57-f14e2e9d952d": Phase="Pending", Reason="", readiness=false. Elapsed: 183.303477ms
Mar 27 14:29:57.948: INFO: Pod "downwardapi-volume-b06d227c-2cde-45e4-9d57-f14e2e9d952d": Phase="Running", Reason="", readiness=true. Elapsed: 2.189122927s
Mar 27 14:29:59.949: INFO: Pod "downwardapi-volume-b06d227c-2cde-45e4-9d57-f14e2e9d952d": Phase="Running", Reason="", readiness=false. Elapsed: 4.189252942s
Mar 27 14:30:01.948: INFO: Pod "downwardapi-volume-b06d227c-2cde-45e4-9d57-f14e2e9d952d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.188473426s
STEP: Saw pod success 03/27/23 14:30:01.948
Mar 27 14:30:01.948: INFO: Pod "downwardapi-volume-b06d227c-2cde-45e4-9d57-f14e2e9d952d" satisfied condition "Succeeded or Failed"
Mar 27 14:30:01.955: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-b06d227c-2cde-45e4-9d57-f14e2e9d952d container client-container: <nil>
STEP: delete the pod 03/27/23 14:30:01.965
Mar 27 14:30:01.999: INFO: Waiting for pod downwardapi-volume-b06d227c-2cde-45e4-9d57-f14e2e9d952d to disappear
Mar 27 14:30:02.004: INFO: Pod downwardapi-volume-b06d227c-2cde-45e4-9d57-f14e2e9d952d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 27 14:30:02.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4008" for this suite. 03/27/23 14:30:02.011
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":73,"skipped":1521,"failed":0}
------------------------------
• [SLOW TEST] [6.380 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:29:55.657
    Mar 27 14:29:55.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename downward-api 03/27/23 14:29:55.658
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:29:55.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:29:55.746
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 03/27/23 14:29:55.749
    Mar 27 14:29:55.759: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b06d227c-2cde-45e4-9d57-f14e2e9d952d" in namespace "downward-api-4008" to be "Succeeded or Failed"
    Mar 27 14:29:55.943: INFO: Pod "downwardapi-volume-b06d227c-2cde-45e4-9d57-f14e2e9d952d": Phase="Pending", Reason="", readiness=false. Elapsed: 183.303477ms
    Mar 27 14:29:57.948: INFO: Pod "downwardapi-volume-b06d227c-2cde-45e4-9d57-f14e2e9d952d": Phase="Running", Reason="", readiness=true. Elapsed: 2.189122927s
    Mar 27 14:29:59.949: INFO: Pod "downwardapi-volume-b06d227c-2cde-45e4-9d57-f14e2e9d952d": Phase="Running", Reason="", readiness=false. Elapsed: 4.189252942s
    Mar 27 14:30:01.948: INFO: Pod "downwardapi-volume-b06d227c-2cde-45e4-9d57-f14e2e9d952d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.188473426s
    STEP: Saw pod success 03/27/23 14:30:01.948
    Mar 27 14:30:01.948: INFO: Pod "downwardapi-volume-b06d227c-2cde-45e4-9d57-f14e2e9d952d" satisfied condition "Succeeded or Failed"
    Mar 27 14:30:01.955: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-b06d227c-2cde-45e4-9d57-f14e2e9d952d container client-container: <nil>
    STEP: delete the pod 03/27/23 14:30:01.965
    Mar 27 14:30:01.999: INFO: Waiting for pod downwardapi-volume-b06d227c-2cde-45e4-9d57-f14e2e9d952d to disappear
    Mar 27 14:30:02.004: INFO: Pod downwardapi-volume-b06d227c-2cde-45e4-9d57-f14e2e9d952d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 27 14:30:02.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4008" for this suite. 03/27/23 14:30:02.011
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:30:02.04
Mar 27 14:30:02.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename pods 03/27/23 14:30:02.042
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:30:02.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:30:02.204
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 03/27/23 14:30:02.209
Mar 27 14:30:02.218: INFO: Waiting up to 5m0s for pod "pod-2gsvb" in namespace "pods-7310" to be "running"
Mar 27 14:30:02.684: INFO: Pod "pod-2gsvb": Phase="Pending", Reason="", readiness=false. Elapsed: 465.972136ms
Mar 27 14:30:04.690: INFO: Pod "pod-2gsvb": Phase="Running", Reason="", readiness=true. Elapsed: 2.472180506s
Mar 27 14:30:04.690: INFO: Pod "pod-2gsvb" satisfied condition "running"
STEP: patching /status 03/27/23 14:30:04.69
Mar 27 14:30:04.814: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 27 14:30:04.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7310" for this suite. 03/27/23 14:30:04.821
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":74,"skipped":1549,"failed":0}
------------------------------
• [3.021 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:30:02.04
    Mar 27 14:30:02.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename pods 03/27/23 14:30:02.042
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:30:02.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:30:02.204
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 03/27/23 14:30:02.209
    Mar 27 14:30:02.218: INFO: Waiting up to 5m0s for pod "pod-2gsvb" in namespace "pods-7310" to be "running"
    Mar 27 14:30:02.684: INFO: Pod "pod-2gsvb": Phase="Pending", Reason="", readiness=false. Elapsed: 465.972136ms
    Mar 27 14:30:04.690: INFO: Pod "pod-2gsvb": Phase="Running", Reason="", readiness=true. Elapsed: 2.472180506s
    Mar 27 14:30:04.690: INFO: Pod "pod-2gsvb" satisfied condition "running"
    STEP: patching /status 03/27/23 14:30:04.69
    Mar 27 14:30:04.814: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 27 14:30:04.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7310" for this suite. 03/27/23 14:30:04.821
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:30:05.063
Mar 27 14:30:05.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 14:30:05.064
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:30:05.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:30:05.329
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-643adc4b-c9fc-4b65-8dd4-092dc81b487e 03/27/23 14:30:05.332
STEP: Creating a pod to test consume configMaps 03/27/23 14:30:05.342
Mar 27 14:30:05.361: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-51955c9a-35ef-4c09-a5a3-0980c6c1c659" in namespace "projected-9424" to be "Succeeded or Failed"
Mar 27 14:30:05.365: INFO: Pod "pod-projected-configmaps-51955c9a-35ef-4c09-a5a3-0980c6c1c659": Phase="Pending", Reason="", readiness=false. Elapsed: 4.160597ms
Mar 27 14:30:07.373: INFO: Pod "pod-projected-configmaps-51955c9a-35ef-4c09-a5a3-0980c6c1c659": Phase="Running", Reason="", readiness=true. Elapsed: 2.012567851s
Mar 27 14:30:09.371: INFO: Pod "pod-projected-configmaps-51955c9a-35ef-4c09-a5a3-0980c6c1c659": Phase="Running", Reason="", readiness=false. Elapsed: 4.010727137s
Mar 27 14:30:11.370: INFO: Pod "pod-projected-configmaps-51955c9a-35ef-4c09-a5a3-0980c6c1c659": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009142347s
STEP: Saw pod success 03/27/23 14:30:11.37
Mar 27 14:30:11.370: INFO: Pod "pod-projected-configmaps-51955c9a-35ef-4c09-a5a3-0980c6c1c659" satisfied condition "Succeeded or Failed"
Mar 27 14:30:11.373: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-configmaps-51955c9a-35ef-4c09-a5a3-0980c6c1c659 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 14:30:11.382
Mar 27 14:30:11.401: INFO: Waiting for pod pod-projected-configmaps-51955c9a-35ef-4c09-a5a3-0980c6c1c659 to disappear
Mar 27 14:30:11.405: INFO: Pod pod-projected-configmaps-51955c9a-35ef-4c09-a5a3-0980c6c1c659 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 27 14:30:11.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9424" for this suite. 03/27/23 14:30:11.412
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":75,"skipped":1591,"failed":0}
------------------------------
• [SLOW TEST] [6.361 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:30:05.063
    Mar 27 14:30:05.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 14:30:05.064
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:30:05.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:30:05.329
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-643adc4b-c9fc-4b65-8dd4-092dc81b487e 03/27/23 14:30:05.332
    STEP: Creating a pod to test consume configMaps 03/27/23 14:30:05.342
    Mar 27 14:30:05.361: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-51955c9a-35ef-4c09-a5a3-0980c6c1c659" in namespace "projected-9424" to be "Succeeded or Failed"
    Mar 27 14:30:05.365: INFO: Pod "pod-projected-configmaps-51955c9a-35ef-4c09-a5a3-0980c6c1c659": Phase="Pending", Reason="", readiness=false. Elapsed: 4.160597ms
    Mar 27 14:30:07.373: INFO: Pod "pod-projected-configmaps-51955c9a-35ef-4c09-a5a3-0980c6c1c659": Phase="Running", Reason="", readiness=true. Elapsed: 2.012567851s
    Mar 27 14:30:09.371: INFO: Pod "pod-projected-configmaps-51955c9a-35ef-4c09-a5a3-0980c6c1c659": Phase="Running", Reason="", readiness=false. Elapsed: 4.010727137s
    Mar 27 14:30:11.370: INFO: Pod "pod-projected-configmaps-51955c9a-35ef-4c09-a5a3-0980c6c1c659": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009142347s
    STEP: Saw pod success 03/27/23 14:30:11.37
    Mar 27 14:30:11.370: INFO: Pod "pod-projected-configmaps-51955c9a-35ef-4c09-a5a3-0980c6c1c659" satisfied condition "Succeeded or Failed"
    Mar 27 14:30:11.373: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-configmaps-51955c9a-35ef-4c09-a5a3-0980c6c1c659 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 14:30:11.382
    Mar 27 14:30:11.401: INFO: Waiting for pod pod-projected-configmaps-51955c9a-35ef-4c09-a5a3-0980c6c1c659 to disappear
    Mar 27 14:30:11.405: INFO: Pod pod-projected-configmaps-51955c9a-35ef-4c09-a5a3-0980c6c1c659 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 27 14:30:11.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9424" for this suite. 03/27/23 14:30:11.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:30:11.425
Mar 27 14:30:11.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename cronjob 03/27/23 14:30:11.425
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:30:11.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:30:11.455
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 03/27/23 14:30:11.458
STEP: Ensuring more than one job is running at a time 03/27/23 14:30:11.468
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/27/23 14:32:01.474
STEP: Removing cronjob 03/27/23 14:32:01.478
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar 27 14:32:01.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6670" for this suite. 03/27/23 14:32:01.491
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":76,"skipped":1596,"failed":0}
------------------------------
• [SLOW TEST] [110.074 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:30:11.425
    Mar 27 14:30:11.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename cronjob 03/27/23 14:30:11.425
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:30:11.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:30:11.455
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 03/27/23 14:30:11.458
    STEP: Ensuring more than one job is running at a time 03/27/23 14:30:11.468
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/27/23 14:32:01.474
    STEP: Removing cronjob 03/27/23 14:32:01.478
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar 27 14:32:01.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6670" for this suite. 03/27/23 14:32:01.491
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:32:01.499
Mar 27 14:32:01.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 14:32:01.5
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:32:01.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:32:01.527
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-01608013-5f42-4f34-8344-c48422dd289d 03/27/23 14:32:01.532
STEP: Creating a pod to test consume configMaps 03/27/23 14:32:01.542
Mar 27 14:32:01.553: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ca16a7bd-97b5-4c54-9b7f-c3a5271ff123" in namespace "projected-8713" to be "Succeeded or Failed"
Mar 27 14:32:01.557: INFO: Pod "pod-projected-configmaps-ca16a7bd-97b5-4c54-9b7f-c3a5271ff123": Phase="Pending", Reason="", readiness=false. Elapsed: 3.338105ms
Mar 27 14:32:03.564: INFO: Pod "pod-projected-configmaps-ca16a7bd-97b5-4c54-9b7f-c3a5271ff123": Phase="Running", Reason="", readiness=true. Elapsed: 2.010683846s
Mar 27 14:32:05.563: INFO: Pod "pod-projected-configmaps-ca16a7bd-97b5-4c54-9b7f-c3a5271ff123": Phase="Running", Reason="", readiness=false. Elapsed: 4.00975407s
Mar 27 14:32:07.563: INFO: Pod "pod-projected-configmaps-ca16a7bd-97b5-4c54-9b7f-c3a5271ff123": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009513076s
STEP: Saw pod success 03/27/23 14:32:07.563
Mar 27 14:32:07.563: INFO: Pod "pod-projected-configmaps-ca16a7bd-97b5-4c54-9b7f-c3a5271ff123" satisfied condition "Succeeded or Failed"
Mar 27 14:32:07.569: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-projected-configmaps-ca16a7bd-97b5-4c54-9b7f-c3a5271ff123 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 14:32:07.584
Mar 27 14:32:07.597: INFO: Waiting for pod pod-projected-configmaps-ca16a7bd-97b5-4c54-9b7f-c3a5271ff123 to disappear
Mar 27 14:32:07.601: INFO: Pod pod-projected-configmaps-ca16a7bd-97b5-4c54-9b7f-c3a5271ff123 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 27 14:32:07.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8713" for this suite. 03/27/23 14:32:07.606
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":77,"skipped":1596,"failed":0}
------------------------------
• [SLOW TEST] [6.115 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:32:01.499
    Mar 27 14:32:01.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 14:32:01.5
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:32:01.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:32:01.527
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-01608013-5f42-4f34-8344-c48422dd289d 03/27/23 14:32:01.532
    STEP: Creating a pod to test consume configMaps 03/27/23 14:32:01.542
    Mar 27 14:32:01.553: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ca16a7bd-97b5-4c54-9b7f-c3a5271ff123" in namespace "projected-8713" to be "Succeeded or Failed"
    Mar 27 14:32:01.557: INFO: Pod "pod-projected-configmaps-ca16a7bd-97b5-4c54-9b7f-c3a5271ff123": Phase="Pending", Reason="", readiness=false. Elapsed: 3.338105ms
    Mar 27 14:32:03.564: INFO: Pod "pod-projected-configmaps-ca16a7bd-97b5-4c54-9b7f-c3a5271ff123": Phase="Running", Reason="", readiness=true. Elapsed: 2.010683846s
    Mar 27 14:32:05.563: INFO: Pod "pod-projected-configmaps-ca16a7bd-97b5-4c54-9b7f-c3a5271ff123": Phase="Running", Reason="", readiness=false. Elapsed: 4.00975407s
    Mar 27 14:32:07.563: INFO: Pod "pod-projected-configmaps-ca16a7bd-97b5-4c54-9b7f-c3a5271ff123": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009513076s
    STEP: Saw pod success 03/27/23 14:32:07.563
    Mar 27 14:32:07.563: INFO: Pod "pod-projected-configmaps-ca16a7bd-97b5-4c54-9b7f-c3a5271ff123" satisfied condition "Succeeded or Failed"
    Mar 27 14:32:07.569: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-projected-configmaps-ca16a7bd-97b5-4c54-9b7f-c3a5271ff123 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 14:32:07.584
    Mar 27 14:32:07.597: INFO: Waiting for pod pod-projected-configmaps-ca16a7bd-97b5-4c54-9b7f-c3a5271ff123 to disappear
    Mar 27 14:32:07.601: INFO: Pod pod-projected-configmaps-ca16a7bd-97b5-4c54-9b7f-c3a5271ff123 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 27 14:32:07.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8713" for this suite. 03/27/23 14:32:07.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:32:07.616
Mar 27 14:32:07.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename pod-network-test 03/27/23 14:32:07.617
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:32:07.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:32:07.643
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-2845 03/27/23 14:32:07.647
STEP: creating a selector 03/27/23 14:32:07.648
STEP: Creating the service pods in kubernetes 03/27/23 14:32:07.648
Mar 27 14:32:07.648: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 27 14:32:07.691: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2845" to be "running and ready"
Mar 27 14:32:07.700: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.193461ms
Mar 27 14:32:07.700: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:32:09.705: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013849913s
Mar 27 14:32:09.705: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:32:11.706: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014882874s
Mar 27 14:32:11.706: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:32:13.710: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019562896s
Mar 27 14:32:13.710: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:32:15.706: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015073548s
Mar 27 14:32:15.706: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:32:17.705: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014200232s
Mar 27 14:32:17.705: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:32:19.706: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.015104331s
Mar 27 14:32:19.706: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 27 14:32:19.706: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 27 14:32:19.709: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2845" to be "running and ready"
Mar 27 14:32:19.712: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.224913ms
Mar 27 14:32:19.713: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 27 14:32:19.713: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 27 14:32:19.716: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2845" to be "running and ready"
Mar 27 14:32:19.720: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.11108ms
Mar 27 14:32:19.720: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 27 14:32:19.720: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/27/23 14:32:19.723
Mar 27 14:32:19.735: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2845" to be "running"
Mar 27 14:32:19.739: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03304ms
Mar 27 14:32:21.745: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01024992s
Mar 27 14:32:21.746: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 27 14:32:21.749: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 27 14:32:21.749: INFO: Breadth first check of 172.25.1.65 on host 192.168.1.3...
Mar 27 14:32:21.752: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.66:9080/dial?request=hostname&protocol=udp&host=172.25.1.65&port=8081&tries=1'] Namespace:pod-network-test-2845 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:32:21.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:32:21.753: INFO: ExecWithOptions: Clientset creation
Mar 27 14:32:21.753: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-2845/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.66%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.1.65%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 27 14:32:21.852: INFO: Waiting for responses: map[]
Mar 27 14:32:21.852: INFO: reached 172.25.1.65 after 0/1 tries
Mar 27 14:32:21.852: INFO: Breadth first check of 172.25.2.105 on host 192.168.1.4...
Mar 27 14:32:21.856: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.66:9080/dial?request=hostname&protocol=udp&host=172.25.2.105&port=8081&tries=1'] Namespace:pod-network-test-2845 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:32:21.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:32:21.857: INFO: ExecWithOptions: Clientset creation
Mar 27 14:32:21.857: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-2845/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.66%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.2.105%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 27 14:32:21.949: INFO: Waiting for responses: map[]
Mar 27 14:32:21.949: INFO: reached 172.25.2.105 after 0/1 tries
Mar 27 14:32:21.949: INFO: Breadth first check of 172.25.0.58 on host 192.168.1.7...
Mar 27 14:32:21.957: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.66:9080/dial?request=hostname&protocol=udp&host=172.25.0.58&port=8081&tries=1'] Namespace:pod-network-test-2845 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:32:21.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:32:21.958: INFO: ExecWithOptions: Clientset creation
Mar 27 14:32:21.958: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-2845/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.66%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.0.58%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 27 14:32:22.052: INFO: Waiting for responses: map[]
Mar 27 14:32:22.052: INFO: reached 172.25.0.58 after 0/1 tries
Mar 27 14:32:22.052: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar 27 14:32:22.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2845" for this suite. 03/27/23 14:32:22.059
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":78,"skipped":1612,"failed":0}
------------------------------
• [SLOW TEST] [14.450 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:32:07.616
    Mar 27 14:32:07.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename pod-network-test 03/27/23 14:32:07.617
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:32:07.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:32:07.643
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-2845 03/27/23 14:32:07.647
    STEP: creating a selector 03/27/23 14:32:07.648
    STEP: Creating the service pods in kubernetes 03/27/23 14:32:07.648
    Mar 27 14:32:07.648: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 27 14:32:07.691: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2845" to be "running and ready"
    Mar 27 14:32:07.700: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.193461ms
    Mar 27 14:32:07.700: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:32:09.705: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013849913s
    Mar 27 14:32:09.705: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:32:11.706: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014882874s
    Mar 27 14:32:11.706: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:32:13.710: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019562896s
    Mar 27 14:32:13.710: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:32:15.706: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015073548s
    Mar 27 14:32:15.706: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:32:17.705: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014200232s
    Mar 27 14:32:17.705: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:32:19.706: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.015104331s
    Mar 27 14:32:19.706: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 27 14:32:19.706: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 27 14:32:19.709: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2845" to be "running and ready"
    Mar 27 14:32:19.712: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.224913ms
    Mar 27 14:32:19.713: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 27 14:32:19.713: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 27 14:32:19.716: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2845" to be "running and ready"
    Mar 27 14:32:19.720: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.11108ms
    Mar 27 14:32:19.720: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 27 14:32:19.720: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/27/23 14:32:19.723
    Mar 27 14:32:19.735: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2845" to be "running"
    Mar 27 14:32:19.739: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03304ms
    Mar 27 14:32:21.745: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01024992s
    Mar 27 14:32:21.746: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 27 14:32:21.749: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 27 14:32:21.749: INFO: Breadth first check of 172.25.1.65 on host 192.168.1.3...
    Mar 27 14:32:21.752: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.66:9080/dial?request=hostname&protocol=udp&host=172.25.1.65&port=8081&tries=1'] Namespace:pod-network-test-2845 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:32:21.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:32:21.753: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:32:21.753: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-2845/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.66%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.1.65%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 27 14:32:21.852: INFO: Waiting for responses: map[]
    Mar 27 14:32:21.852: INFO: reached 172.25.1.65 after 0/1 tries
    Mar 27 14:32:21.852: INFO: Breadth first check of 172.25.2.105 on host 192.168.1.4...
    Mar 27 14:32:21.856: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.66:9080/dial?request=hostname&protocol=udp&host=172.25.2.105&port=8081&tries=1'] Namespace:pod-network-test-2845 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:32:21.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:32:21.857: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:32:21.857: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-2845/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.66%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.2.105%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 27 14:32:21.949: INFO: Waiting for responses: map[]
    Mar 27 14:32:21.949: INFO: reached 172.25.2.105 after 0/1 tries
    Mar 27 14:32:21.949: INFO: Breadth first check of 172.25.0.58 on host 192.168.1.7...
    Mar 27 14:32:21.957: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.66:9080/dial?request=hostname&protocol=udp&host=172.25.0.58&port=8081&tries=1'] Namespace:pod-network-test-2845 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:32:21.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:32:21.958: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:32:21.958: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-2845/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.66%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.0.58%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 27 14:32:22.052: INFO: Waiting for responses: map[]
    Mar 27 14:32:22.052: INFO: reached 172.25.0.58 after 0/1 tries
    Mar 27 14:32:22.052: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar 27 14:32:22.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-2845" for this suite. 03/27/23 14:32:22.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:32:22.068
Mar 27 14:32:22.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename pods 03/27/23 14:32:22.069
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:32:22.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:32:22.094
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 03/27/23 14:32:22.097
Mar 27 14:32:22.108: INFO: Waiting up to 5m0s for pod "pod-hostip-f6054d65-ad0d-4ef6-83bb-3a3ef627b5bf" in namespace "pods-8737" to be "running and ready"
Mar 27 14:32:22.111: INFO: Pod "pod-hostip-f6054d65-ad0d-4ef6-83bb-3a3ef627b5bf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.658722ms
Mar 27 14:32:22.112: INFO: The phase of Pod pod-hostip-f6054d65-ad0d-4ef6-83bb-3a3ef627b5bf is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:32:24.118: INFO: Pod "pod-hostip-f6054d65-ad0d-4ef6-83bb-3a3ef627b5bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.010114798s
Mar 27 14:32:24.118: INFO: The phase of Pod pod-hostip-f6054d65-ad0d-4ef6-83bb-3a3ef627b5bf is Running (Ready = true)
Mar 27 14:32:24.118: INFO: Pod "pod-hostip-f6054d65-ad0d-4ef6-83bb-3a3ef627b5bf" satisfied condition "running and ready"
Mar 27 14:32:24.126: INFO: Pod pod-hostip-f6054d65-ad0d-4ef6-83bb-3a3ef627b5bf has hostIP: 192.168.1.3
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 27 14:32:24.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8737" for this suite. 03/27/23 14:32:24.133
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":79,"skipped":1617,"failed":0}
------------------------------
• [2.073 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:32:22.068
    Mar 27 14:32:22.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename pods 03/27/23 14:32:22.069
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:32:22.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:32:22.094
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 03/27/23 14:32:22.097
    Mar 27 14:32:22.108: INFO: Waiting up to 5m0s for pod "pod-hostip-f6054d65-ad0d-4ef6-83bb-3a3ef627b5bf" in namespace "pods-8737" to be "running and ready"
    Mar 27 14:32:22.111: INFO: Pod "pod-hostip-f6054d65-ad0d-4ef6-83bb-3a3ef627b5bf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.658722ms
    Mar 27 14:32:22.112: INFO: The phase of Pod pod-hostip-f6054d65-ad0d-4ef6-83bb-3a3ef627b5bf is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:32:24.118: INFO: Pod "pod-hostip-f6054d65-ad0d-4ef6-83bb-3a3ef627b5bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.010114798s
    Mar 27 14:32:24.118: INFO: The phase of Pod pod-hostip-f6054d65-ad0d-4ef6-83bb-3a3ef627b5bf is Running (Ready = true)
    Mar 27 14:32:24.118: INFO: Pod "pod-hostip-f6054d65-ad0d-4ef6-83bb-3a3ef627b5bf" satisfied condition "running and ready"
    Mar 27 14:32:24.126: INFO: Pod pod-hostip-f6054d65-ad0d-4ef6-83bb-3a3ef627b5bf has hostIP: 192.168.1.3
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 27 14:32:24.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8737" for this suite. 03/27/23 14:32:24.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:32:24.143
Mar 27 14:32:24.143: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename sysctl 03/27/23 14:32:24.144
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:32:24.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:32:24.168
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/27/23 14:32:24.171
STEP: Watching for error events or started pod 03/27/23 14:32:24.18
STEP: Waiting for pod completion 03/27/23 14:32:26.186
Mar 27 14:32:26.186: INFO: Waiting up to 3m0s for pod "sysctl-dfb9b837-738b-4e06-9e3b-fb1d19a4a1cf" in namespace "sysctl-3612" to be "completed"
Mar 27 14:32:26.190: INFO: Pod "sysctl-dfb9b837-738b-4e06-9e3b-fb1d19a4a1cf": Phase="Running", Reason="", readiness=true. Elapsed: 3.727321ms
Mar 27 14:32:28.197: INFO: Pod "sysctl-dfb9b837-738b-4e06-9e3b-fb1d19a4a1cf": Phase="Running", Reason="", readiness=false. Elapsed: 2.010747703s
Mar 27 14:32:30.196: INFO: Pod "sysctl-dfb9b837-738b-4e06-9e3b-fb1d19a4a1cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010247943s
Mar 27 14:32:30.196: INFO: Pod "sysctl-dfb9b837-738b-4e06-9e3b-fb1d19a4a1cf" satisfied condition "completed"
STEP: Checking that the pod succeeded 03/27/23 14:32:30.2
STEP: Getting logs from the pod 03/27/23 14:32:30.2
STEP: Checking that the sysctl is actually updated 03/27/23 14:32:30.209
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 27 14:32:30.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3612" for this suite. 03/27/23 14:32:30.215
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":80,"skipped":1631,"failed":0}
------------------------------
• [SLOW TEST] [6.079 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:32:24.143
    Mar 27 14:32:24.143: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename sysctl 03/27/23 14:32:24.144
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:32:24.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:32:24.168
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/27/23 14:32:24.171
    STEP: Watching for error events or started pod 03/27/23 14:32:24.18
    STEP: Waiting for pod completion 03/27/23 14:32:26.186
    Mar 27 14:32:26.186: INFO: Waiting up to 3m0s for pod "sysctl-dfb9b837-738b-4e06-9e3b-fb1d19a4a1cf" in namespace "sysctl-3612" to be "completed"
    Mar 27 14:32:26.190: INFO: Pod "sysctl-dfb9b837-738b-4e06-9e3b-fb1d19a4a1cf": Phase="Running", Reason="", readiness=true. Elapsed: 3.727321ms
    Mar 27 14:32:28.197: INFO: Pod "sysctl-dfb9b837-738b-4e06-9e3b-fb1d19a4a1cf": Phase="Running", Reason="", readiness=false. Elapsed: 2.010747703s
    Mar 27 14:32:30.196: INFO: Pod "sysctl-dfb9b837-738b-4e06-9e3b-fb1d19a4a1cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010247943s
    Mar 27 14:32:30.196: INFO: Pod "sysctl-dfb9b837-738b-4e06-9e3b-fb1d19a4a1cf" satisfied condition "completed"
    STEP: Checking that the pod succeeded 03/27/23 14:32:30.2
    STEP: Getting logs from the pod 03/27/23 14:32:30.2
    STEP: Checking that the sysctl is actually updated 03/27/23 14:32:30.209
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 27 14:32:30.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-3612" for this suite. 03/27/23 14:32:30.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:32:30.224
Mar 27 14:32:30.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename subpath 03/27/23 14:32:30.225
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:32:30.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:32:30.258
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/27/23 14:32:30.262
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-n7sf 03/27/23 14:32:30.273
STEP: Creating a pod to test atomic-volume-subpath 03/27/23 14:32:30.273
Mar 27 14:32:30.295: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-n7sf" in namespace "subpath-4441" to be "Succeeded or Failed"
Mar 27 14:32:30.298: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.63915ms
Mar 27 14:32:32.304: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 2.008799376s
Mar 27 14:32:34.303: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 4.008562617s
Mar 27 14:32:36.304: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 6.009156076s
Mar 27 14:32:38.303: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 8.008535358s
Mar 27 14:32:40.304: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 10.00915185s
Mar 27 14:32:42.304: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 12.009492115s
Mar 27 14:32:44.305: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 14.010248196s
Mar 27 14:32:46.303: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 16.008164602s
Mar 27 14:32:48.303: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 18.008535913s
Mar 27 14:32:50.304: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 20.009617379s
Mar 27 14:32:52.303: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=false. Elapsed: 22.00846739s
Mar 27 14:32:54.303: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008682406s
STEP: Saw pod success 03/27/23 14:32:54.303
Mar 27 14:32:54.304: INFO: Pod "pod-subpath-test-projected-n7sf" satisfied condition "Succeeded or Failed"
Mar 27 14:32:54.308: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-subpath-test-projected-n7sf container test-container-subpath-projected-n7sf: <nil>
STEP: delete the pod 03/27/23 14:32:54.317
Mar 27 14:32:54.327: INFO: Waiting for pod pod-subpath-test-projected-n7sf to disappear
Mar 27 14:32:54.331: INFO: Pod pod-subpath-test-projected-n7sf no longer exists
STEP: Deleting pod pod-subpath-test-projected-n7sf 03/27/23 14:32:54.331
Mar 27 14:32:54.331: INFO: Deleting pod "pod-subpath-test-projected-n7sf" in namespace "subpath-4441"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar 27 14:32:54.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4441" for this suite. 03/27/23 14:32:54.339
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":81,"skipped":1646,"failed":0}
------------------------------
• [SLOW TEST] [24.127 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:32:30.224
    Mar 27 14:32:30.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename subpath 03/27/23 14:32:30.225
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:32:30.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:32:30.258
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/27/23 14:32:30.262
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-n7sf 03/27/23 14:32:30.273
    STEP: Creating a pod to test atomic-volume-subpath 03/27/23 14:32:30.273
    Mar 27 14:32:30.295: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-n7sf" in namespace "subpath-4441" to be "Succeeded or Failed"
    Mar 27 14:32:30.298: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.63915ms
    Mar 27 14:32:32.304: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 2.008799376s
    Mar 27 14:32:34.303: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 4.008562617s
    Mar 27 14:32:36.304: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 6.009156076s
    Mar 27 14:32:38.303: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 8.008535358s
    Mar 27 14:32:40.304: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 10.00915185s
    Mar 27 14:32:42.304: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 12.009492115s
    Mar 27 14:32:44.305: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 14.010248196s
    Mar 27 14:32:46.303: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 16.008164602s
    Mar 27 14:32:48.303: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 18.008535913s
    Mar 27 14:32:50.304: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=true. Elapsed: 20.009617379s
    Mar 27 14:32:52.303: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Running", Reason="", readiness=false. Elapsed: 22.00846739s
    Mar 27 14:32:54.303: INFO: Pod "pod-subpath-test-projected-n7sf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008682406s
    STEP: Saw pod success 03/27/23 14:32:54.303
    Mar 27 14:32:54.304: INFO: Pod "pod-subpath-test-projected-n7sf" satisfied condition "Succeeded or Failed"
    Mar 27 14:32:54.308: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-subpath-test-projected-n7sf container test-container-subpath-projected-n7sf: <nil>
    STEP: delete the pod 03/27/23 14:32:54.317
    Mar 27 14:32:54.327: INFO: Waiting for pod pod-subpath-test-projected-n7sf to disappear
    Mar 27 14:32:54.331: INFO: Pod pod-subpath-test-projected-n7sf no longer exists
    STEP: Deleting pod pod-subpath-test-projected-n7sf 03/27/23 14:32:54.331
    Mar 27 14:32:54.331: INFO: Deleting pod "pod-subpath-test-projected-n7sf" in namespace "subpath-4441"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar 27 14:32:54.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-4441" for this suite. 03/27/23 14:32:54.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:32:54.356
Mar 27 14:32:54.357: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename pod-network-test 03/27/23 14:32:54.357
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:32:54.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:32:54.38
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-6442 03/27/23 14:32:54.383
STEP: creating a selector 03/27/23 14:32:54.383
STEP: Creating the service pods in kubernetes 03/27/23 14:32:54.383
Mar 27 14:32:54.383: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 27 14:32:54.440: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6442" to be "running and ready"
Mar 27 14:32:54.453: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010441ms
Mar 27 14:32:54.453: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:32:56.459: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.018465373s
Mar 27 14:32:56.459: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:32:58.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017356518s
Mar 27 14:32:58.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:33:00.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017875124s
Mar 27 14:33:00.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:33:02.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.017874506s
Mar 27 14:33:02.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:33:04.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.017486911s
Mar 27 14:33:04.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:33:06.459: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.018228237s
Mar 27 14:33:06.459: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:33:08.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.017098697s
Mar 27 14:33:08.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:33:10.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016763531s
Mar 27 14:33:10.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:33:12.459: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.018531315s
Mar 27 14:33:12.459: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:33:14.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.017745499s
Mar 27 14:33:14.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:33:16.459: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.018818702s
Mar 27 14:33:16.459: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 27 14:33:16.459: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 27 14:33:16.463: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6442" to be "running and ready"
Mar 27 14:33:16.467: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.269565ms
Mar 27 14:33:16.467: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 27 14:33:16.467: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 27 14:33:16.471: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6442" to be "running and ready"
Mar 27 14:33:16.474: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.541219ms
Mar 27 14:33:16.474: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 27 14:33:16.474: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/27/23 14:33:16.48
Mar 27 14:33:16.492: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6442" to be "running"
Mar 27 14:33:16.497: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.813313ms
Mar 27 14:33:18.503: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011829996s
Mar 27 14:33:20.571: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.079013856s
Mar 27 14:33:20.571: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 27 14:33:20.576: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6442" to be "running"
Mar 27 14:33:20.579: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.492652ms
Mar 27 14:33:20.579: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar 27 14:33:21.022: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 27 14:33:21.022: INFO: Going to poll 172.25.1.69 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 27 14:33:21.026: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.1.69:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6442 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:33:21.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:33:21.026: INFO: ExecWithOptions: Clientset creation
Mar 27 14:33:21.027: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-6442/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.1.69%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 27 14:33:21.140: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 27 14:33:21.140: INFO: Going to poll 172.25.2.107 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 27 14:33:22.892: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.2.107:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6442 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:33:22.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:33:22.892: INFO: ExecWithOptions: Clientset creation
Mar 27 14:33:22.892: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-6442/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.2.107%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 27 14:33:22.995: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 27 14:33:22.995: INFO: Going to poll 172.25.0.59 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 27 14:33:22.999: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.0.59:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6442 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:33:22.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:33:23.000: INFO: ExecWithOptions: Clientset creation
Mar 27 14:33:23.000: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-6442/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.0.59%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 27 14:33:29.031: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar 27 14:33:29.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6442" for this suite. 03/27/23 14:33:29.038
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":82,"skipped":1692,"failed":0}
------------------------------
• [SLOW TEST] [34.698 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:32:54.356
    Mar 27 14:32:54.357: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename pod-network-test 03/27/23 14:32:54.357
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:32:54.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:32:54.38
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-6442 03/27/23 14:32:54.383
    STEP: creating a selector 03/27/23 14:32:54.383
    STEP: Creating the service pods in kubernetes 03/27/23 14:32:54.383
    Mar 27 14:32:54.383: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 27 14:32:54.440: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6442" to be "running and ready"
    Mar 27 14:32:54.453: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010441ms
    Mar 27 14:32:54.453: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:32:56.459: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.018465373s
    Mar 27 14:32:56.459: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:32:58.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017356518s
    Mar 27 14:32:58.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:33:00.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017875124s
    Mar 27 14:33:00.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:33:02.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.017874506s
    Mar 27 14:33:02.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:33:04.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.017486911s
    Mar 27 14:33:04.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:33:06.459: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.018228237s
    Mar 27 14:33:06.459: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:33:08.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.017098697s
    Mar 27 14:33:08.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:33:10.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016763531s
    Mar 27 14:33:10.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:33:12.459: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.018531315s
    Mar 27 14:33:12.459: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:33:14.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.017745499s
    Mar 27 14:33:14.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:33:16.459: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.018818702s
    Mar 27 14:33:16.459: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 27 14:33:16.459: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 27 14:33:16.463: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6442" to be "running and ready"
    Mar 27 14:33:16.467: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.269565ms
    Mar 27 14:33:16.467: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 27 14:33:16.467: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 27 14:33:16.471: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6442" to be "running and ready"
    Mar 27 14:33:16.474: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.541219ms
    Mar 27 14:33:16.474: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 27 14:33:16.474: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/27/23 14:33:16.48
    Mar 27 14:33:16.492: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6442" to be "running"
    Mar 27 14:33:16.497: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.813313ms
    Mar 27 14:33:18.503: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011829996s
    Mar 27 14:33:20.571: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.079013856s
    Mar 27 14:33:20.571: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 27 14:33:20.576: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6442" to be "running"
    Mar 27 14:33:20.579: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.492652ms
    Mar 27 14:33:20.579: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar 27 14:33:21.022: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 27 14:33:21.022: INFO: Going to poll 172.25.1.69 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 27 14:33:21.026: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.1.69:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6442 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:33:21.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:33:21.026: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:33:21.027: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-6442/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.1.69%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 27 14:33:21.140: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar 27 14:33:21.140: INFO: Going to poll 172.25.2.107 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 27 14:33:22.892: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.2.107:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6442 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:33:22.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:33:22.892: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:33:22.892: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-6442/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.2.107%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 27 14:33:22.995: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar 27 14:33:22.995: INFO: Going to poll 172.25.0.59 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 27 14:33:22.999: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.0.59:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6442 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:33:22.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:33:23.000: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:33:23.000: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-6442/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.0.59%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 27 14:33:29.031: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar 27 14:33:29.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-6442" for this suite. 03/27/23 14:33:29.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:33:29.055
Mar 27 14:33:29.055: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename disruption 03/27/23 14:33:29.056
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:33:31.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:33:31.398
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 03/27/23 14:33:31.755
STEP: Waiting for the pdb to be processed 03/27/23 14:33:32.06
STEP: updating the pdb 03/27/23 14:33:32.069
STEP: Waiting for the pdb to be processed 03/27/23 14:33:32.571
STEP: patching the pdb 03/27/23 14:33:32.575
STEP: Waiting for the pdb to be processed 03/27/23 14:33:32.827
STEP: Waiting for the pdb to be deleted 03/27/23 14:33:33.11
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar 27 14:33:33.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3305" for this suite. 03/27/23 14:33:33.12
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":83,"skipped":1701,"failed":0}
------------------------------
• [4.073 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:33:29.055
    Mar 27 14:33:29.055: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename disruption 03/27/23 14:33:29.056
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:33:31.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:33:31.398
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 03/27/23 14:33:31.755
    STEP: Waiting for the pdb to be processed 03/27/23 14:33:32.06
    STEP: updating the pdb 03/27/23 14:33:32.069
    STEP: Waiting for the pdb to be processed 03/27/23 14:33:32.571
    STEP: patching the pdb 03/27/23 14:33:32.575
    STEP: Waiting for the pdb to be processed 03/27/23 14:33:32.827
    STEP: Waiting for the pdb to be deleted 03/27/23 14:33:33.11
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar 27 14:33:33.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-3305" for this suite. 03/27/23 14:33:33.12
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:33:33.128
Mar 27 14:33:33.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename deployment 03/27/23 14:33:33.129
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:33:33.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:33:33.402
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Mar 27 14:33:33.436: INFO: Creating deployment "test-recreate-deployment"
Mar 27 14:33:33.446: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 27 14:33:33.453: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Mar 27 14:33:35.462: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 27 14:33:35.466: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 27 14:33:35.489: INFO: Updating deployment test-recreate-deployment
Mar 27 14:33:35.489: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 27 14:33:35.616: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8184  0a9b7dda-ff11-467f-b3e0-b82d4e3b4cd7 19320 2 2023-03-27 14:33:33 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-27 14:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a92228 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-27 14:33:35 +0000 UTC,LastTransitionTime:2023-03-27 14:33:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-03-27 14:33:35 +0000 UTC,LastTransitionTime:2023-03-27 14:33:33 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 27 14:33:35.620: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-8184  a7b4b7b3-1e83-4bbd-9f06-f77a2edfc16f 19318 1 2023-03-27 14:33:35 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 0a9b7dda-ff11-467f-b3e0-b82d4e3b4cd7 0xc003a926f0 0xc003a926f1}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a9b7dda-ff11-467f-b3e0-b82d4e3b4cd7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:33:35 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a92788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 27 14:33:35.620: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 27 14:33:35.620: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-8184  c189d6da-83f1-409f-9da2-3ed3753b3fc4 19308 2 2023-03-27 14:33:33 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 0a9b7dda-ff11-467f-b3e0-b82d4e3b4cd7 0xc003a925d7 0xc003a925d8}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a9b7dda-ff11-467f-b3e0-b82d4e3b4cd7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:33:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a92688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 27 14:33:35.624: INFO: Pod "test-recreate-deployment-9d58999df-2gkr2" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-2gkr2 test-recreate-deployment-9d58999df- deployment-8184  98e20475-8293-4abb-b8b5-5b6fb4e00731 19319 0 2023-03-27 14:33:35 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df a7b4b7b3-1e83-4bbd-9f06-f77a2edfc16f 0xc003a92bd0 0xc003a92bd1}] [] [{kube-controller-manager Update v1 2023-03-27 14:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7b4b7b3-1e83-4bbd-9f06-f77a2edfc16f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:33:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5rbqd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5rbqd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:33:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:33:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:33:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:33:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:33:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 27 14:33:35.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8184" for this suite. 03/27/23 14:33:35.63
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":84,"skipped":1704,"failed":0}
------------------------------
• [2.509 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:33:33.128
    Mar 27 14:33:33.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename deployment 03/27/23 14:33:33.129
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:33:33.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:33:33.402
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Mar 27 14:33:33.436: INFO: Creating deployment "test-recreate-deployment"
    Mar 27 14:33:33.446: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Mar 27 14:33:33.453: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
    Mar 27 14:33:35.462: INFO: Waiting deployment "test-recreate-deployment" to complete
    Mar 27 14:33:35.466: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Mar 27 14:33:35.489: INFO: Updating deployment test-recreate-deployment
    Mar 27 14:33:35.489: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 27 14:33:35.616: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-8184  0a9b7dda-ff11-467f-b3e0-b82d4e3b4cd7 19320 2 2023-03-27 14:33:33 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-27 14:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a92228 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-27 14:33:35 +0000 UTC,LastTransitionTime:2023-03-27 14:33:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-03-27 14:33:35 +0000 UTC,LastTransitionTime:2023-03-27 14:33:33 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Mar 27 14:33:35.620: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-8184  a7b4b7b3-1e83-4bbd-9f06-f77a2edfc16f 19318 1 2023-03-27 14:33:35 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 0a9b7dda-ff11-467f-b3e0-b82d4e3b4cd7 0xc003a926f0 0xc003a926f1}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a9b7dda-ff11-467f-b3e0-b82d4e3b4cd7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:33:35 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a92788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 14:33:35.620: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Mar 27 14:33:35.620: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-8184  c189d6da-83f1-409f-9da2-3ed3753b3fc4 19308 2 2023-03-27 14:33:33 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 0a9b7dda-ff11-467f-b3e0-b82d4e3b4cd7 0xc003a925d7 0xc003a925d8}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a9b7dda-ff11-467f-b3e0-b82d4e3b4cd7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:33:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a92688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 14:33:35.624: INFO: Pod "test-recreate-deployment-9d58999df-2gkr2" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-2gkr2 test-recreate-deployment-9d58999df- deployment-8184  98e20475-8293-4abb-b8b5-5b6fb4e00731 19319 0 2023-03-27 14:33:35 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df a7b4b7b3-1e83-4bbd-9f06-f77a2edfc16f 0xc003a92bd0 0xc003a92bd1}] [] [{kube-controller-manager Update v1 2023-03-27 14:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7b4b7b3-1e83-4bbd-9f06-f77a2edfc16f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:33:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5rbqd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5rbqd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:33:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:33:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:33:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:33:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:33:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 27 14:33:35.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8184" for this suite. 03/27/23 14:33:35.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:33:35.646
Mar 27 14:33:35.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 14:33:35.647
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:33:35.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:33:35.674
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 03/27/23 14:33:35.679
Mar 27 14:33:35.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: rename a version 03/27/23 14:33:58.32
STEP: check the new version name is served 03/27/23 14:33:58.404
STEP: check the old version name is removed 03/27/23 14:34:04.921
STEP: check the other version is not changed 03/27/23 14:34:11.186
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:34:31.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3700" for this suite. 03/27/23 14:34:31.923
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":85,"skipped":1754,"failed":0}
------------------------------
• [SLOW TEST] [56.295 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:33:35.646
    Mar 27 14:33:35.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 14:33:35.647
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:33:35.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:33:35.674
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 03/27/23 14:33:35.679
    Mar 27 14:33:35.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: rename a version 03/27/23 14:33:58.32
    STEP: check the new version name is served 03/27/23 14:33:58.404
    STEP: check the old version name is removed 03/27/23 14:34:04.921
    STEP: check the other version is not changed 03/27/23 14:34:11.186
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:34:31.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3700" for this suite. 03/27/23 14:34:31.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:34:31.943
Mar 27 14:34:31.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename var-expansion 03/27/23 14:34:31.945
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:34:31.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:34:31.98
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 03/27/23 14:34:31.985
Mar 27 14:34:31.996: INFO: Waiting up to 2m0s for pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee" in namespace "var-expansion-8084" to be "running"
Mar 27 14:34:32.002: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 5.04707ms
Mar 27 14:34:34.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011607951s
Mar 27 14:34:36.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011220612s
Mar 27 14:34:38.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011312853s
Mar 27 14:34:40.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010918431s
Mar 27 14:34:42.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011689608s
Mar 27 14:34:44.006: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009405223s
Mar 27 14:34:46.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010732966s
Mar 27 14:34:48.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011445071s
Mar 27 14:34:50.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010006994s
Mar 27 14:34:52.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 20.0114276s
Mar 27 14:34:54.009: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 22.012583111s
Mar 27 14:34:56.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 24.01172951s
Mar 27 14:34:58.006: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009681309s
Mar 27 14:35:00.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010871206s
Mar 27 14:35:02.009: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012580244s
Mar 27 14:35:04.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010730658s
Mar 27 14:35:06.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 34.011538147s
Mar 27 14:35:08.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 36.011442626s
Mar 27 14:35:10.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011229264s
Mar 27 14:35:12.009: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 40.012625857s
Mar 27 14:35:14.009: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012721451s
Mar 27 14:35:16.006: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 44.00965375s
Mar 27 14:35:18.009: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012295755s
Mar 27 14:35:20.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011041847s
Mar 27 14:35:22.014: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 50.017632841s
Mar 27 14:35:24.006: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 52.009766636s
Mar 27 14:35:26.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 54.011233644s
Mar 27 14:35:28.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 56.011804819s
Mar 27 14:35:30.006: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009436198s
Mar 27 14:35:32.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010278573s
Mar 27 14:35:34.006: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009618788s
Mar 27 14:35:36.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.010445034s
Mar 27 14:35:38.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.010439845s
Mar 27 14:35:40.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010042945s
Mar 27 14:35:42.011: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.014487064s
Mar 27 14:35:44.013: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.016334906s
Mar 27 14:35:46.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010847053s
Mar 27 14:35:48.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010575275s
Mar 27 14:35:50.011: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.01420244s
Mar 27 14:35:52.009: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.012115025s
Mar 27 14:35:54.011: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.014783198s
Mar 27 14:35:56.006: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009824513s
Mar 27 14:35:58.016: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.019017734s
Mar 27 14:36:00.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010672072s
Mar 27 14:36:02.014: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.017329624s
Mar 27 14:36:04.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.010476865s
Mar 27 14:36:06.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010006445s
Mar 27 14:36:08.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011203862s
Mar 27 14:36:10.011: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.014012239s
Mar 27 14:36:12.009: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.011877677s
Mar 27 14:36:14.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.01154275s
Mar 27 14:36:16.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.011719158s
Mar 27 14:36:18.010: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.013649258s
Mar 27 14:36:20.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.010492312s
Mar 27 14:36:22.009: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.012008415s
Mar 27 14:36:24.009: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.011980106s
Mar 27 14:36:26.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011221515s
Mar 27 14:36:28.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010321897s
Mar 27 14:36:30.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.011826084s
Mar 27 14:36:32.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011818228s
Mar 27 14:36:32.013: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.016499809s
STEP: updating the pod 03/27/23 14:36:32.013
Mar 27 14:36:32.534: INFO: Successfully updated pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee"
STEP: waiting for pod running 03/27/23 14:36:32.534
Mar 27 14:36:32.534: INFO: Waiting up to 2m0s for pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee" in namespace "var-expansion-8084" to be "running"
Mar 27 14:36:32.544: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 10.131018ms
Mar 27 14:36:34.549: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Running", Reason="", readiness=true. Elapsed: 2.014483522s
Mar 27 14:36:34.549: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee" satisfied condition "running"
STEP: deleting the pod gracefully 03/27/23 14:36:34.549
Mar 27 14:36:34.549: INFO: Deleting pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee" in namespace "var-expansion-8084"
Mar 27 14:36:34.559: INFO: Wait up to 5m0s for pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 27 14:37:06.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8084" for this suite. 03/27/23 14:37:06.579
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":86,"skipped":1784,"failed":0}
------------------------------
• [SLOW TEST] [154.644 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:34:31.943
    Mar 27 14:34:31.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename var-expansion 03/27/23 14:34:31.945
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:34:31.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:34:31.98
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 03/27/23 14:34:31.985
    Mar 27 14:34:31.996: INFO: Waiting up to 2m0s for pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee" in namespace "var-expansion-8084" to be "running"
    Mar 27 14:34:32.002: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 5.04707ms
    Mar 27 14:34:34.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011607951s
    Mar 27 14:34:36.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011220612s
    Mar 27 14:34:38.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011312853s
    Mar 27 14:34:40.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010918431s
    Mar 27 14:34:42.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011689608s
    Mar 27 14:34:44.006: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009405223s
    Mar 27 14:34:46.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010732966s
    Mar 27 14:34:48.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011445071s
    Mar 27 14:34:50.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010006994s
    Mar 27 14:34:52.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 20.0114276s
    Mar 27 14:34:54.009: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 22.012583111s
    Mar 27 14:34:56.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 24.01172951s
    Mar 27 14:34:58.006: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009681309s
    Mar 27 14:35:00.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010871206s
    Mar 27 14:35:02.009: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012580244s
    Mar 27 14:35:04.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010730658s
    Mar 27 14:35:06.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 34.011538147s
    Mar 27 14:35:08.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 36.011442626s
    Mar 27 14:35:10.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011229264s
    Mar 27 14:35:12.009: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 40.012625857s
    Mar 27 14:35:14.009: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012721451s
    Mar 27 14:35:16.006: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 44.00965375s
    Mar 27 14:35:18.009: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012295755s
    Mar 27 14:35:20.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011041847s
    Mar 27 14:35:22.014: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 50.017632841s
    Mar 27 14:35:24.006: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 52.009766636s
    Mar 27 14:35:26.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 54.011233644s
    Mar 27 14:35:28.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 56.011804819s
    Mar 27 14:35:30.006: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009436198s
    Mar 27 14:35:32.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010278573s
    Mar 27 14:35:34.006: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009618788s
    Mar 27 14:35:36.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.010445034s
    Mar 27 14:35:38.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.010439845s
    Mar 27 14:35:40.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010042945s
    Mar 27 14:35:42.011: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.014487064s
    Mar 27 14:35:44.013: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.016334906s
    Mar 27 14:35:46.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010847053s
    Mar 27 14:35:48.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010575275s
    Mar 27 14:35:50.011: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.01420244s
    Mar 27 14:35:52.009: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.012115025s
    Mar 27 14:35:54.011: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.014783198s
    Mar 27 14:35:56.006: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009824513s
    Mar 27 14:35:58.016: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.019017734s
    Mar 27 14:36:00.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010672072s
    Mar 27 14:36:02.014: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.017329624s
    Mar 27 14:36:04.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.010476865s
    Mar 27 14:36:06.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010006445s
    Mar 27 14:36:08.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011203862s
    Mar 27 14:36:10.011: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.014012239s
    Mar 27 14:36:12.009: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.011877677s
    Mar 27 14:36:14.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.01154275s
    Mar 27 14:36:16.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.011719158s
    Mar 27 14:36:18.010: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.013649258s
    Mar 27 14:36:20.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.010492312s
    Mar 27 14:36:22.009: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.012008415s
    Mar 27 14:36:24.009: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.011980106s
    Mar 27 14:36:26.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011221515s
    Mar 27 14:36:28.007: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010321897s
    Mar 27 14:36:30.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.011826084s
    Mar 27 14:36:32.008: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011818228s
    Mar 27 14:36:32.013: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.016499809s
    STEP: updating the pod 03/27/23 14:36:32.013
    Mar 27 14:36:32.534: INFO: Successfully updated pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee"
    STEP: waiting for pod running 03/27/23 14:36:32.534
    Mar 27 14:36:32.534: INFO: Waiting up to 2m0s for pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee" in namespace "var-expansion-8084" to be "running"
    Mar 27 14:36:32.544: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Pending", Reason="", readiness=false. Elapsed: 10.131018ms
    Mar 27 14:36:34.549: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee": Phase="Running", Reason="", readiness=true. Elapsed: 2.014483522s
    Mar 27 14:36:34.549: INFO: Pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee" satisfied condition "running"
    STEP: deleting the pod gracefully 03/27/23 14:36:34.549
    Mar 27 14:36:34.549: INFO: Deleting pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee" in namespace "var-expansion-8084"
    Mar 27 14:36:34.559: INFO: Wait up to 5m0s for pod "var-expansion-f3b1ceb5-62c4-4f5b-a0f8-2bcac3571bee" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 27 14:37:06.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8084" for this suite. 03/27/23 14:37:06.579
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:37:06.588
Mar 27 14:37:06.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 14:37:06.59
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:37:06.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:37:06.617
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-85bf6290-18aa-472f-aa9d-0c911d49b205 03/27/23 14:37:06.625
STEP: Creating configMap with name cm-test-opt-upd-7fbb6beb-3f05-4d8d-a61f-bfd655a3eb00 03/27/23 14:37:06.633
STEP: Creating the pod 03/27/23 14:37:06.639
Mar 27 14:37:06.650: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-737e161f-1be0-4f5a-ad34-d66f91174f29" in namespace "projected-5032" to be "running and ready"
Mar 27 14:37:06.655: INFO: Pod "pod-projected-configmaps-737e161f-1be0-4f5a-ad34-d66f91174f29": Phase="Pending", Reason="", readiness=false. Elapsed: 5.647539ms
Mar 27 14:37:06.655: INFO: The phase of Pod pod-projected-configmaps-737e161f-1be0-4f5a-ad34-d66f91174f29 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:37:08.661: INFO: Pod "pod-projected-configmaps-737e161f-1be0-4f5a-ad34-d66f91174f29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011724454s
Mar 27 14:37:08.661: INFO: The phase of Pod pod-projected-configmaps-737e161f-1be0-4f5a-ad34-d66f91174f29 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:37:10.664: INFO: Pod "pod-projected-configmaps-737e161f-1be0-4f5a-ad34-d66f91174f29": Phase="Running", Reason="", readiness=true. Elapsed: 4.013994735s
Mar 27 14:37:10.664: INFO: The phase of Pod pod-projected-configmaps-737e161f-1be0-4f5a-ad34-d66f91174f29 is Running (Ready = true)
Mar 27 14:37:10.664: INFO: Pod "pod-projected-configmaps-737e161f-1be0-4f5a-ad34-d66f91174f29" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-85bf6290-18aa-472f-aa9d-0c911d49b205 03/27/23 14:37:10.7
STEP: Updating configmap cm-test-opt-upd-7fbb6beb-3f05-4d8d-a61f-bfd655a3eb00 03/27/23 14:37:10.71
STEP: Creating configMap with name cm-test-opt-create-15b7205e-8ba6-47bd-bf04-a947bbeb3b46 03/27/23 14:37:10.717
STEP: waiting to observe update in volume 03/27/23 14:37:10.723
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 27 14:38:15.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5032" for this suite. 03/27/23 14:38:15.33
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":87,"skipped":1785,"failed":0}
------------------------------
• [SLOW TEST] [68.754 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:37:06.588
    Mar 27 14:37:06.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 14:37:06.59
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:37:06.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:37:06.617
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-85bf6290-18aa-472f-aa9d-0c911d49b205 03/27/23 14:37:06.625
    STEP: Creating configMap with name cm-test-opt-upd-7fbb6beb-3f05-4d8d-a61f-bfd655a3eb00 03/27/23 14:37:06.633
    STEP: Creating the pod 03/27/23 14:37:06.639
    Mar 27 14:37:06.650: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-737e161f-1be0-4f5a-ad34-d66f91174f29" in namespace "projected-5032" to be "running and ready"
    Mar 27 14:37:06.655: INFO: Pod "pod-projected-configmaps-737e161f-1be0-4f5a-ad34-d66f91174f29": Phase="Pending", Reason="", readiness=false. Elapsed: 5.647539ms
    Mar 27 14:37:06.655: INFO: The phase of Pod pod-projected-configmaps-737e161f-1be0-4f5a-ad34-d66f91174f29 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:37:08.661: INFO: Pod "pod-projected-configmaps-737e161f-1be0-4f5a-ad34-d66f91174f29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011724454s
    Mar 27 14:37:08.661: INFO: The phase of Pod pod-projected-configmaps-737e161f-1be0-4f5a-ad34-d66f91174f29 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:37:10.664: INFO: Pod "pod-projected-configmaps-737e161f-1be0-4f5a-ad34-d66f91174f29": Phase="Running", Reason="", readiness=true. Elapsed: 4.013994735s
    Mar 27 14:37:10.664: INFO: The phase of Pod pod-projected-configmaps-737e161f-1be0-4f5a-ad34-d66f91174f29 is Running (Ready = true)
    Mar 27 14:37:10.664: INFO: Pod "pod-projected-configmaps-737e161f-1be0-4f5a-ad34-d66f91174f29" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-85bf6290-18aa-472f-aa9d-0c911d49b205 03/27/23 14:37:10.7
    STEP: Updating configmap cm-test-opt-upd-7fbb6beb-3f05-4d8d-a61f-bfd655a3eb00 03/27/23 14:37:10.71
    STEP: Creating configMap with name cm-test-opt-create-15b7205e-8ba6-47bd-bf04-a947bbeb3b46 03/27/23 14:37:10.717
    STEP: waiting to observe update in volume 03/27/23 14:37:10.723
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 27 14:38:15.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5032" for this suite. 03/27/23 14:38:15.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:38:15.343
Mar 27 14:38:15.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 14:38:15.345
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:38:15.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:38:15.371
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-008c9513-1d4f-4688-8a62-fa4f0e78d29a 03/27/23 14:38:15.374
STEP: Creating a pod to test consume configMaps 03/27/23 14:38:15.389
Mar 27 14:38:15.399: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0e9ec6bf-16fd-4c69-b136-6bf90c71d25c" in namespace "projected-2489" to be "Succeeded or Failed"
Mar 27 14:38:15.407: INFO: Pod "pod-projected-configmaps-0e9ec6bf-16fd-4c69-b136-6bf90c71d25c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.652895ms
Mar 27 14:38:17.412: INFO: Pod "pod-projected-configmaps-0e9ec6bf-16fd-4c69-b136-6bf90c71d25c": Phase="Running", Reason="", readiness=true. Elapsed: 2.012553146s
Mar 27 14:38:19.413: INFO: Pod "pod-projected-configmaps-0e9ec6bf-16fd-4c69-b136-6bf90c71d25c": Phase="Running", Reason="", readiness=false. Elapsed: 4.013505203s
Mar 27 14:38:21.412: INFO: Pod "pod-projected-configmaps-0e9ec6bf-16fd-4c69-b136-6bf90c71d25c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012673297s
STEP: Saw pod success 03/27/23 14:38:21.412
Mar 27 14:38:21.412: INFO: Pod "pod-projected-configmaps-0e9ec6bf-16fd-4c69-b136-6bf90c71d25c" satisfied condition "Succeeded or Failed"
Mar 27 14:38:21.416: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-projected-configmaps-0e9ec6bf-16fd-4c69-b136-6bf90c71d25c container projected-configmap-volume-test: <nil>
STEP: delete the pod 03/27/23 14:38:21.429
Mar 27 14:38:21.444: INFO: Waiting for pod pod-projected-configmaps-0e9ec6bf-16fd-4c69-b136-6bf90c71d25c to disappear
Mar 27 14:38:21.448: INFO: Pod pod-projected-configmaps-0e9ec6bf-16fd-4c69-b136-6bf90c71d25c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 27 14:38:21.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2489" for this suite. 03/27/23 14:38:21.455
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":88,"skipped":1795,"failed":0}
------------------------------
• [SLOW TEST] [6.121 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:38:15.343
    Mar 27 14:38:15.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 14:38:15.345
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:38:15.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:38:15.371
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-008c9513-1d4f-4688-8a62-fa4f0e78d29a 03/27/23 14:38:15.374
    STEP: Creating a pod to test consume configMaps 03/27/23 14:38:15.389
    Mar 27 14:38:15.399: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0e9ec6bf-16fd-4c69-b136-6bf90c71d25c" in namespace "projected-2489" to be "Succeeded or Failed"
    Mar 27 14:38:15.407: INFO: Pod "pod-projected-configmaps-0e9ec6bf-16fd-4c69-b136-6bf90c71d25c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.652895ms
    Mar 27 14:38:17.412: INFO: Pod "pod-projected-configmaps-0e9ec6bf-16fd-4c69-b136-6bf90c71d25c": Phase="Running", Reason="", readiness=true. Elapsed: 2.012553146s
    Mar 27 14:38:19.413: INFO: Pod "pod-projected-configmaps-0e9ec6bf-16fd-4c69-b136-6bf90c71d25c": Phase="Running", Reason="", readiness=false. Elapsed: 4.013505203s
    Mar 27 14:38:21.412: INFO: Pod "pod-projected-configmaps-0e9ec6bf-16fd-4c69-b136-6bf90c71d25c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012673297s
    STEP: Saw pod success 03/27/23 14:38:21.412
    Mar 27 14:38:21.412: INFO: Pod "pod-projected-configmaps-0e9ec6bf-16fd-4c69-b136-6bf90c71d25c" satisfied condition "Succeeded or Failed"
    Mar 27 14:38:21.416: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-projected-configmaps-0e9ec6bf-16fd-4c69-b136-6bf90c71d25c container projected-configmap-volume-test: <nil>
    STEP: delete the pod 03/27/23 14:38:21.429
    Mar 27 14:38:21.444: INFO: Waiting for pod pod-projected-configmaps-0e9ec6bf-16fd-4c69-b136-6bf90c71d25c to disappear
    Mar 27 14:38:21.448: INFO: Pod pod-projected-configmaps-0e9ec6bf-16fd-4c69-b136-6bf90c71d25c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 27 14:38:21.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2489" for this suite. 03/27/23 14:38:21.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:38:21.466
Mar 27 14:38:21.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename job 03/27/23 14:38:21.468
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:38:21.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:38:21.495
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 03/27/23 14:38:21.498
STEP: Ensuring job reaches completions 03/27/23 14:38:21.506
STEP: Ensuring pods with index for job exist 03/27/23 14:38:31.512
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar 27 14:38:31.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9481" for this suite. 03/27/23 14:38:31.522
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":89,"skipped":1807,"failed":0}
------------------------------
• [SLOW TEST] [10.062 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:38:21.466
    Mar 27 14:38:21.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename job 03/27/23 14:38:21.468
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:38:21.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:38:21.495
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 03/27/23 14:38:21.498
    STEP: Ensuring job reaches completions 03/27/23 14:38:21.506
    STEP: Ensuring pods with index for job exist 03/27/23 14:38:31.512
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar 27 14:38:31.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-9481" for this suite. 03/27/23 14:38:31.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:38:31.532
Mar 27 14:38:31.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename pods 03/27/23 14:38:31.533
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:38:31.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:38:31.553
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 03/27/23 14:38:31.565
STEP: watching for Pod to be ready 03/27/23 14:38:31.576
Mar 27 14:38:31.578: INFO: observed Pod pod-test in namespace pods-5804 in phase Pending with labels: map[test-pod-static:true] & conditions []
Mar 27 14:38:31.586: INFO: observed Pod pod-test in namespace pods-5804 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC  }]
Mar 27 14:38:31.601: INFO: observed Pod pod-test in namespace pods-5804 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC  }]
Mar 27 14:38:32.039: INFO: observed Pod pod-test in namespace pods-5804 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC  }]
Mar 27 14:38:33.462: INFO: Found Pod pod-test in namespace pods-5804 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 03/27/23 14:38:33.471
STEP: getting the Pod and ensuring that it's patched 03/27/23 14:38:33.489
STEP: replacing the Pod's status Ready condition to False 03/27/23 14:38:33.494
STEP: check the Pod again to ensure its Ready conditions are False 03/27/23 14:38:33.511
STEP: deleting the Pod via a Collection with a LabelSelector 03/27/23 14:38:33.511
STEP: watching for the Pod to be deleted 03/27/23 14:38:33.522
Mar 27 14:38:33.525: INFO: observed event type MODIFIED
Mar 27 14:38:35.465: INFO: observed event type MODIFIED
Mar 27 14:38:36.253: INFO: observed event type MODIFIED
Mar 27 14:38:36.468: INFO: observed event type MODIFIED
Mar 27 14:38:36.481: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 27 14:38:36.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5804" for this suite. 03/27/23 14:38:36.496
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":90,"skipped":1866,"failed":0}
------------------------------
• [4.971 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:38:31.532
    Mar 27 14:38:31.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename pods 03/27/23 14:38:31.533
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:38:31.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:38:31.553
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 03/27/23 14:38:31.565
    STEP: watching for Pod to be ready 03/27/23 14:38:31.576
    Mar 27 14:38:31.578: INFO: observed Pod pod-test in namespace pods-5804 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Mar 27 14:38:31.586: INFO: observed Pod pod-test in namespace pods-5804 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC  }]
    Mar 27 14:38:31.601: INFO: observed Pod pod-test in namespace pods-5804 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC  }]
    Mar 27 14:38:32.039: INFO: observed Pod pod-test in namespace pods-5804 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC  }]
    Mar 27 14:38:33.462: INFO: Found Pod pod-test in namespace pods-5804 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 14:38:31 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 03/27/23 14:38:33.471
    STEP: getting the Pod and ensuring that it's patched 03/27/23 14:38:33.489
    STEP: replacing the Pod's status Ready condition to False 03/27/23 14:38:33.494
    STEP: check the Pod again to ensure its Ready conditions are False 03/27/23 14:38:33.511
    STEP: deleting the Pod via a Collection with a LabelSelector 03/27/23 14:38:33.511
    STEP: watching for the Pod to be deleted 03/27/23 14:38:33.522
    Mar 27 14:38:33.525: INFO: observed event type MODIFIED
    Mar 27 14:38:35.465: INFO: observed event type MODIFIED
    Mar 27 14:38:36.253: INFO: observed event type MODIFIED
    Mar 27 14:38:36.468: INFO: observed event type MODIFIED
    Mar 27 14:38:36.481: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 27 14:38:36.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5804" for this suite. 03/27/23 14:38:36.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:38:36.506
Mar 27 14:38:36.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 14:38:36.507
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:38:36.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:38:36.529
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Mar 27 14:38:36.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:38:37.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4429" for this suite. 03/27/23 14:38:37.099
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":91,"skipped":1873,"failed":0}
------------------------------
• [0.603 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:38:36.506
    Mar 27 14:38:36.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 14:38:36.507
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:38:36.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:38:36.529
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Mar 27 14:38:36.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:38:37.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-4429" for this suite. 03/27/23 14:38:37.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:38:37.11
Mar 27 14:38:37.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename job 03/27/23 14:38:37.111
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:38:37.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:38:37.133
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 03/27/23 14:38:37.136
STEP: Ensure pods equal to paralellism count is attached to the job 03/27/23 14:38:37.145
STEP: patching /status 03/27/23 14:38:41.151
STEP: updating /status 03/27/23 14:38:41.166
STEP: get /status 03/27/23 14:38:41.176
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar 27 14:38:41.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3991" for this suite. 03/27/23 14:38:41.185
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":92,"skipped":1896,"failed":0}
------------------------------
• [4.087 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:38:37.11
    Mar 27 14:38:37.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename job 03/27/23 14:38:37.111
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:38:37.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:38:37.133
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 03/27/23 14:38:37.136
    STEP: Ensure pods equal to paralellism count is attached to the job 03/27/23 14:38:37.145
    STEP: patching /status 03/27/23 14:38:41.151
    STEP: updating /status 03/27/23 14:38:41.166
    STEP: get /status 03/27/23 14:38:41.176
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar 27 14:38:41.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3991" for this suite. 03/27/23 14:38:41.185
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:38:41.197
Mar 27 14:38:41.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename svcaccounts 03/27/23 14:38:41.199
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:38:41.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:38:41.225
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Mar 27 14:38:41.249: INFO: created pod
Mar 27 14:38:41.249: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8950" to be "Succeeded or Failed"
Mar 27 14:38:41.253: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.174304ms
Mar 27 14:38:43.259: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009493329s
Mar 27 14:38:45.258: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 4.008322889s
Mar 27 14:38:47.258: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008530775s
STEP: Saw pod success 03/27/23 14:38:47.258
Mar 27 14:38:47.258: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Mar 27 14:39:17.261: INFO: polling logs
Mar 27 14:39:17.273: INFO: Pod logs: 
I0327 14:38:42.629808       1 log.go:195] OK: Got token
I0327 14:38:42.629906       1 log.go:195] validating with in-cluster discovery
I0327 14:38:42.630269       1 log.go:195] OK: got issuer https://n6d2clvr86.bki1.metakube.syseleven.de:30518
I0327 14:38:42.630321       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://n6d2clvr86.bki1.metakube.syseleven.de:30518", Subject:"system:serviceaccount:svcaccounts-8950:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1679928521, NotBefore:1679927921, IssuedAt:1679927921, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8950", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c883350f-8fc3-40c8-8c35-873820ff4195"}}}
I0327 14:38:42.644360       1 log.go:195] OK: Constructed OIDC provider for issuer https://n6d2clvr86.bki1.metakube.syseleven.de:30518
I0327 14:38:42.646206       1 log.go:195] OK: Validated signature on JWT
I0327 14:38:42.646328       1 log.go:195] OK: Got valid claims from token!
I0327 14:38:42.646364       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://n6d2clvr86.bki1.metakube.syseleven.de:30518", Subject:"system:serviceaccount:svcaccounts-8950:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1679928521, NotBefore:1679927921, IssuedAt:1679927921, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8950", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c883350f-8fc3-40c8-8c35-873820ff4195"}}}

Mar 27 14:39:17.273: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar 27 14:39:17.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8950" for this suite. 03/27/23 14:39:17.286
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":93,"skipped":1898,"failed":0}
------------------------------
• [SLOW TEST] [36.096 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:38:41.197
    Mar 27 14:38:41.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename svcaccounts 03/27/23 14:38:41.199
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:38:41.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:38:41.225
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Mar 27 14:38:41.249: INFO: created pod
    Mar 27 14:38:41.249: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8950" to be "Succeeded or Failed"
    Mar 27 14:38:41.253: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.174304ms
    Mar 27 14:38:43.259: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009493329s
    Mar 27 14:38:45.258: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 4.008322889s
    Mar 27 14:38:47.258: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008530775s
    STEP: Saw pod success 03/27/23 14:38:47.258
    Mar 27 14:38:47.258: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Mar 27 14:39:17.261: INFO: polling logs
    Mar 27 14:39:17.273: INFO: Pod logs: 
    I0327 14:38:42.629808       1 log.go:195] OK: Got token
    I0327 14:38:42.629906       1 log.go:195] validating with in-cluster discovery
    I0327 14:38:42.630269       1 log.go:195] OK: got issuer https://n6d2clvr86.bki1.metakube.syseleven.de:30518
    I0327 14:38:42.630321       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://n6d2clvr86.bki1.metakube.syseleven.de:30518", Subject:"system:serviceaccount:svcaccounts-8950:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1679928521, NotBefore:1679927921, IssuedAt:1679927921, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8950", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c883350f-8fc3-40c8-8c35-873820ff4195"}}}
    I0327 14:38:42.644360       1 log.go:195] OK: Constructed OIDC provider for issuer https://n6d2clvr86.bki1.metakube.syseleven.de:30518
    I0327 14:38:42.646206       1 log.go:195] OK: Validated signature on JWT
    I0327 14:38:42.646328       1 log.go:195] OK: Got valid claims from token!
    I0327 14:38:42.646364       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://n6d2clvr86.bki1.metakube.syseleven.de:30518", Subject:"system:serviceaccount:svcaccounts-8950:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1679928521, NotBefore:1679927921, IssuedAt:1679927921, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8950", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c883350f-8fc3-40c8-8c35-873820ff4195"}}}

    Mar 27 14:39:17.273: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar 27 14:39:17.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-8950" for this suite. 03/27/23 14:39:17.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:39:17.297
Mar 27 14:39:17.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename emptydir 03/27/23 14:39:17.299
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:39:17.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:39:17.417
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/27/23 14:39:17.421
Mar 27 14:39:17.433: INFO: Waiting up to 5m0s for pod "pod-bf831653-2e0b-4d67-b930-4e26ae339ef2" in namespace "emptydir-9748" to be "Succeeded or Failed"
Mar 27 14:39:17.437: INFO: Pod "pod-bf831653-2e0b-4d67-b930-4e26ae339ef2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.098064ms
Mar 27 14:39:19.443: INFO: Pod "pod-bf831653-2e0b-4d67-b930-4e26ae339ef2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010329526s
Mar 27 14:39:21.443: INFO: Pod "pod-bf831653-2e0b-4d67-b930-4e26ae339ef2": Phase="Running", Reason="", readiness=false. Elapsed: 4.01006297s
Mar 27 14:39:23.446: INFO: Pod "pod-bf831653-2e0b-4d67-b930-4e26ae339ef2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013338542s
STEP: Saw pod success 03/27/23 14:39:23.446
Mar 27 14:39:23.446: INFO: Pod "pod-bf831653-2e0b-4d67-b930-4e26ae339ef2" satisfied condition "Succeeded or Failed"
Mar 27 14:39:23.449: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-bf831653-2e0b-4d67-b930-4e26ae339ef2 container test-container: <nil>
STEP: delete the pod 03/27/23 14:39:23.456
Mar 27 14:39:23.473: INFO: Waiting for pod pod-bf831653-2e0b-4d67-b930-4e26ae339ef2 to disappear
Mar 27 14:39:23.477: INFO: Pod pod-bf831653-2e0b-4d67-b930-4e26ae339ef2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 27 14:39:23.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9748" for this suite. 03/27/23 14:39:23.482
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":94,"skipped":1906,"failed":0}
------------------------------
• [SLOW TEST] [6.191 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:39:17.297
    Mar 27 14:39:17.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename emptydir 03/27/23 14:39:17.299
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:39:17.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:39:17.417
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/27/23 14:39:17.421
    Mar 27 14:39:17.433: INFO: Waiting up to 5m0s for pod "pod-bf831653-2e0b-4d67-b930-4e26ae339ef2" in namespace "emptydir-9748" to be "Succeeded or Failed"
    Mar 27 14:39:17.437: INFO: Pod "pod-bf831653-2e0b-4d67-b930-4e26ae339ef2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.098064ms
    Mar 27 14:39:19.443: INFO: Pod "pod-bf831653-2e0b-4d67-b930-4e26ae339ef2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010329526s
    Mar 27 14:39:21.443: INFO: Pod "pod-bf831653-2e0b-4d67-b930-4e26ae339ef2": Phase="Running", Reason="", readiness=false. Elapsed: 4.01006297s
    Mar 27 14:39:23.446: INFO: Pod "pod-bf831653-2e0b-4d67-b930-4e26ae339ef2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013338542s
    STEP: Saw pod success 03/27/23 14:39:23.446
    Mar 27 14:39:23.446: INFO: Pod "pod-bf831653-2e0b-4d67-b930-4e26ae339ef2" satisfied condition "Succeeded or Failed"
    Mar 27 14:39:23.449: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-bf831653-2e0b-4d67-b930-4e26ae339ef2 container test-container: <nil>
    STEP: delete the pod 03/27/23 14:39:23.456
    Mar 27 14:39:23.473: INFO: Waiting for pod pod-bf831653-2e0b-4d67-b930-4e26ae339ef2 to disappear
    Mar 27 14:39:23.477: INFO: Pod pod-bf831653-2e0b-4d67-b930-4e26ae339ef2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 27 14:39:23.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9748" for this suite. 03/27/23 14:39:23.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:39:23.489
Mar 27 14:39:23.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename podtemplate 03/27/23 14:39:23.49
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:39:23.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:39:23.514
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 03/27/23 14:39:23.517
STEP: Replace a pod template 03/27/23 14:39:23.53
Mar 27 14:39:23.541: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar 27 14:39:23.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8391" for this suite. 03/27/23 14:39:23.549
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":95,"skipped":1917,"failed":0}
------------------------------
• [0.069 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:39:23.489
    Mar 27 14:39:23.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename podtemplate 03/27/23 14:39:23.49
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:39:23.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:39:23.514
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 03/27/23 14:39:23.517
    STEP: Replace a pod template 03/27/23 14:39:23.53
    Mar 27 14:39:23.541: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar 27 14:39:23.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-8391" for this suite. 03/27/23 14:39:23.549
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:39:23.56
Mar 27 14:39:23.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename runtimeclass 03/27/23 14:39:23.561
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:39:23.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:39:23.586
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 03/27/23 14:39:23.59
STEP: getting /apis/node.k8s.io 03/27/23 14:39:23.593
STEP: getting /apis/node.k8s.io/v1 03/27/23 14:39:23.595
STEP: creating 03/27/23 14:39:23.597
STEP: watching 03/27/23 14:39:23.614
Mar 27 14:39:23.615: INFO: starting watch
STEP: getting 03/27/23 14:39:23.622
STEP: listing 03/27/23 14:39:23.626
STEP: patching 03/27/23 14:39:23.631
STEP: updating 03/27/23 14:39:23.638
Mar 27 14:39:23.643: INFO: waiting for watch events with expected annotations
STEP: deleting 03/27/23 14:39:23.643
STEP: deleting a collection 03/27/23 14:39:23.657
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar 27 14:39:23.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1526" for this suite. 03/27/23 14:39:23.681
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":96,"skipped":1929,"failed":0}
------------------------------
• [0.128 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:39:23.56
    Mar 27 14:39:23.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename runtimeclass 03/27/23 14:39:23.561
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:39:23.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:39:23.586
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 03/27/23 14:39:23.59
    STEP: getting /apis/node.k8s.io 03/27/23 14:39:23.593
    STEP: getting /apis/node.k8s.io/v1 03/27/23 14:39:23.595
    STEP: creating 03/27/23 14:39:23.597
    STEP: watching 03/27/23 14:39:23.614
    Mar 27 14:39:23.615: INFO: starting watch
    STEP: getting 03/27/23 14:39:23.622
    STEP: listing 03/27/23 14:39:23.626
    STEP: patching 03/27/23 14:39:23.631
    STEP: updating 03/27/23 14:39:23.638
    Mar 27 14:39:23.643: INFO: waiting for watch events with expected annotations
    STEP: deleting 03/27/23 14:39:23.643
    STEP: deleting a collection 03/27/23 14:39:23.657
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar 27 14:39:23.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1526" for this suite. 03/27/23 14:39:23.681
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:39:23.689
Mar 27 14:39:23.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename gc 03/27/23 14:39:23.69
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:39:23.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:39:23.712
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 03/27/23 14:39:23.727
STEP: create the rc2 03/27/23 14:39:23.74
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/27/23 14:39:28.752
STEP: delete the rc simpletest-rc-to-be-deleted 03/27/23 14:39:29.444
STEP: wait for the rc to be deleted 03/27/23 14:39:29.457
Mar 27 14:39:34.475: INFO: 66 pods remaining
Mar 27 14:39:34.475: INFO: 66 pods has nil DeletionTimestamp
Mar 27 14:39:34.475: INFO: 
STEP: Gathering metrics 03/27/23 14:39:39.485
W0327 14:39:39.498243      24 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 27 14:39:39.498: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 27 14:39:39.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-258pd" in namespace "gc-4417"
Mar 27 14:39:39.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-2blmt" in namespace "gc-4417"
Mar 27 14:39:39.533: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bt7l" in namespace "gc-4417"
Mar 27 14:39:39.555: INFO: Deleting pod "simpletest-rc-to-be-deleted-2cs8x" in namespace "gc-4417"
Mar 27 14:39:39.581: INFO: Deleting pod "simpletest-rc-to-be-deleted-2f94c" in namespace "gc-4417"
Mar 27 14:39:39.596: INFO: Deleting pod "simpletest-rc-to-be-deleted-2z2vz" in namespace "gc-4417"
Mar 27 14:39:39.613: INFO: Deleting pod "simpletest-rc-to-be-deleted-55csd" in namespace "gc-4417"
Mar 27 14:39:39.632: INFO: Deleting pod "simpletest-rc-to-be-deleted-59h88" in namespace "gc-4417"
Mar 27 14:39:39.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-5d4bd" in namespace "gc-4417"
Mar 27 14:39:39.666: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rpph" in namespace "gc-4417"
Mar 27 14:39:39.687: INFO: Deleting pod "simpletest-rc-to-be-deleted-8wtzx" in namespace "gc-4417"
Mar 27 14:39:39.704: INFO: Deleting pod "simpletest-rc-to-be-deleted-9d9t8" in namespace "gc-4417"
Mar 27 14:39:39.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-9r6bx" in namespace "gc-4417"
Mar 27 14:39:39.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rnqt" in namespace "gc-4417"
Mar 27 14:39:39.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-9tp6g" in namespace "gc-4417"
Mar 27 14:39:39.767: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5vbw" in namespace "gc-4417"
Mar 27 14:39:39.788: INFO: Deleting pod "simpletest-rc-to-be-deleted-bphkb" in namespace "gc-4417"
Mar 27 14:39:39.802: INFO: Deleting pod "simpletest-rc-to-be-deleted-bx6vk" in namespace "gc-4417"
Mar 27 14:39:39.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-cb8kp" in namespace "gc-4417"
Mar 27 14:39:39.831: INFO: Deleting pod "simpletest-rc-to-be-deleted-cf4lf" in namespace "gc-4417"
Mar 27 14:39:39.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-cfcrs" in namespace "gc-4417"
Mar 27 14:39:39.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-ck2jb" in namespace "gc-4417"
Mar 27 14:39:39.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-clxtn" in namespace "gc-4417"
Mar 27 14:39:39.891: INFO: Deleting pod "simpletest-rc-to-be-deleted-cz5nw" in namespace "gc-4417"
Mar 27 14:39:39.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2nx5" in namespace "gc-4417"
Mar 27 14:39:39.919: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2vjl" in namespace "gc-4417"
Mar 27 14:39:39.934: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6jf2" in namespace "gc-4417"
Mar 27 14:39:39.945: INFO: Deleting pod "simpletest-rc-to-be-deleted-dbs6s" in namespace "gc-4417"
Mar 27 14:39:39.960: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgdjz" in namespace "gc-4417"
Mar 27 14:39:39.981: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtfjk" in namespace "gc-4417"
Mar 27 14:39:39.994: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvtfk" in namespace "gc-4417"
Mar 27 14:39:40.012: INFO: Deleting pod "simpletest-rc-to-be-deleted-f266l" in namespace "gc-4417"
Mar 27 14:39:40.027: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2dnt" in namespace "gc-4417"
Mar 27 14:39:40.037: INFO: Deleting pod "simpletest-rc-to-be-deleted-fb2bv" in namespace "gc-4417"
Mar 27 14:39:40.049: INFO: Deleting pod "simpletest-rc-to-be-deleted-flvc8" in namespace "gc-4417"
Mar 27 14:39:40.066: INFO: Deleting pod "simpletest-rc-to-be-deleted-fns52" in namespace "gc-4417"
Mar 27 14:39:40.080: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwdlk" in namespace "gc-4417"
Mar 27 14:39:40.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjbj6" in namespace "gc-4417"
Mar 27 14:39:40.104: INFO: Deleting pod "simpletest-rc-to-be-deleted-gldxm" in namespace "gc-4417"
Mar 27 14:39:40.115: INFO: Deleting pod "simpletest-rc-to-be-deleted-glfpj" in namespace "gc-4417"
Mar 27 14:39:40.128: INFO: Deleting pod "simpletest-rc-to-be-deleted-gptw2" in namespace "gc-4417"
Mar 27 14:39:40.140: INFO: Deleting pod "simpletest-rc-to-be-deleted-grw9f" in namespace "gc-4417"
Mar 27 14:39:40.150: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtvjc" in namespace "gc-4417"
Mar 27 14:39:40.164: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtwn9" in namespace "gc-4417"
Mar 27 14:39:40.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwqff" in namespace "gc-4417"
Mar 27 14:39:40.198: INFO: Deleting pod "simpletest-rc-to-be-deleted-h7wlf" in namespace "gc-4417"
Mar 27 14:39:40.224: INFO: Deleting pod "simpletest-rc-to-be-deleted-hz98z" in namespace "gc-4417"
Mar 27 14:39:40.236: INFO: Deleting pod "simpletest-rc-to-be-deleted-jcwt9" in namespace "gc-4417"
Mar 27 14:39:40.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-jz5t6" in namespace "gc-4417"
Mar 27 14:39:40.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-k4t8s" in namespace "gc-4417"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 27 14:39:40.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4417" for this suite. 03/27/23 14:39:40.284
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":97,"skipped":1932,"failed":0}
------------------------------
• [SLOW TEST] [16.603 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:39:23.689
    Mar 27 14:39:23.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename gc 03/27/23 14:39:23.69
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:39:23.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:39:23.712
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 03/27/23 14:39:23.727
    STEP: create the rc2 03/27/23 14:39:23.74
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/27/23 14:39:28.752
    STEP: delete the rc simpletest-rc-to-be-deleted 03/27/23 14:39:29.444
    STEP: wait for the rc to be deleted 03/27/23 14:39:29.457
    Mar 27 14:39:34.475: INFO: 66 pods remaining
    Mar 27 14:39:34.475: INFO: 66 pods has nil DeletionTimestamp
    Mar 27 14:39:34.475: INFO: 
    STEP: Gathering metrics 03/27/23 14:39:39.485
    W0327 14:39:39.498243      24 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 27 14:39:39.498: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar 27 14:39:39.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-258pd" in namespace "gc-4417"
    Mar 27 14:39:39.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-2blmt" in namespace "gc-4417"
    Mar 27 14:39:39.533: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bt7l" in namespace "gc-4417"
    Mar 27 14:39:39.555: INFO: Deleting pod "simpletest-rc-to-be-deleted-2cs8x" in namespace "gc-4417"
    Mar 27 14:39:39.581: INFO: Deleting pod "simpletest-rc-to-be-deleted-2f94c" in namespace "gc-4417"
    Mar 27 14:39:39.596: INFO: Deleting pod "simpletest-rc-to-be-deleted-2z2vz" in namespace "gc-4417"
    Mar 27 14:39:39.613: INFO: Deleting pod "simpletest-rc-to-be-deleted-55csd" in namespace "gc-4417"
    Mar 27 14:39:39.632: INFO: Deleting pod "simpletest-rc-to-be-deleted-59h88" in namespace "gc-4417"
    Mar 27 14:39:39.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-5d4bd" in namespace "gc-4417"
    Mar 27 14:39:39.666: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rpph" in namespace "gc-4417"
    Mar 27 14:39:39.687: INFO: Deleting pod "simpletest-rc-to-be-deleted-8wtzx" in namespace "gc-4417"
    Mar 27 14:39:39.704: INFO: Deleting pod "simpletest-rc-to-be-deleted-9d9t8" in namespace "gc-4417"
    Mar 27 14:39:39.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-9r6bx" in namespace "gc-4417"
    Mar 27 14:39:39.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rnqt" in namespace "gc-4417"
    Mar 27 14:39:39.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-9tp6g" in namespace "gc-4417"
    Mar 27 14:39:39.767: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5vbw" in namespace "gc-4417"
    Mar 27 14:39:39.788: INFO: Deleting pod "simpletest-rc-to-be-deleted-bphkb" in namespace "gc-4417"
    Mar 27 14:39:39.802: INFO: Deleting pod "simpletest-rc-to-be-deleted-bx6vk" in namespace "gc-4417"
    Mar 27 14:39:39.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-cb8kp" in namespace "gc-4417"
    Mar 27 14:39:39.831: INFO: Deleting pod "simpletest-rc-to-be-deleted-cf4lf" in namespace "gc-4417"
    Mar 27 14:39:39.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-cfcrs" in namespace "gc-4417"
    Mar 27 14:39:39.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-ck2jb" in namespace "gc-4417"
    Mar 27 14:39:39.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-clxtn" in namespace "gc-4417"
    Mar 27 14:39:39.891: INFO: Deleting pod "simpletest-rc-to-be-deleted-cz5nw" in namespace "gc-4417"
    Mar 27 14:39:39.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2nx5" in namespace "gc-4417"
    Mar 27 14:39:39.919: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2vjl" in namespace "gc-4417"
    Mar 27 14:39:39.934: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6jf2" in namespace "gc-4417"
    Mar 27 14:39:39.945: INFO: Deleting pod "simpletest-rc-to-be-deleted-dbs6s" in namespace "gc-4417"
    Mar 27 14:39:39.960: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgdjz" in namespace "gc-4417"
    Mar 27 14:39:39.981: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtfjk" in namespace "gc-4417"
    Mar 27 14:39:39.994: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvtfk" in namespace "gc-4417"
    Mar 27 14:39:40.012: INFO: Deleting pod "simpletest-rc-to-be-deleted-f266l" in namespace "gc-4417"
    Mar 27 14:39:40.027: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2dnt" in namespace "gc-4417"
    Mar 27 14:39:40.037: INFO: Deleting pod "simpletest-rc-to-be-deleted-fb2bv" in namespace "gc-4417"
    Mar 27 14:39:40.049: INFO: Deleting pod "simpletest-rc-to-be-deleted-flvc8" in namespace "gc-4417"
    Mar 27 14:39:40.066: INFO: Deleting pod "simpletest-rc-to-be-deleted-fns52" in namespace "gc-4417"
    Mar 27 14:39:40.080: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwdlk" in namespace "gc-4417"
    Mar 27 14:39:40.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjbj6" in namespace "gc-4417"
    Mar 27 14:39:40.104: INFO: Deleting pod "simpletest-rc-to-be-deleted-gldxm" in namespace "gc-4417"
    Mar 27 14:39:40.115: INFO: Deleting pod "simpletest-rc-to-be-deleted-glfpj" in namespace "gc-4417"
    Mar 27 14:39:40.128: INFO: Deleting pod "simpletest-rc-to-be-deleted-gptw2" in namespace "gc-4417"
    Mar 27 14:39:40.140: INFO: Deleting pod "simpletest-rc-to-be-deleted-grw9f" in namespace "gc-4417"
    Mar 27 14:39:40.150: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtvjc" in namespace "gc-4417"
    Mar 27 14:39:40.164: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtwn9" in namespace "gc-4417"
    Mar 27 14:39:40.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwqff" in namespace "gc-4417"
    Mar 27 14:39:40.198: INFO: Deleting pod "simpletest-rc-to-be-deleted-h7wlf" in namespace "gc-4417"
    Mar 27 14:39:40.224: INFO: Deleting pod "simpletest-rc-to-be-deleted-hz98z" in namespace "gc-4417"
    Mar 27 14:39:40.236: INFO: Deleting pod "simpletest-rc-to-be-deleted-jcwt9" in namespace "gc-4417"
    Mar 27 14:39:40.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-jz5t6" in namespace "gc-4417"
    Mar 27 14:39:40.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-k4t8s" in namespace "gc-4417"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 27 14:39:40.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4417" for this suite. 03/27/23 14:39:40.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:39:40.292
Mar 27 14:39:40.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename init-container 03/27/23 14:39:40.293
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:39:40.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:39:40.312
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 03/27/23 14:39:40.315
Mar 27 14:39:40.316: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 27 14:39:59.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-889" for this suite. 03/27/23 14:39:59.63
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":98,"skipped":1944,"failed":0}
------------------------------
• [SLOW TEST] [19.351 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:39:40.292
    Mar 27 14:39:40.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename init-container 03/27/23 14:39:40.293
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:39:40.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:39:40.312
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 03/27/23 14:39:40.315
    Mar 27 14:39:40.316: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 27 14:39:59.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-889" for this suite. 03/27/23 14:39:59.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:39:59.644
Mar 27 14:39:59.644: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename prestop 03/27/23 14:39:59.645
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:39:59.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:39:59.666
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-33 03/27/23 14:39:59.672
STEP: Waiting for pods to come up. 03/27/23 14:39:59.692
Mar 27 14:39:59.692: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-33" to be "running"
Mar 27 14:39:59.698: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.914421ms
Mar 27 14:40:01.704: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012006359s
Mar 27 14:40:03.704: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.012370316s
Mar 27 14:40:03.704: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-33 03/27/23 14:40:03.709
Mar 27 14:40:03.717: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-33" to be "running"
Mar 27 14:40:03.721: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.458229ms
Mar 27 14:40:05.727: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.00989899s
Mar 27 14:40:05.727: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 03/27/23 14:40:05.727
Mar 27 14:40:10.749: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 03/27/23 14:40:10.75
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Mar 27 14:40:10.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-33" for this suite. 03/27/23 14:40:10.864
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":99,"skipped":1966,"failed":0}
------------------------------
• [SLOW TEST] [11.227 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:39:59.644
    Mar 27 14:39:59.644: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename prestop 03/27/23 14:39:59.645
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:39:59.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:39:59.666
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-33 03/27/23 14:39:59.672
    STEP: Waiting for pods to come up. 03/27/23 14:39:59.692
    Mar 27 14:39:59.692: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-33" to be "running"
    Mar 27 14:39:59.698: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.914421ms
    Mar 27 14:40:01.704: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012006359s
    Mar 27 14:40:03.704: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.012370316s
    Mar 27 14:40:03.704: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-33 03/27/23 14:40:03.709
    Mar 27 14:40:03.717: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-33" to be "running"
    Mar 27 14:40:03.721: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.458229ms
    Mar 27 14:40:05.727: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.00989899s
    Mar 27 14:40:05.727: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 03/27/23 14:40:05.727
    Mar 27 14:40:10.749: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 03/27/23 14:40:10.75
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Mar 27 14:40:10.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-33" for this suite. 03/27/23 14:40:10.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:40:10.876
Mar 27 14:40:10.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename dns 03/27/23 14:40:10.878
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:40:10.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:40:10.908
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/27/23 14:40:10.913
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/27/23 14:40:10.913
STEP: creating a pod to probe DNS 03/27/23 14:40:10.913
STEP: submitting the pod to kubernetes 03/27/23 14:40:10.914
Mar 27 14:40:10.923: INFO: Waiting up to 15m0s for pod "dns-test-fb925b2a-c4db-4a92-bbde-5ea81f53fdcb" in namespace "dns-4109" to be "running"
Mar 27 14:40:10.930: INFO: Pod "dns-test-fb925b2a-c4db-4a92-bbde-5ea81f53fdcb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014838ms
Mar 27 14:40:12.936: INFO: Pod "dns-test-fb925b2a-c4db-4a92-bbde-5ea81f53fdcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012167768s
Mar 27 14:40:14.937: INFO: Pod "dns-test-fb925b2a-c4db-4a92-bbde-5ea81f53fdcb": Phase="Running", Reason="", readiness=true. Elapsed: 4.013100845s
Mar 27 14:40:14.937: INFO: Pod "dns-test-fb925b2a-c4db-4a92-bbde-5ea81f53fdcb" satisfied condition "running"
STEP: retrieving the pod 03/27/23 14:40:14.937
STEP: looking for the results for each expected name from probers 03/27/23 14:40:14.942
Mar 27 14:40:14.969: INFO: DNS probes using dns-4109/dns-test-fb925b2a-c4db-4a92-bbde-5ea81f53fdcb succeeded

STEP: deleting the pod 03/27/23 14:40:14.969
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 27 14:40:14.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4109" for this suite. 03/27/23 14:40:14.994
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":100,"skipped":2039,"failed":0}
------------------------------
• [4.130 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:40:10.876
    Mar 27 14:40:10.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename dns 03/27/23 14:40:10.878
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:40:10.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:40:10.908
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/27/23 14:40:10.913
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/27/23 14:40:10.913
    STEP: creating a pod to probe DNS 03/27/23 14:40:10.913
    STEP: submitting the pod to kubernetes 03/27/23 14:40:10.914
    Mar 27 14:40:10.923: INFO: Waiting up to 15m0s for pod "dns-test-fb925b2a-c4db-4a92-bbde-5ea81f53fdcb" in namespace "dns-4109" to be "running"
    Mar 27 14:40:10.930: INFO: Pod "dns-test-fb925b2a-c4db-4a92-bbde-5ea81f53fdcb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014838ms
    Mar 27 14:40:12.936: INFO: Pod "dns-test-fb925b2a-c4db-4a92-bbde-5ea81f53fdcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012167768s
    Mar 27 14:40:14.937: INFO: Pod "dns-test-fb925b2a-c4db-4a92-bbde-5ea81f53fdcb": Phase="Running", Reason="", readiness=true. Elapsed: 4.013100845s
    Mar 27 14:40:14.937: INFO: Pod "dns-test-fb925b2a-c4db-4a92-bbde-5ea81f53fdcb" satisfied condition "running"
    STEP: retrieving the pod 03/27/23 14:40:14.937
    STEP: looking for the results for each expected name from probers 03/27/23 14:40:14.942
    Mar 27 14:40:14.969: INFO: DNS probes using dns-4109/dns-test-fb925b2a-c4db-4a92-bbde-5ea81f53fdcb succeeded

    STEP: deleting the pod 03/27/23 14:40:14.969
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 27 14:40:14.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4109" for this suite. 03/27/23 14:40:14.994
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:40:15.007
Mar 27 14:40:15.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename downward-api 03/27/23 14:40:15.008
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:40:15.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:40:15.03
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 03/27/23 14:40:15.034
Mar 27 14:40:15.044: INFO: Waiting up to 5m0s for pod "downward-api-31394599-fcde-4af6-aca2-5ec215f2c914" in namespace "downward-api-6815" to be "Succeeded or Failed"
Mar 27 14:40:15.047: INFO: Pod "downward-api-31394599-fcde-4af6-aca2-5ec215f2c914": Phase="Pending", Reason="", readiness=false. Elapsed: 2.738395ms
Mar 27 14:40:17.053: INFO: Pod "downward-api-31394599-fcde-4af6-aca2-5ec215f2c914": Phase="Running", Reason="", readiness=true. Elapsed: 2.008746683s
Mar 27 14:40:19.053: INFO: Pod "downward-api-31394599-fcde-4af6-aca2-5ec215f2c914": Phase="Running", Reason="", readiness=false. Elapsed: 4.009337043s
Mar 27 14:40:21.052: INFO: Pod "downward-api-31394599-fcde-4af6-aca2-5ec215f2c914": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008595191s
STEP: Saw pod success 03/27/23 14:40:21.052
Mar 27 14:40:21.053: INFO: Pod "downward-api-31394599-fcde-4af6-aca2-5ec215f2c914" satisfied condition "Succeeded or Failed"
Mar 27 14:40:21.181: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod downward-api-31394599-fcde-4af6-aca2-5ec215f2c914 container dapi-container: <nil>
STEP: delete the pod 03/27/23 14:40:21.189
Mar 27 14:40:21.368: INFO: Waiting for pod downward-api-31394599-fcde-4af6-aca2-5ec215f2c914 to disappear
Mar 27 14:40:21.371: INFO: Pod downward-api-31394599-fcde-4af6-aca2-5ec215f2c914 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar 27 14:40:21.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6815" for this suite. 03/27/23 14:40:21.398
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":101,"skipped":2044,"failed":0}
------------------------------
• [SLOW TEST] [6.429 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:40:15.007
    Mar 27 14:40:15.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename downward-api 03/27/23 14:40:15.008
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:40:15.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:40:15.03
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 03/27/23 14:40:15.034
    Mar 27 14:40:15.044: INFO: Waiting up to 5m0s for pod "downward-api-31394599-fcde-4af6-aca2-5ec215f2c914" in namespace "downward-api-6815" to be "Succeeded or Failed"
    Mar 27 14:40:15.047: INFO: Pod "downward-api-31394599-fcde-4af6-aca2-5ec215f2c914": Phase="Pending", Reason="", readiness=false. Elapsed: 2.738395ms
    Mar 27 14:40:17.053: INFO: Pod "downward-api-31394599-fcde-4af6-aca2-5ec215f2c914": Phase="Running", Reason="", readiness=true. Elapsed: 2.008746683s
    Mar 27 14:40:19.053: INFO: Pod "downward-api-31394599-fcde-4af6-aca2-5ec215f2c914": Phase="Running", Reason="", readiness=false. Elapsed: 4.009337043s
    Mar 27 14:40:21.052: INFO: Pod "downward-api-31394599-fcde-4af6-aca2-5ec215f2c914": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008595191s
    STEP: Saw pod success 03/27/23 14:40:21.052
    Mar 27 14:40:21.053: INFO: Pod "downward-api-31394599-fcde-4af6-aca2-5ec215f2c914" satisfied condition "Succeeded or Failed"
    Mar 27 14:40:21.181: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod downward-api-31394599-fcde-4af6-aca2-5ec215f2c914 container dapi-container: <nil>
    STEP: delete the pod 03/27/23 14:40:21.189
    Mar 27 14:40:21.368: INFO: Waiting for pod downward-api-31394599-fcde-4af6-aca2-5ec215f2c914 to disappear
    Mar 27 14:40:21.371: INFO: Pod downward-api-31394599-fcde-4af6-aca2-5ec215f2c914 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar 27 14:40:21.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6815" for this suite. 03/27/23 14:40:21.398
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:40:21.436
Mar 27 14:40:21.436: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename secrets 03/27/23 14:40:21.437
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:40:21.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:40:21.516
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-6a5830a7-7057-4e22-a280-fb6f4b5c782b 03/27/23 14:40:21.521
STEP: Creating a pod to test consume secrets 03/27/23 14:40:21.527
Mar 27 14:40:21.539: INFO: Waiting up to 5m0s for pod "pod-secrets-3d308900-c824-44b6-b1e4-a021dbb0e93a" in namespace "secrets-677" to be "Succeeded or Failed"
Mar 27 14:40:21.542: INFO: Pod "pod-secrets-3d308900-c824-44b6-b1e4-a021dbb0e93a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.186759ms
Mar 27 14:40:23.547: INFO: Pod "pod-secrets-3d308900-c824-44b6-b1e4-a021dbb0e93a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008052974s
Mar 27 14:40:25.547: INFO: Pod "pod-secrets-3d308900-c824-44b6-b1e4-a021dbb0e93a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008119625s
STEP: Saw pod success 03/27/23 14:40:25.547
Mar 27 14:40:25.547: INFO: Pod "pod-secrets-3d308900-c824-44b6-b1e4-a021dbb0e93a" satisfied condition "Succeeded or Failed"
Mar 27 14:40:25.552: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-secrets-3d308900-c824-44b6-b1e4-a021dbb0e93a container secret-volume-test: <nil>
STEP: delete the pod 03/27/23 14:40:25.561
Mar 27 14:40:25.576: INFO: Waiting for pod pod-secrets-3d308900-c824-44b6-b1e4-a021dbb0e93a to disappear
Mar 27 14:40:25.579: INFO: Pod pod-secrets-3d308900-c824-44b6-b1e4-a021dbb0e93a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 27 14:40:25.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-677" for this suite. 03/27/23 14:40:25.586
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":102,"skipped":2048,"failed":0}
------------------------------
• [4.160 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:40:21.436
    Mar 27 14:40:21.436: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename secrets 03/27/23 14:40:21.437
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:40:21.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:40:21.516
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-6a5830a7-7057-4e22-a280-fb6f4b5c782b 03/27/23 14:40:21.521
    STEP: Creating a pod to test consume secrets 03/27/23 14:40:21.527
    Mar 27 14:40:21.539: INFO: Waiting up to 5m0s for pod "pod-secrets-3d308900-c824-44b6-b1e4-a021dbb0e93a" in namespace "secrets-677" to be "Succeeded or Failed"
    Mar 27 14:40:21.542: INFO: Pod "pod-secrets-3d308900-c824-44b6-b1e4-a021dbb0e93a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.186759ms
    Mar 27 14:40:23.547: INFO: Pod "pod-secrets-3d308900-c824-44b6-b1e4-a021dbb0e93a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008052974s
    Mar 27 14:40:25.547: INFO: Pod "pod-secrets-3d308900-c824-44b6-b1e4-a021dbb0e93a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008119625s
    STEP: Saw pod success 03/27/23 14:40:25.547
    Mar 27 14:40:25.547: INFO: Pod "pod-secrets-3d308900-c824-44b6-b1e4-a021dbb0e93a" satisfied condition "Succeeded or Failed"
    Mar 27 14:40:25.552: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-secrets-3d308900-c824-44b6-b1e4-a021dbb0e93a container secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 14:40:25.561
    Mar 27 14:40:25.576: INFO: Waiting for pod pod-secrets-3d308900-c824-44b6-b1e4-a021dbb0e93a to disappear
    Mar 27 14:40:25.579: INFO: Pod pod-secrets-3d308900-c824-44b6-b1e4-a021dbb0e93a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 27 14:40:25.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-677" for this suite. 03/27/23 14:40:25.586
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:40:25.597
Mar 27 14:40:25.597: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubectl 03/27/23 14:40:25.598
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:40:25.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:40:25.614
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 03/27/23 14:40:25.616
Mar 27 14:40:25.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-6075 cluster-info'
Mar 27 14:40:25.683: INFO: stderr: ""
Mar 27 14:40:25.683: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 27 14:40:25.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6075" for this suite. 03/27/23 14:40:25.688
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":103,"skipped":2086,"failed":0}
------------------------------
• [0.098 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:40:25.597
    Mar 27 14:40:25.597: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubectl 03/27/23 14:40:25.598
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:40:25.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:40:25.614
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 03/27/23 14:40:25.616
    Mar 27 14:40:25.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-6075 cluster-info'
    Mar 27 14:40:25.683: INFO: stderr: ""
    Mar 27 14:40:25.683: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 27 14:40:25.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6075" for this suite. 03/27/23 14:40:25.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:40:25.697
Mar 27 14:40:25.697: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 14:40:25.698
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:40:25.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:40:25.721
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 03/27/23 14:40:25.725
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/27/23 14:40:25.726
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/27/23 14:40:25.726
STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/27/23 14:40:25.726
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/27/23 14:40:25.728
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/27/23 14:40:25.728
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/27/23 14:40:25.729
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:40:25.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3692" for this suite. 03/27/23 14:40:25.735
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":104,"skipped":2103,"failed":0}
------------------------------
• [0.045 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:40:25.697
    Mar 27 14:40:25.697: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 14:40:25.698
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:40:25.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:40:25.721
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 03/27/23 14:40:25.725
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/27/23 14:40:25.726
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/27/23 14:40:25.726
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/27/23 14:40:25.726
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/27/23 14:40:25.728
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/27/23 14:40:25.728
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/27/23 14:40:25.729
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:40:25.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-3692" for this suite. 03/27/23 14:40:25.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:40:25.747
Mar 27 14:40:25.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubectl 03/27/23 14:40:25.748
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:40:25.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:40:25.768
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/27/23 14:40:25.772
Mar 27 14:40:25.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-8653 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Mar 27 14:40:25.848: INFO: stderr: ""
Mar 27 14:40:25.848: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 03/27/23 14:40:25.848
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Mar 27 14:40:25.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-8653 delete pods e2e-test-httpd-pod'
Mar 27 14:40:28.365: INFO: stderr: ""
Mar 27 14:40:28.365: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 27 14:40:28.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8653" for this suite. 03/27/23 14:40:28.37
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":105,"skipped":2129,"failed":0}
------------------------------
• [2.684 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:40:25.747
    Mar 27 14:40:25.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubectl 03/27/23 14:40:25.748
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:40:25.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:40:25.768
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/27/23 14:40:25.772
    Mar 27 14:40:25.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-8653 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Mar 27 14:40:25.848: INFO: stderr: ""
    Mar 27 14:40:25.848: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 03/27/23 14:40:25.848
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Mar 27 14:40:25.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-8653 delete pods e2e-test-httpd-pod'
    Mar 27 14:40:28.365: INFO: stderr: ""
    Mar 27 14:40:28.365: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 27 14:40:28.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8653" for this suite. 03/27/23 14:40:28.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:40:28.432
Mar 27 14:40:28.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename container-probe 03/27/23 14:40:28.433
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:40:28.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:40:28.46
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 27 14:41:28.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5032" for this suite. 03/27/23 14:41:28.487
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":106,"skipped":2134,"failed":0}
------------------------------
• [SLOW TEST] [60.062 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:40:28.432
    Mar 27 14:40:28.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename container-probe 03/27/23 14:40:28.433
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:40:28.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:40:28.46
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 27 14:41:28.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5032" for this suite. 03/27/23 14:41:28.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:41:28.498
Mar 27 14:41:28.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 14:41:28.499
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:41:28.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:41:28.524
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 03/27/23 14:41:28.528
Mar 27 14:41:28.528: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: mark a version not serverd 03/27/23 14:41:35.689
STEP: check the unserved version gets removed 03/27/23 14:41:35.712
STEP: check the other version is not changed 03/27/23 14:41:38.738
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:41:44.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9587" for this suite. 03/27/23 14:41:44.259
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":107,"skipped":2172,"failed":0}
------------------------------
• [SLOW TEST] [15.768 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:41:28.498
    Mar 27 14:41:28.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 14:41:28.499
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:41:28.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:41:28.524
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 03/27/23 14:41:28.528
    Mar 27 14:41:28.528: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: mark a version not serverd 03/27/23 14:41:35.689
    STEP: check the unserved version gets removed 03/27/23 14:41:35.712
    STEP: check the other version is not changed 03/27/23 14:41:38.738
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:41:44.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9587" for this suite. 03/27/23 14:41:44.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:41:44.267
Mar 27 14:41:44.267: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename emptydir 03/27/23 14:41:44.268
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:41:44.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:41:44.284
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 03/27/23 14:41:44.287
Mar 27 14:41:44.298: INFO: Waiting up to 5m0s for pod "pod-ca253f08-ade3-4d92-b5d5-5a894c1317ae" in namespace "emptydir-7231" to be "Succeeded or Failed"
Mar 27 14:41:44.309: INFO: Pod "pod-ca253f08-ade3-4d92-b5d5-5a894c1317ae": Phase="Pending", Reason="", readiness=false. Elapsed: 11.038842ms
Mar 27 14:41:46.315: INFO: Pod "pod-ca253f08-ade3-4d92-b5d5-5a894c1317ae": Phase="Running", Reason="", readiness=true. Elapsed: 2.016600677s
Mar 27 14:41:48.315: INFO: Pod "pod-ca253f08-ade3-4d92-b5d5-5a894c1317ae": Phase="Running", Reason="", readiness=false. Elapsed: 4.016443645s
Mar 27 14:41:50.315: INFO: Pod "pod-ca253f08-ade3-4d92-b5d5-5a894c1317ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016873641s
STEP: Saw pod success 03/27/23 14:41:50.315
Mar 27 14:41:50.315: INFO: Pod "pod-ca253f08-ade3-4d92-b5d5-5a894c1317ae" satisfied condition "Succeeded or Failed"
Mar 27 14:41:50.323: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-ca253f08-ade3-4d92-b5d5-5a894c1317ae container test-container: <nil>
STEP: delete the pod 03/27/23 14:41:50.334
Mar 27 14:41:50.349: INFO: Waiting for pod pod-ca253f08-ade3-4d92-b5d5-5a894c1317ae to disappear
Mar 27 14:41:50.355: INFO: Pod pod-ca253f08-ade3-4d92-b5d5-5a894c1317ae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 27 14:41:50.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7231" for this suite. 03/27/23 14:41:50.363
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":108,"skipped":2194,"failed":0}
------------------------------
• [SLOW TEST] [6.106 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:41:44.267
    Mar 27 14:41:44.267: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename emptydir 03/27/23 14:41:44.268
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:41:44.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:41:44.284
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 03/27/23 14:41:44.287
    Mar 27 14:41:44.298: INFO: Waiting up to 5m0s for pod "pod-ca253f08-ade3-4d92-b5d5-5a894c1317ae" in namespace "emptydir-7231" to be "Succeeded or Failed"
    Mar 27 14:41:44.309: INFO: Pod "pod-ca253f08-ade3-4d92-b5d5-5a894c1317ae": Phase="Pending", Reason="", readiness=false. Elapsed: 11.038842ms
    Mar 27 14:41:46.315: INFO: Pod "pod-ca253f08-ade3-4d92-b5d5-5a894c1317ae": Phase="Running", Reason="", readiness=true. Elapsed: 2.016600677s
    Mar 27 14:41:48.315: INFO: Pod "pod-ca253f08-ade3-4d92-b5d5-5a894c1317ae": Phase="Running", Reason="", readiness=false. Elapsed: 4.016443645s
    Mar 27 14:41:50.315: INFO: Pod "pod-ca253f08-ade3-4d92-b5d5-5a894c1317ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016873641s
    STEP: Saw pod success 03/27/23 14:41:50.315
    Mar 27 14:41:50.315: INFO: Pod "pod-ca253f08-ade3-4d92-b5d5-5a894c1317ae" satisfied condition "Succeeded or Failed"
    Mar 27 14:41:50.323: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-ca253f08-ade3-4d92-b5d5-5a894c1317ae container test-container: <nil>
    STEP: delete the pod 03/27/23 14:41:50.334
    Mar 27 14:41:50.349: INFO: Waiting for pod pod-ca253f08-ade3-4d92-b5d5-5a894c1317ae to disappear
    Mar 27 14:41:50.355: INFO: Pod pod-ca253f08-ade3-4d92-b5d5-5a894c1317ae no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 27 14:41:50.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7231" for this suite. 03/27/23 14:41:50.363
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:41:50.377
Mar 27 14:41:50.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename gc 03/27/23 14:41:50.378
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:41:50.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:41:50.41
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 03/27/23 14:41:50.415
STEP: Wait for the Deployment to create new ReplicaSet 03/27/23 14:41:50.425
STEP: delete the deployment 03/27/23 14:41:50.935
STEP: wait for all rs to be garbage collected 03/27/23 14:41:50.944
STEP: expected 0 rs, got 1 rs 03/27/23 14:41:50.951
STEP: expected 0 pods, got 2 pods 03/27/23 14:41:50.955
STEP: Gathering metrics 03/27/23 14:41:51.467
W0327 14:41:51.486871      24 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 27 14:41:51.486: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 27 14:41:51.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7112" for this suite. 03/27/23 14:41:51.493
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":109,"skipped":2220,"failed":0}
------------------------------
• [1.124 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:41:50.377
    Mar 27 14:41:50.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename gc 03/27/23 14:41:50.378
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:41:50.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:41:50.41
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 03/27/23 14:41:50.415
    STEP: Wait for the Deployment to create new ReplicaSet 03/27/23 14:41:50.425
    STEP: delete the deployment 03/27/23 14:41:50.935
    STEP: wait for all rs to be garbage collected 03/27/23 14:41:50.944
    STEP: expected 0 rs, got 1 rs 03/27/23 14:41:50.951
    STEP: expected 0 pods, got 2 pods 03/27/23 14:41:50.955
    STEP: Gathering metrics 03/27/23 14:41:51.467
    W0327 14:41:51.486871      24 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 27 14:41:51.486: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 27 14:41:51.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7112" for this suite. 03/27/23 14:41:51.493
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:41:51.502
Mar 27 14:41:51.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename container-runtime 03/27/23 14:41:51.502
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:41:51.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:41:51.528
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/27/23 14:41:51.551
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/27/23 14:42:12.675
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/27/23 14:42:12.679
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/27/23 14:42:12.687
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/27/23 14:42:12.687
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/27/23 14:42:12.723
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/27/23 14:42:16.753
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/27/23 14:42:18.769
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/27/23 14:42:18.779
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/27/23 14:42:18.779
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/27/23 14:42:18.803
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/27/23 14:42:19.811
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/27/23 14:42:23.839
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/27/23 14:42:23.846
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/27/23 14:42:23.846
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar 27 14:42:23.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9592" for this suite. 03/27/23 14:42:23.884
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":110,"skipped":2224,"failed":0}
------------------------------
• [SLOW TEST] [32.390 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:41:51.502
    Mar 27 14:41:51.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename container-runtime 03/27/23 14:41:51.502
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:41:51.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:41:51.528
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/27/23 14:41:51.551
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/27/23 14:42:12.675
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/27/23 14:42:12.679
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/27/23 14:42:12.687
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/27/23 14:42:12.687
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/27/23 14:42:12.723
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/27/23 14:42:16.753
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/27/23 14:42:18.769
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/27/23 14:42:18.779
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/27/23 14:42:18.779
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/27/23 14:42:18.803
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/27/23 14:42:19.811
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/27/23 14:42:23.839
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/27/23 14:42:23.846
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/27/23 14:42:23.846
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar 27 14:42:23.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-9592" for this suite. 03/27/23 14:42:23.884
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:42:23.893
Mar 27 14:42:23.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename emptydir 03/27/23 14:42:23.896
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:42:23.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:42:23.916
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 03/27/23 14:42:23.92
Mar 27 14:42:23.932: INFO: Waiting up to 5m0s for pod "pod-a2286f71-ca74-44cc-9e53-44439f0cff1c" in namespace "emptydir-2734" to be "Succeeded or Failed"
Mar 27 14:42:23.936: INFO: Pod "pod-a2286f71-ca74-44cc-9e53-44439f0cff1c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.217703ms
Mar 27 14:42:25.942: INFO: Pod "pod-a2286f71-ca74-44cc-9e53-44439f0cff1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009927425s
Mar 27 14:42:27.941: INFO: Pod "pod-a2286f71-ca74-44cc-9e53-44439f0cff1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008777966s
Mar 27 14:42:29.940: INFO: Pod "pod-a2286f71-ca74-44cc-9e53-44439f0cff1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008086406s
STEP: Saw pod success 03/27/23 14:42:29.941
Mar 27 14:42:29.941: INFO: Pod "pod-a2286f71-ca74-44cc-9e53-44439f0cff1c" satisfied condition "Succeeded or Failed"
Mar 27 14:42:29.944: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-a2286f71-ca74-44cc-9e53-44439f0cff1c container test-container: <nil>
STEP: delete the pod 03/27/23 14:42:29.955
Mar 27 14:42:29.972: INFO: Waiting for pod pod-a2286f71-ca74-44cc-9e53-44439f0cff1c to disappear
Mar 27 14:42:29.975: INFO: Pod pod-a2286f71-ca74-44cc-9e53-44439f0cff1c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 27 14:42:29.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2734" for this suite. 03/27/23 14:42:29.98
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":111,"skipped":2228,"failed":0}
------------------------------
• [SLOW TEST] [6.095 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:42:23.893
    Mar 27 14:42:23.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename emptydir 03/27/23 14:42:23.896
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:42:23.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:42:23.916
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 03/27/23 14:42:23.92
    Mar 27 14:42:23.932: INFO: Waiting up to 5m0s for pod "pod-a2286f71-ca74-44cc-9e53-44439f0cff1c" in namespace "emptydir-2734" to be "Succeeded or Failed"
    Mar 27 14:42:23.936: INFO: Pod "pod-a2286f71-ca74-44cc-9e53-44439f0cff1c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.217703ms
    Mar 27 14:42:25.942: INFO: Pod "pod-a2286f71-ca74-44cc-9e53-44439f0cff1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009927425s
    Mar 27 14:42:27.941: INFO: Pod "pod-a2286f71-ca74-44cc-9e53-44439f0cff1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008777966s
    Mar 27 14:42:29.940: INFO: Pod "pod-a2286f71-ca74-44cc-9e53-44439f0cff1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008086406s
    STEP: Saw pod success 03/27/23 14:42:29.941
    Mar 27 14:42:29.941: INFO: Pod "pod-a2286f71-ca74-44cc-9e53-44439f0cff1c" satisfied condition "Succeeded or Failed"
    Mar 27 14:42:29.944: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-a2286f71-ca74-44cc-9e53-44439f0cff1c container test-container: <nil>
    STEP: delete the pod 03/27/23 14:42:29.955
    Mar 27 14:42:29.972: INFO: Waiting for pod pod-a2286f71-ca74-44cc-9e53-44439f0cff1c to disappear
    Mar 27 14:42:29.975: INFO: Pod pod-a2286f71-ca74-44cc-9e53-44439f0cff1c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 27 14:42:29.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2734" for this suite. 03/27/23 14:42:29.98
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:42:29.988
Mar 27 14:42:29.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename dns 03/27/23 14:42:29.989
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:42:30.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:42:30.013
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/27/23 14:42:30.017
Mar 27 14:42:30.026: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5162  8f10155e-ef79-4af6-ae12-b30fd0704bf1 25798 0 2023-03-27 14:42:30 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-27 14:42:30 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cnkzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cnkzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:42:30.026: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5162" to be "running and ready"
Mar 27 14:42:30.034: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 7.908289ms
Mar 27 14:42:30.034: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:42:32.040: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013060799s
Mar 27 14:42:32.040: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:42:34.040: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.013240377s
Mar 27 14:42:34.040: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Mar 27 14:42:34.040: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 03/27/23 14:42:34.04
Mar 27 14:42:34.040: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5162 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:42:34.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:42:34.041: INFO: ExecWithOptions: Clientset creation
Mar 27 14:42:34.041: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/dns-5162/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 03/27/23 14:42:34.158
Mar 27 14:42:34.158: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5162 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:42:34.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:42:34.159: INFO: ExecWithOptions: Clientset creation
Mar 27 14:42:34.159: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/dns-5162/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 27 14:42:34.276: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 27 14:42:34.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5162" for this suite. 03/27/23 14:42:34.301
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":112,"skipped":2230,"failed":0}
------------------------------
• [4.321 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:42:29.988
    Mar 27 14:42:29.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename dns 03/27/23 14:42:29.989
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:42:30.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:42:30.013
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/27/23 14:42:30.017
    Mar 27 14:42:30.026: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5162  8f10155e-ef79-4af6-ae12-b30fd0704bf1 25798 0 2023-03-27 14:42:30 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-27 14:42:30 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cnkzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cnkzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:42:30.026: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5162" to be "running and ready"
    Mar 27 14:42:30.034: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 7.908289ms
    Mar 27 14:42:30.034: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:42:32.040: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013060799s
    Mar 27 14:42:32.040: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:42:34.040: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.013240377s
    Mar 27 14:42:34.040: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Mar 27 14:42:34.040: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 03/27/23 14:42:34.04
    Mar 27 14:42:34.040: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5162 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:42:34.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:42:34.041: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:42:34.041: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/dns-5162/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 03/27/23 14:42:34.158
    Mar 27 14:42:34.158: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5162 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:42:34.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:42:34.159: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:42:34.159: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/dns-5162/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 27 14:42:34.276: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 27 14:42:34.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5162" for this suite. 03/27/23 14:42:34.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:42:34.313
Mar 27 14:42:34.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename daemonsets 03/27/23 14:42:34.314
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:42:34.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:42:34.338
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Mar 27 14:42:34.370: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 14:42:34.377
Mar 27 14:42:34.385: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 14:42:34.385: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 14:42:35.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 14:42:35.395: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 14:42:36.398: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 27 14:42:36.398: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 03/27/23 14:42:36.414
STEP: Check that daemon pods images are updated. 03/27/23 14:42:36.427
Mar 27 14:42:36.432: INFO: Wrong image for pod: daemon-set-2r87z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 27 14:42:36.432: INFO: Wrong image for pod: daemon-set-8nfx8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 27 14:42:36.432: INFO: Wrong image for pod: daemon-set-b8vrb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 27 14:42:37.443: INFO: Wrong image for pod: daemon-set-8nfx8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 27 14:42:37.444: INFO: Wrong image for pod: daemon-set-b8vrb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 27 14:42:38.443: INFO: Wrong image for pod: daemon-set-8nfx8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 27 14:42:38.443: INFO: Wrong image for pod: daemon-set-b8vrb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 27 14:42:39.443: INFO: Wrong image for pod: daemon-set-8nfx8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 27 14:42:39.443: INFO: Wrong image for pod: daemon-set-b8vrb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 27 14:42:40.444: INFO: Wrong image for pod: daemon-set-8nfx8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 27 14:42:40.444: INFO: Wrong image for pod: daemon-set-b8vrb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 27 14:42:40.444: INFO: Pod daemon-set-dkzm4 is not available
Mar 27 14:42:41.444: INFO: Wrong image for pod: daemon-set-b8vrb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 27 14:42:42.444: INFO: Wrong image for pod: daemon-set-b8vrb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 27 14:42:42.444: INFO: Pod daemon-set-vcxsp is not available
Mar 27 14:42:43.443: INFO: Wrong image for pod: daemon-set-b8vrb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 27 14:42:43.443: INFO: Pod daemon-set-vcxsp is not available
Mar 27 14:42:44.444: INFO: Wrong image for pod: daemon-set-b8vrb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 27 14:42:44.444: INFO: Pod daemon-set-vcxsp is not available
Mar 27 14:42:47.443: INFO: Pod daemon-set-v8wws is not available
STEP: Check that daemon pods are still running on every node of the cluster. 03/27/23 14:42:47.451
Mar 27 14:42:47.467: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 14:42:47.467: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 14:42:48.478: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 27 14:42:48.478: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/27/23 14:42:48.5
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9832, will wait for the garbage collector to delete the pods 03/27/23 14:42:48.501
Mar 27 14:42:48.563: INFO: Deleting DaemonSet.extensions daemon-set took: 7.320858ms
Mar 27 14:42:48.664: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.172406ms
Mar 27 14:42:50.672: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 14:42:50.672: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 27 14:42:50.677: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26064"},"items":null}

Mar 27 14:42:50.687: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26064"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 27 14:42:50.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9832" for this suite. 03/27/23 14:42:50.744
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":113,"skipped":2258,"failed":0}
------------------------------
• [SLOW TEST] [16.447 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:42:34.313
    Mar 27 14:42:34.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename daemonsets 03/27/23 14:42:34.314
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:42:34.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:42:34.338
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Mar 27 14:42:34.370: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 14:42:34.377
    Mar 27 14:42:34.385: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 14:42:34.385: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 14:42:35.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 14:42:35.395: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 14:42:36.398: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 27 14:42:36.398: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 03/27/23 14:42:36.414
    STEP: Check that daemon pods images are updated. 03/27/23 14:42:36.427
    Mar 27 14:42:36.432: INFO: Wrong image for pod: daemon-set-2r87z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 27 14:42:36.432: INFO: Wrong image for pod: daemon-set-8nfx8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 27 14:42:36.432: INFO: Wrong image for pod: daemon-set-b8vrb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 27 14:42:37.443: INFO: Wrong image for pod: daemon-set-8nfx8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 27 14:42:37.444: INFO: Wrong image for pod: daemon-set-b8vrb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 27 14:42:38.443: INFO: Wrong image for pod: daemon-set-8nfx8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 27 14:42:38.443: INFO: Wrong image for pod: daemon-set-b8vrb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 27 14:42:39.443: INFO: Wrong image for pod: daemon-set-8nfx8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 27 14:42:39.443: INFO: Wrong image for pod: daemon-set-b8vrb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 27 14:42:40.444: INFO: Wrong image for pod: daemon-set-8nfx8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 27 14:42:40.444: INFO: Wrong image for pod: daemon-set-b8vrb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 27 14:42:40.444: INFO: Pod daemon-set-dkzm4 is not available
    Mar 27 14:42:41.444: INFO: Wrong image for pod: daemon-set-b8vrb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 27 14:42:42.444: INFO: Wrong image for pod: daemon-set-b8vrb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 27 14:42:42.444: INFO: Pod daemon-set-vcxsp is not available
    Mar 27 14:42:43.443: INFO: Wrong image for pod: daemon-set-b8vrb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 27 14:42:43.443: INFO: Pod daemon-set-vcxsp is not available
    Mar 27 14:42:44.444: INFO: Wrong image for pod: daemon-set-b8vrb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 27 14:42:44.444: INFO: Pod daemon-set-vcxsp is not available
    Mar 27 14:42:47.443: INFO: Pod daemon-set-v8wws is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 03/27/23 14:42:47.451
    Mar 27 14:42:47.467: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 14:42:47.467: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 14:42:48.478: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 27 14:42:48.478: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/27/23 14:42:48.5
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9832, will wait for the garbage collector to delete the pods 03/27/23 14:42:48.501
    Mar 27 14:42:48.563: INFO: Deleting DaemonSet.extensions daemon-set took: 7.320858ms
    Mar 27 14:42:48.664: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.172406ms
    Mar 27 14:42:50.672: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 14:42:50.672: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 27 14:42:50.677: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26064"},"items":null}

    Mar 27 14:42:50.687: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26064"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 14:42:50.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9832" for this suite. 03/27/23 14:42:50.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:42:50.761
Mar 27 14:42:50.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename svcaccounts 03/27/23 14:42:50.762
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:42:50.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:42:50.849
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  03/27/23 14:42:50.853
Mar 27 14:42:50.861: INFO: Waiting up to 5m0s for pod "test-pod-665b1531-a19e-446d-947a-1a13b5c16b43" in namespace "svcaccounts-4143" to be "Succeeded or Failed"
Mar 27 14:42:50.866: INFO: Pod "test-pod-665b1531-a19e-446d-947a-1a13b5c16b43": Phase="Pending", Reason="", readiness=false. Elapsed: 4.825894ms
Mar 27 14:42:52.871: INFO: Pod "test-pod-665b1531-a19e-446d-947a-1a13b5c16b43": Phase="Running", Reason="", readiness=true. Elapsed: 2.00991035s
Mar 27 14:42:54.871: INFO: Pod "test-pod-665b1531-a19e-446d-947a-1a13b5c16b43": Phase="Running", Reason="", readiness=false. Elapsed: 4.009985416s
Mar 27 14:42:56.870: INFO: Pod "test-pod-665b1531-a19e-446d-947a-1a13b5c16b43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009003057s
STEP: Saw pod success 03/27/23 14:42:56.87
Mar 27 14:42:56.871: INFO: Pod "test-pod-665b1531-a19e-446d-947a-1a13b5c16b43" satisfied condition "Succeeded or Failed"
Mar 27 14:42:56.874: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod test-pod-665b1531-a19e-446d-947a-1a13b5c16b43 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 14:42:56.886
Mar 27 14:42:56.901: INFO: Waiting for pod test-pod-665b1531-a19e-446d-947a-1a13b5c16b43 to disappear
Mar 27 14:42:56.905: INFO: Pod test-pod-665b1531-a19e-446d-947a-1a13b5c16b43 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar 27 14:42:56.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4143" for this suite. 03/27/23 14:42:56.911
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":114,"skipped":2272,"failed":0}
------------------------------
• [SLOW TEST] [6.156 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:42:50.761
    Mar 27 14:42:50.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename svcaccounts 03/27/23 14:42:50.762
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:42:50.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:42:50.849
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  03/27/23 14:42:50.853
    Mar 27 14:42:50.861: INFO: Waiting up to 5m0s for pod "test-pod-665b1531-a19e-446d-947a-1a13b5c16b43" in namespace "svcaccounts-4143" to be "Succeeded or Failed"
    Mar 27 14:42:50.866: INFO: Pod "test-pod-665b1531-a19e-446d-947a-1a13b5c16b43": Phase="Pending", Reason="", readiness=false. Elapsed: 4.825894ms
    Mar 27 14:42:52.871: INFO: Pod "test-pod-665b1531-a19e-446d-947a-1a13b5c16b43": Phase="Running", Reason="", readiness=true. Elapsed: 2.00991035s
    Mar 27 14:42:54.871: INFO: Pod "test-pod-665b1531-a19e-446d-947a-1a13b5c16b43": Phase="Running", Reason="", readiness=false. Elapsed: 4.009985416s
    Mar 27 14:42:56.870: INFO: Pod "test-pod-665b1531-a19e-446d-947a-1a13b5c16b43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009003057s
    STEP: Saw pod success 03/27/23 14:42:56.87
    Mar 27 14:42:56.871: INFO: Pod "test-pod-665b1531-a19e-446d-947a-1a13b5c16b43" satisfied condition "Succeeded or Failed"
    Mar 27 14:42:56.874: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod test-pod-665b1531-a19e-446d-947a-1a13b5c16b43 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 14:42:56.886
    Mar 27 14:42:56.901: INFO: Waiting for pod test-pod-665b1531-a19e-446d-947a-1a13b5c16b43 to disappear
    Mar 27 14:42:56.905: INFO: Pod test-pod-665b1531-a19e-446d-947a-1a13b5c16b43 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar 27 14:42:56.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4143" for this suite. 03/27/23 14:42:56.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:42:56.918
Mar 27 14:42:56.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename emptydir 03/27/23 14:42:56.919
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:42:56.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:42:56.94
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 03/27/23 14:42:56.943
Mar 27 14:42:56.955: INFO: Waiting up to 5m0s for pod "pod-31a18324-bfca-4fa6-a50d-8292c222eb12" in namespace "emptydir-4206" to be "Succeeded or Failed"
Mar 27 14:42:56.959: INFO: Pod "pod-31a18324-bfca-4fa6-a50d-8292c222eb12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03203ms
Mar 27 14:42:58.968: INFO: Pod "pod-31a18324-bfca-4fa6-a50d-8292c222eb12": Phase="Running", Reason="", readiness=true. Elapsed: 2.013603895s
Mar 27 14:43:00.968: INFO: Pod "pod-31a18324-bfca-4fa6-a50d-8292c222eb12": Phase="Running", Reason="", readiness=false. Elapsed: 4.013166162s
Mar 27 14:43:02.963: INFO: Pod "pod-31a18324-bfca-4fa6-a50d-8292c222eb12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008311679s
STEP: Saw pod success 03/27/23 14:43:02.963
Mar 27 14:43:02.963: INFO: Pod "pod-31a18324-bfca-4fa6-a50d-8292c222eb12" satisfied condition "Succeeded or Failed"
Mar 27 14:43:02.967: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-31a18324-bfca-4fa6-a50d-8292c222eb12 container test-container: <nil>
STEP: delete the pod 03/27/23 14:43:02.981
Mar 27 14:43:02.995: INFO: Waiting for pod pod-31a18324-bfca-4fa6-a50d-8292c222eb12 to disappear
Mar 27 14:43:02.999: INFO: Pod pod-31a18324-bfca-4fa6-a50d-8292c222eb12 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 27 14:43:02.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4206" for this suite. 03/27/23 14:43:03.004
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":115,"skipped":2289,"failed":0}
------------------------------
• [SLOW TEST] [6.092 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:42:56.918
    Mar 27 14:42:56.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename emptydir 03/27/23 14:42:56.919
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:42:56.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:42:56.94
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/27/23 14:42:56.943
    Mar 27 14:42:56.955: INFO: Waiting up to 5m0s for pod "pod-31a18324-bfca-4fa6-a50d-8292c222eb12" in namespace "emptydir-4206" to be "Succeeded or Failed"
    Mar 27 14:42:56.959: INFO: Pod "pod-31a18324-bfca-4fa6-a50d-8292c222eb12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03203ms
    Mar 27 14:42:58.968: INFO: Pod "pod-31a18324-bfca-4fa6-a50d-8292c222eb12": Phase="Running", Reason="", readiness=true. Elapsed: 2.013603895s
    Mar 27 14:43:00.968: INFO: Pod "pod-31a18324-bfca-4fa6-a50d-8292c222eb12": Phase="Running", Reason="", readiness=false. Elapsed: 4.013166162s
    Mar 27 14:43:02.963: INFO: Pod "pod-31a18324-bfca-4fa6-a50d-8292c222eb12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008311679s
    STEP: Saw pod success 03/27/23 14:43:02.963
    Mar 27 14:43:02.963: INFO: Pod "pod-31a18324-bfca-4fa6-a50d-8292c222eb12" satisfied condition "Succeeded or Failed"
    Mar 27 14:43:02.967: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-31a18324-bfca-4fa6-a50d-8292c222eb12 container test-container: <nil>
    STEP: delete the pod 03/27/23 14:43:02.981
    Mar 27 14:43:02.995: INFO: Waiting for pod pod-31a18324-bfca-4fa6-a50d-8292c222eb12 to disappear
    Mar 27 14:43:02.999: INFO: Pod pod-31a18324-bfca-4fa6-a50d-8292c222eb12 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 27 14:43:02.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4206" for this suite. 03/27/23 14:43:03.004
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:43:03.01
Mar 27 14:43:03.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename emptydir 03/27/23 14:43:03.011
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:43:03.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:43:03.032
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 03/27/23 14:43:03.037
Mar 27 14:43:03.050: INFO: Waiting up to 5m0s for pod "pod-2e2cbe0d-a2b1-4ed1-929d-4054b8bb2bb3" in namespace "emptydir-7071" to be "Succeeded or Failed"
Mar 27 14:43:03.062: INFO: Pod "pod-2e2cbe0d-a2b1-4ed1-929d-4054b8bb2bb3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.449527ms
Mar 27 14:43:05.068: INFO: Pod "pod-2e2cbe0d-a2b1-4ed1-929d-4054b8bb2bb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017928746s
Mar 27 14:43:07.068: INFO: Pod "pod-2e2cbe0d-a2b1-4ed1-929d-4054b8bb2bb3": Phase="Running", Reason="", readiness=false. Elapsed: 4.01771317s
Mar 27 14:43:09.078: INFO: Pod "pod-2e2cbe0d-a2b1-4ed1-929d-4054b8bb2bb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028128217s
STEP: Saw pod success 03/27/23 14:43:09.078
Mar 27 14:43:09.078: INFO: Pod "pod-2e2cbe0d-a2b1-4ed1-929d-4054b8bb2bb3" satisfied condition "Succeeded or Failed"
Mar 27 14:43:09.089: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-2e2cbe0d-a2b1-4ed1-929d-4054b8bb2bb3 container test-container: <nil>
STEP: delete the pod 03/27/23 14:43:09.098
Mar 27 14:43:09.119: INFO: Waiting for pod pod-2e2cbe0d-a2b1-4ed1-929d-4054b8bb2bb3 to disappear
Mar 27 14:43:09.126: INFO: Pod pod-2e2cbe0d-a2b1-4ed1-929d-4054b8bb2bb3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 27 14:43:09.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7071" for this suite. 03/27/23 14:43:09.132
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":116,"skipped":2289,"failed":0}
------------------------------
• [SLOW TEST] [6.139 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:43:03.01
    Mar 27 14:43:03.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename emptydir 03/27/23 14:43:03.011
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:43:03.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:43:03.032
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/27/23 14:43:03.037
    Mar 27 14:43:03.050: INFO: Waiting up to 5m0s for pod "pod-2e2cbe0d-a2b1-4ed1-929d-4054b8bb2bb3" in namespace "emptydir-7071" to be "Succeeded or Failed"
    Mar 27 14:43:03.062: INFO: Pod "pod-2e2cbe0d-a2b1-4ed1-929d-4054b8bb2bb3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.449527ms
    Mar 27 14:43:05.068: INFO: Pod "pod-2e2cbe0d-a2b1-4ed1-929d-4054b8bb2bb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017928746s
    Mar 27 14:43:07.068: INFO: Pod "pod-2e2cbe0d-a2b1-4ed1-929d-4054b8bb2bb3": Phase="Running", Reason="", readiness=false. Elapsed: 4.01771317s
    Mar 27 14:43:09.078: INFO: Pod "pod-2e2cbe0d-a2b1-4ed1-929d-4054b8bb2bb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028128217s
    STEP: Saw pod success 03/27/23 14:43:09.078
    Mar 27 14:43:09.078: INFO: Pod "pod-2e2cbe0d-a2b1-4ed1-929d-4054b8bb2bb3" satisfied condition "Succeeded or Failed"
    Mar 27 14:43:09.089: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-2e2cbe0d-a2b1-4ed1-929d-4054b8bb2bb3 container test-container: <nil>
    STEP: delete the pod 03/27/23 14:43:09.098
    Mar 27 14:43:09.119: INFO: Waiting for pod pod-2e2cbe0d-a2b1-4ed1-929d-4054b8bb2bb3 to disappear
    Mar 27 14:43:09.126: INFO: Pod pod-2e2cbe0d-a2b1-4ed1-929d-4054b8bb2bb3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 27 14:43:09.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7071" for this suite. 03/27/23 14:43:09.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:43:09.152
Mar 27 14:43:09.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename emptydir 03/27/23 14:43:09.153
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:43:09.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:43:09.173
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/27/23 14:43:09.226
Mar 27 14:43:09.236: INFO: Waiting up to 5m0s for pod "pod-e169465a-47a0-4c1a-93f3-0bd7c4ea7ea9" in namespace "emptydir-4015" to be "Succeeded or Failed"
Mar 27 14:43:09.239: INFO: Pod "pod-e169465a-47a0-4c1a-93f3-0bd7c4ea7ea9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.456246ms
Mar 27 14:43:11.247: INFO: Pod "pod-e169465a-47a0-4c1a-93f3-0bd7c4ea7ea9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010822572s
Mar 27 14:43:13.245: INFO: Pod "pod-e169465a-47a0-4c1a-93f3-0bd7c4ea7ea9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00873319s
STEP: Saw pod success 03/27/23 14:43:13.245
Mar 27 14:43:13.245: INFO: Pod "pod-e169465a-47a0-4c1a-93f3-0bd7c4ea7ea9" satisfied condition "Succeeded or Failed"
Mar 27 14:43:13.250: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-e169465a-47a0-4c1a-93f3-0bd7c4ea7ea9 container test-container: <nil>
STEP: delete the pod 03/27/23 14:43:13.259
Mar 27 14:43:13.273: INFO: Waiting for pod pod-e169465a-47a0-4c1a-93f3-0bd7c4ea7ea9 to disappear
Mar 27 14:43:13.276: INFO: Pod pod-e169465a-47a0-4c1a-93f3-0bd7c4ea7ea9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 27 14:43:13.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4015" for this suite. 03/27/23 14:43:13.281
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":117,"skipped":2298,"failed":0}
------------------------------
• [4.137 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:43:09.152
    Mar 27 14:43:09.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename emptydir 03/27/23 14:43:09.153
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:43:09.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:43:09.173
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/27/23 14:43:09.226
    Mar 27 14:43:09.236: INFO: Waiting up to 5m0s for pod "pod-e169465a-47a0-4c1a-93f3-0bd7c4ea7ea9" in namespace "emptydir-4015" to be "Succeeded or Failed"
    Mar 27 14:43:09.239: INFO: Pod "pod-e169465a-47a0-4c1a-93f3-0bd7c4ea7ea9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.456246ms
    Mar 27 14:43:11.247: INFO: Pod "pod-e169465a-47a0-4c1a-93f3-0bd7c4ea7ea9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010822572s
    Mar 27 14:43:13.245: INFO: Pod "pod-e169465a-47a0-4c1a-93f3-0bd7c4ea7ea9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00873319s
    STEP: Saw pod success 03/27/23 14:43:13.245
    Mar 27 14:43:13.245: INFO: Pod "pod-e169465a-47a0-4c1a-93f3-0bd7c4ea7ea9" satisfied condition "Succeeded or Failed"
    Mar 27 14:43:13.250: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-e169465a-47a0-4c1a-93f3-0bd7c4ea7ea9 container test-container: <nil>
    STEP: delete the pod 03/27/23 14:43:13.259
    Mar 27 14:43:13.273: INFO: Waiting for pod pod-e169465a-47a0-4c1a-93f3-0bd7c4ea7ea9 to disappear
    Mar 27 14:43:13.276: INFO: Pod pod-e169465a-47a0-4c1a-93f3-0bd7c4ea7ea9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 27 14:43:13.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4015" for this suite. 03/27/23 14:43:13.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:43:13.289
Mar 27 14:43:13.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename secrets 03/27/23 14:43:13.29
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:43:13.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:43:13.315
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-8641b4e2-5f9c-4f8e-bc68-95d42f3b4798 03/27/23 14:43:13.324
STEP: Creating secret with name s-test-opt-upd-92099e5b-9f2c-41b4-8da5-4db9a8ff635b 03/27/23 14:43:13.329
STEP: Creating the pod 03/27/23 14:43:13.337
Mar 27 14:43:13.349: INFO: Waiting up to 5m0s for pod "pod-secrets-def7e0ec-ab8e-4006-b682-5203cba3f3b2" in namespace "secrets-3558" to be "running and ready"
Mar 27 14:43:13.354: INFO: Pod "pod-secrets-def7e0ec-ab8e-4006-b682-5203cba3f3b2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.16742ms
Mar 27 14:43:13.354: INFO: The phase of Pod pod-secrets-def7e0ec-ab8e-4006-b682-5203cba3f3b2 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:43:15.361: INFO: Pod "pod-secrets-def7e0ec-ab8e-4006-b682-5203cba3f3b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.012277489s
Mar 27 14:43:15.361: INFO: The phase of Pod pod-secrets-def7e0ec-ab8e-4006-b682-5203cba3f3b2 is Running (Ready = true)
Mar 27 14:43:15.361: INFO: Pod "pod-secrets-def7e0ec-ab8e-4006-b682-5203cba3f3b2" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-8641b4e2-5f9c-4f8e-bc68-95d42f3b4798 03/27/23 14:43:15.395
STEP: Updating secret s-test-opt-upd-92099e5b-9f2c-41b4-8da5-4db9a8ff635b 03/27/23 14:43:15.404
STEP: Creating secret with name s-test-opt-create-70797b5e-c74f-4ddd-b936-fa21c1b24c14 03/27/23 14:43:15.409
STEP: waiting to observe update in volume 03/27/23 14:43:15.413
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 27 14:43:17.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3558" for this suite. 03/27/23 14:43:17.465
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":118,"skipped":2311,"failed":0}
------------------------------
• [4.186 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:43:13.289
    Mar 27 14:43:13.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename secrets 03/27/23 14:43:13.29
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:43:13.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:43:13.315
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-8641b4e2-5f9c-4f8e-bc68-95d42f3b4798 03/27/23 14:43:13.324
    STEP: Creating secret with name s-test-opt-upd-92099e5b-9f2c-41b4-8da5-4db9a8ff635b 03/27/23 14:43:13.329
    STEP: Creating the pod 03/27/23 14:43:13.337
    Mar 27 14:43:13.349: INFO: Waiting up to 5m0s for pod "pod-secrets-def7e0ec-ab8e-4006-b682-5203cba3f3b2" in namespace "secrets-3558" to be "running and ready"
    Mar 27 14:43:13.354: INFO: Pod "pod-secrets-def7e0ec-ab8e-4006-b682-5203cba3f3b2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.16742ms
    Mar 27 14:43:13.354: INFO: The phase of Pod pod-secrets-def7e0ec-ab8e-4006-b682-5203cba3f3b2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:43:15.361: INFO: Pod "pod-secrets-def7e0ec-ab8e-4006-b682-5203cba3f3b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.012277489s
    Mar 27 14:43:15.361: INFO: The phase of Pod pod-secrets-def7e0ec-ab8e-4006-b682-5203cba3f3b2 is Running (Ready = true)
    Mar 27 14:43:15.361: INFO: Pod "pod-secrets-def7e0ec-ab8e-4006-b682-5203cba3f3b2" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-8641b4e2-5f9c-4f8e-bc68-95d42f3b4798 03/27/23 14:43:15.395
    STEP: Updating secret s-test-opt-upd-92099e5b-9f2c-41b4-8da5-4db9a8ff635b 03/27/23 14:43:15.404
    STEP: Creating secret with name s-test-opt-create-70797b5e-c74f-4ddd-b936-fa21c1b24c14 03/27/23 14:43:15.409
    STEP: waiting to observe update in volume 03/27/23 14:43:15.413
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 27 14:43:17.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3558" for this suite. 03/27/23 14:43:17.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:43:17.475
Mar 27 14:43:17.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename container-runtime 03/27/23 14:43:17.476
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:43:17.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:43:17.507
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 03/27/23 14:43:17.51
STEP: wait for the container to reach Succeeded 03/27/23 14:43:17.52
STEP: get the container status 03/27/23 14:43:22.559
STEP: the container should be terminated 03/27/23 14:43:22.566
STEP: the termination message should be set 03/27/23 14:43:22.566
Mar 27 14:43:22.566: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 03/27/23 14:43:22.566
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar 27 14:43:22.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8497" for this suite. 03/27/23 14:43:22.602
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":119,"skipped":2316,"failed":0}
------------------------------
• [SLOW TEST] [5.137 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:43:17.475
    Mar 27 14:43:17.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename container-runtime 03/27/23 14:43:17.476
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:43:17.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:43:17.507
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 03/27/23 14:43:17.51
    STEP: wait for the container to reach Succeeded 03/27/23 14:43:17.52
    STEP: get the container status 03/27/23 14:43:22.559
    STEP: the container should be terminated 03/27/23 14:43:22.566
    STEP: the termination message should be set 03/27/23 14:43:22.566
    Mar 27 14:43:22.566: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 03/27/23 14:43:22.566
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar 27 14:43:22.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8497" for this suite. 03/27/23 14:43:22.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:43:22.614
Mar 27 14:43:22.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename security-context-test 03/27/23 14:43:22.615
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:43:22.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:43:22.64
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Mar 27 14:43:22.661: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-5763cd0c-16e7-4971-95b4-e864e604c6ee" in namespace "security-context-test-101" to be "Succeeded or Failed"
Mar 27 14:43:22.669: INFO: Pod "busybox-readonly-false-5763cd0c-16e7-4971-95b4-e864e604c6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 7.903221ms
Mar 27 14:43:24.675: INFO: Pod "busybox-readonly-false-5763cd0c-16e7-4971-95b4-e864e604c6ee": Phase="Running", Reason="", readiness=true. Elapsed: 2.013734527s
Mar 27 14:43:26.675: INFO: Pod "busybox-readonly-false-5763cd0c-16e7-4971-95b4-e864e604c6ee": Phase="Running", Reason="", readiness=false. Elapsed: 4.013787549s
Mar 27 14:43:28.674: INFO: Pod "busybox-readonly-false-5763cd0c-16e7-4971-95b4-e864e604c6ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013134154s
Mar 27 14:43:28.674: INFO: Pod "busybox-readonly-false-5763cd0c-16e7-4971-95b4-e864e604c6ee" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar 27 14:43:28.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-101" for this suite. 03/27/23 14:43:28.679
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":120,"skipped":2334,"failed":0}
------------------------------
• [SLOW TEST] [6.073 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:43:22.614
    Mar 27 14:43:22.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename security-context-test 03/27/23 14:43:22.615
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:43:22.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:43:22.64
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Mar 27 14:43:22.661: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-5763cd0c-16e7-4971-95b4-e864e604c6ee" in namespace "security-context-test-101" to be "Succeeded or Failed"
    Mar 27 14:43:22.669: INFO: Pod "busybox-readonly-false-5763cd0c-16e7-4971-95b4-e864e604c6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 7.903221ms
    Mar 27 14:43:24.675: INFO: Pod "busybox-readonly-false-5763cd0c-16e7-4971-95b4-e864e604c6ee": Phase="Running", Reason="", readiness=true. Elapsed: 2.013734527s
    Mar 27 14:43:26.675: INFO: Pod "busybox-readonly-false-5763cd0c-16e7-4971-95b4-e864e604c6ee": Phase="Running", Reason="", readiness=false. Elapsed: 4.013787549s
    Mar 27 14:43:28.674: INFO: Pod "busybox-readonly-false-5763cd0c-16e7-4971-95b4-e864e604c6ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013134154s
    Mar 27 14:43:28.674: INFO: Pod "busybox-readonly-false-5763cd0c-16e7-4971-95b4-e864e604c6ee" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar 27 14:43:28.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-101" for this suite. 03/27/23 14:43:28.679
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:43:28.69
Mar 27 14:43:28.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 14:43:28.691
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:43:28.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:43:28.713
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-7f756f1a-4bf3-4d73-b7cd-7140141b72ec 03/27/23 14:43:28.717
STEP: Creating a pod to test consume configMaps 03/27/23 14:43:28.723
Mar 27 14:43:28.735: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e15f0c5e-5009-4753-b651-8519462720dc" in namespace "projected-9016" to be "Succeeded or Failed"
Mar 27 14:43:28.738: INFO: Pod "pod-projected-configmaps-e15f0c5e-5009-4753-b651-8519462720dc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.618576ms
Mar 27 14:43:30.744: INFO: Pod "pod-projected-configmaps-e15f0c5e-5009-4753-b651-8519462720dc": Phase="Running", Reason="", readiness=true. Elapsed: 2.009042282s
Mar 27 14:43:32.744: INFO: Pod "pod-projected-configmaps-e15f0c5e-5009-4753-b651-8519462720dc": Phase="Running", Reason="", readiness=false. Elapsed: 4.009269298s
Mar 27 14:43:34.745: INFO: Pod "pod-projected-configmaps-e15f0c5e-5009-4753-b651-8519462720dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010680013s
STEP: Saw pod success 03/27/23 14:43:34.745
Mar 27 14:43:34.746: INFO: Pod "pod-projected-configmaps-e15f0c5e-5009-4753-b651-8519462720dc" satisfied condition "Succeeded or Failed"
Mar 27 14:43:34.749: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-configmaps-e15f0c5e-5009-4753-b651-8519462720dc container agnhost-container: <nil>
STEP: delete the pod 03/27/23 14:43:34.759
Mar 27 14:43:34.776: INFO: Waiting for pod pod-projected-configmaps-e15f0c5e-5009-4753-b651-8519462720dc to disappear
Mar 27 14:43:34.781: INFO: Pod pod-projected-configmaps-e15f0c5e-5009-4753-b651-8519462720dc no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 27 14:43:34.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9016" for this suite. 03/27/23 14:43:34.786
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":121,"skipped":2356,"failed":0}
------------------------------
• [SLOW TEST] [6.102 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:43:28.69
    Mar 27 14:43:28.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 14:43:28.691
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:43:28.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:43:28.713
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-7f756f1a-4bf3-4d73-b7cd-7140141b72ec 03/27/23 14:43:28.717
    STEP: Creating a pod to test consume configMaps 03/27/23 14:43:28.723
    Mar 27 14:43:28.735: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e15f0c5e-5009-4753-b651-8519462720dc" in namespace "projected-9016" to be "Succeeded or Failed"
    Mar 27 14:43:28.738: INFO: Pod "pod-projected-configmaps-e15f0c5e-5009-4753-b651-8519462720dc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.618576ms
    Mar 27 14:43:30.744: INFO: Pod "pod-projected-configmaps-e15f0c5e-5009-4753-b651-8519462720dc": Phase="Running", Reason="", readiness=true. Elapsed: 2.009042282s
    Mar 27 14:43:32.744: INFO: Pod "pod-projected-configmaps-e15f0c5e-5009-4753-b651-8519462720dc": Phase="Running", Reason="", readiness=false. Elapsed: 4.009269298s
    Mar 27 14:43:34.745: INFO: Pod "pod-projected-configmaps-e15f0c5e-5009-4753-b651-8519462720dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010680013s
    STEP: Saw pod success 03/27/23 14:43:34.745
    Mar 27 14:43:34.746: INFO: Pod "pod-projected-configmaps-e15f0c5e-5009-4753-b651-8519462720dc" satisfied condition "Succeeded or Failed"
    Mar 27 14:43:34.749: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-configmaps-e15f0c5e-5009-4753-b651-8519462720dc container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 14:43:34.759
    Mar 27 14:43:34.776: INFO: Waiting for pod pod-projected-configmaps-e15f0c5e-5009-4753-b651-8519462720dc to disappear
    Mar 27 14:43:34.781: INFO: Pod pod-projected-configmaps-e15f0c5e-5009-4753-b651-8519462720dc no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 27 14:43:34.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9016" for this suite. 03/27/23 14:43:34.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:43:34.794
Mar 27 14:43:34.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename container-probe 03/27/23 14:43:34.795
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:43:34.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:43:34.815
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-61b7e4b3-a213-4ee4-bbae-56294f69c841 in namespace container-probe-6499 03/27/23 14:43:34.82
Mar 27 14:43:34.833: INFO: Waiting up to 5m0s for pod "liveness-61b7e4b3-a213-4ee4-bbae-56294f69c841" in namespace "container-probe-6499" to be "not pending"
Mar 27 14:43:34.837: INFO: Pod "liveness-61b7e4b3-a213-4ee4-bbae-56294f69c841": Phase="Pending", Reason="", readiness=false. Elapsed: 3.751593ms
Mar 27 14:43:36.842: INFO: Pod "liveness-61b7e4b3-a213-4ee4-bbae-56294f69c841": Phase="Running", Reason="", readiness=true. Elapsed: 2.00941927s
Mar 27 14:43:36.842: INFO: Pod "liveness-61b7e4b3-a213-4ee4-bbae-56294f69c841" satisfied condition "not pending"
Mar 27 14:43:36.842: INFO: Started pod liveness-61b7e4b3-a213-4ee4-bbae-56294f69c841 in namespace container-probe-6499
STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 14:43:36.842
Mar 27 14:43:36.846: INFO: Initial restart count of pod liveness-61b7e4b3-a213-4ee4-bbae-56294f69c841 is 0
Mar 27 14:43:56.909: INFO: Restart count of pod container-probe-6499/liveness-61b7e4b3-a213-4ee4-bbae-56294f69c841 is now 1 (20.063007521s elapsed)
STEP: deleting the pod 03/27/23 14:43:56.909
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 27 14:43:56.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6499" for this suite. 03/27/23 14:43:56.929
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":122,"skipped":2365,"failed":0}
------------------------------
• [SLOW TEST] [22.145 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:43:34.794
    Mar 27 14:43:34.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename container-probe 03/27/23 14:43:34.795
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:43:34.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:43:34.815
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-61b7e4b3-a213-4ee4-bbae-56294f69c841 in namespace container-probe-6499 03/27/23 14:43:34.82
    Mar 27 14:43:34.833: INFO: Waiting up to 5m0s for pod "liveness-61b7e4b3-a213-4ee4-bbae-56294f69c841" in namespace "container-probe-6499" to be "not pending"
    Mar 27 14:43:34.837: INFO: Pod "liveness-61b7e4b3-a213-4ee4-bbae-56294f69c841": Phase="Pending", Reason="", readiness=false. Elapsed: 3.751593ms
    Mar 27 14:43:36.842: INFO: Pod "liveness-61b7e4b3-a213-4ee4-bbae-56294f69c841": Phase="Running", Reason="", readiness=true. Elapsed: 2.00941927s
    Mar 27 14:43:36.842: INFO: Pod "liveness-61b7e4b3-a213-4ee4-bbae-56294f69c841" satisfied condition "not pending"
    Mar 27 14:43:36.842: INFO: Started pod liveness-61b7e4b3-a213-4ee4-bbae-56294f69c841 in namespace container-probe-6499
    STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 14:43:36.842
    Mar 27 14:43:36.846: INFO: Initial restart count of pod liveness-61b7e4b3-a213-4ee4-bbae-56294f69c841 is 0
    Mar 27 14:43:56.909: INFO: Restart count of pod container-probe-6499/liveness-61b7e4b3-a213-4ee4-bbae-56294f69c841 is now 1 (20.063007521s elapsed)
    STEP: deleting the pod 03/27/23 14:43:56.909
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 27 14:43:56.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6499" for this suite. 03/27/23 14:43:56.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:43:56.942
Mar 27 14:43:56.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename statefulset 03/27/23 14:43:56.943
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:43:56.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:43:56.967
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1021 03/27/23 14:43:56.971
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Mar 27 14:43:56.992: INFO: Found 0 stateful pods, waiting for 1
Mar 27 14:44:07.008: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 03/27/23 14:44:07.016
W0327 14:44:07.030480      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 27 14:44:07.042: INFO: Found 1 stateful pods, waiting for 2
Mar 27 14:44:17.056: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 14:44:17.056: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 03/27/23 14:44:17.066
STEP: Delete all of the StatefulSets 03/27/23 14:44:17.071
STEP: Verify that StatefulSets have been deleted 03/27/23 14:44:17.08
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 27 14:44:17.083: INFO: Deleting all statefulset in ns statefulset-1021
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 27 14:44:17.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1021" for this suite. 03/27/23 14:44:17.099
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":123,"skipped":2384,"failed":0}
------------------------------
• [SLOW TEST] [20.171 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:43:56.942
    Mar 27 14:43:56.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename statefulset 03/27/23 14:43:56.943
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:43:56.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:43:56.967
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1021 03/27/23 14:43:56.971
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Mar 27 14:43:56.992: INFO: Found 0 stateful pods, waiting for 1
    Mar 27 14:44:07.008: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 03/27/23 14:44:07.016
    W0327 14:44:07.030480      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 27 14:44:07.042: INFO: Found 1 stateful pods, waiting for 2
    Mar 27 14:44:17.056: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 14:44:17.056: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 03/27/23 14:44:17.066
    STEP: Delete all of the StatefulSets 03/27/23 14:44:17.071
    STEP: Verify that StatefulSets have been deleted 03/27/23 14:44:17.08
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 27 14:44:17.083: INFO: Deleting all statefulset in ns statefulset-1021
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 27 14:44:17.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1021" for this suite. 03/27/23 14:44:17.099
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:44:17.114
Mar 27 14:44:17.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename replication-controller 03/27/23 14:44:17.115
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:44:17.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:44:17.14
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 03/27/23 14:44:17.147
STEP: waiting for RC to be added 03/27/23 14:44:17.153
STEP: waiting for available Replicas 03/27/23 14:44:17.154
STEP: patching ReplicationController 03/27/23 14:44:19.389
STEP: waiting for RC to be modified 03/27/23 14:44:19.403
STEP: patching ReplicationController status 03/27/23 14:44:19.403
STEP: waiting for RC to be modified 03/27/23 14:44:19.413
STEP: waiting for available Replicas 03/27/23 14:44:19.413
STEP: fetching ReplicationController status 03/27/23 14:44:19.422
STEP: patching ReplicationController scale 03/27/23 14:44:19.427
STEP: waiting for RC to be modified 03/27/23 14:44:19.437
STEP: waiting for ReplicationController's scale to be the max amount 03/27/23 14:44:19.437
STEP: fetching ReplicationController; ensuring that it's patched 03/27/23 14:44:20.823
STEP: updating ReplicationController status 03/27/23 14:44:20.829
STEP: waiting for RC to be modified 03/27/23 14:44:20.841
STEP: listing all ReplicationControllers 03/27/23 14:44:20.842
STEP: checking that ReplicationController has expected values 03/27/23 14:44:20.847
STEP: deleting ReplicationControllers by collection 03/27/23 14:44:20.847
STEP: waiting for ReplicationController to have a DELETED watchEvent 03/27/23 14:44:20.863
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar 27 14:44:20.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2601" for this suite. 03/27/23 14:44:20.943
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":124,"skipped":2386,"failed":0}
------------------------------
• [3.838 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:44:17.114
    Mar 27 14:44:17.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename replication-controller 03/27/23 14:44:17.115
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:44:17.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:44:17.14
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 03/27/23 14:44:17.147
    STEP: waiting for RC to be added 03/27/23 14:44:17.153
    STEP: waiting for available Replicas 03/27/23 14:44:17.154
    STEP: patching ReplicationController 03/27/23 14:44:19.389
    STEP: waiting for RC to be modified 03/27/23 14:44:19.403
    STEP: patching ReplicationController status 03/27/23 14:44:19.403
    STEP: waiting for RC to be modified 03/27/23 14:44:19.413
    STEP: waiting for available Replicas 03/27/23 14:44:19.413
    STEP: fetching ReplicationController status 03/27/23 14:44:19.422
    STEP: patching ReplicationController scale 03/27/23 14:44:19.427
    STEP: waiting for RC to be modified 03/27/23 14:44:19.437
    STEP: waiting for ReplicationController's scale to be the max amount 03/27/23 14:44:19.437
    STEP: fetching ReplicationController; ensuring that it's patched 03/27/23 14:44:20.823
    STEP: updating ReplicationController status 03/27/23 14:44:20.829
    STEP: waiting for RC to be modified 03/27/23 14:44:20.841
    STEP: listing all ReplicationControllers 03/27/23 14:44:20.842
    STEP: checking that ReplicationController has expected values 03/27/23 14:44:20.847
    STEP: deleting ReplicationControllers by collection 03/27/23 14:44:20.847
    STEP: waiting for ReplicationController to have a DELETED watchEvent 03/27/23 14:44:20.863
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar 27 14:44:20.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-2601" for this suite. 03/27/23 14:44:20.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:44:20.952
Mar 27 14:44:20.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename pod-network-test 03/27/23 14:44:20.953
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:44:20.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:44:20.983
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-7181 03/27/23 14:44:20.987
STEP: creating a selector 03/27/23 14:44:20.987
STEP: Creating the service pods in kubernetes 03/27/23 14:44:20.987
Mar 27 14:44:20.987: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 27 14:44:21.026: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7181" to be "running and ready"
Mar 27 14:44:21.035: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.38452ms
Mar 27 14:44:21.035: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:44:23.047: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.021302786s
Mar 27 14:44:23.047: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:44:25.041: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014855238s
Mar 27 14:44:25.041: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:44:27.041: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014921232s
Mar 27 14:44:27.041: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:44:29.043: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.017255125s
Mar 27 14:44:29.043: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:44:31.041: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014869089s
Mar 27 14:44:31.041: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 14:44:33.040: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.01445868s
Mar 27 14:44:33.041: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 27 14:44:33.041: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 27 14:44:33.044: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7181" to be "running and ready"
Mar 27 14:44:33.048: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 3.823241ms
Mar 27 14:44:33.048: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 27 14:44:35.057: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.012661089s
Mar 27 14:44:35.057: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 27 14:44:37.055: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.010415752s
Mar 27 14:44:37.055: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 27 14:44:39.054: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.009155411s
Mar 27 14:44:39.054: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 27 14:44:41.054: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.009513888s
Mar 27 14:44:41.054: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 27 14:44:43.055: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.01081501s
Mar 27 14:44:43.055: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 27 14:44:43.055: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 27 14:44:43.059: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7181" to be "running and ready"
Mar 27 14:44:43.063: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.619745ms
Mar 27 14:44:43.063: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 27 14:44:43.063: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/27/23 14:44:43.066
Mar 27 14:44:43.081: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7181" to be "running"
Mar 27 14:44:43.087: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.942862ms
Mar 27 14:44:45.092: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011118495s
Mar 27 14:44:45.092: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 27 14:44:45.096: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7181" to be "running"
Mar 27 14:44:45.100: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.399145ms
Mar 27 14:44:45.100: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar 27 14:44:45.103: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 27 14:44:45.103: INFO: Going to poll 172.25.1.121 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 27 14:44:45.106: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.1.121 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7181 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:44:45.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:44:45.106: INFO: ExecWithOptions: Clientset creation
Mar 27 14:44:45.106: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-7181/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.1.121+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 27 14:44:46.297: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 27 14:44:46.297: INFO: Going to poll 172.25.2.171 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 27 14:44:46.301: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.2.171 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7181 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:44:46.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:44:46.302: INFO: ExecWithOptions: Clientset creation
Mar 27 14:44:46.302: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-7181/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.2.171+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 27 14:44:47.400: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 27 14:44:47.400: INFO: Going to poll 172.25.0.95 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 27 14:44:47.405: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.0.95 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7181 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 14:44:47.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 14:44:47.406: INFO: ExecWithOptions: Clientset creation
Mar 27 14:44:47.406: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-7181/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.0.95+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 27 14:44:48.494: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar 27 14:44:48.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7181" for this suite. 03/27/23 14:44:48.5
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":125,"skipped":2393,"failed":0}
------------------------------
• [SLOW TEST] [27.555 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:44:20.952
    Mar 27 14:44:20.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename pod-network-test 03/27/23 14:44:20.953
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:44:20.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:44:20.983
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-7181 03/27/23 14:44:20.987
    STEP: creating a selector 03/27/23 14:44:20.987
    STEP: Creating the service pods in kubernetes 03/27/23 14:44:20.987
    Mar 27 14:44:20.987: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 27 14:44:21.026: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7181" to be "running and ready"
    Mar 27 14:44:21.035: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.38452ms
    Mar 27 14:44:21.035: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:44:23.047: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.021302786s
    Mar 27 14:44:23.047: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:44:25.041: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014855238s
    Mar 27 14:44:25.041: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:44:27.041: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014921232s
    Mar 27 14:44:27.041: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:44:29.043: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.017255125s
    Mar 27 14:44:29.043: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:44:31.041: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014869089s
    Mar 27 14:44:31.041: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 14:44:33.040: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.01445868s
    Mar 27 14:44:33.041: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 27 14:44:33.041: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 27 14:44:33.044: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7181" to be "running and ready"
    Mar 27 14:44:33.048: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 3.823241ms
    Mar 27 14:44:33.048: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 27 14:44:35.057: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.012661089s
    Mar 27 14:44:35.057: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 27 14:44:37.055: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.010415752s
    Mar 27 14:44:37.055: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 27 14:44:39.054: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.009155411s
    Mar 27 14:44:39.054: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 27 14:44:41.054: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.009513888s
    Mar 27 14:44:41.054: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 27 14:44:43.055: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.01081501s
    Mar 27 14:44:43.055: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 27 14:44:43.055: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 27 14:44:43.059: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7181" to be "running and ready"
    Mar 27 14:44:43.063: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.619745ms
    Mar 27 14:44:43.063: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 27 14:44:43.063: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/27/23 14:44:43.066
    Mar 27 14:44:43.081: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7181" to be "running"
    Mar 27 14:44:43.087: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.942862ms
    Mar 27 14:44:45.092: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011118495s
    Mar 27 14:44:45.092: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 27 14:44:45.096: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7181" to be "running"
    Mar 27 14:44:45.100: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.399145ms
    Mar 27 14:44:45.100: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar 27 14:44:45.103: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 27 14:44:45.103: INFO: Going to poll 172.25.1.121 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 27 14:44:45.106: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.1.121 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7181 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:44:45.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:44:45.106: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:44:45.106: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-7181/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.1.121+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 27 14:44:46.297: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar 27 14:44:46.297: INFO: Going to poll 172.25.2.171 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 27 14:44:46.301: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.2.171 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7181 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:44:46.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:44:46.302: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:44:46.302: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-7181/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.2.171+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 27 14:44:47.400: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar 27 14:44:47.400: INFO: Going to poll 172.25.0.95 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 27 14:44:47.405: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.0.95 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7181 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 14:44:47.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 14:44:47.406: INFO: ExecWithOptions: Clientset creation
    Mar 27 14:44:47.406: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-7181/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.0.95+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 27 14:44:48.494: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar 27 14:44:48.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-7181" for this suite. 03/27/23 14:44:48.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:44:48.509
Mar 27 14:44:48.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename webhook 03/27/23 14:44:48.509
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:44:48.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:44:48.533
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/27/23 14:44:48.551
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 14:44:48.98
STEP: Deploying the webhook pod 03/27/23 14:44:48.989
STEP: Wait for the deployment to be ready 03/27/23 14:44:49.007
Mar 27 14:44:49.015: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 14:44:51.028
STEP: Verifying the service has paired with the endpoint 03/27/23 14:44:51.044
Mar 27 14:44:52.045: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 03/27/23 14:44:52.125
STEP: Creating a configMap that should be mutated 03/27/23 14:44:52.149
STEP: Deleting the collection of validation webhooks 03/27/23 14:44:52.218
STEP: Creating a configMap that should not be mutated 03/27/23 14:44:52.282
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:44:52.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9701" for this suite. 03/27/23 14:44:52.305
STEP: Destroying namespace "webhook-9701-markers" for this suite. 03/27/23 14:44:52.316
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":126,"skipped":2423,"failed":0}
------------------------------
• [3.857 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:44:48.509
    Mar 27 14:44:48.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename webhook 03/27/23 14:44:48.509
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:44:48.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:44:48.533
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/27/23 14:44:48.551
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 14:44:48.98
    STEP: Deploying the webhook pod 03/27/23 14:44:48.989
    STEP: Wait for the deployment to be ready 03/27/23 14:44:49.007
    Mar 27 14:44:49.015: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 14:44:51.028
    STEP: Verifying the service has paired with the endpoint 03/27/23 14:44:51.044
    Mar 27 14:44:52.045: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 03/27/23 14:44:52.125
    STEP: Creating a configMap that should be mutated 03/27/23 14:44:52.149
    STEP: Deleting the collection of validation webhooks 03/27/23 14:44:52.218
    STEP: Creating a configMap that should not be mutated 03/27/23 14:44:52.282
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:44:52.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9701" for this suite. 03/27/23 14:44:52.305
    STEP: Destroying namespace "webhook-9701-markers" for this suite. 03/27/23 14:44:52.316
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:44:52.367
Mar 27 14:44:52.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename runtimeclass 03/27/23 14:44:52.368
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:44:52.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:44:52.388
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar 27 14:44:52.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6689" for this suite. 03/27/23 14:44:52.405
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":127,"skipped":2442,"failed":0}
------------------------------
• [0.046 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:44:52.367
    Mar 27 14:44:52.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename runtimeclass 03/27/23 14:44:52.368
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:44:52.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:44:52.388
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar 27 14:44:52.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-6689" for this suite. 03/27/23 14:44:52.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:44:52.415
Mar 27 14:44:52.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename services 03/27/23 14:44:52.416
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:44:52.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:44:52.435
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-907 03/27/23 14:44:52.438
STEP: creating replication controller nodeport-test in namespace services-907 03/27/23 14:44:52.464
I0327 14:44:52.473011      24 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-907, replica count: 2
I0327 14:44:55.525098      24 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 14:44:55.525: INFO: Creating new exec pod
Mar 27 14:44:55.537: INFO: Waiting up to 5m0s for pod "execpod8mmr5" in namespace "services-907" to be "running"
Mar 27 14:44:55.543: INFO: Pod "execpod8mmr5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.096156ms
Mar 27 14:44:57.549: INFO: Pod "execpod8mmr5": Phase="Running", Reason="", readiness=true. Elapsed: 2.011785156s
Mar 27 14:44:57.549: INFO: Pod "execpod8mmr5" satisfied condition "running"
Mar 27 14:44:58.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-907 exec execpod8mmr5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Mar 27 14:44:58.756: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 27 14:44:58.756: INFO: stdout: ""
Mar 27 14:44:59.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-907 exec execpod8mmr5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Mar 27 14:44:59.937: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 27 14:44:59.937: INFO: stdout: "nodeport-test-m29fk"
Mar 27 14:44:59.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-907 exec execpod8mmr5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.17.217 80'
Mar 27 14:45:01.158: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.17.217 80\nConnection to 10.240.17.217 80 port [tcp/http] succeeded!\n"
Mar 27 14:45:01.158: INFO: stdout: "nodeport-test-69f2s"
Mar 27 14:45:01.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-907 exec execpod8mmr5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.7 30448'
Mar 27 14:45:01.330: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.7 30448\nConnection to 192.168.1.7 30448 port [tcp/*] succeeded!\n"
Mar 27 14:45:01.330: INFO: stdout: "nodeport-test-69f2s"
Mar 27 14:45:01.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-907 exec execpod8mmr5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.3 30448'
Mar 27 14:45:01.531: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.3 30448\nConnection to 192.168.1.3 30448 port [tcp/*] succeeded!\n"
Mar 27 14:45:01.531: INFO: stdout: "nodeport-test-69f2s"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 27 14:45:01.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-907" for this suite. 03/27/23 14:45:01.539
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":128,"skipped":2480,"failed":0}
------------------------------
• [SLOW TEST] [9.130 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:44:52.415
    Mar 27 14:44:52.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename services 03/27/23 14:44:52.416
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:44:52.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:44:52.435
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-907 03/27/23 14:44:52.438
    STEP: creating replication controller nodeport-test in namespace services-907 03/27/23 14:44:52.464
    I0327 14:44:52.473011      24 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-907, replica count: 2
    I0327 14:44:55.525098      24 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 14:44:55.525: INFO: Creating new exec pod
    Mar 27 14:44:55.537: INFO: Waiting up to 5m0s for pod "execpod8mmr5" in namespace "services-907" to be "running"
    Mar 27 14:44:55.543: INFO: Pod "execpod8mmr5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.096156ms
    Mar 27 14:44:57.549: INFO: Pod "execpod8mmr5": Phase="Running", Reason="", readiness=true. Elapsed: 2.011785156s
    Mar 27 14:44:57.549: INFO: Pod "execpod8mmr5" satisfied condition "running"
    Mar 27 14:44:58.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-907 exec execpod8mmr5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Mar 27 14:44:58.756: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Mar 27 14:44:58.756: INFO: stdout: ""
    Mar 27 14:44:59.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-907 exec execpod8mmr5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Mar 27 14:44:59.937: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Mar 27 14:44:59.937: INFO: stdout: "nodeport-test-m29fk"
    Mar 27 14:44:59.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-907 exec execpod8mmr5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.17.217 80'
    Mar 27 14:45:01.158: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.17.217 80\nConnection to 10.240.17.217 80 port [tcp/http] succeeded!\n"
    Mar 27 14:45:01.158: INFO: stdout: "nodeport-test-69f2s"
    Mar 27 14:45:01.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-907 exec execpod8mmr5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.7 30448'
    Mar 27 14:45:01.330: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.7 30448\nConnection to 192.168.1.7 30448 port [tcp/*] succeeded!\n"
    Mar 27 14:45:01.330: INFO: stdout: "nodeport-test-69f2s"
    Mar 27 14:45:01.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-907 exec execpod8mmr5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.3 30448'
    Mar 27 14:45:01.531: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.3 30448\nConnection to 192.168.1.3 30448 port [tcp/*] succeeded!\n"
    Mar 27 14:45:01.531: INFO: stdout: "nodeport-test-69f2s"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 27 14:45:01.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-907" for this suite. 03/27/23 14:45:01.539
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:45:01.552
Mar 27 14:45:01.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename services 03/27/23 14:45:01.553
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:45:01.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:45:01.577
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 03/27/23 14:45:01.585
STEP: watching for the Service to be added 03/27/23 14:45:01.603
Mar 27 14:45:01.606: INFO: Found Service test-service-xxrjl in namespace services-3575 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Mar 27 14:45:01.606: INFO: Service test-service-xxrjl created
STEP: Getting /status 03/27/23 14:45:01.606
Mar 27 14:45:01.611: INFO: Service test-service-xxrjl has LoadBalancer: {[]}
STEP: patching the ServiceStatus 03/27/23 14:45:01.611
STEP: watching for the Service to be patched 03/27/23 14:45:01.623
Mar 27 14:45:01.626: INFO: observed Service test-service-xxrjl in namespace services-3575 with annotations: map[] & LoadBalancer: {[]}
Mar 27 14:45:01.626: INFO: Found Service test-service-xxrjl in namespace services-3575 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Mar 27 14:45:01.626: INFO: Service test-service-xxrjl has service status patched
STEP: updating the ServiceStatus 03/27/23 14:45:01.626
Mar 27 14:45:01.637: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 03/27/23 14:45:01.637
Mar 27 14:45:01.639: INFO: Observed Service test-service-xxrjl in namespace services-3575 with annotations: map[] & Conditions: {[]}
Mar 27 14:45:01.639: INFO: Observed event: &Service{ObjectMeta:{test-service-xxrjl  services-3575  c2b3555a-af32-44ee-8314-9730bb9979f7 27527 0 2023-03-27 14:45:01 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-27 14:45:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-27 14:45:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.240.24.115,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.240.24.115],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Mar 27 14:45:01.641: INFO: Found Service test-service-xxrjl in namespace services-3575 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 27 14:45:01.641: INFO: Service test-service-xxrjl has service status updated
STEP: patching the service 03/27/23 14:45:01.641
STEP: watching for the Service to be patched 03/27/23 14:45:01.66
Mar 27 14:45:01.663: INFO: observed Service test-service-xxrjl in namespace services-3575 with labels: map[test-service-static:true]
Mar 27 14:45:01.663: INFO: observed Service test-service-xxrjl in namespace services-3575 with labels: map[test-service-static:true]
Mar 27 14:45:01.663: INFO: observed Service test-service-xxrjl in namespace services-3575 with labels: map[test-service-static:true]
Mar 27 14:45:01.663: INFO: Found Service test-service-xxrjl in namespace services-3575 with labels: map[test-service:patched test-service-static:true]
Mar 27 14:45:01.663: INFO: Service test-service-xxrjl patched
STEP: deleting the service 03/27/23 14:45:01.663
STEP: watching for the Service to be deleted 03/27/23 14:45:01.675
Mar 27 14:45:01.677: INFO: Observed event: ADDED
Mar 27 14:45:01.677: INFO: Observed event: MODIFIED
Mar 27 14:45:01.677: INFO: Observed event: MODIFIED
Mar 27 14:45:01.678: INFO: Observed event: MODIFIED
Mar 27 14:45:01.678: INFO: Found Service test-service-xxrjl in namespace services-3575 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Mar 27 14:45:01.678: INFO: Service test-service-xxrjl deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 27 14:45:01.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3575" for this suite. 03/27/23 14:45:01.683
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":129,"skipped":2483,"failed":0}
------------------------------
• [0.141 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:45:01.552
    Mar 27 14:45:01.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename services 03/27/23 14:45:01.553
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:45:01.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:45:01.577
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 03/27/23 14:45:01.585
    STEP: watching for the Service to be added 03/27/23 14:45:01.603
    Mar 27 14:45:01.606: INFO: Found Service test-service-xxrjl in namespace services-3575 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Mar 27 14:45:01.606: INFO: Service test-service-xxrjl created
    STEP: Getting /status 03/27/23 14:45:01.606
    Mar 27 14:45:01.611: INFO: Service test-service-xxrjl has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 03/27/23 14:45:01.611
    STEP: watching for the Service to be patched 03/27/23 14:45:01.623
    Mar 27 14:45:01.626: INFO: observed Service test-service-xxrjl in namespace services-3575 with annotations: map[] & LoadBalancer: {[]}
    Mar 27 14:45:01.626: INFO: Found Service test-service-xxrjl in namespace services-3575 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Mar 27 14:45:01.626: INFO: Service test-service-xxrjl has service status patched
    STEP: updating the ServiceStatus 03/27/23 14:45:01.626
    Mar 27 14:45:01.637: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 03/27/23 14:45:01.637
    Mar 27 14:45:01.639: INFO: Observed Service test-service-xxrjl in namespace services-3575 with annotations: map[] & Conditions: {[]}
    Mar 27 14:45:01.639: INFO: Observed event: &Service{ObjectMeta:{test-service-xxrjl  services-3575  c2b3555a-af32-44ee-8314-9730bb9979f7 27527 0 2023-03-27 14:45:01 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-27 14:45:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-27 14:45:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.240.24.115,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.240.24.115],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Mar 27 14:45:01.641: INFO: Found Service test-service-xxrjl in namespace services-3575 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 27 14:45:01.641: INFO: Service test-service-xxrjl has service status updated
    STEP: patching the service 03/27/23 14:45:01.641
    STEP: watching for the Service to be patched 03/27/23 14:45:01.66
    Mar 27 14:45:01.663: INFO: observed Service test-service-xxrjl in namespace services-3575 with labels: map[test-service-static:true]
    Mar 27 14:45:01.663: INFO: observed Service test-service-xxrjl in namespace services-3575 with labels: map[test-service-static:true]
    Mar 27 14:45:01.663: INFO: observed Service test-service-xxrjl in namespace services-3575 with labels: map[test-service-static:true]
    Mar 27 14:45:01.663: INFO: Found Service test-service-xxrjl in namespace services-3575 with labels: map[test-service:patched test-service-static:true]
    Mar 27 14:45:01.663: INFO: Service test-service-xxrjl patched
    STEP: deleting the service 03/27/23 14:45:01.663
    STEP: watching for the Service to be deleted 03/27/23 14:45:01.675
    Mar 27 14:45:01.677: INFO: Observed event: ADDED
    Mar 27 14:45:01.677: INFO: Observed event: MODIFIED
    Mar 27 14:45:01.677: INFO: Observed event: MODIFIED
    Mar 27 14:45:01.678: INFO: Observed event: MODIFIED
    Mar 27 14:45:01.678: INFO: Found Service test-service-xxrjl in namespace services-3575 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Mar 27 14:45:01.678: INFO: Service test-service-xxrjl deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 27 14:45:01.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3575" for this suite. 03/27/23 14:45:01.683
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:45:01.695
Mar 27 14:45:01.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 14:45:01.696
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:45:01.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:45:01.716
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-f53bd549-08e2-4395-a88a-3a9a195cc4cc 03/27/23 14:45:01.725
STEP: Creating a pod to test consume secrets 03/27/23 14:45:01.731
Mar 27 14:45:01.748: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6271a9a0-25b1-4840-8eef-c006d75730d4" in namespace "projected-1626" to be "Succeeded or Failed"
Mar 27 14:45:01.752: INFO: Pod "pod-projected-secrets-6271a9a0-25b1-4840-8eef-c006d75730d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.796876ms
Mar 27 14:45:03.758: INFO: Pod "pod-projected-secrets-6271a9a0-25b1-4840-8eef-c006d75730d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009914805s
Mar 27 14:45:05.758: INFO: Pod "pod-projected-secrets-6271a9a0-25b1-4840-8eef-c006d75730d4": Phase="Running", Reason="", readiness=false. Elapsed: 4.010401614s
Mar 27 14:45:07.757: INFO: Pod "pod-projected-secrets-6271a9a0-25b1-4840-8eef-c006d75730d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0088963s
STEP: Saw pod success 03/27/23 14:45:07.757
Mar 27 14:45:07.757: INFO: Pod "pod-projected-secrets-6271a9a0-25b1-4840-8eef-c006d75730d4" satisfied condition "Succeeded or Failed"
Mar 27 14:45:07.761: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-secrets-6271a9a0-25b1-4840-8eef-c006d75730d4 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/27/23 14:45:07.772
Mar 27 14:45:07.795: INFO: Waiting for pod pod-projected-secrets-6271a9a0-25b1-4840-8eef-c006d75730d4 to disappear
Mar 27 14:45:07.798: INFO: Pod pod-projected-secrets-6271a9a0-25b1-4840-8eef-c006d75730d4 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 27 14:45:07.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1626" for this suite. 03/27/23 14:45:07.806
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":130,"skipped":2517,"failed":0}
------------------------------
• [SLOW TEST] [6.121 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:45:01.695
    Mar 27 14:45:01.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 14:45:01.696
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:45:01.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:45:01.716
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-f53bd549-08e2-4395-a88a-3a9a195cc4cc 03/27/23 14:45:01.725
    STEP: Creating a pod to test consume secrets 03/27/23 14:45:01.731
    Mar 27 14:45:01.748: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6271a9a0-25b1-4840-8eef-c006d75730d4" in namespace "projected-1626" to be "Succeeded or Failed"
    Mar 27 14:45:01.752: INFO: Pod "pod-projected-secrets-6271a9a0-25b1-4840-8eef-c006d75730d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.796876ms
    Mar 27 14:45:03.758: INFO: Pod "pod-projected-secrets-6271a9a0-25b1-4840-8eef-c006d75730d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009914805s
    Mar 27 14:45:05.758: INFO: Pod "pod-projected-secrets-6271a9a0-25b1-4840-8eef-c006d75730d4": Phase="Running", Reason="", readiness=false. Elapsed: 4.010401614s
    Mar 27 14:45:07.757: INFO: Pod "pod-projected-secrets-6271a9a0-25b1-4840-8eef-c006d75730d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0088963s
    STEP: Saw pod success 03/27/23 14:45:07.757
    Mar 27 14:45:07.757: INFO: Pod "pod-projected-secrets-6271a9a0-25b1-4840-8eef-c006d75730d4" satisfied condition "Succeeded or Failed"
    Mar 27 14:45:07.761: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-secrets-6271a9a0-25b1-4840-8eef-c006d75730d4 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 14:45:07.772
    Mar 27 14:45:07.795: INFO: Waiting for pod pod-projected-secrets-6271a9a0-25b1-4840-8eef-c006d75730d4 to disappear
    Mar 27 14:45:07.798: INFO: Pod pod-projected-secrets-6271a9a0-25b1-4840-8eef-c006d75730d4 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 27 14:45:07.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1626" for this suite. 03/27/23 14:45:07.806
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:45:07.816
Mar 27 14:45:07.816: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubectl 03/27/23 14:45:07.817
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:45:07.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:45:07.841
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 03/27/23 14:45:07.844
Mar 27 14:45:07.845: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-1510 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 03/27/23 14:45:07.897
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 27 14:45:07.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1510" for this suite. 03/27/23 14:45:07.911
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":131,"skipped":2522,"failed":0}
------------------------------
• [0.101 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:45:07.816
    Mar 27 14:45:07.816: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubectl 03/27/23 14:45:07.817
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:45:07.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:45:07.841
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 03/27/23 14:45:07.844
    Mar 27 14:45:07.845: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-1510 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 03/27/23 14:45:07.897
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 27 14:45:07.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1510" for this suite. 03/27/23 14:45:07.911
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:45:07.918
Mar 27 14:45:07.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename configmap 03/27/23 14:45:07.919
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:45:07.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:45:07.94
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-dce24b97-a5d6-43de-b4d5-7725224625c2 03/27/23 14:45:07.949
STEP: Creating a pod to test consume configMaps 03/27/23 14:45:07.959
Mar 27 14:45:07.970: INFO: Waiting up to 5m0s for pod "pod-configmaps-cc949f11-bcf7-4e74-ac73-415418b0c3c0" in namespace "configmap-9894" to be "Succeeded or Failed"
Mar 27 14:45:07.973: INFO: Pod "pod-configmaps-cc949f11-bcf7-4e74-ac73-415418b0c3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.793666ms
Mar 27 14:45:09.981: INFO: Pod "pod-configmaps-cc949f11-bcf7-4e74-ac73-415418b0c3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011620584s
Mar 27 14:45:11.978: INFO: Pod "pod-configmaps-cc949f11-bcf7-4e74-ac73-415418b0c3c0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008241366s
Mar 27 14:45:13.979: INFO: Pod "pod-configmaps-cc949f11-bcf7-4e74-ac73-415418b0c3c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009062054s
STEP: Saw pod success 03/27/23 14:45:13.979
Mar 27 14:45:13.979: INFO: Pod "pod-configmaps-cc949f11-bcf7-4e74-ac73-415418b0c3c0" satisfied condition "Succeeded or Failed"
Mar 27 14:45:13.982: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-configmaps-cc949f11-bcf7-4e74-ac73-415418b0c3c0 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 14:45:13.99
Mar 27 14:45:14.003: INFO: Waiting for pod pod-configmaps-cc949f11-bcf7-4e74-ac73-415418b0c3c0 to disappear
Mar 27 14:45:14.009: INFO: Pod pod-configmaps-cc949f11-bcf7-4e74-ac73-415418b0c3c0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 27 14:45:14.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9894" for this suite. 03/27/23 14:45:14.015
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":132,"skipped":2523,"failed":0}
------------------------------
• [SLOW TEST] [6.104 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:45:07.918
    Mar 27 14:45:07.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename configmap 03/27/23 14:45:07.919
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:45:07.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:45:07.94
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-dce24b97-a5d6-43de-b4d5-7725224625c2 03/27/23 14:45:07.949
    STEP: Creating a pod to test consume configMaps 03/27/23 14:45:07.959
    Mar 27 14:45:07.970: INFO: Waiting up to 5m0s for pod "pod-configmaps-cc949f11-bcf7-4e74-ac73-415418b0c3c0" in namespace "configmap-9894" to be "Succeeded or Failed"
    Mar 27 14:45:07.973: INFO: Pod "pod-configmaps-cc949f11-bcf7-4e74-ac73-415418b0c3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.793666ms
    Mar 27 14:45:09.981: INFO: Pod "pod-configmaps-cc949f11-bcf7-4e74-ac73-415418b0c3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011620584s
    Mar 27 14:45:11.978: INFO: Pod "pod-configmaps-cc949f11-bcf7-4e74-ac73-415418b0c3c0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008241366s
    Mar 27 14:45:13.979: INFO: Pod "pod-configmaps-cc949f11-bcf7-4e74-ac73-415418b0c3c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009062054s
    STEP: Saw pod success 03/27/23 14:45:13.979
    Mar 27 14:45:13.979: INFO: Pod "pod-configmaps-cc949f11-bcf7-4e74-ac73-415418b0c3c0" satisfied condition "Succeeded or Failed"
    Mar 27 14:45:13.982: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-configmaps-cc949f11-bcf7-4e74-ac73-415418b0c3c0 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 14:45:13.99
    Mar 27 14:45:14.003: INFO: Waiting for pod pod-configmaps-cc949f11-bcf7-4e74-ac73-415418b0c3c0 to disappear
    Mar 27 14:45:14.009: INFO: Pod pod-configmaps-cc949f11-bcf7-4e74-ac73-415418b0c3c0 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 27 14:45:14.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9894" for this suite. 03/27/23 14:45:14.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:45:14.023
Mar 27 14:45:14.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubectl 03/27/23 14:45:14.024
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:45:14.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:45:14.043
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 03/27/23 14:45:14.047
Mar 27 14:45:14.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 create -f -'
Mar 27 14:45:14.763: INFO: stderr: ""
Mar 27 14:45:14.763: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/27/23 14:45:14.764
Mar 27 14:45:14.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 27 14:45:14.849: INFO: stderr: ""
Mar 27 14:45:14.849: INFO: stdout: "update-demo-nautilus-8t9m9 update-demo-nautilus-mgnkq "
Mar 27 14:45:14.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods update-demo-nautilus-8t9m9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 14:45:14.913: INFO: stderr: ""
Mar 27 14:45:14.913: INFO: stdout: ""
Mar 27 14:45:14.913: INFO: update-demo-nautilus-8t9m9 is created but not running
Mar 27 14:45:19.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 27 14:45:19.983: INFO: stderr: ""
Mar 27 14:45:19.983: INFO: stdout: "update-demo-nautilus-8t9m9 update-demo-nautilus-mgnkq "
Mar 27 14:45:19.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods update-demo-nautilus-8t9m9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 14:45:20.049: INFO: stderr: ""
Mar 27 14:45:20.049: INFO: stdout: ""
Mar 27 14:45:20.049: INFO: update-demo-nautilus-8t9m9 is created but not running
Mar 27 14:45:25.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 27 14:45:25.118: INFO: stderr: ""
Mar 27 14:45:25.118: INFO: stdout: "update-demo-nautilus-8t9m9 update-demo-nautilus-mgnkq "
Mar 27 14:45:25.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods update-demo-nautilus-8t9m9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 14:45:25.186: INFO: stderr: ""
Mar 27 14:45:25.186: INFO: stdout: "true"
Mar 27 14:45:25.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods update-demo-nautilus-8t9m9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 27 14:45:25.258: INFO: stderr: ""
Mar 27 14:45:25.258: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 27 14:45:25.258: INFO: validating pod update-demo-nautilus-8t9m9
Mar 27 14:45:25.270: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 27 14:45:25.270: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 27 14:45:25.270: INFO: update-demo-nautilus-8t9m9 is verified up and running
Mar 27 14:45:25.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods update-demo-nautilus-mgnkq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 14:45:25.335: INFO: stderr: ""
Mar 27 14:45:25.335: INFO: stdout: "true"
Mar 27 14:45:25.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods update-demo-nautilus-mgnkq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 27 14:45:25.403: INFO: stderr: ""
Mar 27 14:45:25.403: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 27 14:45:25.403: INFO: validating pod update-demo-nautilus-mgnkq
Mar 27 14:45:25.414: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 27 14:45:25.414: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 27 14:45:25.414: INFO: update-demo-nautilus-mgnkq is verified up and running
STEP: using delete to clean up resources 03/27/23 14:45:25.414
Mar 27 14:45:25.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 delete --grace-period=0 --force -f -'
Mar 27 14:45:25.487: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 27 14:45:25.487: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 27 14:45:25.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get rc,svc -l name=update-demo --no-headers'
Mar 27 14:45:25.579: INFO: stderr: "No resources found in kubectl-3716 namespace.\n"
Mar 27 14:45:25.579: INFO: stdout: ""
Mar 27 14:45:25.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 27 14:45:25.654: INFO: stderr: ""
Mar 27 14:45:25.654: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 27 14:45:25.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3716" for this suite. 03/27/23 14:45:25.662
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":133,"skipped":2528,"failed":0}
------------------------------
• [SLOW TEST] [11.647 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:45:14.023
    Mar 27 14:45:14.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubectl 03/27/23 14:45:14.024
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:45:14.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:45:14.043
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 03/27/23 14:45:14.047
    Mar 27 14:45:14.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 create -f -'
    Mar 27 14:45:14.763: INFO: stderr: ""
    Mar 27 14:45:14.763: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/27/23 14:45:14.764
    Mar 27 14:45:14.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 27 14:45:14.849: INFO: stderr: ""
    Mar 27 14:45:14.849: INFO: stdout: "update-demo-nautilus-8t9m9 update-demo-nautilus-mgnkq "
    Mar 27 14:45:14.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods update-demo-nautilus-8t9m9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 14:45:14.913: INFO: stderr: ""
    Mar 27 14:45:14.913: INFO: stdout: ""
    Mar 27 14:45:14.913: INFO: update-demo-nautilus-8t9m9 is created but not running
    Mar 27 14:45:19.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 27 14:45:19.983: INFO: stderr: ""
    Mar 27 14:45:19.983: INFO: stdout: "update-demo-nautilus-8t9m9 update-demo-nautilus-mgnkq "
    Mar 27 14:45:19.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods update-demo-nautilus-8t9m9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 14:45:20.049: INFO: stderr: ""
    Mar 27 14:45:20.049: INFO: stdout: ""
    Mar 27 14:45:20.049: INFO: update-demo-nautilus-8t9m9 is created but not running
    Mar 27 14:45:25.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 27 14:45:25.118: INFO: stderr: ""
    Mar 27 14:45:25.118: INFO: stdout: "update-demo-nautilus-8t9m9 update-demo-nautilus-mgnkq "
    Mar 27 14:45:25.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods update-demo-nautilus-8t9m9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 14:45:25.186: INFO: stderr: ""
    Mar 27 14:45:25.186: INFO: stdout: "true"
    Mar 27 14:45:25.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods update-demo-nautilus-8t9m9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 27 14:45:25.258: INFO: stderr: ""
    Mar 27 14:45:25.258: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 27 14:45:25.258: INFO: validating pod update-demo-nautilus-8t9m9
    Mar 27 14:45:25.270: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 27 14:45:25.270: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 27 14:45:25.270: INFO: update-demo-nautilus-8t9m9 is verified up and running
    Mar 27 14:45:25.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods update-demo-nautilus-mgnkq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 14:45:25.335: INFO: stderr: ""
    Mar 27 14:45:25.335: INFO: stdout: "true"
    Mar 27 14:45:25.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods update-demo-nautilus-mgnkq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 27 14:45:25.403: INFO: stderr: ""
    Mar 27 14:45:25.403: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 27 14:45:25.403: INFO: validating pod update-demo-nautilus-mgnkq
    Mar 27 14:45:25.414: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 27 14:45:25.414: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 27 14:45:25.414: INFO: update-demo-nautilus-mgnkq is verified up and running
    STEP: using delete to clean up resources 03/27/23 14:45:25.414
    Mar 27 14:45:25.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 delete --grace-period=0 --force -f -'
    Mar 27 14:45:25.487: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 27 14:45:25.487: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar 27 14:45:25.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get rc,svc -l name=update-demo --no-headers'
    Mar 27 14:45:25.579: INFO: stderr: "No resources found in kubectl-3716 namespace.\n"
    Mar 27 14:45:25.579: INFO: stdout: ""
    Mar 27 14:45:25.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-3716 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 27 14:45:25.654: INFO: stderr: ""
    Mar 27 14:45:25.654: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 27 14:45:25.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3716" for this suite. 03/27/23 14:45:25.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:45:25.671
Mar 27 14:45:25.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename downward-api 03/27/23 14:45:25.675
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:45:25.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:45:25.699
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 03/27/23 14:45:25.703
Mar 27 14:45:25.715: INFO: Waiting up to 5m0s for pod "downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2" in namespace "downward-api-1988" to be "Succeeded or Failed"
Mar 27 14:45:25.718: INFO: Pod "downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.082086ms
Mar 27 14:45:27.723: INFO: Pod "downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008033254s
Mar 27 14:45:29.723: INFO: Pod "downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007427707s
Mar 27 14:45:31.725: INFO: Pod "downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009592325s
Mar 27 14:45:33.724: INFO: Pod "downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009015802s
STEP: Saw pod success 03/27/23 14:45:33.724
Mar 27 14:45:33.724: INFO: Pod "downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2" satisfied condition "Succeeded or Failed"
Mar 27 14:45:33.729: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2 container dapi-container: <nil>
STEP: delete the pod 03/27/23 14:45:33.741
Mar 27 14:45:33.763: INFO: Waiting for pod downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2 to disappear
Mar 27 14:45:33.766: INFO: Pod downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar 27 14:45:33.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1988" for this suite. 03/27/23 14:45:33.772
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":134,"skipped":2569,"failed":0}
------------------------------
• [SLOW TEST] [8.108 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:45:25.671
    Mar 27 14:45:25.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename downward-api 03/27/23 14:45:25.675
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:45:25.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:45:25.699
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 03/27/23 14:45:25.703
    Mar 27 14:45:25.715: INFO: Waiting up to 5m0s for pod "downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2" in namespace "downward-api-1988" to be "Succeeded or Failed"
    Mar 27 14:45:25.718: INFO: Pod "downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.082086ms
    Mar 27 14:45:27.723: INFO: Pod "downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008033254s
    Mar 27 14:45:29.723: INFO: Pod "downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007427707s
    Mar 27 14:45:31.725: INFO: Pod "downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009592325s
    Mar 27 14:45:33.724: INFO: Pod "downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009015802s
    STEP: Saw pod success 03/27/23 14:45:33.724
    Mar 27 14:45:33.724: INFO: Pod "downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2" satisfied condition "Succeeded or Failed"
    Mar 27 14:45:33.729: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2 container dapi-container: <nil>
    STEP: delete the pod 03/27/23 14:45:33.741
    Mar 27 14:45:33.763: INFO: Waiting for pod downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2 to disappear
    Mar 27 14:45:33.766: INFO: Pod downward-api-ddf4b77e-b7e6-40ca-bed4-082c5ec85ed2 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar 27 14:45:33.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1988" for this suite. 03/27/23 14:45:33.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:45:33.781
Mar 27 14:45:33.781: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename svcaccounts 03/27/23 14:45:33.781
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:45:33.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:45:33.809
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Mar 27 14:45:33.815: INFO: Got root ca configmap in namespace "svcaccounts-3255"
Mar 27 14:45:33.828: INFO: Deleted root ca configmap in namespace "svcaccounts-3255"
STEP: waiting for a new root ca configmap created 03/27/23 14:45:34.329
Mar 27 14:45:34.333: INFO: Recreated root ca configmap in namespace "svcaccounts-3255"
Mar 27 14:45:34.338: INFO: Updated root ca configmap in namespace "svcaccounts-3255"
STEP: waiting for the root ca configmap reconciled 03/27/23 14:45:34.839
Mar 27 14:45:34.846: INFO: Reconciled root ca configmap in namespace "svcaccounts-3255"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar 27 14:45:34.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3255" for this suite. 03/27/23 14:45:34.852
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":135,"skipped":2582,"failed":0}
------------------------------
• [1.081 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:45:33.781
    Mar 27 14:45:33.781: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename svcaccounts 03/27/23 14:45:33.781
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:45:33.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:45:33.809
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Mar 27 14:45:33.815: INFO: Got root ca configmap in namespace "svcaccounts-3255"
    Mar 27 14:45:33.828: INFO: Deleted root ca configmap in namespace "svcaccounts-3255"
    STEP: waiting for a new root ca configmap created 03/27/23 14:45:34.329
    Mar 27 14:45:34.333: INFO: Recreated root ca configmap in namespace "svcaccounts-3255"
    Mar 27 14:45:34.338: INFO: Updated root ca configmap in namespace "svcaccounts-3255"
    STEP: waiting for the root ca configmap reconciled 03/27/23 14:45:34.839
    Mar 27 14:45:34.846: INFO: Reconciled root ca configmap in namespace "svcaccounts-3255"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar 27 14:45:34.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-3255" for this suite. 03/27/23 14:45:34.852
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:45:34.863
Mar 27 14:45:34.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename subpath 03/27/23 14:45:34.864
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:45:34.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:45:34.883
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/27/23 14:45:34.886
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-qb2q 03/27/23 14:45:34.901
STEP: Creating a pod to test atomic-volume-subpath 03/27/23 14:45:34.901
Mar 27 14:45:34.913: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-qb2q" in namespace "subpath-7919" to be "Succeeded or Failed"
Mar 27 14:45:34.920: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Pending", Reason="", readiness=false. Elapsed: 7.340811ms
Mar 27 14:45:36.926: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 2.013253142s
Mar 27 14:45:38.927: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 4.014349733s
Mar 27 14:45:40.926: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 6.013185082s
Mar 27 14:45:42.926: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 8.013695288s
Mar 27 14:45:44.925: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 10.012638975s
Mar 27 14:45:46.926: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 12.013392388s
Mar 27 14:45:48.926: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 14.013770473s
Mar 27 14:45:50.926: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 16.013435799s
Mar 27 14:45:52.929: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 18.01610741s
Mar 27 14:45:54.924: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 20.011784836s
Mar 27 14:45:56.927: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 22.014136729s
Mar 27 14:45:58.928: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=false. Elapsed: 24.015349275s
Mar 27 14:46:00.925: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.012846389s
STEP: Saw pod success 03/27/23 14:46:00.925
Mar 27 14:46:00.926: INFO: Pod "pod-subpath-test-downwardapi-qb2q" satisfied condition "Succeeded or Failed"
Mar 27 14:46:00.930: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-subpath-test-downwardapi-qb2q container test-container-subpath-downwardapi-qb2q: <nil>
STEP: delete the pod 03/27/23 14:46:00.945
Mar 27 14:46:01.031: INFO: Waiting for pod pod-subpath-test-downwardapi-qb2q to disappear
Mar 27 14:46:01.035: INFO: Pod pod-subpath-test-downwardapi-qb2q no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-qb2q 03/27/23 14:46:01.035
Mar 27 14:46:01.035: INFO: Deleting pod "pod-subpath-test-downwardapi-qb2q" in namespace "subpath-7919"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar 27 14:46:01.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7919" for this suite. 03/27/23 14:46:01.277
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":136,"skipped":2584,"failed":0}
------------------------------
• [SLOW TEST] [26.556 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:45:34.863
    Mar 27 14:45:34.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename subpath 03/27/23 14:45:34.864
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:45:34.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:45:34.883
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/27/23 14:45:34.886
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-qb2q 03/27/23 14:45:34.901
    STEP: Creating a pod to test atomic-volume-subpath 03/27/23 14:45:34.901
    Mar 27 14:45:34.913: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-qb2q" in namespace "subpath-7919" to be "Succeeded or Failed"
    Mar 27 14:45:34.920: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Pending", Reason="", readiness=false. Elapsed: 7.340811ms
    Mar 27 14:45:36.926: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 2.013253142s
    Mar 27 14:45:38.927: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 4.014349733s
    Mar 27 14:45:40.926: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 6.013185082s
    Mar 27 14:45:42.926: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 8.013695288s
    Mar 27 14:45:44.925: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 10.012638975s
    Mar 27 14:45:46.926: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 12.013392388s
    Mar 27 14:45:48.926: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 14.013770473s
    Mar 27 14:45:50.926: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 16.013435799s
    Mar 27 14:45:52.929: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 18.01610741s
    Mar 27 14:45:54.924: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 20.011784836s
    Mar 27 14:45:56.927: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=true. Elapsed: 22.014136729s
    Mar 27 14:45:58.928: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Running", Reason="", readiness=false. Elapsed: 24.015349275s
    Mar 27 14:46:00.925: INFO: Pod "pod-subpath-test-downwardapi-qb2q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.012846389s
    STEP: Saw pod success 03/27/23 14:46:00.925
    Mar 27 14:46:00.926: INFO: Pod "pod-subpath-test-downwardapi-qb2q" satisfied condition "Succeeded or Failed"
    Mar 27 14:46:00.930: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-subpath-test-downwardapi-qb2q container test-container-subpath-downwardapi-qb2q: <nil>
    STEP: delete the pod 03/27/23 14:46:00.945
    Mar 27 14:46:01.031: INFO: Waiting for pod pod-subpath-test-downwardapi-qb2q to disappear
    Mar 27 14:46:01.035: INFO: Pod pod-subpath-test-downwardapi-qb2q no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-qb2q 03/27/23 14:46:01.035
    Mar 27 14:46:01.035: INFO: Deleting pod "pod-subpath-test-downwardapi-qb2q" in namespace "subpath-7919"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar 27 14:46:01.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7919" for this suite. 03/27/23 14:46:01.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:46:01.42
Mar 27 14:46:01.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename var-expansion 03/27/23 14:46:01.421
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:46:01.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:46:01.889
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 03/27/23 14:46:01.894
Mar 27 14:46:02.047: INFO: Waiting up to 5m0s for pod "var-expansion-85e11a7e-5f97-4237-b4bf-ea17c66481c1" in namespace "var-expansion-7926" to be "Succeeded or Failed"
Mar 27 14:46:02.051: INFO: Pod "var-expansion-85e11a7e-5f97-4237-b4bf-ea17c66481c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.689701ms
Mar 27 14:46:04.057: INFO: Pod "var-expansion-85e11a7e-5f97-4237-b4bf-ea17c66481c1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009649097s
Mar 27 14:46:06.055: INFO: Pod "var-expansion-85e11a7e-5f97-4237-b4bf-ea17c66481c1": Phase="Running", Reason="", readiness=false. Elapsed: 4.00725008s
Mar 27 14:46:08.057: INFO: Pod "var-expansion-85e11a7e-5f97-4237-b4bf-ea17c66481c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00961242s
STEP: Saw pod success 03/27/23 14:46:08.057
Mar 27 14:46:08.057: INFO: Pod "var-expansion-85e11a7e-5f97-4237-b4bf-ea17c66481c1" satisfied condition "Succeeded or Failed"
Mar 27 14:46:08.061: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod var-expansion-85e11a7e-5f97-4237-b4bf-ea17c66481c1 container dapi-container: <nil>
STEP: delete the pod 03/27/23 14:46:08.072
Mar 27 14:46:08.086: INFO: Waiting for pod var-expansion-85e11a7e-5f97-4237-b4bf-ea17c66481c1 to disappear
Mar 27 14:46:08.089: INFO: Pod var-expansion-85e11a7e-5f97-4237-b4bf-ea17c66481c1 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 27 14:46:08.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7926" for this suite. 03/27/23 14:46:08.096
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":137,"skipped":2610,"failed":0}
------------------------------
• [SLOW TEST] [6.688 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:46:01.42
    Mar 27 14:46:01.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename var-expansion 03/27/23 14:46:01.421
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:46:01.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:46:01.889
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 03/27/23 14:46:01.894
    Mar 27 14:46:02.047: INFO: Waiting up to 5m0s for pod "var-expansion-85e11a7e-5f97-4237-b4bf-ea17c66481c1" in namespace "var-expansion-7926" to be "Succeeded or Failed"
    Mar 27 14:46:02.051: INFO: Pod "var-expansion-85e11a7e-5f97-4237-b4bf-ea17c66481c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.689701ms
    Mar 27 14:46:04.057: INFO: Pod "var-expansion-85e11a7e-5f97-4237-b4bf-ea17c66481c1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009649097s
    Mar 27 14:46:06.055: INFO: Pod "var-expansion-85e11a7e-5f97-4237-b4bf-ea17c66481c1": Phase="Running", Reason="", readiness=false. Elapsed: 4.00725008s
    Mar 27 14:46:08.057: INFO: Pod "var-expansion-85e11a7e-5f97-4237-b4bf-ea17c66481c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00961242s
    STEP: Saw pod success 03/27/23 14:46:08.057
    Mar 27 14:46:08.057: INFO: Pod "var-expansion-85e11a7e-5f97-4237-b4bf-ea17c66481c1" satisfied condition "Succeeded or Failed"
    Mar 27 14:46:08.061: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod var-expansion-85e11a7e-5f97-4237-b4bf-ea17c66481c1 container dapi-container: <nil>
    STEP: delete the pod 03/27/23 14:46:08.072
    Mar 27 14:46:08.086: INFO: Waiting for pod var-expansion-85e11a7e-5f97-4237-b4bf-ea17c66481c1 to disappear
    Mar 27 14:46:08.089: INFO: Pod var-expansion-85e11a7e-5f97-4237-b4bf-ea17c66481c1 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 27 14:46:08.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7926" for this suite. 03/27/23 14:46:08.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:46:08.109
Mar 27 14:46:08.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubelet-test 03/27/23 14:46:08.11
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:46:08.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:46:08.131
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar 27 14:46:08.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7359" for this suite. 03/27/23 14:46:08.184
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":138,"skipped":2618,"failed":0}
------------------------------
• [0.091 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:46:08.109
    Mar 27 14:46:08.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubelet-test 03/27/23 14:46:08.11
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:46:08.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:46:08.131
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar 27 14:46:08.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7359" for this suite. 03/27/23 14:46:08.184
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:46:08.201
Mar 27 14:46:08.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename crd-watch 03/27/23 14:46:08.202
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:46:08.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:46:08.224
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Mar 27 14:46:08.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Creating first CR  03/27/23 14:46:10.81
Mar 27 14:46:10.820: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T14:46:10Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T14:46:10Z]] name:name1 resourceVersion:28207 uid:6b34364b-f189-4ab9-8e5d-9312ae51293b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 03/27/23 14:46:20.82
Mar 27 14:46:20.828: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T14:46:20Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T14:46:20Z]] name:name2 resourceVersion:28275 uid:cd6912d5-eb11-47c8-9d98-7c78543b7d4a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 03/27/23 14:46:30.829
Mar 27 14:46:30.841: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T14:46:10Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T14:46:30Z]] name:name1 resourceVersion:28324 uid:6b34364b-f189-4ab9-8e5d-9312ae51293b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 03/27/23 14:46:40.842
Mar 27 14:46:40.853: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T14:46:20Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T14:46:40Z]] name:name2 resourceVersion:28373 uid:cd6912d5-eb11-47c8-9d98-7c78543b7d4a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 03/27/23 14:46:50.854
Mar 27 14:46:50.898: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T14:46:10Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T14:46:30Z]] name:name1 resourceVersion:28422 uid:6b34364b-f189-4ab9-8e5d-9312ae51293b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 03/27/23 14:47:00.899
Mar 27 14:47:00.908: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T14:46:20Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T14:46:40Z]] name:name2 resourceVersion:28471 uid:cd6912d5-eb11-47c8-9d98-7c78543b7d4a] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:47:11.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-3908" for this suite. 03/27/23 14:47:11.429
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":139,"skipped":2619,"failed":0}
------------------------------
• [SLOW TEST] [63.238 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:46:08.201
    Mar 27 14:46:08.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename crd-watch 03/27/23 14:46:08.202
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:46:08.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:46:08.224
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Mar 27 14:46:08.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Creating first CR  03/27/23 14:46:10.81
    Mar 27 14:46:10.820: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T14:46:10Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T14:46:10Z]] name:name1 resourceVersion:28207 uid:6b34364b-f189-4ab9-8e5d-9312ae51293b] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 03/27/23 14:46:20.82
    Mar 27 14:46:20.828: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T14:46:20Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T14:46:20Z]] name:name2 resourceVersion:28275 uid:cd6912d5-eb11-47c8-9d98-7c78543b7d4a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 03/27/23 14:46:30.829
    Mar 27 14:46:30.841: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T14:46:10Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T14:46:30Z]] name:name1 resourceVersion:28324 uid:6b34364b-f189-4ab9-8e5d-9312ae51293b] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 03/27/23 14:46:40.842
    Mar 27 14:46:40.853: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T14:46:20Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T14:46:40Z]] name:name2 resourceVersion:28373 uid:cd6912d5-eb11-47c8-9d98-7c78543b7d4a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 03/27/23 14:46:50.854
    Mar 27 14:46:50.898: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T14:46:10Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T14:46:30Z]] name:name1 resourceVersion:28422 uid:6b34364b-f189-4ab9-8e5d-9312ae51293b] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 03/27/23 14:47:00.899
    Mar 27 14:47:00.908: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T14:46:20Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T14:46:40Z]] name:name2 resourceVersion:28471 uid:cd6912d5-eb11-47c8-9d98-7c78543b7d4a] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:47:11.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-3908" for this suite. 03/27/23 14:47:11.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:47:11.44
Mar 27 14:47:11.440: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 14:47:11.441
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:47:11.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:47:11.465
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Mar 27 14:47:11.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/27/23 14:47:15.418
Mar 27 14:47:15.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 --namespace=crd-publish-openapi-5370 create -f -'
Mar 27 14:47:16.032: INFO: stderr: ""
Mar 27 14:47:16.032: INFO: stdout: "e2e-test-crd-publish-openapi-191-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 27 14:47:16.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 --namespace=crd-publish-openapi-5370 delete e2e-test-crd-publish-openapi-191-crds test-foo'
Mar 27 14:47:16.096: INFO: stderr: ""
Mar 27 14:47:16.096: INFO: stdout: "e2e-test-crd-publish-openapi-191-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 27 14:47:16.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 --namespace=crd-publish-openapi-5370 apply -f -'
Mar 27 14:47:16.291: INFO: stderr: ""
Mar 27 14:47:16.292: INFO: stdout: "e2e-test-crd-publish-openapi-191-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 27 14:47:16.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 --namespace=crd-publish-openapi-5370 delete e2e-test-crd-publish-openapi-191-crds test-foo'
Mar 27 14:47:16.357: INFO: stderr: ""
Mar 27 14:47:16.357: INFO: stdout: "e2e-test-crd-publish-openapi-191-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/27/23 14:47:16.357
Mar 27 14:47:16.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 --namespace=crd-publish-openapi-5370 create -f -'
Mar 27 14:47:16.545: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/27/23 14:47:16.545
Mar 27 14:47:16.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 --namespace=crd-publish-openapi-5370 create -f -'
Mar 27 14:47:17.277: INFO: rc: 1
Mar 27 14:47:17.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 --namespace=crd-publish-openapi-5370 apply -f -'
Mar 27 14:47:17.468: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/27/23 14:47:17.468
Mar 27 14:47:17.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 --namespace=crd-publish-openapi-5370 create -f -'
Mar 27 14:47:17.645: INFO: rc: 1
Mar 27 14:47:17.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 --namespace=crd-publish-openapi-5370 apply -f -'
Mar 27 14:47:17.838: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 03/27/23 14:47:17.838
Mar 27 14:47:17.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 explain e2e-test-crd-publish-openapi-191-crds'
Mar 27 14:47:18.019: INFO: stderr: ""
Mar 27 14:47:18.019: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-191-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 03/27/23 14:47:18.019
Mar 27 14:47:18.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 explain e2e-test-crd-publish-openapi-191-crds.metadata'
Mar 27 14:47:18.233: INFO: stderr: ""
Mar 27 14:47:18.233: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-191-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 27 14:47:18.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 explain e2e-test-crd-publish-openapi-191-crds.spec'
Mar 27 14:47:18.411: INFO: stderr: ""
Mar 27 14:47:18.411: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-191-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 27 14:47:18.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 explain e2e-test-crd-publish-openapi-191-crds.spec.bars'
Mar 27 14:47:18.593: INFO: stderr: ""
Mar 27 14:47:18.593: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-191-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/27/23 14:47:18.593
Mar 27 14:47:18.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 explain e2e-test-crd-publish-openapi-191-crds.spec.bars2'
Mar 27 14:47:18.762: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:47:22.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5370" for this suite. 03/27/23 14:47:22.297
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":140,"skipped":2647,"failed":0}
------------------------------
• [SLOW TEST] [10.864 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:47:11.44
    Mar 27 14:47:11.440: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 14:47:11.441
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:47:11.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:47:11.465
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Mar 27 14:47:11.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/27/23 14:47:15.418
    Mar 27 14:47:15.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 --namespace=crd-publish-openapi-5370 create -f -'
    Mar 27 14:47:16.032: INFO: stderr: ""
    Mar 27 14:47:16.032: INFO: stdout: "e2e-test-crd-publish-openapi-191-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar 27 14:47:16.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 --namespace=crd-publish-openapi-5370 delete e2e-test-crd-publish-openapi-191-crds test-foo'
    Mar 27 14:47:16.096: INFO: stderr: ""
    Mar 27 14:47:16.096: INFO: stdout: "e2e-test-crd-publish-openapi-191-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Mar 27 14:47:16.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 --namespace=crd-publish-openapi-5370 apply -f -'
    Mar 27 14:47:16.291: INFO: stderr: ""
    Mar 27 14:47:16.292: INFO: stdout: "e2e-test-crd-publish-openapi-191-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar 27 14:47:16.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 --namespace=crd-publish-openapi-5370 delete e2e-test-crd-publish-openapi-191-crds test-foo'
    Mar 27 14:47:16.357: INFO: stderr: ""
    Mar 27 14:47:16.357: INFO: stdout: "e2e-test-crd-publish-openapi-191-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/27/23 14:47:16.357
    Mar 27 14:47:16.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 --namespace=crd-publish-openapi-5370 create -f -'
    Mar 27 14:47:16.545: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/27/23 14:47:16.545
    Mar 27 14:47:16.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 --namespace=crd-publish-openapi-5370 create -f -'
    Mar 27 14:47:17.277: INFO: rc: 1
    Mar 27 14:47:17.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 --namespace=crd-publish-openapi-5370 apply -f -'
    Mar 27 14:47:17.468: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/27/23 14:47:17.468
    Mar 27 14:47:17.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 --namespace=crd-publish-openapi-5370 create -f -'
    Mar 27 14:47:17.645: INFO: rc: 1
    Mar 27 14:47:17.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 --namespace=crd-publish-openapi-5370 apply -f -'
    Mar 27 14:47:17.838: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 03/27/23 14:47:17.838
    Mar 27 14:47:17.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 explain e2e-test-crd-publish-openapi-191-crds'
    Mar 27 14:47:18.019: INFO: stderr: ""
    Mar 27 14:47:18.019: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-191-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 03/27/23 14:47:18.019
    Mar 27 14:47:18.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 explain e2e-test-crd-publish-openapi-191-crds.metadata'
    Mar 27 14:47:18.233: INFO: stderr: ""
    Mar 27 14:47:18.233: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-191-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Mar 27 14:47:18.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 explain e2e-test-crd-publish-openapi-191-crds.spec'
    Mar 27 14:47:18.411: INFO: stderr: ""
    Mar 27 14:47:18.411: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-191-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Mar 27 14:47:18.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 explain e2e-test-crd-publish-openapi-191-crds.spec.bars'
    Mar 27 14:47:18.593: INFO: stderr: ""
    Mar 27 14:47:18.593: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-191-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/27/23 14:47:18.593
    Mar 27 14:47:18.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-5370 explain e2e-test-crd-publish-openapi-191-crds.spec.bars2'
    Mar 27 14:47:18.762: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:47:22.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5370" for this suite. 03/27/23 14:47:22.297
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:47:22.304
Mar 27 14:47:22.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename downward-api 03/27/23 14:47:22.306
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:47:22.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:47:22.327
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 03/27/23 14:47:22.329
Mar 27 14:47:22.343: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dfd453c6-e55f-4aa6-9dca-790cf29e0675" in namespace "downward-api-8528" to be "Succeeded or Failed"
Mar 27 14:47:22.349: INFO: Pod "downwardapi-volume-dfd453c6-e55f-4aa6-9dca-790cf29e0675": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050519ms
Mar 27 14:47:24.355: INFO: Pod "downwardapi-volume-dfd453c6-e55f-4aa6-9dca-790cf29e0675": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012218404s
Mar 27 14:47:26.355: INFO: Pod "downwardapi-volume-dfd453c6-e55f-4aa6-9dca-790cf29e0675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011658971s
STEP: Saw pod success 03/27/23 14:47:26.355
Mar 27 14:47:26.355: INFO: Pod "downwardapi-volume-dfd453c6-e55f-4aa6-9dca-790cf29e0675" satisfied condition "Succeeded or Failed"
Mar 27 14:47:26.358: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-dfd453c6-e55f-4aa6-9dca-790cf29e0675 container client-container: <nil>
STEP: delete the pod 03/27/23 14:47:26.365
Mar 27 14:47:26.382: INFO: Waiting for pod downwardapi-volume-dfd453c6-e55f-4aa6-9dca-790cf29e0675 to disappear
Mar 27 14:47:26.385: INFO: Pod downwardapi-volume-dfd453c6-e55f-4aa6-9dca-790cf29e0675 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 27 14:47:26.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8528" for this suite. 03/27/23 14:47:26.391
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":141,"skipped":2648,"failed":0}
------------------------------
• [4.091 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:47:22.304
    Mar 27 14:47:22.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename downward-api 03/27/23 14:47:22.306
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:47:22.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:47:22.327
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 03/27/23 14:47:22.329
    Mar 27 14:47:22.343: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dfd453c6-e55f-4aa6-9dca-790cf29e0675" in namespace "downward-api-8528" to be "Succeeded or Failed"
    Mar 27 14:47:22.349: INFO: Pod "downwardapi-volume-dfd453c6-e55f-4aa6-9dca-790cf29e0675": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050519ms
    Mar 27 14:47:24.355: INFO: Pod "downwardapi-volume-dfd453c6-e55f-4aa6-9dca-790cf29e0675": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012218404s
    Mar 27 14:47:26.355: INFO: Pod "downwardapi-volume-dfd453c6-e55f-4aa6-9dca-790cf29e0675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011658971s
    STEP: Saw pod success 03/27/23 14:47:26.355
    Mar 27 14:47:26.355: INFO: Pod "downwardapi-volume-dfd453c6-e55f-4aa6-9dca-790cf29e0675" satisfied condition "Succeeded or Failed"
    Mar 27 14:47:26.358: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-dfd453c6-e55f-4aa6-9dca-790cf29e0675 container client-container: <nil>
    STEP: delete the pod 03/27/23 14:47:26.365
    Mar 27 14:47:26.382: INFO: Waiting for pod downwardapi-volume-dfd453c6-e55f-4aa6-9dca-790cf29e0675 to disappear
    Mar 27 14:47:26.385: INFO: Pod downwardapi-volume-dfd453c6-e55f-4aa6-9dca-790cf29e0675 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 27 14:47:26.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8528" for this suite. 03/27/23 14:47:26.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:47:26.397
Mar 27 14:47:26.397: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename statefulset 03/27/23 14:47:26.398
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:47:26.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:47:26.438
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3195 03/27/23 14:47:26.442
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 03/27/23 14:47:26.449
Mar 27 14:47:26.462: INFO: Found 0 stateful pods, waiting for 3
Mar 27 14:47:36.470: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 14:47:36.470: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 14:47:36.470: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 14:47:36.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3195 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 14:47:36.700: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 14:47:36.700: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 14:47:36.700: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/27/23 14:47:46.714
Mar 27 14:47:46.734: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/27/23 14:47:46.734
STEP: Updating Pods in reverse ordinal order 03/27/23 14:47:56.754
Mar 27 14:47:56.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3195 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 27 14:47:56.908: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 27 14:47:56.908: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 27 14:47:56.908: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 27 14:48:06.930: INFO: Waiting for StatefulSet statefulset-3195/ss2 to complete update
Mar 27 14:48:06.930: INFO: Waiting for Pod statefulset-3195/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Mar 27 14:48:06.930: INFO: Waiting for Pod statefulset-3195/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Mar 27 14:48:16.940: INFO: Waiting for StatefulSet statefulset-3195/ss2 to complete update
Mar 27 14:48:16.940: INFO: Waiting for Pod statefulset-3195/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Mar 27 14:48:26.939: INFO: Waiting for StatefulSet statefulset-3195/ss2 to complete update
STEP: Rolling back to a previous revision 03/27/23 14:48:36.943
Mar 27 14:48:36.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3195 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 14:48:37.096: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 14:48:37.096: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 14:48:37.096: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 27 14:48:47.139: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 03/27/23 14:48:57.156
Mar 27 14:48:57.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3195 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 27 14:48:57.321: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 27 14:48:57.321: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 27 14:48:57.321: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 27 14:49:07.347: INFO: Deleting all statefulset in ns statefulset-3195
Mar 27 14:49:07.350: INFO: Scaling statefulset ss2 to 0
Mar 27 14:49:17.372: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 14:49:17.375: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 27 14:49:17.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3195" for this suite. 03/27/23 14:49:17.391
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":142,"skipped":2659,"failed":0}
------------------------------
• [SLOW TEST] [111.001 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:47:26.397
    Mar 27 14:47:26.397: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename statefulset 03/27/23 14:47:26.398
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:47:26.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:47:26.438
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3195 03/27/23 14:47:26.442
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 03/27/23 14:47:26.449
    Mar 27 14:47:26.462: INFO: Found 0 stateful pods, waiting for 3
    Mar 27 14:47:36.470: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 14:47:36.470: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 14:47:36.470: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 14:47:36.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3195 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 14:47:36.700: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 14:47:36.700: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 14:47:36.700: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/27/23 14:47:46.714
    Mar 27 14:47:46.734: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/27/23 14:47:46.734
    STEP: Updating Pods in reverse ordinal order 03/27/23 14:47:56.754
    Mar 27 14:47:56.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3195 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 27 14:47:56.908: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 27 14:47:56.908: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 27 14:47:56.908: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 27 14:48:06.930: INFO: Waiting for StatefulSet statefulset-3195/ss2 to complete update
    Mar 27 14:48:06.930: INFO: Waiting for Pod statefulset-3195/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Mar 27 14:48:06.930: INFO: Waiting for Pod statefulset-3195/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Mar 27 14:48:16.940: INFO: Waiting for StatefulSet statefulset-3195/ss2 to complete update
    Mar 27 14:48:16.940: INFO: Waiting for Pod statefulset-3195/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Mar 27 14:48:26.939: INFO: Waiting for StatefulSet statefulset-3195/ss2 to complete update
    STEP: Rolling back to a previous revision 03/27/23 14:48:36.943
    Mar 27 14:48:36.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3195 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 14:48:37.096: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 14:48:37.096: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 14:48:37.096: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 27 14:48:47.139: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 03/27/23 14:48:57.156
    Mar 27 14:48:57.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3195 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 27 14:48:57.321: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 27 14:48:57.321: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 27 14:48:57.321: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 27 14:49:07.347: INFO: Deleting all statefulset in ns statefulset-3195
    Mar 27 14:49:07.350: INFO: Scaling statefulset ss2 to 0
    Mar 27 14:49:17.372: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 14:49:17.375: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 27 14:49:17.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3195" for this suite. 03/27/23 14:49:17.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:49:17.399
Mar 27 14:49:17.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename downward-api 03/27/23 14:49:17.4
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:49:17.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:49:17.421
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 03/27/23 14:49:17.425
Mar 27 14:49:17.433: INFO: Waiting up to 5m0s for pod "downward-api-0dfd029b-d6bf-41ac-b2d8-93da4c330b06" in namespace "downward-api-4809" to be "Succeeded or Failed"
Mar 27 14:49:17.436: INFO: Pod "downward-api-0dfd029b-d6bf-41ac-b2d8-93da4c330b06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.799528ms
Mar 27 14:49:19.441: INFO: Pod "downward-api-0dfd029b-d6bf-41ac-b2d8-93da4c330b06": Phase="Running", Reason="", readiness=true. Elapsed: 2.007789528s
Mar 27 14:49:21.442: INFO: Pod "downward-api-0dfd029b-d6bf-41ac-b2d8-93da4c330b06": Phase="Running", Reason="", readiness=false. Elapsed: 4.008391198s
Mar 27 14:49:23.440: INFO: Pod "downward-api-0dfd029b-d6bf-41ac-b2d8-93da4c330b06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00667938s
STEP: Saw pod success 03/27/23 14:49:23.44
Mar 27 14:49:23.440: INFO: Pod "downward-api-0dfd029b-d6bf-41ac-b2d8-93da4c330b06" satisfied condition "Succeeded or Failed"
Mar 27 14:49:23.444: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downward-api-0dfd029b-d6bf-41ac-b2d8-93da4c330b06 container dapi-container: <nil>
STEP: delete the pod 03/27/23 14:49:23.457
Mar 27 14:49:23.473: INFO: Waiting for pod downward-api-0dfd029b-d6bf-41ac-b2d8-93da4c330b06 to disappear
Mar 27 14:49:23.476: INFO: Pod downward-api-0dfd029b-d6bf-41ac-b2d8-93da4c330b06 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar 27 14:49:23.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4809" for this suite. 03/27/23 14:49:23.481
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":143,"skipped":2664,"failed":0}
------------------------------
• [SLOW TEST] [6.089 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:49:17.399
    Mar 27 14:49:17.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename downward-api 03/27/23 14:49:17.4
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:49:17.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:49:17.421
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 03/27/23 14:49:17.425
    Mar 27 14:49:17.433: INFO: Waiting up to 5m0s for pod "downward-api-0dfd029b-d6bf-41ac-b2d8-93da4c330b06" in namespace "downward-api-4809" to be "Succeeded or Failed"
    Mar 27 14:49:17.436: INFO: Pod "downward-api-0dfd029b-d6bf-41ac-b2d8-93da4c330b06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.799528ms
    Mar 27 14:49:19.441: INFO: Pod "downward-api-0dfd029b-d6bf-41ac-b2d8-93da4c330b06": Phase="Running", Reason="", readiness=true. Elapsed: 2.007789528s
    Mar 27 14:49:21.442: INFO: Pod "downward-api-0dfd029b-d6bf-41ac-b2d8-93da4c330b06": Phase="Running", Reason="", readiness=false. Elapsed: 4.008391198s
    Mar 27 14:49:23.440: INFO: Pod "downward-api-0dfd029b-d6bf-41ac-b2d8-93da4c330b06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00667938s
    STEP: Saw pod success 03/27/23 14:49:23.44
    Mar 27 14:49:23.440: INFO: Pod "downward-api-0dfd029b-d6bf-41ac-b2d8-93da4c330b06" satisfied condition "Succeeded or Failed"
    Mar 27 14:49:23.444: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downward-api-0dfd029b-d6bf-41ac-b2d8-93da4c330b06 container dapi-container: <nil>
    STEP: delete the pod 03/27/23 14:49:23.457
    Mar 27 14:49:23.473: INFO: Waiting for pod downward-api-0dfd029b-d6bf-41ac-b2d8-93da4c330b06 to disappear
    Mar 27 14:49:23.476: INFO: Pod downward-api-0dfd029b-d6bf-41ac-b2d8-93da4c330b06 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar 27 14:49:23.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4809" for this suite. 03/27/23 14:49:23.481
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:49:23.49
Mar 27 14:49:23.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubectl 03/27/23 14:49:23.491
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:49:23.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:49:23.516
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 03/27/23 14:49:23.519
Mar 27 14:49:23.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-6961 api-versions'
Mar 27 14:49:23.580: INFO: stderr: ""
Mar 27 14:49:23.580: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 27 14:49:23.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6961" for this suite. 03/27/23 14:49:23.585
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":144,"skipped":2683,"failed":0}
------------------------------
• [0.101 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:49:23.49
    Mar 27 14:49:23.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubectl 03/27/23 14:49:23.491
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:49:23.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:49:23.516
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 03/27/23 14:49:23.519
    Mar 27 14:49:23.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-6961 api-versions'
    Mar 27 14:49:23.580: INFO: stderr: ""
    Mar 27 14:49:23.580: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 27 14:49:23.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6961" for this suite. 03/27/23 14:49:23.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:49:23.593
Mar 27 14:49:23.593: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename var-expansion 03/27/23 14:49:23.594
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:49:23.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:49:23.632
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 03/27/23 14:49:23.635
Mar 27 14:49:23.643: INFO: Waiting up to 5m0s for pod "var-expansion-1d80e3e5-6dd8-41bd-83a2-d5892dfea399" in namespace "var-expansion-5477" to be "Succeeded or Failed"
Mar 27 14:49:23.646: INFO: Pod "var-expansion-1d80e3e5-6dd8-41bd-83a2-d5892dfea399": Phase="Pending", Reason="", readiness=false. Elapsed: 2.296665ms
Mar 27 14:49:25.650: INFO: Pod "var-expansion-1d80e3e5-6dd8-41bd-83a2-d5892dfea399": Phase="Running", Reason="", readiness=true. Elapsed: 2.006644621s
Mar 27 14:49:27.653: INFO: Pod "var-expansion-1d80e3e5-6dd8-41bd-83a2-d5892dfea399": Phase="Running", Reason="", readiness=false. Elapsed: 4.009976061s
Mar 27 14:49:29.650: INFO: Pod "var-expansion-1d80e3e5-6dd8-41bd-83a2-d5892dfea399": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006610422s
STEP: Saw pod success 03/27/23 14:49:29.65
Mar 27 14:49:29.650: INFO: Pod "var-expansion-1d80e3e5-6dd8-41bd-83a2-d5892dfea399" satisfied condition "Succeeded or Failed"
Mar 27 14:49:29.654: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod var-expansion-1d80e3e5-6dd8-41bd-83a2-d5892dfea399 container dapi-container: <nil>
STEP: delete the pod 03/27/23 14:49:29.662
Mar 27 14:49:29.676: INFO: Waiting for pod var-expansion-1d80e3e5-6dd8-41bd-83a2-d5892dfea399 to disappear
Mar 27 14:49:29.678: INFO: Pod var-expansion-1d80e3e5-6dd8-41bd-83a2-d5892dfea399 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 27 14:49:29.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5477" for this suite. 03/27/23 14:49:29.682
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":145,"skipped":2718,"failed":0}
------------------------------
• [SLOW TEST] [6.094 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:49:23.593
    Mar 27 14:49:23.593: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename var-expansion 03/27/23 14:49:23.594
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:49:23.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:49:23.632
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 03/27/23 14:49:23.635
    Mar 27 14:49:23.643: INFO: Waiting up to 5m0s for pod "var-expansion-1d80e3e5-6dd8-41bd-83a2-d5892dfea399" in namespace "var-expansion-5477" to be "Succeeded or Failed"
    Mar 27 14:49:23.646: INFO: Pod "var-expansion-1d80e3e5-6dd8-41bd-83a2-d5892dfea399": Phase="Pending", Reason="", readiness=false. Elapsed: 2.296665ms
    Mar 27 14:49:25.650: INFO: Pod "var-expansion-1d80e3e5-6dd8-41bd-83a2-d5892dfea399": Phase="Running", Reason="", readiness=true. Elapsed: 2.006644621s
    Mar 27 14:49:27.653: INFO: Pod "var-expansion-1d80e3e5-6dd8-41bd-83a2-d5892dfea399": Phase="Running", Reason="", readiness=false. Elapsed: 4.009976061s
    Mar 27 14:49:29.650: INFO: Pod "var-expansion-1d80e3e5-6dd8-41bd-83a2-d5892dfea399": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006610422s
    STEP: Saw pod success 03/27/23 14:49:29.65
    Mar 27 14:49:29.650: INFO: Pod "var-expansion-1d80e3e5-6dd8-41bd-83a2-d5892dfea399" satisfied condition "Succeeded or Failed"
    Mar 27 14:49:29.654: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod var-expansion-1d80e3e5-6dd8-41bd-83a2-d5892dfea399 container dapi-container: <nil>
    STEP: delete the pod 03/27/23 14:49:29.662
    Mar 27 14:49:29.676: INFO: Waiting for pod var-expansion-1d80e3e5-6dd8-41bd-83a2-d5892dfea399 to disappear
    Mar 27 14:49:29.678: INFO: Pod var-expansion-1d80e3e5-6dd8-41bd-83a2-d5892dfea399 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 27 14:49:29.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5477" for this suite. 03/27/23 14:49:29.682
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:49:29.688
Mar 27 14:49:29.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename deployment 03/27/23 14:49:29.689
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:49:29.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:49:29.712
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Mar 27 14:49:29.714: INFO: Creating deployment "webserver-deployment"
Mar 27 14:49:29.721: INFO: Waiting for observed generation 1
Mar 27 14:49:31.729: INFO: Waiting for all required pods to come up
Mar 27 14:49:31.735: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 03/27/23 14:49:31.735
Mar 27 14:49:31.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-468jq" in namespace "deployment-7793" to be "running"
Mar 27 14:49:31.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-95bpv" in namespace "deployment-7793" to be "running"
Mar 27 14:49:31.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-bv9mh" in namespace "deployment-7793" to be "running"
Mar 27 14:49:31.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-pwg4r" in namespace "deployment-7793" to be "running"
Mar 27 14:49:31.739: INFO: Pod "webserver-deployment-845c8977d9-468jq": Phase="Pending", Reason="", readiness=false. Elapsed: 3.468329ms
Mar 27 14:49:31.739: INFO: Pod "webserver-deployment-845c8977d9-bv9mh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.416739ms
Mar 27 14:49:31.740: INFO: Pod "webserver-deployment-845c8977d9-95bpv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.963648ms
Mar 27 14:49:31.740: INFO: Pod "webserver-deployment-845c8977d9-pwg4r": Phase="Pending", Reason="", readiness=false. Elapsed: 3.918056ms
Mar 27 14:49:33.744: INFO: Pod "webserver-deployment-845c8977d9-pwg4r": Phase="Running", Reason="", readiness=true. Elapsed: 2.008554119s
Mar 27 14:49:33.744: INFO: Pod "webserver-deployment-845c8977d9-pwg4r" satisfied condition "running"
Mar 27 14:49:33.744: INFO: Pod "webserver-deployment-845c8977d9-bv9mh": Phase="Running", Reason="", readiness=true. Elapsed: 2.008588096s
Mar 27 14:49:33.744: INFO: Pod "webserver-deployment-845c8977d9-bv9mh" satisfied condition "running"
Mar 27 14:49:33.745: INFO: Pod "webserver-deployment-845c8977d9-95bpv": Phase="Running", Reason="", readiness=true. Elapsed: 2.008942012s
Mar 27 14:49:33.745: INFO: Pod "webserver-deployment-845c8977d9-95bpv" satisfied condition "running"
Mar 27 14:49:33.745: INFO: Pod "webserver-deployment-845c8977d9-468jq": Phase="Running", Reason="", readiness=true. Elapsed: 2.009245179s
Mar 27 14:49:33.745: INFO: Pod "webserver-deployment-845c8977d9-468jq" satisfied condition "running"
Mar 27 14:49:33.745: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 27 14:49:33.751: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 27 14:49:33.761: INFO: Updating deployment webserver-deployment
Mar 27 14:49:33.761: INFO: Waiting for observed generation 2
Mar 27 14:49:35.769: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 27 14:49:35.773: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 27 14:49:35.775: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 27 14:49:35.783: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 27 14:49:35.783: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 27 14:49:35.786: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 27 14:49:35.793: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 27 14:49:35.793: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 27 14:49:35.802: INFO: Updating deployment webserver-deployment
Mar 27 14:49:35.802: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 27 14:49:35.830: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 27 14:49:37.839: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 27 14:49:37.845: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7793  1e2abd7f-ad19-4791-97b4-090088efe28e 30081 3 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0062d5328 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-27 14:49:35 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-03-27 14:49:36 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 27 14:49:37.850: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-7793  59ba893b-0a47-421d-b4f3-a3ad12666f0c 30073 3 2023-03-27 14:49:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1e2abd7f-ad19-4791-97b4-090088efe28e 0xc0062d5737 0xc0062d5738}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e2abd7f-ad19-4791-97b4-090088efe28e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0062d57d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 27 14:49:37.850: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 27 14:49:37.850: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-7793  3b2ed2d4-1181-488e-945c-07a68a136b55 30074 3 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1e2abd7f-ad19-4791-97b4-090088efe28e 0xc0062d5837 0xc0062d5838}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e2abd7f-ad19-4791-97b4-090088efe28e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0062d58c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 27 14:49:37.859: INFO: Pod "webserver-deployment-69b7448995-5klgw" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-5klgw webserver-deployment-69b7448995- deployment-7793  2d884d58-2bbf-4c42-8fb5-e5726afcc1a6 30120 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:259d8e429c3ecdad37247082126d827a024a293a3fff5a6408a32a839416c226 cni.projectcalico.org/podIP:172.25.1.134/32 cni.projectcalico.org/podIPs:172.25.1.134/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc0062d5dc7 0xc0062d5dc8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nv2g6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nv2g6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.859: INFO: Pod "webserver-deployment-69b7448995-5wr9n" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-5wr9n webserver-deployment-69b7448995- deployment-7793  02ed4195-f6bf-4442-89f1-0f29a19b84b8 30164 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:2aab566215e3b4f984e885c77b26284c34b4abb785df7e35e762a02188222af9 cni.projectcalico.org/podIP:172.25.1.135/32 cni.projectcalico.org/podIPs:172.25.1.135/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc0062d5fe0 0xc0062d5fe1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-khzj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-khzj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.859: INFO: Pod "webserver-deployment-69b7448995-b4rvq" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-b4rvq webserver-deployment-69b7448995- deployment-7793  46000df9-2128-4bb7-a94d-f1c36eeed009 29959 0 2023-03-27 14:49:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:6b1bb632cf0a195d71c064f816b333552ef6b3212877a08aa4c55073c0c8c4de cni.projectcalico.org/podIP:172.25.1.130/32 cni.projectcalico.org/podIPs:172.25.1.130/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fca1f0 0xc003fca1f1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-49q6n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-49q6n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:49:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.859: INFO: Pod "webserver-deployment-69b7448995-clw5v" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-clw5v webserver-deployment-69b7448995- deployment-7793  d705d678-e67a-421d-b817-93c49c62345a 30138 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:b1ff1a7e521567ae1f533de35fef65d235c03a67efecbaeaed697b9eb3f88c3e cni.projectcalico.org/podIP:172.25.2.199/32 cni.projectcalico.org/podIPs:172.25.2.199/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fca400 0xc003fca401}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b4xl5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b4xl5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.859: INFO: Pod "webserver-deployment-69b7448995-cngch" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-cngch webserver-deployment-69b7448995- deployment-7793  fe0d763f-883f-4391-9fc1-3ffe1d2396b2 30154 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:a238df68acdfeab2836ac888eb1871341909caa82e0ab81a4bd99154e0b16476 cni.projectcalico.org/podIP:172.25.2.197/32 cni.projectcalico.org/podIPs:172.25.2.197/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fca5a0 0xc003fca5a1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-smfhg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-smfhg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.860: INFO: Pod "webserver-deployment-69b7448995-crlpf" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-crlpf webserver-deployment-69b7448995- deployment-7793  4d92bb25-df32-4a53-ad58-479f50d990d7 29960 0 2023-03-27 14:49:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:325bcd07608ad5eaa950596dce82e1c01a59a3b5878cbb992251fb0bf57d3436 cni.projectcalico.org/podIP:172.25.2.192/32 cni.projectcalico.org/podIPs:172.25.2.192/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fca7b0 0xc003fca7b1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6hjl4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6hjl4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2023-03-27 14:49:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.860: INFO: Pod "webserver-deployment-69b7448995-hkksv" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-hkksv webserver-deployment-69b7448995- deployment-7793  637c5583-3067-4e0b-87bb-5469c0a50c17 29966 0 2023-03-27 14:49:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:4dd8c04f856cf93590289afa0ff362520ab49adfa69dae099164b2b6679537ce cni.projectcalico.org/podIP:172.25.1.131/32 cni.projectcalico.org/podIPs:172.25.1.131/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fca9c0 0xc003fca9c1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wrs5h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wrs5h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:49:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.860: INFO: Pod "webserver-deployment-69b7448995-hpkk2" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-hpkk2 webserver-deployment-69b7448995- deployment-7793  edff8f33-2bae-4e50-9bfa-76e406cf01e8 29965 0 2023-03-27 14:49:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:6f96cb66695f89e9ac95debe61fc219b37cca7eab107aedd47410d9686639e1e cni.projectcalico.org/podIP:172.25.2.193/32 cni.projectcalico.org/podIPs:172.25.2.193/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fcabd0 0xc003fcabd1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l89vj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l89vj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2023-03-27 14:49:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.860: INFO: Pod "webserver-deployment-69b7448995-kwxmj" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-kwxmj webserver-deployment-69b7448995- deployment-7793  cfbb8dc7-7002-4159-8f7e-e84dbabe4b5d 30099 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:987187ca6bdb9c01ccbbc2e7ed5016f5d5a90503daabc834b94e64e039826659 cni.projectcalico.org/podIP:172.25.0.103/32 cni.projectcalico.org/podIPs:172.25.0.103/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fcadf0 0xc003fcadf1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q6xfh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q6xfh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.860: INFO: Pod "webserver-deployment-69b7448995-kzdq2" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-kzdq2 webserver-deployment-69b7448995- deployment-7793  01070643-8350-43ac-b5a6-af2201ecf0f2 30163 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:7d7e31d91f69b5ba6107b3daae9616b8991efdd993f32be7c03228d8e2f587b5 cni.projectcalico.org/podIP:172.25.0.108/32 cni.projectcalico.org/podIPs:172.25.0.108/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fcb000 0xc003fcb001}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pvjh2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pvjh2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2023-03-27 14:49:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.861: INFO: Pod "webserver-deployment-69b7448995-m64z9" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-m64z9 webserver-deployment-69b7448995- deployment-7793  ae8e27d8-f6b9-44a3-8376-b7c8a6e2ecc2 29961 0 2023-03-27 14:49:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:b8c81fecc7403a74871fc186dc989d1bf152d4106e42683c90199a82f25c9805 cni.projectcalico.org/podIP:172.25.0.102/32 cni.projectcalico.org/podIPs:172.25.0.102/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fcb210 0xc003fcb211}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5gtvr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5gtvr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2023-03-27 14:49:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.861: INFO: Pod "webserver-deployment-69b7448995-nc6fk" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-nc6fk webserver-deployment-69b7448995- deployment-7793  2bc9c2ad-ce17-4daf-9a5d-a42dc4c5e4cd 30156 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:3a321a11811dc095bdb4a6c552d8ceebf2a66ddc7c6f93a703521ffd956d1ec1 cni.projectcalico.org/podIP:172.25.2.201/32 cni.projectcalico.org/podIPs:172.25.2.201/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fcb430 0xc003fcb431}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-696bp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-696bp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.861: INFO: Pod "webserver-deployment-69b7448995-ncvsm" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-ncvsm webserver-deployment-69b7448995- deployment-7793  c5f6e88a-b3dc-4f00-8e31-b3d362c27332 30140 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:83794a4f495b1ae3462c7977b9084654638f4327bd24f1b6021fbea4c9cb3acc cni.projectcalico.org/podIP:172.25.0.106/32 cni.projectcalico.org/podIPs:172.25.0.106/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fcb5d0 0xc003fcb5d1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-srdvz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-srdvz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.861: INFO: Pod "webserver-deployment-845c8977d9-25zcm" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-25zcm webserver-deployment-845c8977d9- deployment-7793  300c80e4-3466-4236-bcf7-eff7ae63e1de 29873 0 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e861dfafb3627dbfb8ffd232192c512cf5bd2206ebb4603300b14f14e727aed9 cni.projectcalico.org/podIP:172.25.1.129/32 cni.projectcalico.org/podIPs:172.25.1.129/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc003fcb7e0 0xc003fcb7e1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.129\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bwcmc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bwcmc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.129,StartTime:2023-03-27 14:49:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:49:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b62a7bae70bf9ebce0c65254a22156ace68edaa7b67854a692e51ac8875114bb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.129,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.861: INFO: Pod "webserver-deployment-845c8977d9-4mjzd" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-4mjzd webserver-deployment-845c8977d9- deployment-7793  0271368f-4a57-4102-beb4-effea7504b47 30100 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d54756abd18f87d6105f2f056b1b897c8edfa1b5a1b5ca35cc54b7bfaba6da3b cni.projectcalico.org/podIP:172.25.1.132/32 cni.projectcalico.org/podIPs:172.25.1.132/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc003fcb9f7 0xc003fcb9f8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xvr6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xvr6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.862: INFO: Pod "webserver-deployment-845c8977d9-5j6fq" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-5j6fq webserver-deployment-845c8977d9- deployment-7793  0cfa3013-302b-4bd0-81ae-c368489a536f 30107 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:cd2e6f25a96b519e393fd9b00d034c5083429a9e5ea4f968a131c57f4524af4f cni.projectcalico.org/podIP:172.25.1.133/32 cni.projectcalico.org/podIPs:172.25.1.133/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc003fcbbf7 0xc003fcbbf8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dmrlb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dmrlb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.862: INFO: Pod "webserver-deployment-845c8977d9-8bh95" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-8bh95 webserver-deployment-845c8977d9- deployment-7793  4d6d41ea-3757-4a0e-9c34-54edba272661 30139 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:13091bd4bc435c0275d8aea1b00cfc48d58037f9b39bc938c3d228a29a6c97ac cni.projectcalico.org/podIP:172.25.2.198/32 cni.projectcalico.org/podIPs:172.25.2.198/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc003fcbdf7 0xc003fcbdf8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tkwsj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tkwsj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.862: INFO: Pod "webserver-deployment-845c8977d9-8wbnk" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-8wbnk webserver-deployment-845c8977d9- deployment-7793  328a833f-23b6-4a67-8658-4bd313f56344 30171 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:599e332445b6ff9cfdee13edb510aba3f98f668752ffab9c3d3812366f8f33e2 cni.projectcalico.org/podIP:172.25.2.200/32 cni.projectcalico.org/podIPs:172.25.2.200/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc003fcbff7 0xc003fcbff8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l62wm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l62wm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.862: INFO: Pod "webserver-deployment-845c8977d9-bv9mh" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-bv9mh webserver-deployment-845c8977d9- deployment-7793  18fa79f7-1585-4b97-8777-0ff9193fe429 29890 0 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:dcb99d32ca0f5e1f3c7e593bcf659f3802539c7fe5d824c26952bcdc1b54e450 cni.projectcalico.org/podIP:172.25.2.189/32 cni.projectcalico.org/podIPs:172.25.2.189/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d2217 0xc0020d2218}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bmqfd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bmqfd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:172.25.2.189,StartTime:2023-03-27 14:49:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:49:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9e1b23f63e2a54cb34f14569c3effe7887dbdb74fd8773bd964a6a6896490da6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.862: INFO: Pod "webserver-deployment-845c8977d9-dwfkl" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-dwfkl webserver-deployment-845c8977d9- deployment-7793  7f82e849-570e-4f4c-9fb8-bcf62a370d4b 29876 0 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9895119b25e5e4ecbf4fda9601fd2ac79b2335bff27d65b690c8a46f6d114183 cni.projectcalico.org/podIP:172.25.1.127/32 cni.projectcalico.org/podIPs:172.25.1.127/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d2437 0xc0020d2438}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zmpl7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zmpl7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.127,StartTime:2023-03-27 14:49:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:49:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7b6aac347c29701c75174135d81ef76b96977b6213ee5d1ba4482b4b7994f6a8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.863: INFO: Pod "webserver-deployment-845c8977d9-dzphq" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-dzphq webserver-deployment-845c8977d9- deployment-7793  0868b9f2-afbc-4d66-bbda-26a8b0e6e2d6 30101 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e60478c6d883d7f9066b32e0daf0142be6cb653d3b14a43f38027d749f1f5da5 cni.projectcalico.org/podIP:172.25.2.194/32 cni.projectcalico.org/podIPs:172.25.2.194/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d2657 0xc0020d2658}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bzx9j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bzx9j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.863: INFO: Pod "webserver-deployment-845c8977d9-f7h5z" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-f7h5z webserver-deployment-845c8977d9- deployment-7793  22ad0b47-6f32-49d4-bb11-e86d04c3f321 29864 0 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:0000406394bb50afa506942272438f99d7ed9bfb0a5d7687d3d9029bd21b5f82 cni.projectcalico.org/podIP:172.25.0.101/32 cni.projectcalico.org/podIPs:172.25.0.101/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d2857 0xc0020d2858}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2pr7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2pr7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:172.25.0.101,StartTime:2023-03-27 14:49:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:49:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://d3d88479c7a7e6b2dd55a58a27646c3bc0e4a505437e64c3e412a945052275bf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.863: INFO: Pod "webserver-deployment-845c8977d9-gd84d" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-gd84d webserver-deployment-845c8977d9- deployment-7793  3930548d-9279-4e46-a04d-9354a014b576 29871 0 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9221e9160896dd5b1ed403ad0e26da8cbf230b11e29509f902d7aabeb83a6e19 cni.projectcalico.org/podIP:172.25.1.128/32 cni.projectcalico.org/podIPs:172.25.1.128/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d2a77 0xc0020d2a78}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.128\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9ttvm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9ttvm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.128,StartTime:2023-03-27 14:49:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:49:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://11104619fb707e8834887ec1363b19a9800410800a4c4dd129764580d3ca381b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.128,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.863: INFO: Pod "webserver-deployment-845c8977d9-gljvv" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-gljvv webserver-deployment-845c8977d9- deployment-7793  fb13f7df-f5aa-4d77-9845-225d86c86a6c 29862 0 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e5c5c1191330750e104fcce4358851c9cb4ae0fcc106023ab887e21bd3b43c9e cni.projectcalico.org/podIP:172.25.0.99/32 cni.projectcalico.org/podIPs:172.25.0.99/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d2c97 0xc0020d2c98}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rnbqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rnbqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:172.25.0.99,StartTime:2023-03-27 14:49:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:49:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1a7c57cecf05d1b0f30a7eee7bd2d27e3db9030c83ad876ba1aa9aa042f165bc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.863: INFO: Pod "webserver-deployment-845c8977d9-h8sr5" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-h8sr5 webserver-deployment-845c8977d9- deployment-7793  205a8ae1-6cd0-47ff-8853-92f79feb54f2 30136 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6771be9e236dd9c502aeb300b3e342717ff56024a9515fd084d4b90d95932fef cni.projectcalico.org/podIP:172.25.0.105/32 cni.projectcalico.org/podIPs:172.25.0.105/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d2eb0 0xc0020d2eb1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tdqqs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tdqqs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.863: INFO: Pod "webserver-deployment-845c8977d9-hvt4r" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hvt4r webserver-deployment-845c8977d9- deployment-7793  5d2b4515-a5c2-40a0-821f-8bfb21ecd46b 30145 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1fb68e1655b8e03d2f1a381f14836baa279f5e45d4bf6e45a391694805f95761 cni.projectcalico.org/podIP:172.25.1.136/32 cni.projectcalico.org/podIPs:172.25.1.136/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d30a7 0xc0020d30a8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ghsqx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ghsqx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.863: INFO: Pod "webserver-deployment-845c8977d9-p52dg" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-p52dg webserver-deployment-845c8977d9- deployment-7793  24263857-f52d-4eea-bc6a-b9bd19acd12d 30103 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4b172e90e6fe82b6780e0c2760c09419a88c041c09f12ed6196ecfc0024a90b8 cni.projectcalico.org/podIP:172.25.2.196/32 cni.projectcalico.org/podIPs:172.25.2.196/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d32b7 0xc0020d32b8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wzcnj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wzcnj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.864: INFO: Pod "webserver-deployment-845c8977d9-pwg4r" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-pwg4r webserver-deployment-845c8977d9- deployment-7793  461123c0-5521-4fd9-9285-6ade7404bf41 29895 0 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:c06bc1911c41c3611e14369e4ed39ae7dc5782542b8ec739ff73430704a1a4b1 cni.projectcalico.org/podIP:172.25.2.188/32 cni.projectcalico.org/podIPs:172.25.2.188/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d34b7 0xc0020d34b8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-csz5k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-csz5k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:172.25.2.188,StartTime:2023-03-27 14:49:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:49:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://82099fcfb87509e1e86da17fa79e6603f06acb6d4d81b6567ed9b1a51a96b52c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.864: INFO: Pod "webserver-deployment-845c8977d9-shvcx" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-shvcx webserver-deployment-845c8977d9- deployment-7793  555648f3-52b2-4bad-9757-0c4dbeae288a 30104 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:c2300318db6084bd9bea5aba56e19ade353780381da40a3a27878f287d028db4 cni.projectcalico.org/podIP:172.25.2.195/32 cni.projectcalico.org/podIPs:172.25.2.195/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d36d7 0xc0020d36d8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8hxjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8hxjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.864: INFO: Pod "webserver-deployment-845c8977d9-wwjn8" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-wwjn8 webserver-deployment-845c8977d9- deployment-7793  24843f37-fad4-4069-98bd-4071b204faff 29867 0 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3a4182fd0d3f5bce4bdf08bdccc4d6b4be532c28f4758204569bda7e3ce36f0e cni.projectcalico.org/podIP:172.25.0.100/32 cni.projectcalico.org/podIPs:172.25.0.100/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d38e7 0xc0020d38e8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sx8fc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sx8fc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:172.25.0.100,StartTime:2023-03-27 14:49:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:49:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cb3fcb730e5090279711ad70e1d874dafcd8e83339ece23168f72a954bfd4a99,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.864: INFO: Pod "webserver-deployment-845c8977d9-xb52m" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xb52m webserver-deployment-845c8977d9- deployment-7793  890222ac-39c5-412f-a85f-33141fe14e02 30119 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7759c8a5dc3689291a8c7a191ba0a76056cce40093276e614ddafce3c684767a cni.projectcalico.org/podIP:172.25.0.104/32 cni.projectcalico.org/podIPs:172.25.0.104/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d3b07 0xc0020d3b08}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fvtjf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fvtjf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.865: INFO: Pod "webserver-deployment-845c8977d9-xjkhn" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xjkhn webserver-deployment-845c8977d9- deployment-7793  f7772075-80eb-4b1e-afa6-991d252ce696 30150 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:5046b60fe82304e37b042a7d0233b9f354d2f6c6571e0c514eae0c6fbf111ac2 cni.projectcalico.org/podIP:172.25.0.107/32 cni.projectcalico.org/podIPs:172.25.0.107/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d3d07 0xc0020d3d08}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vrs8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vrs8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 14:49:37.865: INFO: Pod "webserver-deployment-845c8977d9-xwdhb" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xwdhb webserver-deployment-845c8977d9- deployment-7793  0c50d534-8615-4fd8-802b-cd7724dacae9 30151 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:78e72a2c18a18a5e83183688277a8b5137077821af5b0e487a4dd90201448bac cni.projectcalico.org/podIP:172.25.1.137/32 cni.projectcalico.org/podIPs:172.25.1.137/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d3f07 0xc0020d3f08}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4jp2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4jp2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 27 14:49:37.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7793" for this suite. 03/27/23 14:49:37.87
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":146,"skipped":2728,"failed":0}
------------------------------
• [SLOW TEST] [8.193 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:49:29.688
    Mar 27 14:49:29.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename deployment 03/27/23 14:49:29.689
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:49:29.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:49:29.712
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Mar 27 14:49:29.714: INFO: Creating deployment "webserver-deployment"
    Mar 27 14:49:29.721: INFO: Waiting for observed generation 1
    Mar 27 14:49:31.729: INFO: Waiting for all required pods to come up
    Mar 27 14:49:31.735: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 03/27/23 14:49:31.735
    Mar 27 14:49:31.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-468jq" in namespace "deployment-7793" to be "running"
    Mar 27 14:49:31.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-95bpv" in namespace "deployment-7793" to be "running"
    Mar 27 14:49:31.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-bv9mh" in namespace "deployment-7793" to be "running"
    Mar 27 14:49:31.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-pwg4r" in namespace "deployment-7793" to be "running"
    Mar 27 14:49:31.739: INFO: Pod "webserver-deployment-845c8977d9-468jq": Phase="Pending", Reason="", readiness=false. Elapsed: 3.468329ms
    Mar 27 14:49:31.739: INFO: Pod "webserver-deployment-845c8977d9-bv9mh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.416739ms
    Mar 27 14:49:31.740: INFO: Pod "webserver-deployment-845c8977d9-95bpv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.963648ms
    Mar 27 14:49:31.740: INFO: Pod "webserver-deployment-845c8977d9-pwg4r": Phase="Pending", Reason="", readiness=false. Elapsed: 3.918056ms
    Mar 27 14:49:33.744: INFO: Pod "webserver-deployment-845c8977d9-pwg4r": Phase="Running", Reason="", readiness=true. Elapsed: 2.008554119s
    Mar 27 14:49:33.744: INFO: Pod "webserver-deployment-845c8977d9-pwg4r" satisfied condition "running"
    Mar 27 14:49:33.744: INFO: Pod "webserver-deployment-845c8977d9-bv9mh": Phase="Running", Reason="", readiness=true. Elapsed: 2.008588096s
    Mar 27 14:49:33.744: INFO: Pod "webserver-deployment-845c8977d9-bv9mh" satisfied condition "running"
    Mar 27 14:49:33.745: INFO: Pod "webserver-deployment-845c8977d9-95bpv": Phase="Running", Reason="", readiness=true. Elapsed: 2.008942012s
    Mar 27 14:49:33.745: INFO: Pod "webserver-deployment-845c8977d9-95bpv" satisfied condition "running"
    Mar 27 14:49:33.745: INFO: Pod "webserver-deployment-845c8977d9-468jq": Phase="Running", Reason="", readiness=true. Elapsed: 2.009245179s
    Mar 27 14:49:33.745: INFO: Pod "webserver-deployment-845c8977d9-468jq" satisfied condition "running"
    Mar 27 14:49:33.745: INFO: Waiting for deployment "webserver-deployment" to complete
    Mar 27 14:49:33.751: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Mar 27 14:49:33.761: INFO: Updating deployment webserver-deployment
    Mar 27 14:49:33.761: INFO: Waiting for observed generation 2
    Mar 27 14:49:35.769: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Mar 27 14:49:35.773: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Mar 27 14:49:35.775: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar 27 14:49:35.783: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Mar 27 14:49:35.783: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Mar 27 14:49:35.786: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar 27 14:49:35.793: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Mar 27 14:49:35.793: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Mar 27 14:49:35.802: INFO: Updating deployment webserver-deployment
    Mar 27 14:49:35.802: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Mar 27 14:49:35.830: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Mar 27 14:49:37.839: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 27 14:49:37.845: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-7793  1e2abd7f-ad19-4791-97b4-090088efe28e 30081 3 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0062d5328 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-27 14:49:35 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-03-27 14:49:36 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Mar 27 14:49:37.850: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-7793  59ba893b-0a47-421d-b4f3-a3ad12666f0c 30073 3 2023-03-27 14:49:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1e2abd7f-ad19-4791-97b4-090088efe28e 0xc0062d5737 0xc0062d5738}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e2abd7f-ad19-4791-97b4-090088efe28e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0062d57d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 14:49:37.850: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Mar 27 14:49:37.850: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-7793  3b2ed2d4-1181-488e-945c-07a68a136b55 30074 3 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1e2abd7f-ad19-4791-97b4-090088efe28e 0xc0062d5837 0xc0062d5838}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e2abd7f-ad19-4791-97b4-090088efe28e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0062d58c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 14:49:37.859: INFO: Pod "webserver-deployment-69b7448995-5klgw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-5klgw webserver-deployment-69b7448995- deployment-7793  2d884d58-2bbf-4c42-8fb5-e5726afcc1a6 30120 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:259d8e429c3ecdad37247082126d827a024a293a3fff5a6408a32a839416c226 cni.projectcalico.org/podIP:172.25.1.134/32 cni.projectcalico.org/podIPs:172.25.1.134/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc0062d5dc7 0xc0062d5dc8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nv2g6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nv2g6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.859: INFO: Pod "webserver-deployment-69b7448995-5wr9n" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-5wr9n webserver-deployment-69b7448995- deployment-7793  02ed4195-f6bf-4442-89f1-0f29a19b84b8 30164 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:2aab566215e3b4f984e885c77b26284c34b4abb785df7e35e762a02188222af9 cni.projectcalico.org/podIP:172.25.1.135/32 cni.projectcalico.org/podIPs:172.25.1.135/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc0062d5fe0 0xc0062d5fe1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-khzj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-khzj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.859: INFO: Pod "webserver-deployment-69b7448995-b4rvq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-b4rvq webserver-deployment-69b7448995- deployment-7793  46000df9-2128-4bb7-a94d-f1c36eeed009 29959 0 2023-03-27 14:49:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:6b1bb632cf0a195d71c064f816b333552ef6b3212877a08aa4c55073c0c8c4de cni.projectcalico.org/podIP:172.25.1.130/32 cni.projectcalico.org/podIPs:172.25.1.130/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fca1f0 0xc003fca1f1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-49q6n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-49q6n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:49:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.859: INFO: Pod "webserver-deployment-69b7448995-clw5v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-clw5v webserver-deployment-69b7448995- deployment-7793  d705d678-e67a-421d-b817-93c49c62345a 30138 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:b1ff1a7e521567ae1f533de35fef65d235c03a67efecbaeaed697b9eb3f88c3e cni.projectcalico.org/podIP:172.25.2.199/32 cni.projectcalico.org/podIPs:172.25.2.199/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fca400 0xc003fca401}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b4xl5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b4xl5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.859: INFO: Pod "webserver-deployment-69b7448995-cngch" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-cngch webserver-deployment-69b7448995- deployment-7793  fe0d763f-883f-4391-9fc1-3ffe1d2396b2 30154 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:a238df68acdfeab2836ac888eb1871341909caa82e0ab81a4bd99154e0b16476 cni.projectcalico.org/podIP:172.25.2.197/32 cni.projectcalico.org/podIPs:172.25.2.197/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fca5a0 0xc003fca5a1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-smfhg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-smfhg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.860: INFO: Pod "webserver-deployment-69b7448995-crlpf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-crlpf webserver-deployment-69b7448995- deployment-7793  4d92bb25-df32-4a53-ad58-479f50d990d7 29960 0 2023-03-27 14:49:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:325bcd07608ad5eaa950596dce82e1c01a59a3b5878cbb992251fb0bf57d3436 cni.projectcalico.org/podIP:172.25.2.192/32 cni.projectcalico.org/podIPs:172.25.2.192/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fca7b0 0xc003fca7b1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6hjl4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6hjl4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2023-03-27 14:49:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.860: INFO: Pod "webserver-deployment-69b7448995-hkksv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-hkksv webserver-deployment-69b7448995- deployment-7793  637c5583-3067-4e0b-87bb-5469c0a50c17 29966 0 2023-03-27 14:49:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:4dd8c04f856cf93590289afa0ff362520ab49adfa69dae099164b2b6679537ce cni.projectcalico.org/podIP:172.25.1.131/32 cni.projectcalico.org/podIPs:172.25.1.131/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fca9c0 0xc003fca9c1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wrs5h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wrs5h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:49:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.860: INFO: Pod "webserver-deployment-69b7448995-hpkk2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-hpkk2 webserver-deployment-69b7448995- deployment-7793  edff8f33-2bae-4e50-9bfa-76e406cf01e8 29965 0 2023-03-27 14:49:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:6f96cb66695f89e9ac95debe61fc219b37cca7eab107aedd47410d9686639e1e cni.projectcalico.org/podIP:172.25.2.193/32 cni.projectcalico.org/podIPs:172.25.2.193/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fcabd0 0xc003fcabd1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l89vj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l89vj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2023-03-27 14:49:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.860: INFO: Pod "webserver-deployment-69b7448995-kwxmj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-kwxmj webserver-deployment-69b7448995- deployment-7793  cfbb8dc7-7002-4159-8f7e-e84dbabe4b5d 30099 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:987187ca6bdb9c01ccbbc2e7ed5016f5d5a90503daabc834b94e64e039826659 cni.projectcalico.org/podIP:172.25.0.103/32 cni.projectcalico.org/podIPs:172.25.0.103/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fcadf0 0xc003fcadf1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q6xfh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q6xfh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.860: INFO: Pod "webserver-deployment-69b7448995-kzdq2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-kzdq2 webserver-deployment-69b7448995- deployment-7793  01070643-8350-43ac-b5a6-af2201ecf0f2 30163 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:7d7e31d91f69b5ba6107b3daae9616b8991efdd993f32be7c03228d8e2f587b5 cni.projectcalico.org/podIP:172.25.0.108/32 cni.projectcalico.org/podIPs:172.25.0.108/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fcb000 0xc003fcb001}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pvjh2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pvjh2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2023-03-27 14:49:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.861: INFO: Pod "webserver-deployment-69b7448995-m64z9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-m64z9 webserver-deployment-69b7448995- deployment-7793  ae8e27d8-f6b9-44a3-8376-b7c8a6e2ecc2 29961 0 2023-03-27 14:49:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:b8c81fecc7403a74871fc186dc989d1bf152d4106e42683c90199a82f25c9805 cni.projectcalico.org/podIP:172.25.0.102/32 cni.projectcalico.org/podIPs:172.25.0.102/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fcb210 0xc003fcb211}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5gtvr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5gtvr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2023-03-27 14:49:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.861: INFO: Pod "webserver-deployment-69b7448995-nc6fk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-nc6fk webserver-deployment-69b7448995- deployment-7793  2bc9c2ad-ce17-4daf-9a5d-a42dc4c5e4cd 30156 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:3a321a11811dc095bdb4a6c552d8ceebf2a66ddc7c6f93a703521ffd956d1ec1 cni.projectcalico.org/podIP:172.25.2.201/32 cni.projectcalico.org/podIPs:172.25.2.201/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fcb430 0xc003fcb431}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-696bp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-696bp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.861: INFO: Pod "webserver-deployment-69b7448995-ncvsm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-ncvsm webserver-deployment-69b7448995- deployment-7793  c5f6e88a-b3dc-4f00-8e31-b3d362c27332 30140 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:83794a4f495b1ae3462c7977b9084654638f4327bd24f1b6021fbea4c9cb3acc cni.projectcalico.org/podIP:172.25.0.106/32 cni.projectcalico.org/podIPs:172.25.0.106/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 59ba893b-0a47-421d-b4f3-a3ad12666f0c 0xc003fcb5d0 0xc003fcb5d1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59ba893b-0a47-421d-b4f3-a3ad12666f0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-srdvz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-srdvz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.861: INFO: Pod "webserver-deployment-845c8977d9-25zcm" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-25zcm webserver-deployment-845c8977d9- deployment-7793  300c80e4-3466-4236-bcf7-eff7ae63e1de 29873 0 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e861dfafb3627dbfb8ffd232192c512cf5bd2206ebb4603300b14f14e727aed9 cni.projectcalico.org/podIP:172.25.1.129/32 cni.projectcalico.org/podIPs:172.25.1.129/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc003fcb7e0 0xc003fcb7e1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.129\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bwcmc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bwcmc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.129,StartTime:2023-03-27 14:49:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:49:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b62a7bae70bf9ebce0c65254a22156ace68edaa7b67854a692e51ac8875114bb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.129,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.861: INFO: Pod "webserver-deployment-845c8977d9-4mjzd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-4mjzd webserver-deployment-845c8977d9- deployment-7793  0271368f-4a57-4102-beb4-effea7504b47 30100 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d54756abd18f87d6105f2f056b1b897c8edfa1b5a1b5ca35cc54b7bfaba6da3b cni.projectcalico.org/podIP:172.25.1.132/32 cni.projectcalico.org/podIPs:172.25.1.132/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc003fcb9f7 0xc003fcb9f8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xvr6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xvr6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.862: INFO: Pod "webserver-deployment-845c8977d9-5j6fq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-5j6fq webserver-deployment-845c8977d9- deployment-7793  0cfa3013-302b-4bd0-81ae-c368489a536f 30107 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:cd2e6f25a96b519e393fd9b00d034c5083429a9e5ea4f968a131c57f4524af4f cni.projectcalico.org/podIP:172.25.1.133/32 cni.projectcalico.org/podIPs:172.25.1.133/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc003fcbbf7 0xc003fcbbf8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dmrlb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dmrlb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.862: INFO: Pod "webserver-deployment-845c8977d9-8bh95" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-8bh95 webserver-deployment-845c8977d9- deployment-7793  4d6d41ea-3757-4a0e-9c34-54edba272661 30139 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:13091bd4bc435c0275d8aea1b00cfc48d58037f9b39bc938c3d228a29a6c97ac cni.projectcalico.org/podIP:172.25.2.198/32 cni.projectcalico.org/podIPs:172.25.2.198/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc003fcbdf7 0xc003fcbdf8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tkwsj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tkwsj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.862: INFO: Pod "webserver-deployment-845c8977d9-8wbnk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-8wbnk webserver-deployment-845c8977d9- deployment-7793  328a833f-23b6-4a67-8658-4bd313f56344 30171 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:599e332445b6ff9cfdee13edb510aba3f98f668752ffab9c3d3812366f8f33e2 cni.projectcalico.org/podIP:172.25.2.200/32 cni.projectcalico.org/podIPs:172.25.2.200/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc003fcbff7 0xc003fcbff8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l62wm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l62wm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.862: INFO: Pod "webserver-deployment-845c8977d9-bv9mh" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-bv9mh webserver-deployment-845c8977d9- deployment-7793  18fa79f7-1585-4b97-8777-0ff9193fe429 29890 0 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:dcb99d32ca0f5e1f3c7e593bcf659f3802539c7fe5d824c26952bcdc1b54e450 cni.projectcalico.org/podIP:172.25.2.189/32 cni.projectcalico.org/podIPs:172.25.2.189/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d2217 0xc0020d2218}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bmqfd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bmqfd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:172.25.2.189,StartTime:2023-03-27 14:49:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:49:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9e1b23f63e2a54cb34f14569c3effe7887dbdb74fd8773bd964a6a6896490da6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.862: INFO: Pod "webserver-deployment-845c8977d9-dwfkl" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-dwfkl webserver-deployment-845c8977d9- deployment-7793  7f82e849-570e-4f4c-9fb8-bcf62a370d4b 29876 0 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9895119b25e5e4ecbf4fda9601fd2ac79b2335bff27d65b690c8a46f6d114183 cni.projectcalico.org/podIP:172.25.1.127/32 cni.projectcalico.org/podIPs:172.25.1.127/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d2437 0xc0020d2438}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zmpl7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zmpl7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.127,StartTime:2023-03-27 14:49:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:49:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7b6aac347c29701c75174135d81ef76b96977b6213ee5d1ba4482b4b7994f6a8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.863: INFO: Pod "webserver-deployment-845c8977d9-dzphq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-dzphq webserver-deployment-845c8977d9- deployment-7793  0868b9f2-afbc-4d66-bbda-26a8b0e6e2d6 30101 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e60478c6d883d7f9066b32e0daf0142be6cb653d3b14a43f38027d749f1f5da5 cni.projectcalico.org/podIP:172.25.2.194/32 cni.projectcalico.org/podIPs:172.25.2.194/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d2657 0xc0020d2658}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bzx9j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bzx9j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.863: INFO: Pod "webserver-deployment-845c8977d9-f7h5z" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-f7h5z webserver-deployment-845c8977d9- deployment-7793  22ad0b47-6f32-49d4-bb11-e86d04c3f321 29864 0 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:0000406394bb50afa506942272438f99d7ed9bfb0a5d7687d3d9029bd21b5f82 cni.projectcalico.org/podIP:172.25.0.101/32 cni.projectcalico.org/podIPs:172.25.0.101/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d2857 0xc0020d2858}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2pr7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2pr7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:172.25.0.101,StartTime:2023-03-27 14:49:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:49:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://d3d88479c7a7e6b2dd55a58a27646c3bc0e4a505437e64c3e412a945052275bf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.863: INFO: Pod "webserver-deployment-845c8977d9-gd84d" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-gd84d webserver-deployment-845c8977d9- deployment-7793  3930548d-9279-4e46-a04d-9354a014b576 29871 0 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9221e9160896dd5b1ed403ad0e26da8cbf230b11e29509f902d7aabeb83a6e19 cni.projectcalico.org/podIP:172.25.1.128/32 cni.projectcalico.org/podIPs:172.25.1.128/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d2a77 0xc0020d2a78}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.128\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9ttvm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9ttvm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.128,StartTime:2023-03-27 14:49:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:49:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://11104619fb707e8834887ec1363b19a9800410800a4c4dd129764580d3ca381b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.128,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.863: INFO: Pod "webserver-deployment-845c8977d9-gljvv" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-gljvv webserver-deployment-845c8977d9- deployment-7793  fb13f7df-f5aa-4d77-9845-225d86c86a6c 29862 0 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e5c5c1191330750e104fcce4358851c9cb4ae0fcc106023ab887e21bd3b43c9e cni.projectcalico.org/podIP:172.25.0.99/32 cni.projectcalico.org/podIPs:172.25.0.99/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d2c97 0xc0020d2c98}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rnbqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rnbqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:172.25.0.99,StartTime:2023-03-27 14:49:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:49:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1a7c57cecf05d1b0f30a7eee7bd2d27e3db9030c83ad876ba1aa9aa042f165bc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.863: INFO: Pod "webserver-deployment-845c8977d9-h8sr5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-h8sr5 webserver-deployment-845c8977d9- deployment-7793  205a8ae1-6cd0-47ff-8853-92f79feb54f2 30136 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6771be9e236dd9c502aeb300b3e342717ff56024a9515fd084d4b90d95932fef cni.projectcalico.org/podIP:172.25.0.105/32 cni.projectcalico.org/podIPs:172.25.0.105/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d2eb0 0xc0020d2eb1}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tdqqs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tdqqs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.863: INFO: Pod "webserver-deployment-845c8977d9-hvt4r" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hvt4r webserver-deployment-845c8977d9- deployment-7793  5d2b4515-a5c2-40a0-821f-8bfb21ecd46b 30145 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1fb68e1655b8e03d2f1a381f14836baa279f5e45d4bf6e45a391694805f95761 cni.projectcalico.org/podIP:172.25.1.136/32 cni.projectcalico.org/podIPs:172.25.1.136/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d30a7 0xc0020d30a8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ghsqx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ghsqx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.863: INFO: Pod "webserver-deployment-845c8977d9-p52dg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-p52dg webserver-deployment-845c8977d9- deployment-7793  24263857-f52d-4eea-bc6a-b9bd19acd12d 30103 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4b172e90e6fe82b6780e0c2760c09419a88c041c09f12ed6196ecfc0024a90b8 cni.projectcalico.org/podIP:172.25.2.196/32 cni.projectcalico.org/podIPs:172.25.2.196/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d32b7 0xc0020d32b8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wzcnj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wzcnj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.864: INFO: Pod "webserver-deployment-845c8977d9-pwg4r" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-pwg4r webserver-deployment-845c8977d9- deployment-7793  461123c0-5521-4fd9-9285-6ade7404bf41 29895 0 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:c06bc1911c41c3611e14369e4ed39ae7dc5782542b8ec739ff73430704a1a4b1 cni.projectcalico.org/podIP:172.25.2.188/32 cni.projectcalico.org/podIPs:172.25.2.188/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d34b7 0xc0020d34b8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-csz5k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-csz5k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:172.25.2.188,StartTime:2023-03-27 14:49:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:49:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://82099fcfb87509e1e86da17fa79e6603f06acb6d4d81b6567ed9b1a51a96b52c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.864: INFO: Pod "webserver-deployment-845c8977d9-shvcx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-shvcx webserver-deployment-845c8977d9- deployment-7793  555648f3-52b2-4bad-9757-0c4dbeae288a 30104 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:c2300318db6084bd9bea5aba56e19ade353780381da40a3a27878f287d028db4 cni.projectcalico.org/podIP:172.25.2.195/32 cni.projectcalico.org/podIPs:172.25.2.195/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d36d7 0xc0020d36d8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8hxjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8hxjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.864: INFO: Pod "webserver-deployment-845c8977d9-wwjn8" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-wwjn8 webserver-deployment-845c8977d9- deployment-7793  24843f37-fad4-4069-98bd-4071b204faff 29867 0 2023-03-27 14:49:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3a4182fd0d3f5bce4bdf08bdccc4d6b4be532c28f4758204569bda7e3ce36f0e cni.projectcalico.org/podIP:172.25.0.100/32 cni.projectcalico.org/podIPs:172.25.0.100/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d38e7 0xc0020d38e8}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:49:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:49:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sx8fc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sx8fc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:172.25.0.100,StartTime:2023-03-27 14:49:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:49:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cb3fcb730e5090279711ad70e1d874dafcd8e83339ece23168f72a954bfd4a99,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.864: INFO: Pod "webserver-deployment-845c8977d9-xb52m" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xb52m webserver-deployment-845c8977d9- deployment-7793  890222ac-39c5-412f-a85f-33141fe14e02 30119 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7759c8a5dc3689291a8c7a191ba0a76056cce40093276e614ddafce3c684767a cni.projectcalico.org/podIP:172.25.0.104/32 cni.projectcalico.org/podIPs:172.25.0.104/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d3b07 0xc0020d3b08}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fvtjf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fvtjf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.865: INFO: Pod "webserver-deployment-845c8977d9-xjkhn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xjkhn webserver-deployment-845c8977d9- deployment-7793  f7772075-80eb-4b1e-afa6-991d252ce696 30150 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:5046b60fe82304e37b042a7d0233b9f354d2f6c6571e0c514eae0c6fbf111ac2 cni.projectcalico.org/podIP:172.25.0.107/32 cni.projectcalico.org/podIPs:172.25.0.107/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d3d07 0xc0020d3d08}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vrs8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vrs8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 14:49:37.865: INFO: Pod "webserver-deployment-845c8977d9-xwdhb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xwdhb webserver-deployment-845c8977d9- deployment-7793  0c50d534-8615-4fd8-802b-cd7724dacae9 30151 0 2023-03-27 14:49:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:78e72a2c18a18a5e83183688277a8b5137077821af5b0e487a4dd90201448bac cni.projectcalico.org/podIP:172.25.1.137/32 cni.projectcalico.org/podIPs:172.25.1.137/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3b2ed2d4-1181-488e-945c-07a68a136b55 0xc0020d3f07 0xc0020d3f08}] [] [{kube-controller-manager Update v1 2023-03-27 14:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2ed2d4-1181-488e-945c-07a68a136b55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 14:49:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 14:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4jp2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4jp2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2023-03-27 14:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 27 14:49:37.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7793" for this suite. 03/27/23 14:49:37.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:49:37.885
Mar 27 14:49:37.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename var-expansion 03/27/23 14:49:37.887
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:49:37.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:49:37.91
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Mar 27 14:49:37.920: INFO: Waiting up to 2m0s for pod "var-expansion-534a37e9-4e75-4ad3-ac3a-f4fd47c962d4" in namespace "var-expansion-7417" to be "container 0 failed with reason CreateContainerConfigError"
Mar 27 14:49:37.922: INFO: Pod "var-expansion-534a37e9-4e75-4ad3-ac3a-f4fd47c962d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.814182ms
Mar 27 14:49:39.926: INFO: Pod "var-expansion-534a37e9-4e75-4ad3-ac3a-f4fd47c962d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00659524s
Mar 27 14:49:41.927: INFO: Pod "var-expansion-534a37e9-4e75-4ad3-ac3a-f4fd47c962d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006894132s
Mar 27 14:49:41.927: INFO: Pod "var-expansion-534a37e9-4e75-4ad3-ac3a-f4fd47c962d4" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar 27 14:49:41.927: INFO: Deleting pod "var-expansion-534a37e9-4e75-4ad3-ac3a-f4fd47c962d4" in namespace "var-expansion-7417"
Mar 27 14:49:41.936: INFO: Wait up to 5m0s for pod "var-expansion-534a37e9-4e75-4ad3-ac3a-f4fd47c962d4" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 27 14:49:45.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7417" for this suite. 03/27/23 14:49:45.95
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":147,"skipped":2748,"failed":0}
------------------------------
• [SLOW TEST] [8.073 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:49:37.885
    Mar 27 14:49:37.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename var-expansion 03/27/23 14:49:37.887
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:49:37.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:49:37.91
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Mar 27 14:49:37.920: INFO: Waiting up to 2m0s for pod "var-expansion-534a37e9-4e75-4ad3-ac3a-f4fd47c962d4" in namespace "var-expansion-7417" to be "container 0 failed with reason CreateContainerConfigError"
    Mar 27 14:49:37.922: INFO: Pod "var-expansion-534a37e9-4e75-4ad3-ac3a-f4fd47c962d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.814182ms
    Mar 27 14:49:39.926: INFO: Pod "var-expansion-534a37e9-4e75-4ad3-ac3a-f4fd47c962d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00659524s
    Mar 27 14:49:41.927: INFO: Pod "var-expansion-534a37e9-4e75-4ad3-ac3a-f4fd47c962d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006894132s
    Mar 27 14:49:41.927: INFO: Pod "var-expansion-534a37e9-4e75-4ad3-ac3a-f4fd47c962d4" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar 27 14:49:41.927: INFO: Deleting pod "var-expansion-534a37e9-4e75-4ad3-ac3a-f4fd47c962d4" in namespace "var-expansion-7417"
    Mar 27 14:49:41.936: INFO: Wait up to 5m0s for pod "var-expansion-534a37e9-4e75-4ad3-ac3a-f4fd47c962d4" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 27 14:49:45.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7417" for this suite. 03/27/23 14:49:45.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:49:45.959
Mar 27 14:49:45.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename security-context 03/27/23 14:49:45.96
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:49:45.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:49:45.983
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/27/23 14:49:45.985
Mar 27 14:49:45.996: INFO: Waiting up to 5m0s for pod "security-context-8b109424-703d-4155-8ea5-ab8ef0ddca14" in namespace "security-context-3475" to be "Succeeded or Failed"
Mar 27 14:49:46.000: INFO: Pod "security-context-8b109424-703d-4155-8ea5-ab8ef0ddca14": Phase="Pending", Reason="", readiness=false. Elapsed: 3.84091ms
Mar 27 14:49:48.007: INFO: Pod "security-context-8b109424-703d-4155-8ea5-ab8ef0ddca14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011315861s
Mar 27 14:49:50.007: INFO: Pod "security-context-8b109424-703d-4155-8ea5-ab8ef0ddca14": Phase="Running", Reason="", readiness=false. Elapsed: 4.010823145s
Mar 27 14:49:52.009: INFO: Pod "security-context-8b109424-703d-4155-8ea5-ab8ef0ddca14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012754132s
STEP: Saw pod success 03/27/23 14:49:52.009
Mar 27 14:49:52.009: INFO: Pod "security-context-8b109424-703d-4155-8ea5-ab8ef0ddca14" satisfied condition "Succeeded or Failed"
Mar 27 14:49:52.012: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod security-context-8b109424-703d-4155-8ea5-ab8ef0ddca14 container test-container: <nil>
STEP: delete the pod 03/27/23 14:49:52.026
Mar 27 14:49:52.045: INFO: Waiting for pod security-context-8b109424-703d-4155-8ea5-ab8ef0ddca14 to disappear
Mar 27 14:49:52.048: INFO: Pod security-context-8b109424-703d-4155-8ea5-ab8ef0ddca14 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar 27 14:49:52.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3475" for this suite. 03/27/23 14:49:52.055
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":148,"skipped":2772,"failed":0}
------------------------------
• [SLOW TEST] [6.103 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:49:45.959
    Mar 27 14:49:45.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename security-context 03/27/23 14:49:45.96
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:49:45.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:49:45.983
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/27/23 14:49:45.985
    Mar 27 14:49:45.996: INFO: Waiting up to 5m0s for pod "security-context-8b109424-703d-4155-8ea5-ab8ef0ddca14" in namespace "security-context-3475" to be "Succeeded or Failed"
    Mar 27 14:49:46.000: INFO: Pod "security-context-8b109424-703d-4155-8ea5-ab8ef0ddca14": Phase="Pending", Reason="", readiness=false. Elapsed: 3.84091ms
    Mar 27 14:49:48.007: INFO: Pod "security-context-8b109424-703d-4155-8ea5-ab8ef0ddca14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011315861s
    Mar 27 14:49:50.007: INFO: Pod "security-context-8b109424-703d-4155-8ea5-ab8ef0ddca14": Phase="Running", Reason="", readiness=false. Elapsed: 4.010823145s
    Mar 27 14:49:52.009: INFO: Pod "security-context-8b109424-703d-4155-8ea5-ab8ef0ddca14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012754132s
    STEP: Saw pod success 03/27/23 14:49:52.009
    Mar 27 14:49:52.009: INFO: Pod "security-context-8b109424-703d-4155-8ea5-ab8ef0ddca14" satisfied condition "Succeeded or Failed"
    Mar 27 14:49:52.012: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod security-context-8b109424-703d-4155-8ea5-ab8ef0ddca14 container test-container: <nil>
    STEP: delete the pod 03/27/23 14:49:52.026
    Mar 27 14:49:52.045: INFO: Waiting for pod security-context-8b109424-703d-4155-8ea5-ab8ef0ddca14 to disappear
    Mar 27 14:49:52.048: INFO: Pod security-context-8b109424-703d-4155-8ea5-ab8ef0ddca14 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar 27 14:49:52.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-3475" for this suite. 03/27/23 14:49:52.055
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:49:52.062
Mar 27 14:49:52.062: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename webhook 03/27/23 14:49:52.064
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:49:52.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:49:52.092
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/27/23 14:49:52.111
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 14:49:52.516
STEP: Deploying the webhook pod 03/27/23 14:49:52.524
STEP: Wait for the deployment to be ready 03/27/23 14:49:52.537
Mar 27 14:49:52.543: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 03/27/23 14:49:54.554
STEP: Verifying the service has paired with the endpoint 03/27/23 14:49:54.564
Mar 27 14:49:55.564: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Mar 27 14:49:55.571: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/27/23 14:49:56.082
STEP: Creating a custom resource that should be denied by the webhook 03/27/23 14:49:56.109
STEP: Creating a custom resource whose deletion would be denied by the webhook 03/27/23 14:49:58.147
STEP: Updating the custom resource with disallowed data should be denied 03/27/23 14:49:58.161
STEP: Deleting the custom resource should be denied 03/27/23 14:49:58.172
STEP: Remove the offending key and value from the custom resource data 03/27/23 14:49:58.181
STEP: Deleting the updated custom resource should be successful 03/27/23 14:49:58.197
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:49:58.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1116" for this suite. 03/27/23 14:49:58.734
STEP: Destroying namespace "webhook-1116-markers" for this suite. 03/27/23 14:49:58.748
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":149,"skipped":2776,"failed":0}
------------------------------
• [SLOW TEST] [6.732 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:49:52.062
    Mar 27 14:49:52.062: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename webhook 03/27/23 14:49:52.064
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:49:52.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:49:52.092
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/27/23 14:49:52.111
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 14:49:52.516
    STEP: Deploying the webhook pod 03/27/23 14:49:52.524
    STEP: Wait for the deployment to be ready 03/27/23 14:49:52.537
    Mar 27 14:49:52.543: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 03/27/23 14:49:54.554
    STEP: Verifying the service has paired with the endpoint 03/27/23 14:49:54.564
    Mar 27 14:49:55.564: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Mar 27 14:49:55.571: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/27/23 14:49:56.082
    STEP: Creating a custom resource that should be denied by the webhook 03/27/23 14:49:56.109
    STEP: Creating a custom resource whose deletion would be denied by the webhook 03/27/23 14:49:58.147
    STEP: Updating the custom resource with disallowed data should be denied 03/27/23 14:49:58.161
    STEP: Deleting the custom resource should be denied 03/27/23 14:49:58.172
    STEP: Remove the offending key and value from the custom resource data 03/27/23 14:49:58.181
    STEP: Deleting the updated custom resource should be successful 03/27/23 14:49:58.197
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:49:58.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1116" for this suite. 03/27/23 14:49:58.734
    STEP: Destroying namespace "webhook-1116-markers" for this suite. 03/27/23 14:49:58.748
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:49:58.797
Mar 27 14:49:58.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename subpath 03/27/23 14:49:58.797
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:49:58.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:49:58.82
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/27/23 14:49:58.823
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-dhmd 03/27/23 14:49:58.834
STEP: Creating a pod to test atomic-volume-subpath 03/27/23 14:49:58.834
Mar 27 14:49:58.845: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-dhmd" in namespace "subpath-5308" to be "Succeeded or Failed"
Mar 27 14:49:58.848: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.846947ms
Mar 27 14:50:00.857: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012559164s
Mar 27 14:50:02.853: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 4.008273864s
Mar 27 14:50:04.854: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 6.008802358s
Mar 27 14:50:06.851: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 8.006460643s
Mar 27 14:50:08.853: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 10.00822287s
Mar 27 14:50:10.852: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 12.006849804s
Mar 27 14:50:12.852: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 14.007325991s
Mar 27 14:50:14.855: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 16.010351427s
Mar 27 14:50:16.853: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 18.008107905s
Mar 27 14:50:18.853: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 20.008279876s
Mar 27 14:50:20.852: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 22.007450824s
Mar 27 14:50:22.852: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=false. Elapsed: 24.006807159s
Mar 27 14:50:24.851: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.006370409s
STEP: Saw pod success 03/27/23 14:50:24.851
Mar 27 14:50:24.851: INFO: Pod "pod-subpath-test-configmap-dhmd" satisfied condition "Succeeded or Failed"
Mar 27 14:50:24.855: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-subpath-test-configmap-dhmd container test-container-subpath-configmap-dhmd: <nil>
STEP: delete the pod 03/27/23 14:50:24.866
Mar 27 14:50:24.884: INFO: Waiting for pod pod-subpath-test-configmap-dhmd to disappear
Mar 27 14:50:24.889: INFO: Pod pod-subpath-test-configmap-dhmd no longer exists
STEP: Deleting pod pod-subpath-test-configmap-dhmd 03/27/23 14:50:24.889
Mar 27 14:50:24.889: INFO: Deleting pod "pod-subpath-test-configmap-dhmd" in namespace "subpath-5308"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar 27 14:50:24.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5308" for this suite. 03/27/23 14:50:24.899
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":150,"skipped":2795,"failed":0}
------------------------------
• [SLOW TEST] [26.110 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:49:58.797
    Mar 27 14:49:58.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename subpath 03/27/23 14:49:58.797
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:49:58.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:49:58.82
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/27/23 14:49:58.823
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-dhmd 03/27/23 14:49:58.834
    STEP: Creating a pod to test atomic-volume-subpath 03/27/23 14:49:58.834
    Mar 27 14:49:58.845: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-dhmd" in namespace "subpath-5308" to be "Succeeded or Failed"
    Mar 27 14:49:58.848: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.846947ms
    Mar 27 14:50:00.857: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012559164s
    Mar 27 14:50:02.853: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 4.008273864s
    Mar 27 14:50:04.854: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 6.008802358s
    Mar 27 14:50:06.851: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 8.006460643s
    Mar 27 14:50:08.853: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 10.00822287s
    Mar 27 14:50:10.852: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 12.006849804s
    Mar 27 14:50:12.852: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 14.007325991s
    Mar 27 14:50:14.855: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 16.010351427s
    Mar 27 14:50:16.853: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 18.008107905s
    Mar 27 14:50:18.853: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 20.008279876s
    Mar 27 14:50:20.852: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=true. Elapsed: 22.007450824s
    Mar 27 14:50:22.852: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Running", Reason="", readiness=false. Elapsed: 24.006807159s
    Mar 27 14:50:24.851: INFO: Pod "pod-subpath-test-configmap-dhmd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.006370409s
    STEP: Saw pod success 03/27/23 14:50:24.851
    Mar 27 14:50:24.851: INFO: Pod "pod-subpath-test-configmap-dhmd" satisfied condition "Succeeded or Failed"
    Mar 27 14:50:24.855: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-subpath-test-configmap-dhmd container test-container-subpath-configmap-dhmd: <nil>
    STEP: delete the pod 03/27/23 14:50:24.866
    Mar 27 14:50:24.884: INFO: Waiting for pod pod-subpath-test-configmap-dhmd to disappear
    Mar 27 14:50:24.889: INFO: Pod pod-subpath-test-configmap-dhmd no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-dhmd 03/27/23 14:50:24.889
    Mar 27 14:50:24.889: INFO: Deleting pod "pod-subpath-test-configmap-dhmd" in namespace "subpath-5308"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar 27 14:50:24.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-5308" for this suite. 03/27/23 14:50:24.899
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:50:24.909
Mar 27 14:50:24.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename deployment 03/27/23 14:50:24.91
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:50:24.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:50:24.932
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 03/27/23 14:50:24.939
STEP: waiting for Deployment to be created 03/27/23 14:50:24.946
STEP: waiting for all Replicas to be Ready 03/27/23 14:50:24.948
Mar 27 14:50:24.949: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 27 14:50:24.949: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 27 14:50:24.967: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 27 14:50:24.967: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 27 14:50:24.981: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 27 14:50:24.981: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 27 14:50:25.015: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 27 14:50:25.015: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 27 14:50:26.556: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 27 14:50:26.556: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 27 14:50:26.728: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 03/27/23 14:50:26.728
W0327 14:50:26.737202      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 27 14:50:26.738: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 03/27/23 14:50:26.739
Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0
Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0
Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0
Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0
Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0
Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0
Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0
Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0
Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
Mar 27 14:50:26.757: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
Mar 27 14:50:26.757: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
Mar 27 14:50:26.786: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
Mar 27 14:50:26.786: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
Mar 27 14:50:26.794: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
Mar 27 14:50:26.795: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
Mar 27 14:50:26.803: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
Mar 27 14:50:26.803: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
Mar 27 14:50:28.577: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
Mar 27 14:50:28.577: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
Mar 27 14:50:28.605: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
STEP: listing Deployments 03/27/23 14:50:28.605
Mar 27 14:50:28.610: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 03/27/23 14:50:28.61
Mar 27 14:50:28.627: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 03/27/23 14:50:28.627
Mar 27 14:50:28.638: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 14:50:28.643: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 14:50:28.663: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 14:50:28.680: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 14:50:28.693: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 14:50:28.706: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 14:50:30.587: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 14:50:30.610: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 14:50:30.625: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 14:50:30.652: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 14:50:30.667: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 14:50:31.758: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 03/27/23 14:50:31.798
STEP: fetching the DeploymentStatus 03/27/23 14:50:31.808
Mar 27 14:50:31.813: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
Mar 27 14:50:31.813: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
Mar 27 14:50:31.813: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
Mar 27 14:50:31.813: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
Mar 27 14:50:31.813: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
Mar 27 14:50:31.813: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
Mar 27 14:50:31.813: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
Mar 27 14:50:31.814: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
Mar 27 14:50:31.814: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
Mar 27 14:50:31.814: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
Mar 27 14:50:31.814: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
Mar 27 14:50:31.814: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 3
STEP: deleting the Deployment 03/27/23 14:50:31.814
Mar 27 14:50:31.824: INFO: observed event type MODIFIED
Mar 27 14:50:31.824: INFO: observed event type MODIFIED
Mar 27 14:50:31.824: INFO: observed event type MODIFIED
Mar 27 14:50:31.824: INFO: observed event type MODIFIED
Mar 27 14:50:31.824: INFO: observed event type MODIFIED
Mar 27 14:50:31.825: INFO: observed event type MODIFIED
Mar 27 14:50:31.825: INFO: observed event type MODIFIED
Mar 27 14:50:31.825: INFO: observed event type MODIFIED
Mar 27 14:50:31.825: INFO: observed event type MODIFIED
Mar 27 14:50:31.825: INFO: observed event type MODIFIED
Mar 27 14:50:31.825: INFO: observed event type MODIFIED
Mar 27 14:50:31.825: INFO: observed event type MODIFIED
Mar 27 14:50:31.825: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 27 14:50:31.827: INFO: Log out all the ReplicaSets if there is no deployment created
Mar 27 14:50:31.830: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-2074  f4a4c0f0-cfd0-4954-9638-86b999bc7984 31041 4 2023-03-27 14:50:26 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 258f5286-c700-4571-9f69-eb6888b6ec04 0xc004094f67 0xc004094f68}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:50:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"258f5286-c700-4571-9f69-eb6888b6ec04\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:50:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004094ff0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Mar 27 14:50:31.834: INFO: pod: "test-deployment-54cc775c4b-d7cqz":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-d7cqz test-deployment-54cc775c4b- deployment-2074  7a80427f-2ae9-4eac-9426-f8608ec4b8ce 31020 0 2023-03-27 14:50:28 +0000 UTC 2023-03-27 14:50:31 +0000 UTC 0xc0040954e8 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:941ea4c072d9a41205d476730a73c6b1a8ec867a23608a29795cd3347e1b83c8 cni.projectcalico.org/podIP:172.25.1.140/32 cni.projectcalico.org/podIPs:172.25.1.140/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b f4a4c0f0-cfd0-4954-9638-86b999bc7984 0xc004095537 0xc004095538}] [] [{kube-controller-manager Update v1 2023-03-27 14:50:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a4c0f0-cfd0-4954-9638-86b999bc7984\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:50:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:50:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9gfwl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9gfwl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.140,StartTime:2023-03-27 14:50:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:50:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://72e16579957b43aae3704186ad004061891257e205be72286c15af01cf370ebd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 27 14:50:31.834: INFO: pod: "test-deployment-54cc775c4b-rv9nc":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-rv9nc test-deployment-54cc775c4b- deployment-2074  6e11ab2b-0016-40c1-9538-a00769cb7a35 31037 0 2023-03-27 14:50:26 +0000 UTC 2023-03-27 14:50:32 +0000 UTC 0xc004095710 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:5de0a99eb853cada8a13498367eba5a8cc4db99ab4dd816bb0bb518514aa9c14 cni.projectcalico.org/podIP:172.25.2.206/32 cni.projectcalico.org/podIPs:172.25.2.206/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b f4a4c0f0-cfd0-4954-9638-86b999bc7984 0xc004095767 0xc004095768}] [] [{kube-controller-manager Update v1 2023-03-27 14:50:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a4c0f0-cfd0-4954-9638-86b999bc7984\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:50:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:50:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8ndgz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8ndgz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:172.25.2.206,StartTime:2023-03-27 14:50:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:50:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://fc5bd1489791098f9070642040129df8da184679378ccc9966bf1a81fb31c166,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 27 14:50:31.834: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-2074  377e7da4-0a32-4c36-9c37-c14b1b931e48 31033 2 2023-03-27 14:50:28 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 258f5286-c700-4571-9f69-eb6888b6ec04 0xc004095057 0xc004095058}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:50:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"258f5286-c700-4571-9f69-eb6888b6ec04\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:50:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040950e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Mar 27 14:50:31.840: INFO: pod: "test-deployment-7c7d8d58c8-jwx4s":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-jwx4s test-deployment-7c7d8d58c8- deployment-2074  6810f0a0-6b4b-40d0-b924-c9808d53f876 30993 0 2023-03-27 14:50:28 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:10807c9f089166368950ebce249037831a98e13ab588c89fb190cc64b9efc7f4 cni.projectcalico.org/podIP:172.25.2.207/32 cni.projectcalico.org/podIPs:172.25.2.207/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 377e7da4-0a32-4c36-9c37-c14b1b931e48 0xc003fcabc7 0xc003fcabc8}] [] [{kube-controller-manager Update v1 2023-03-27 14:50:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"377e7da4-0a32-4c36-9c37-c14b1b931e48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:50:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:50:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qhrxl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qhrxl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:172.25.2.207,StartTime:2023-03-27 14:50:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:50:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6dc443ba9529b7c5ac3c5f8326df6eddc664d3081e1717b6b2d68e4aa7b1c1d5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 27 14:50:31.841: INFO: pod: "test-deployment-7c7d8d58c8-psfnj":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-psfnj test-deployment-7c7d8d58c8- deployment-2074  d126aa67-3ad5-4ed5-ada1-06af61907511 31032 0 2023-03-27 14:50:30 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:1c846b1c8b1f56d2f7662f60c9c9f66781e4f7f014359b35a722cc6f00b6ce31 cni.projectcalico.org/podIP:172.25.1.141/32 cni.projectcalico.org/podIPs:172.25.1.141/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 377e7da4-0a32-4c36-9c37-c14b1b931e48 0xc003fcade7 0xc003fcade8}] [] [{kube-controller-manager Update v1 2023-03-27 14:50:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"377e7da4-0a32-4c36-9c37-c14b1b931e48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:50:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:50:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p22b4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p22b4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.141,StartTime:2023-03-27 14:50:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:50:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a3013987095ed40f3a921c7f08e67b66bb20ef1fac0ac1b14dc48072232b6f8d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.141,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 27 14:50:31.841: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-2074  447f0c64-f10c-448f-abed-d80a01785119 30934 3 2023-03-27 14:50:24 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 258f5286-c700-4571-9f69-eb6888b6ec04 0xc004095147 0xc004095148}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:50:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"258f5286-c700-4571-9f69-eb6888b6ec04\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:50:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040951d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 27 14:50:31.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2074" for this suite. 03/27/23 14:50:31.853
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":151,"skipped":2796,"failed":0}
------------------------------
• [SLOW TEST] [6.955 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:50:24.909
    Mar 27 14:50:24.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename deployment 03/27/23 14:50:24.91
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:50:24.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:50:24.932
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 03/27/23 14:50:24.939
    STEP: waiting for Deployment to be created 03/27/23 14:50:24.946
    STEP: waiting for all Replicas to be Ready 03/27/23 14:50:24.948
    Mar 27 14:50:24.949: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 27 14:50:24.949: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 27 14:50:24.967: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 27 14:50:24.967: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 27 14:50:24.981: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 27 14:50:24.981: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 27 14:50:25.015: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 27 14:50:25.015: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 27 14:50:26.556: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar 27 14:50:26.556: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar 27 14:50:26.728: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 03/27/23 14:50:26.728
    W0327 14:50:26.737202      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 27 14:50:26.738: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 03/27/23 14:50:26.739
    Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0
    Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0
    Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0
    Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0
    Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0
    Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0
    Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0
    Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 0
    Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
    Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
    Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
    Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
    Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
    Mar 27 14:50:26.740: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
    Mar 27 14:50:26.757: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
    Mar 27 14:50:26.757: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
    Mar 27 14:50:26.786: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
    Mar 27 14:50:26.786: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
    Mar 27 14:50:26.794: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
    Mar 27 14:50:26.795: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
    Mar 27 14:50:26.803: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
    Mar 27 14:50:26.803: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
    Mar 27 14:50:28.577: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
    Mar 27 14:50:28.577: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
    Mar 27 14:50:28.605: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
    STEP: listing Deployments 03/27/23 14:50:28.605
    Mar 27 14:50:28.610: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 03/27/23 14:50:28.61
    Mar 27 14:50:28.627: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 03/27/23 14:50:28.627
    Mar 27 14:50:28.638: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 14:50:28.643: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 14:50:28.663: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 14:50:28.680: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 14:50:28.693: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 14:50:28.706: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 14:50:30.587: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 14:50:30.610: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 14:50:30.625: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 14:50:30.652: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 14:50:30.667: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 14:50:31.758: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 03/27/23 14:50:31.798
    STEP: fetching the DeploymentStatus 03/27/23 14:50:31.808
    Mar 27 14:50:31.813: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
    Mar 27 14:50:31.813: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
    Mar 27 14:50:31.813: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
    Mar 27 14:50:31.813: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
    Mar 27 14:50:31.813: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
    Mar 27 14:50:31.813: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 1
    Mar 27 14:50:31.813: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
    Mar 27 14:50:31.814: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
    Mar 27 14:50:31.814: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
    Mar 27 14:50:31.814: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
    Mar 27 14:50:31.814: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 2
    Mar 27 14:50:31.814: INFO: observed Deployment test-deployment in namespace deployment-2074 with ReadyReplicas 3
    STEP: deleting the Deployment 03/27/23 14:50:31.814
    Mar 27 14:50:31.824: INFO: observed event type MODIFIED
    Mar 27 14:50:31.824: INFO: observed event type MODIFIED
    Mar 27 14:50:31.824: INFO: observed event type MODIFIED
    Mar 27 14:50:31.824: INFO: observed event type MODIFIED
    Mar 27 14:50:31.824: INFO: observed event type MODIFIED
    Mar 27 14:50:31.825: INFO: observed event type MODIFIED
    Mar 27 14:50:31.825: INFO: observed event type MODIFIED
    Mar 27 14:50:31.825: INFO: observed event type MODIFIED
    Mar 27 14:50:31.825: INFO: observed event type MODIFIED
    Mar 27 14:50:31.825: INFO: observed event type MODIFIED
    Mar 27 14:50:31.825: INFO: observed event type MODIFIED
    Mar 27 14:50:31.825: INFO: observed event type MODIFIED
    Mar 27 14:50:31.825: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 27 14:50:31.827: INFO: Log out all the ReplicaSets if there is no deployment created
    Mar 27 14:50:31.830: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-2074  f4a4c0f0-cfd0-4954-9638-86b999bc7984 31041 4 2023-03-27 14:50:26 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 258f5286-c700-4571-9f69-eb6888b6ec04 0xc004094f67 0xc004094f68}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:50:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"258f5286-c700-4571-9f69-eb6888b6ec04\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:50:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004094ff0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Mar 27 14:50:31.834: INFO: pod: "test-deployment-54cc775c4b-d7cqz":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-d7cqz test-deployment-54cc775c4b- deployment-2074  7a80427f-2ae9-4eac-9426-f8608ec4b8ce 31020 0 2023-03-27 14:50:28 +0000 UTC 2023-03-27 14:50:31 +0000 UTC 0xc0040954e8 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:941ea4c072d9a41205d476730a73c6b1a8ec867a23608a29795cd3347e1b83c8 cni.projectcalico.org/podIP:172.25.1.140/32 cni.projectcalico.org/podIPs:172.25.1.140/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b f4a4c0f0-cfd0-4954-9638-86b999bc7984 0xc004095537 0xc004095538}] [] [{kube-controller-manager Update v1 2023-03-27 14:50:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a4c0f0-cfd0-4954-9638-86b999bc7984\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:50:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:50:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9gfwl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9gfwl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.140,StartTime:2023-03-27 14:50:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:50:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://72e16579957b43aae3704186ad004061891257e205be72286c15af01cf370ebd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 27 14:50:31.834: INFO: pod: "test-deployment-54cc775c4b-rv9nc":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-rv9nc test-deployment-54cc775c4b- deployment-2074  6e11ab2b-0016-40c1-9538-a00769cb7a35 31037 0 2023-03-27 14:50:26 +0000 UTC 2023-03-27 14:50:32 +0000 UTC 0xc004095710 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:5de0a99eb853cada8a13498367eba5a8cc4db99ab4dd816bb0bb518514aa9c14 cni.projectcalico.org/podIP:172.25.2.206/32 cni.projectcalico.org/podIPs:172.25.2.206/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b f4a4c0f0-cfd0-4954-9638-86b999bc7984 0xc004095767 0xc004095768}] [] [{kube-controller-manager Update v1 2023-03-27 14:50:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a4c0f0-cfd0-4954-9638-86b999bc7984\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:50:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:50:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8ndgz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8ndgz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:172.25.2.206,StartTime:2023-03-27 14:50:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:50:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://fc5bd1489791098f9070642040129df8da184679378ccc9966bf1a81fb31c166,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 27 14:50:31.834: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-2074  377e7da4-0a32-4c36-9c37-c14b1b931e48 31033 2 2023-03-27 14:50:28 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 258f5286-c700-4571-9f69-eb6888b6ec04 0xc004095057 0xc004095058}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:50:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"258f5286-c700-4571-9f69-eb6888b6ec04\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:50:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040950e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Mar 27 14:50:31.840: INFO: pod: "test-deployment-7c7d8d58c8-jwx4s":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-jwx4s test-deployment-7c7d8d58c8- deployment-2074  6810f0a0-6b4b-40d0-b924-c9808d53f876 30993 0 2023-03-27 14:50:28 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:10807c9f089166368950ebce249037831a98e13ab588c89fb190cc64b9efc7f4 cni.projectcalico.org/podIP:172.25.2.207/32 cni.projectcalico.org/podIPs:172.25.2.207/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 377e7da4-0a32-4c36-9c37-c14b1b931e48 0xc003fcabc7 0xc003fcabc8}] [] [{kube-controller-manager Update v1 2023-03-27 14:50:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"377e7da4-0a32-4c36-9c37-c14b1b931e48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:50:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:50:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qhrxl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qhrxl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:172.25.2.207,StartTime:2023-03-27 14:50:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:50:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6dc443ba9529b7c5ac3c5f8326df6eddc664d3081e1717b6b2d68e4aa7b1c1d5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 27 14:50:31.841: INFO: pod: "test-deployment-7c7d8d58c8-psfnj":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-psfnj test-deployment-7c7d8d58c8- deployment-2074  d126aa67-3ad5-4ed5-ada1-06af61907511 31032 0 2023-03-27 14:50:30 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:1c846b1c8b1f56d2f7662f60c9c9f66781e4f7f014359b35a722cc6f00b6ce31 cni.projectcalico.org/podIP:172.25.1.141/32 cni.projectcalico.org/podIPs:172.25.1.141/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 377e7da4-0a32-4c36-9c37-c14b1b931e48 0xc003fcade7 0xc003fcade8}] [] [{kube-controller-manager Update v1 2023-03-27 14:50:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"377e7da4-0a32-4c36-9c37-c14b1b931e48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 14:50:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 14:50:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p22b4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p22b4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 14:50:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.141,StartTime:2023-03-27 14:50:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 14:50:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a3013987095ed40f3a921c7f08e67b66bb20ef1fac0ac1b14dc48072232b6f8d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.141,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 27 14:50:31.841: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-2074  447f0c64-f10c-448f-abed-d80a01785119 30934 3 2023-03-27 14:50:24 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 258f5286-c700-4571-9f69-eb6888b6ec04 0xc004095147 0xc004095148}] [] [{kube-controller-manager Update apps/v1 2023-03-27 14:50:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"258f5286-c700-4571-9f69-eb6888b6ec04\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 14:50:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040951d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 27 14:50:31.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2074" for this suite. 03/27/23 14:50:31.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:50:31.864
Mar 27 14:50:31.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename dns 03/27/23 14:50:31.866
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:50:31.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:50:31.884
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 03/27/23 14:50:31.887
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5972 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5972;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5972 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5972;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5972.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5972.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5972.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5972.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5972.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5972.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5972.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5972.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5972.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5972.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5972.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5972.svc;check="$$(dig +notcp +noall +answer +search 182.30.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.30.182_udp@PTR;check="$$(dig +tcp +noall +answer +search 182.30.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.30.182_tcp@PTR;sleep 1; done
 03/27/23 14:50:31.908
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5972 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5972;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5972 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5972;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5972.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5972.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5972.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5972.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5972.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5972.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5972.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5972.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5972.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5972.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5972.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5972.svc;check="$$(dig +notcp +noall +answer +search 182.30.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.30.182_udp@PTR;check="$$(dig +tcp +noall +answer +search 182.30.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.30.182_tcp@PTR;sleep 1; done
 03/27/23 14:50:31.908
STEP: creating a pod to probe DNS 03/27/23 14:50:31.908
STEP: submitting the pod to kubernetes 03/27/23 14:50:31.908
Mar 27 14:50:31.922: INFO: Waiting up to 15m0s for pod "dns-test-795b9f95-0392-4f60-aec9-976ed101b27e" in namespace "dns-5972" to be "running"
Mar 27 14:50:31.927: INFO: Pod "dns-test-795b9f95-0392-4f60-aec9-976ed101b27e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.815537ms
Mar 27 14:50:33.931: INFO: Pod "dns-test-795b9f95-0392-4f60-aec9-976ed101b27e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008215821s
Mar 27 14:50:35.932: INFO: Pod "dns-test-795b9f95-0392-4f60-aec9-976ed101b27e": Phase="Running", Reason="", readiness=true. Elapsed: 4.009510589s
Mar 27 14:50:35.932: INFO: Pod "dns-test-795b9f95-0392-4f60-aec9-976ed101b27e" satisfied condition "running"
STEP: retrieving the pod 03/27/23 14:50:35.932
STEP: looking for the results for each expected name from probers 03/27/23 14:50:36.117
Mar 27 14:50:36.127: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
Mar 27 14:50:36.133: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
Mar 27 14:50:36.543: INFO: Unable to read wheezy_udp@dns-test-service.dns-5972 from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
Mar 27 14:50:36.548: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5972 from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
Mar 27 14:50:36.552: INFO: Unable to read wheezy_udp@dns-test-service.dns-5972.svc from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
Mar 27 14:50:36.557: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5972.svc from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
Mar 27 14:50:36.563: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5972.svc from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
Mar 27 14:50:36.569: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5972.svc from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
Mar 27 14:50:36.656: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
Mar 27 14:50:36.663: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
Mar 27 14:50:36.679: INFO: Unable to read jessie_udp@dns-test-service.dns-5972 from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
Mar 27 14:50:36.684: INFO: Unable to read jessie_tcp@dns-test-service.dns-5972 from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
Mar 27 14:50:36.689: INFO: Unable to read jessie_udp@dns-test-service.dns-5972.svc from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
Mar 27 14:50:36.693: INFO: Unable to read jessie_tcp@dns-test-service.dns-5972.svc from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
Mar 27 14:50:36.697: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5972.svc from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
Mar 27 14:50:36.702: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5972.svc from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
Mar 27 14:50:36.718: INFO: Lookups using dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5972 wheezy_tcp@dns-test-service.dns-5972 wheezy_udp@dns-test-service.dns-5972.svc wheezy_tcp@dns-test-service.dns-5972.svc wheezy_udp@_http._tcp.dns-test-service.dns-5972.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5972.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5972 jessie_tcp@dns-test-service.dns-5972 jessie_udp@dns-test-service.dns-5972.svc jessie_tcp@dns-test-service.dns-5972.svc jessie_udp@_http._tcp.dns-test-service.dns-5972.svc jessie_tcp@_http._tcp.dns-test-service.dns-5972.svc]

Mar 27 14:50:41.846: INFO: DNS probes using dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e succeeded

STEP: deleting the pod 03/27/23 14:50:41.846
STEP: deleting the test service 03/27/23 14:50:41.886
STEP: deleting the test headless service 03/27/23 14:50:41.907
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 27 14:50:41.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5972" for this suite. 03/27/23 14:50:41.926
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":152,"skipped":2806,"failed":0}
------------------------------
• [SLOW TEST] [10.067 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:50:31.864
    Mar 27 14:50:31.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename dns 03/27/23 14:50:31.866
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:50:31.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:50:31.884
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 03/27/23 14:50:31.887
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5972 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5972;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5972 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5972;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5972.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5972.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5972.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5972.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5972.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5972.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5972.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5972.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5972.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5972.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5972.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5972.svc;check="$$(dig +notcp +noall +answer +search 182.30.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.30.182_udp@PTR;check="$$(dig +tcp +noall +answer +search 182.30.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.30.182_tcp@PTR;sleep 1; done
     03/27/23 14:50:31.908
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5972 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5972;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5972 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5972;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5972.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5972.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5972.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5972.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5972.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5972.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5972.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5972.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5972.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5972.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5972.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5972.svc;check="$$(dig +notcp +noall +answer +search 182.30.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.30.182_udp@PTR;check="$$(dig +tcp +noall +answer +search 182.30.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.30.182_tcp@PTR;sleep 1; done
     03/27/23 14:50:31.908
    STEP: creating a pod to probe DNS 03/27/23 14:50:31.908
    STEP: submitting the pod to kubernetes 03/27/23 14:50:31.908
    Mar 27 14:50:31.922: INFO: Waiting up to 15m0s for pod "dns-test-795b9f95-0392-4f60-aec9-976ed101b27e" in namespace "dns-5972" to be "running"
    Mar 27 14:50:31.927: INFO: Pod "dns-test-795b9f95-0392-4f60-aec9-976ed101b27e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.815537ms
    Mar 27 14:50:33.931: INFO: Pod "dns-test-795b9f95-0392-4f60-aec9-976ed101b27e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008215821s
    Mar 27 14:50:35.932: INFO: Pod "dns-test-795b9f95-0392-4f60-aec9-976ed101b27e": Phase="Running", Reason="", readiness=true. Elapsed: 4.009510589s
    Mar 27 14:50:35.932: INFO: Pod "dns-test-795b9f95-0392-4f60-aec9-976ed101b27e" satisfied condition "running"
    STEP: retrieving the pod 03/27/23 14:50:35.932
    STEP: looking for the results for each expected name from probers 03/27/23 14:50:36.117
    Mar 27 14:50:36.127: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
    Mar 27 14:50:36.133: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
    Mar 27 14:50:36.543: INFO: Unable to read wheezy_udp@dns-test-service.dns-5972 from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
    Mar 27 14:50:36.548: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5972 from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
    Mar 27 14:50:36.552: INFO: Unable to read wheezy_udp@dns-test-service.dns-5972.svc from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
    Mar 27 14:50:36.557: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5972.svc from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
    Mar 27 14:50:36.563: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5972.svc from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
    Mar 27 14:50:36.569: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5972.svc from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
    Mar 27 14:50:36.656: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
    Mar 27 14:50:36.663: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
    Mar 27 14:50:36.679: INFO: Unable to read jessie_udp@dns-test-service.dns-5972 from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
    Mar 27 14:50:36.684: INFO: Unable to read jessie_tcp@dns-test-service.dns-5972 from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
    Mar 27 14:50:36.689: INFO: Unable to read jessie_udp@dns-test-service.dns-5972.svc from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
    Mar 27 14:50:36.693: INFO: Unable to read jessie_tcp@dns-test-service.dns-5972.svc from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
    Mar 27 14:50:36.697: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5972.svc from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
    Mar 27 14:50:36.702: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5972.svc from pod dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e: the server could not find the requested resource (get pods dns-test-795b9f95-0392-4f60-aec9-976ed101b27e)
    Mar 27 14:50:36.718: INFO: Lookups using dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5972 wheezy_tcp@dns-test-service.dns-5972 wheezy_udp@dns-test-service.dns-5972.svc wheezy_tcp@dns-test-service.dns-5972.svc wheezy_udp@_http._tcp.dns-test-service.dns-5972.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5972.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5972 jessie_tcp@dns-test-service.dns-5972 jessie_udp@dns-test-service.dns-5972.svc jessie_tcp@dns-test-service.dns-5972.svc jessie_udp@_http._tcp.dns-test-service.dns-5972.svc jessie_tcp@_http._tcp.dns-test-service.dns-5972.svc]

    Mar 27 14:50:41.846: INFO: DNS probes using dns-5972/dns-test-795b9f95-0392-4f60-aec9-976ed101b27e succeeded

    STEP: deleting the pod 03/27/23 14:50:41.846
    STEP: deleting the test service 03/27/23 14:50:41.886
    STEP: deleting the test headless service 03/27/23 14:50:41.907
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 27 14:50:41.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5972" for this suite. 03/27/23 14:50:41.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:50:41.933
Mar 27 14:50:41.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename job 03/27/23 14:50:41.934
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:50:41.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:50:41.961
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 03/27/23 14:50:41.964
STEP: Ensuring active pods == parallelism 03/27/23 14:50:41.972
STEP: delete a job 03/27/23 14:50:45.981
STEP: deleting Job.batch foo in namespace job-7322, will wait for the garbage collector to delete the pods 03/27/23 14:50:45.981
Mar 27 14:50:46.042: INFO: Deleting Job.batch foo took: 8.976987ms
Mar 27 14:50:46.144: INFO: Terminating Job.batch foo pods took: 101.036828ms
STEP: Ensuring job was deleted 03/27/23 14:51:18.144
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar 27 14:51:18.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7322" for this suite. 03/27/23 14:51:18.151
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":153,"skipped":2814,"failed":0}
------------------------------
• [SLOW TEST] [36.227 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:50:41.933
    Mar 27 14:50:41.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename job 03/27/23 14:50:41.934
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:50:41.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:50:41.961
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 03/27/23 14:50:41.964
    STEP: Ensuring active pods == parallelism 03/27/23 14:50:41.972
    STEP: delete a job 03/27/23 14:50:45.981
    STEP: deleting Job.batch foo in namespace job-7322, will wait for the garbage collector to delete the pods 03/27/23 14:50:45.981
    Mar 27 14:50:46.042: INFO: Deleting Job.batch foo took: 8.976987ms
    Mar 27 14:50:46.144: INFO: Terminating Job.batch foo pods took: 101.036828ms
    STEP: Ensuring job was deleted 03/27/23 14:51:18.144
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar 27 14:51:18.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7322" for this suite. 03/27/23 14:51:18.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:51:18.161
Mar 27 14:51:18.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename statefulset 03/27/23 14:51:18.162
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:51:18.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:51:18.188
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8498 03/27/23 14:51:18.191
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-8498 03/27/23 14:51:18.207
Mar 27 14:51:18.217: INFO: Found 0 stateful pods, waiting for 1
Mar 27 14:51:28.222: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 03/27/23 14:51:28.227
STEP: Getting /status 03/27/23 14:51:28.24
Mar 27 14:51:28.243: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 03/27/23 14:51:28.243
Mar 27 14:51:28.252: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 03/27/23 14:51:28.252
Mar 27 14:51:28.254: INFO: Observed &StatefulSet event: ADDED
Mar 27 14:51:28.254: INFO: Found Statefulset ss in namespace statefulset-8498 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 27 14:51:28.254: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 03/27/23 14:51:28.254
Mar 27 14:51:28.254: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 27 14:51:28.262: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 03/27/23 14:51:28.262
Mar 27 14:51:28.264: INFO: Observed &StatefulSet event: ADDED
Mar 27 14:51:28.265: INFO: Observed Statefulset ss in namespace statefulset-8498 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 27 14:51:28.265: INFO: Observed &StatefulSet event: MODIFIED
Mar 27 14:51:28.265: INFO: Found Statefulset ss in namespace statefulset-8498 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 27 14:51:28.265: INFO: Deleting all statefulset in ns statefulset-8498
Mar 27 14:51:28.268: INFO: Scaling statefulset ss to 0
Mar 27 14:51:38.283: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 14:51:38.288: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 27 14:51:38.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8498" for this suite. 03/27/23 14:51:38.47
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":154,"skipped":2826,"failed":0}
------------------------------
• [SLOW TEST] [20.315 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:51:18.161
    Mar 27 14:51:18.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename statefulset 03/27/23 14:51:18.162
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:51:18.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:51:18.188
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8498 03/27/23 14:51:18.191
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-8498 03/27/23 14:51:18.207
    Mar 27 14:51:18.217: INFO: Found 0 stateful pods, waiting for 1
    Mar 27 14:51:28.222: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 03/27/23 14:51:28.227
    STEP: Getting /status 03/27/23 14:51:28.24
    Mar 27 14:51:28.243: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 03/27/23 14:51:28.243
    Mar 27 14:51:28.252: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 03/27/23 14:51:28.252
    Mar 27 14:51:28.254: INFO: Observed &StatefulSet event: ADDED
    Mar 27 14:51:28.254: INFO: Found Statefulset ss in namespace statefulset-8498 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 27 14:51:28.254: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 03/27/23 14:51:28.254
    Mar 27 14:51:28.254: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 27 14:51:28.262: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 03/27/23 14:51:28.262
    Mar 27 14:51:28.264: INFO: Observed &StatefulSet event: ADDED
    Mar 27 14:51:28.265: INFO: Observed Statefulset ss in namespace statefulset-8498 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 27 14:51:28.265: INFO: Observed &StatefulSet event: MODIFIED
    Mar 27 14:51:28.265: INFO: Found Statefulset ss in namespace statefulset-8498 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 27 14:51:28.265: INFO: Deleting all statefulset in ns statefulset-8498
    Mar 27 14:51:28.268: INFO: Scaling statefulset ss to 0
    Mar 27 14:51:38.283: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 14:51:38.288: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 27 14:51:38.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8498" for this suite. 03/27/23 14:51:38.47
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:51:38.477
Mar 27 14:51:38.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename sched-preemption 03/27/23 14:51:38.478
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:51:38.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:51:38.702
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar 27 14:51:38.968: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 27 14:52:39.006: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 03/27/23 14:52:39.009
Mar 27 14:52:39.033: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 27 14:52:39.041: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 27 14:52:39.063: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 27 14:52:39.073: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar 27 14:52:39.106: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar 27 14:52:39.113: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/27/23 14:52:39.113
Mar 27 14:52:39.113: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3553" to be "running"
Mar 27 14:52:39.116: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.706627ms
Mar 27 14:52:41.122: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008429607s
Mar 27 14:52:43.120: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006375965s
Mar 27 14:52:45.120: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00674636s
Mar 27 14:52:47.121: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.008071719s
Mar 27 14:52:47.121: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar 27 14:52:47.122: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3553" to be "running"
Mar 27 14:52:47.125: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.406793ms
Mar 27 14:52:47.125: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 27 14:52:47.125: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3553" to be "running"
Mar 27 14:52:47.127: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.246853ms
Mar 27 14:52:49.133: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007550354s
Mar 27 14:52:51.134: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.008630144s
Mar 27 14:52:51.134: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 27 14:52:51.134: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3553" to be "running"
Mar 27 14:52:51.136: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.719686ms
Mar 27 14:52:51.136: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 27 14:52:51.136: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3553" to be "running"
Mar 27 14:52:51.140: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.542021ms
Mar 27 14:52:51.140: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 27 14:52:51.140: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3553" to be "running"
Mar 27 14:52:51.144: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.665915ms
Mar 27 14:52:51.144: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 03/27/23 14:52:51.144
Mar 27 14:52:51.153: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Mar 27 14:52:51.156: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.289955ms
Mar 27 14:52:53.161: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00788094s
Mar 27 14:52:55.161: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008046999s
Mar 27 14:52:55.161: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar 27 14:52:55.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3553" for this suite. 03/27/23 14:52:55.204
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":155,"skipped":2827,"failed":0}
------------------------------
• [SLOW TEST] [76.778 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:51:38.477
    Mar 27 14:51:38.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename sched-preemption 03/27/23 14:51:38.478
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:51:38.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:51:38.702
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar 27 14:51:38.968: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 27 14:52:39.006: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 03/27/23 14:52:39.009
    Mar 27 14:52:39.033: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar 27 14:52:39.041: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar 27 14:52:39.063: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar 27 14:52:39.073: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar 27 14:52:39.106: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar 27 14:52:39.113: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/27/23 14:52:39.113
    Mar 27 14:52:39.113: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3553" to be "running"
    Mar 27 14:52:39.116: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.706627ms
    Mar 27 14:52:41.122: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008429607s
    Mar 27 14:52:43.120: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006375965s
    Mar 27 14:52:45.120: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00674636s
    Mar 27 14:52:47.121: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.008071719s
    Mar 27 14:52:47.121: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar 27 14:52:47.122: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3553" to be "running"
    Mar 27 14:52:47.125: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.406793ms
    Mar 27 14:52:47.125: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 27 14:52:47.125: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3553" to be "running"
    Mar 27 14:52:47.127: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.246853ms
    Mar 27 14:52:49.133: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007550354s
    Mar 27 14:52:51.134: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.008630144s
    Mar 27 14:52:51.134: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 27 14:52:51.134: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3553" to be "running"
    Mar 27 14:52:51.136: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.719686ms
    Mar 27 14:52:51.136: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 27 14:52:51.136: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3553" to be "running"
    Mar 27 14:52:51.140: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.542021ms
    Mar 27 14:52:51.140: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 27 14:52:51.140: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3553" to be "running"
    Mar 27 14:52:51.144: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.665915ms
    Mar 27 14:52:51.144: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 03/27/23 14:52:51.144
    Mar 27 14:52:51.153: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Mar 27 14:52:51.156: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.289955ms
    Mar 27 14:52:53.161: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00788094s
    Mar 27 14:52:55.161: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008046999s
    Mar 27 14:52:55.161: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 14:52:55.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-3553" for this suite. 03/27/23 14:52:55.204
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:52:55.256
Mar 27 14:52:55.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename resourcequota 03/27/23 14:52:55.258
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:52:55.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:52:55.279
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 03/27/23 14:52:55.282
STEP: Getting a ResourceQuota 03/27/23 14:52:55.288
STEP: Updating a ResourceQuota 03/27/23 14:52:55.29
STEP: Verifying a ResourceQuota was modified 03/27/23 14:52:55.318
STEP: Deleting a ResourceQuota 03/27/23 14:52:55.321
STEP: Verifying the deleted ResourceQuota 03/27/23 14:52:55.331
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 27 14:52:55.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4354" for this suite. 03/27/23 14:52:55.337
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":156,"skipped":2831,"failed":0}
------------------------------
• [0.087 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:52:55.256
    Mar 27 14:52:55.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename resourcequota 03/27/23 14:52:55.258
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:52:55.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:52:55.279
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 03/27/23 14:52:55.282
    STEP: Getting a ResourceQuota 03/27/23 14:52:55.288
    STEP: Updating a ResourceQuota 03/27/23 14:52:55.29
    STEP: Verifying a ResourceQuota was modified 03/27/23 14:52:55.318
    STEP: Deleting a ResourceQuota 03/27/23 14:52:55.321
    STEP: Verifying the deleted ResourceQuota 03/27/23 14:52:55.331
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 27 14:52:55.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4354" for this suite. 03/27/23 14:52:55.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:52:55.345
Mar 27 14:52:55.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename container-lifecycle-hook 03/27/23 14:52:55.346
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:52:55.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:52:55.373
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/27/23 14:52:55.379
Mar 27 14:52:55.388: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7165" to be "running and ready"
Mar 27 14:52:55.390: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.433899ms
Mar 27 14:52:55.390: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:52:57.394: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006165424s
Mar 27 14:52:57.394: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 27 14:52:57.394: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 03/27/23 14:52:57.397
Mar 27 14:52:57.407: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7165" to be "running and ready"
Mar 27 14:52:57.409: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.64944ms
Mar 27 14:52:57.409: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:52:59.418: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011534709s
Mar 27 14:52:59.418: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Mar 27 14:52:59.418: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/27/23 14:52:59.421
Mar 27 14:52:59.437: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 27 14:52:59.441: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 27 14:53:01.442: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 27 14:53:01.446: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 27 14:53:03.442: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 27 14:53:03.446: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 03/27/23 14:53:03.446
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar 27 14:53:03.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7165" for this suite. 03/27/23 14:53:03.461
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":157,"skipped":2873,"failed":0}
------------------------------
• [SLOW TEST] [8.123 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:52:55.345
    Mar 27 14:52:55.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/27/23 14:52:55.346
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:52:55.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:52:55.373
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/27/23 14:52:55.379
    Mar 27 14:52:55.388: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7165" to be "running and ready"
    Mar 27 14:52:55.390: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.433899ms
    Mar 27 14:52:55.390: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:52:57.394: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006165424s
    Mar 27 14:52:57.394: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 27 14:52:57.394: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 03/27/23 14:52:57.397
    Mar 27 14:52:57.407: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7165" to be "running and ready"
    Mar 27 14:52:57.409: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.64944ms
    Mar 27 14:52:57.409: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:52:59.418: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011534709s
    Mar 27 14:52:59.418: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Mar 27 14:52:59.418: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/27/23 14:52:59.421
    Mar 27 14:52:59.437: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 27 14:52:59.441: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar 27 14:53:01.442: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 27 14:53:01.446: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar 27 14:53:03.442: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 27 14:53:03.446: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 03/27/23 14:53:03.446
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar 27 14:53:03.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-7165" for this suite. 03/27/23 14:53:03.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:53:03.469
Mar 27 14:53:03.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename watch 03/27/23 14:53:03.47
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:53:03.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:53:03.493
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 03/27/23 14:53:03.496
STEP: creating a new configmap 03/27/23 14:53:03.497
STEP: modifying the configmap once 03/27/23 14:53:03.501
STEP: changing the label value of the configmap 03/27/23 14:53:03.513
STEP: Expecting to observe a delete notification for the watched object 03/27/23 14:53:03.521
Mar 27 14:53:03.521: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-911  a2e3a668-7ee9-4fdf-b690-433319165a05 32282 0 2023-03-27 14:53:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 14:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 14:53:03.521: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-911  a2e3a668-7ee9-4fdf-b690-433319165a05 32284 0 2023-03-27 14:53:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 14:53:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 14:53:03.522: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-911  a2e3a668-7ee9-4fdf-b690-433319165a05 32285 0 2023-03-27 14:53:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 14:53:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 03/27/23 14:53:03.522
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/27/23 14:53:03.53
STEP: changing the label value of the configmap back 03/27/23 14:53:13.53
STEP: modifying the configmap a third time 03/27/23 14:53:13.542
STEP: deleting the configmap 03/27/23 14:53:13.549
STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/27/23 14:53:13.555
Mar 27 14:53:13.555: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-911  a2e3a668-7ee9-4fdf-b690-433319165a05 32354 0 2023-03-27 14:53:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 14:53:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 14:53:13.555: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-911  a2e3a668-7ee9-4fdf-b690-433319165a05 32355 0 2023-03-27 14:53:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 14:53:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 14:53:13.555: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-911  a2e3a668-7ee9-4fdf-b690-433319165a05 32356 0 2023-03-27 14:53:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 14:53:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar 27 14:53:13.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-911" for this suite. 03/27/23 14:53:13.56
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":158,"skipped":2892,"failed":0}
------------------------------
• [SLOW TEST] [10.103 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:53:03.469
    Mar 27 14:53:03.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename watch 03/27/23 14:53:03.47
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:53:03.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:53:03.493
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 03/27/23 14:53:03.496
    STEP: creating a new configmap 03/27/23 14:53:03.497
    STEP: modifying the configmap once 03/27/23 14:53:03.501
    STEP: changing the label value of the configmap 03/27/23 14:53:03.513
    STEP: Expecting to observe a delete notification for the watched object 03/27/23 14:53:03.521
    Mar 27 14:53:03.521: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-911  a2e3a668-7ee9-4fdf-b690-433319165a05 32282 0 2023-03-27 14:53:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 14:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 14:53:03.521: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-911  a2e3a668-7ee9-4fdf-b690-433319165a05 32284 0 2023-03-27 14:53:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 14:53:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 14:53:03.522: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-911  a2e3a668-7ee9-4fdf-b690-433319165a05 32285 0 2023-03-27 14:53:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 14:53:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 03/27/23 14:53:03.522
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/27/23 14:53:03.53
    STEP: changing the label value of the configmap back 03/27/23 14:53:13.53
    STEP: modifying the configmap a third time 03/27/23 14:53:13.542
    STEP: deleting the configmap 03/27/23 14:53:13.549
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/27/23 14:53:13.555
    Mar 27 14:53:13.555: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-911  a2e3a668-7ee9-4fdf-b690-433319165a05 32354 0 2023-03-27 14:53:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 14:53:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 14:53:13.555: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-911  a2e3a668-7ee9-4fdf-b690-433319165a05 32355 0 2023-03-27 14:53:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 14:53:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 14:53:13.555: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-911  a2e3a668-7ee9-4fdf-b690-433319165a05 32356 0 2023-03-27 14:53:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 14:53:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar 27 14:53:13.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-911" for this suite. 03/27/23 14:53:13.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:53:13.573
Mar 27 14:53:13.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename svcaccounts 03/27/23 14:53:13.574
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:53:13.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:53:13.609
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 03/27/23 14:53:13.611
STEP: watching for the ServiceAccount to be added 03/27/23 14:53:13.624
STEP: patching the ServiceAccount 03/27/23 14:53:13.626
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/27/23 14:53:13.634
STEP: deleting the ServiceAccount 03/27/23 14:53:13.637
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar 27 14:53:13.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5681" for this suite. 03/27/23 14:53:13.659
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":159,"skipped":2899,"failed":0}
------------------------------
• [0.096 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:53:13.573
    Mar 27 14:53:13.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename svcaccounts 03/27/23 14:53:13.574
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:53:13.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:53:13.609
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 03/27/23 14:53:13.611
    STEP: watching for the ServiceAccount to be added 03/27/23 14:53:13.624
    STEP: patching the ServiceAccount 03/27/23 14:53:13.626
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/27/23 14:53:13.634
    STEP: deleting the ServiceAccount 03/27/23 14:53:13.637
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar 27 14:53:13.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5681" for this suite. 03/27/23 14:53:13.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:53:13.671
Mar 27 14:53:13.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename services 03/27/23 14:53:13.673
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:53:13.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:53:13.7
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 27 14:53:13.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9991" for this suite. 03/27/23 14:53:13.712
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":160,"skipped":2912,"failed":0}
------------------------------
• [0.050 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:53:13.671
    Mar 27 14:53:13.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename services 03/27/23 14:53:13.673
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:53:13.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:53:13.7
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 27 14:53:13.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9991" for this suite. 03/27/23 14:53:13.712
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:53:13.722
Mar 27 14:53:13.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename security-context-test 03/27/23 14:53:13.723
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:53:13.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:53:13.746
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Mar 27 14:53:13.761: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6cb351cf-a5b4-495b-bf37-944301dc6985" in namespace "security-context-test-9992" to be "Succeeded or Failed"
Mar 27 14:53:13.764: INFO: Pod "busybox-privileged-false-6cb351cf-a5b4-495b-bf37-944301dc6985": Phase="Pending", Reason="", readiness=false. Elapsed: 2.826455ms
Mar 27 14:53:15.771: INFO: Pod "busybox-privileged-false-6cb351cf-a5b4-495b-bf37-944301dc6985": Phase="Running", Reason="", readiness=true. Elapsed: 2.009901992s
Mar 27 14:53:17.769: INFO: Pod "busybox-privileged-false-6cb351cf-a5b4-495b-bf37-944301dc6985": Phase="Running", Reason="", readiness=false. Elapsed: 4.008216147s
Mar 27 14:53:19.768: INFO: Pod "busybox-privileged-false-6cb351cf-a5b4-495b-bf37-944301dc6985": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007262153s
Mar 27 14:53:19.768: INFO: Pod "busybox-privileged-false-6cb351cf-a5b4-495b-bf37-944301dc6985" satisfied condition "Succeeded or Failed"
Mar 27 14:53:19.776: INFO: Got logs for pod "busybox-privileged-false-6cb351cf-a5b4-495b-bf37-944301dc6985": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar 27 14:53:19.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9992" for this suite. 03/27/23 14:53:19.781
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":161,"skipped":2922,"failed":0}
------------------------------
• [SLOW TEST] [6.065 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:53:13.722
    Mar 27 14:53:13.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename security-context-test 03/27/23 14:53:13.723
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:53:13.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:53:13.746
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Mar 27 14:53:13.761: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6cb351cf-a5b4-495b-bf37-944301dc6985" in namespace "security-context-test-9992" to be "Succeeded or Failed"
    Mar 27 14:53:13.764: INFO: Pod "busybox-privileged-false-6cb351cf-a5b4-495b-bf37-944301dc6985": Phase="Pending", Reason="", readiness=false. Elapsed: 2.826455ms
    Mar 27 14:53:15.771: INFO: Pod "busybox-privileged-false-6cb351cf-a5b4-495b-bf37-944301dc6985": Phase="Running", Reason="", readiness=true. Elapsed: 2.009901992s
    Mar 27 14:53:17.769: INFO: Pod "busybox-privileged-false-6cb351cf-a5b4-495b-bf37-944301dc6985": Phase="Running", Reason="", readiness=false. Elapsed: 4.008216147s
    Mar 27 14:53:19.768: INFO: Pod "busybox-privileged-false-6cb351cf-a5b4-495b-bf37-944301dc6985": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007262153s
    Mar 27 14:53:19.768: INFO: Pod "busybox-privileged-false-6cb351cf-a5b4-495b-bf37-944301dc6985" satisfied condition "Succeeded or Failed"
    Mar 27 14:53:19.776: INFO: Got logs for pod "busybox-privileged-false-6cb351cf-a5b4-495b-bf37-944301dc6985": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar 27 14:53:19.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-9992" for this suite. 03/27/23 14:53:19.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:53:19.789
Mar 27 14:53:19.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename pods 03/27/23 14:53:19.789
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:53:19.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:53:19.811
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 03/27/23 14:53:19.814
STEP: submitting the pod to kubernetes 03/27/23 14:53:19.815
STEP: verifying QOS class is set on the pod 03/27/23 14:53:19.824
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Mar 27 14:53:19.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3539" for this suite. 03/27/23 14:53:19.838
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":162,"skipped":2929,"failed":0}
------------------------------
• [0.267 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:53:19.789
    Mar 27 14:53:19.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename pods 03/27/23 14:53:19.789
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:53:19.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:53:19.811
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 03/27/23 14:53:19.814
    STEP: submitting the pod to kubernetes 03/27/23 14:53:19.815
    STEP: verifying QOS class is set on the pod 03/27/23 14:53:19.824
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Mar 27 14:53:19.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3539" for this suite. 03/27/23 14:53:19.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:53:20.06
Mar 27 14:53:20.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename resourcequota 03/27/23 14:53:20.061
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:53:20.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:53:20.096
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 03/27/23 14:53:37.103
STEP: Creating a ResourceQuota 03/27/23 14:53:42.108
STEP: Ensuring resource quota status is calculated 03/27/23 14:53:42.114
STEP: Creating a ConfigMap 03/27/23 14:53:44.118
STEP: Ensuring resource quota status captures configMap creation 03/27/23 14:53:44.134
STEP: Deleting a ConfigMap 03/27/23 14:53:46.144
STEP: Ensuring resource quota status released usage 03/27/23 14:53:46.15
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 27 14:53:48.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5726" for this suite. 03/27/23 14:53:48.16
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":163,"skipped":2997,"failed":0}
------------------------------
• [SLOW TEST] [28.106 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:53:20.06
    Mar 27 14:53:20.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename resourcequota 03/27/23 14:53:20.061
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:53:20.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:53:20.096
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 03/27/23 14:53:37.103
    STEP: Creating a ResourceQuota 03/27/23 14:53:42.108
    STEP: Ensuring resource quota status is calculated 03/27/23 14:53:42.114
    STEP: Creating a ConfigMap 03/27/23 14:53:44.118
    STEP: Ensuring resource quota status captures configMap creation 03/27/23 14:53:44.134
    STEP: Deleting a ConfigMap 03/27/23 14:53:46.144
    STEP: Ensuring resource quota status released usage 03/27/23 14:53:46.15
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 27 14:53:48.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5726" for this suite. 03/27/23 14:53:48.16
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:53:48.166
Mar 27 14:53:48.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 14:53:48.168
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:53:48.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:53:48.196
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Mar 27 14:53:48.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/27/23 14:53:50.701
Mar 27 14:53:50.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-1933 --namespace=crd-publish-openapi-1933 create -f -'
Mar 27 14:53:51.334: INFO: stderr: ""
Mar 27 14:53:51.334: INFO: stdout: "e2e-test-crd-publish-openapi-7399-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 27 14:53:51.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-1933 --namespace=crd-publish-openapi-1933 delete e2e-test-crd-publish-openapi-7399-crds test-cr'
Mar 27 14:53:51.447: INFO: stderr: ""
Mar 27 14:53:51.447: INFO: stdout: "e2e-test-crd-publish-openapi-7399-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 27 14:53:51.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-1933 --namespace=crd-publish-openapi-1933 apply -f -'
Mar 27 14:53:51.636: INFO: stderr: ""
Mar 27 14:53:51.636: INFO: stdout: "e2e-test-crd-publish-openapi-7399-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 27 14:53:51.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-1933 --namespace=crd-publish-openapi-1933 delete e2e-test-crd-publish-openapi-7399-crds test-cr'
Mar 27 14:53:51.701: INFO: stderr: ""
Mar 27 14:53:51.701: INFO: stdout: "e2e-test-crd-publish-openapi-7399-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 03/27/23 14:53:51.701
Mar 27 14:53:51.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-1933 explain e2e-test-crd-publish-openapi-7399-crds'
Mar 27 14:53:51.915: INFO: stderr: ""
Mar 27 14:53:51.916: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7399-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:53:55.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1933" for this suite. 03/27/23 14:53:55.469
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":164,"skipped":2998,"failed":0}
------------------------------
• [SLOW TEST] [7.316 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:53:48.166
    Mar 27 14:53:48.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 14:53:48.168
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:53:48.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:53:48.196
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Mar 27 14:53:48.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/27/23 14:53:50.701
    Mar 27 14:53:50.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-1933 --namespace=crd-publish-openapi-1933 create -f -'
    Mar 27 14:53:51.334: INFO: stderr: ""
    Mar 27 14:53:51.334: INFO: stdout: "e2e-test-crd-publish-openapi-7399-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar 27 14:53:51.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-1933 --namespace=crd-publish-openapi-1933 delete e2e-test-crd-publish-openapi-7399-crds test-cr'
    Mar 27 14:53:51.447: INFO: stderr: ""
    Mar 27 14:53:51.447: INFO: stdout: "e2e-test-crd-publish-openapi-7399-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Mar 27 14:53:51.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-1933 --namespace=crd-publish-openapi-1933 apply -f -'
    Mar 27 14:53:51.636: INFO: stderr: ""
    Mar 27 14:53:51.636: INFO: stdout: "e2e-test-crd-publish-openapi-7399-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar 27 14:53:51.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-1933 --namespace=crd-publish-openapi-1933 delete e2e-test-crd-publish-openapi-7399-crds test-cr'
    Mar 27 14:53:51.701: INFO: stderr: ""
    Mar 27 14:53:51.701: INFO: stdout: "e2e-test-crd-publish-openapi-7399-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 03/27/23 14:53:51.701
    Mar 27 14:53:51.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-1933 explain e2e-test-crd-publish-openapi-7399-crds'
    Mar 27 14:53:51.915: INFO: stderr: ""
    Mar 27 14:53:51.916: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7399-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:53:55.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1933" for this suite. 03/27/23 14:53:55.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:53:55.483
Mar 27 14:53:55.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename svcaccounts 03/27/23 14:53:55.484
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:53:55.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:53:55.511
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Mar 27 14:53:55.534: INFO: created pod pod-service-account-defaultsa
Mar 27 14:53:55.534: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 27 14:53:55.545: INFO: created pod pod-service-account-mountsa
Mar 27 14:53:55.545: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 27 14:53:55.553: INFO: created pod pod-service-account-nomountsa
Mar 27 14:53:55.553: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 27 14:53:55.563: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 27 14:53:55.563: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 27 14:53:55.575: INFO: created pod pod-service-account-mountsa-mountspec
Mar 27 14:53:55.575: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 27 14:53:55.586: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 27 14:53:55.586: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 27 14:53:55.592: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 27 14:53:55.592: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 27 14:53:55.602: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 27 14:53:55.602: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 27 14:53:55.608: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 27 14:53:55.608: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar 27 14:53:55.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8918" for this suite. 03/27/23 14:53:55.62
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":165,"skipped":3009,"failed":0}
------------------------------
• [0.153 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:53:55.483
    Mar 27 14:53:55.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename svcaccounts 03/27/23 14:53:55.484
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:53:55.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:53:55.511
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Mar 27 14:53:55.534: INFO: created pod pod-service-account-defaultsa
    Mar 27 14:53:55.534: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Mar 27 14:53:55.545: INFO: created pod pod-service-account-mountsa
    Mar 27 14:53:55.545: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Mar 27 14:53:55.553: INFO: created pod pod-service-account-nomountsa
    Mar 27 14:53:55.553: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Mar 27 14:53:55.563: INFO: created pod pod-service-account-defaultsa-mountspec
    Mar 27 14:53:55.563: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Mar 27 14:53:55.575: INFO: created pod pod-service-account-mountsa-mountspec
    Mar 27 14:53:55.575: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Mar 27 14:53:55.586: INFO: created pod pod-service-account-nomountsa-mountspec
    Mar 27 14:53:55.586: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Mar 27 14:53:55.592: INFO: created pod pod-service-account-defaultsa-nomountspec
    Mar 27 14:53:55.592: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Mar 27 14:53:55.602: INFO: created pod pod-service-account-mountsa-nomountspec
    Mar 27 14:53:55.602: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Mar 27 14:53:55.608: INFO: created pod pod-service-account-nomountsa-nomountspec
    Mar 27 14:53:55.608: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar 27 14:53:55.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-8918" for this suite. 03/27/23 14:53:55.62
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:53:55.636
Mar 27 14:53:55.636: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 14:53:55.637
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:53:55.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:53:55.659
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Mar 27 14:53:55.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/27/23 14:53:58.18
Mar 27 14:53:58.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-7615 --namespace=crd-publish-openapi-7615 create -f -'
Mar 27 14:53:59.076: INFO: stderr: ""
Mar 27 14:53:59.076: INFO: stdout: "e2e-test-crd-publish-openapi-7121-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 27 14:53:59.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-7615 --namespace=crd-publish-openapi-7615 delete e2e-test-crd-publish-openapi-7121-crds test-cr'
Mar 27 14:53:59.164: INFO: stderr: ""
Mar 27 14:53:59.164: INFO: stdout: "e2e-test-crd-publish-openapi-7121-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 27 14:53:59.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-7615 --namespace=crd-publish-openapi-7615 apply -f -'
Mar 27 14:53:59.721: INFO: stderr: ""
Mar 27 14:53:59.721: INFO: stdout: "e2e-test-crd-publish-openapi-7121-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 27 14:53:59.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-7615 --namespace=crd-publish-openapi-7615 delete e2e-test-crd-publish-openapi-7121-crds test-cr'
Mar 27 14:53:59.795: INFO: stderr: ""
Mar 27 14:53:59.795: INFO: stdout: "e2e-test-crd-publish-openapi-7121-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/27/23 14:53:59.795
Mar 27 14:53:59.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-7615 explain e2e-test-crd-publish-openapi-7121-crds'
Mar 27 14:54:00.000: INFO: stderr: ""
Mar 27 14:54:00.000: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7121-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:54:02.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7615" for this suite. 03/27/23 14:54:02.567
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":166,"skipped":3009,"failed":0}
------------------------------
• [SLOW TEST] [6.947 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:53:55.636
    Mar 27 14:53:55.636: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 14:53:55.637
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:53:55.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:53:55.659
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Mar 27 14:53:55.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/27/23 14:53:58.18
    Mar 27 14:53:58.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-7615 --namespace=crd-publish-openapi-7615 create -f -'
    Mar 27 14:53:59.076: INFO: stderr: ""
    Mar 27 14:53:59.076: INFO: stdout: "e2e-test-crd-publish-openapi-7121-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar 27 14:53:59.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-7615 --namespace=crd-publish-openapi-7615 delete e2e-test-crd-publish-openapi-7121-crds test-cr'
    Mar 27 14:53:59.164: INFO: stderr: ""
    Mar 27 14:53:59.164: INFO: stdout: "e2e-test-crd-publish-openapi-7121-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Mar 27 14:53:59.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-7615 --namespace=crd-publish-openapi-7615 apply -f -'
    Mar 27 14:53:59.721: INFO: stderr: ""
    Mar 27 14:53:59.721: INFO: stdout: "e2e-test-crd-publish-openapi-7121-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar 27 14:53:59.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-7615 --namespace=crd-publish-openapi-7615 delete e2e-test-crd-publish-openapi-7121-crds test-cr'
    Mar 27 14:53:59.795: INFO: stderr: ""
    Mar 27 14:53:59.795: INFO: stdout: "e2e-test-crd-publish-openapi-7121-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/27/23 14:53:59.795
    Mar 27 14:53:59.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=crd-publish-openapi-7615 explain e2e-test-crd-publish-openapi-7121-crds'
    Mar 27 14:54:00.000: INFO: stderr: ""
    Mar 27 14:54:00.000: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7121-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:54:02.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7615" for this suite. 03/27/23 14:54:02.567
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:54:02.584
Mar 27 14:54:02.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 14:54:02.585
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:54:02.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:54:02.613
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-589e1c41-6375-4eeb-80f2-0657a3d21c3f 03/27/23 14:54:02.621
STEP: Creating a pod to test consume secrets 03/27/23 14:54:02.629
Mar 27 14:54:02.644: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8b59f765-f36a-446b-87e3-49f7c91199d3" in namespace "projected-6310" to be "Succeeded or Failed"
Mar 27 14:54:02.653: INFO: Pod "pod-projected-secrets-8b59f765-f36a-446b-87e3-49f7c91199d3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.57339ms
Mar 27 14:54:04.660: INFO: Pod "pod-projected-secrets-8b59f765-f36a-446b-87e3-49f7c91199d3": Phase="Running", Reason="", readiness=true. Elapsed: 2.016162222s
Mar 27 14:54:06.659: INFO: Pod "pod-projected-secrets-8b59f765-f36a-446b-87e3-49f7c91199d3": Phase="Running", Reason="", readiness=false. Elapsed: 4.015340867s
Mar 27 14:54:08.659: INFO: Pod "pod-projected-secrets-8b59f765-f36a-446b-87e3-49f7c91199d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015571368s
STEP: Saw pod success 03/27/23 14:54:08.66
Mar 27 14:54:08.660: INFO: Pod "pod-projected-secrets-8b59f765-f36a-446b-87e3-49f7c91199d3" satisfied condition "Succeeded or Failed"
Mar 27 14:54:08.663: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-secrets-8b59f765-f36a-446b-87e3-49f7c91199d3 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/27/23 14:54:08.673
Mar 27 14:54:08.690: INFO: Waiting for pod pod-projected-secrets-8b59f765-f36a-446b-87e3-49f7c91199d3 to disappear
Mar 27 14:54:08.694: INFO: Pod pod-projected-secrets-8b59f765-f36a-446b-87e3-49f7c91199d3 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 27 14:54:08.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6310" for this suite. 03/27/23 14:54:08.701
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":167,"skipped":3012,"failed":0}
------------------------------
• [SLOW TEST] [6.126 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:54:02.584
    Mar 27 14:54:02.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 14:54:02.585
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:54:02.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:54:02.613
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-589e1c41-6375-4eeb-80f2-0657a3d21c3f 03/27/23 14:54:02.621
    STEP: Creating a pod to test consume secrets 03/27/23 14:54:02.629
    Mar 27 14:54:02.644: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8b59f765-f36a-446b-87e3-49f7c91199d3" in namespace "projected-6310" to be "Succeeded or Failed"
    Mar 27 14:54:02.653: INFO: Pod "pod-projected-secrets-8b59f765-f36a-446b-87e3-49f7c91199d3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.57339ms
    Mar 27 14:54:04.660: INFO: Pod "pod-projected-secrets-8b59f765-f36a-446b-87e3-49f7c91199d3": Phase="Running", Reason="", readiness=true. Elapsed: 2.016162222s
    Mar 27 14:54:06.659: INFO: Pod "pod-projected-secrets-8b59f765-f36a-446b-87e3-49f7c91199d3": Phase="Running", Reason="", readiness=false. Elapsed: 4.015340867s
    Mar 27 14:54:08.659: INFO: Pod "pod-projected-secrets-8b59f765-f36a-446b-87e3-49f7c91199d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015571368s
    STEP: Saw pod success 03/27/23 14:54:08.66
    Mar 27 14:54:08.660: INFO: Pod "pod-projected-secrets-8b59f765-f36a-446b-87e3-49f7c91199d3" satisfied condition "Succeeded or Failed"
    Mar 27 14:54:08.663: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-secrets-8b59f765-f36a-446b-87e3-49f7c91199d3 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 14:54:08.673
    Mar 27 14:54:08.690: INFO: Waiting for pod pod-projected-secrets-8b59f765-f36a-446b-87e3-49f7c91199d3 to disappear
    Mar 27 14:54:08.694: INFO: Pod pod-projected-secrets-8b59f765-f36a-446b-87e3-49f7c91199d3 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 27 14:54:08.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6310" for this suite. 03/27/23 14:54:08.701
  << End Captured GinkgoWriter Output
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:54:08.71
Mar 27 14:54:08.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename conformance-tests 03/27/23 14:54:08.711
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:54:08.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:54:08.743
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 03/27/23 14:54:08.747
Mar 27 14:54:08.747: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Mar 27 14:54:08.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-9319" for this suite. 03/27/23 14:54:08.763
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":168,"skipped":3012,"failed":0}
------------------------------
• [0.063 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:54:08.71
    Mar 27 14:54:08.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename conformance-tests 03/27/23 14:54:08.711
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:54:08.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:54:08.743
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 03/27/23 14:54:08.747
    Mar 27 14:54:08.747: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Mar 27 14:54:08.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-9319" for this suite. 03/27/23 14:54:08.763
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:54:08.78
Mar 27 14:54:08.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename resourcequota 03/27/23 14:54:08.781
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:54:08.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:54:08.806
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 03/27/23 14:54:08.81
STEP: Ensuring ResourceQuota status is calculated 03/27/23 14:54:08.821
STEP: Creating a ResourceQuota with not terminating scope 03/27/23 14:54:10.826
STEP: Ensuring ResourceQuota status is calculated 03/27/23 14:54:10.833
STEP: Creating a long running pod 03/27/23 14:54:12.838
STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/27/23 14:54:12.857
STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/27/23 14:54:14.863
STEP: Deleting the pod 03/27/23 14:54:16.869
STEP: Ensuring resource quota status released the pod usage 03/27/23 14:54:16.885
STEP: Creating a terminating pod 03/27/23 14:54:18.892
STEP: Ensuring resource quota with terminating scope captures the pod usage 03/27/23 14:54:18.91
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/27/23 14:54:20.917
STEP: Deleting the pod 03/27/23 14:54:22.922
STEP: Ensuring resource quota status released the pod usage 03/27/23 14:54:22.937
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 27 14:54:24.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6672" for this suite. 03/27/23 14:54:24.948
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":169,"skipped":3042,"failed":0}
------------------------------
• [SLOW TEST] [16.175 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:54:08.78
    Mar 27 14:54:08.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename resourcequota 03/27/23 14:54:08.781
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:54:08.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:54:08.806
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 03/27/23 14:54:08.81
    STEP: Ensuring ResourceQuota status is calculated 03/27/23 14:54:08.821
    STEP: Creating a ResourceQuota with not terminating scope 03/27/23 14:54:10.826
    STEP: Ensuring ResourceQuota status is calculated 03/27/23 14:54:10.833
    STEP: Creating a long running pod 03/27/23 14:54:12.838
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/27/23 14:54:12.857
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/27/23 14:54:14.863
    STEP: Deleting the pod 03/27/23 14:54:16.869
    STEP: Ensuring resource quota status released the pod usage 03/27/23 14:54:16.885
    STEP: Creating a terminating pod 03/27/23 14:54:18.892
    STEP: Ensuring resource quota with terminating scope captures the pod usage 03/27/23 14:54:18.91
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/27/23 14:54:20.917
    STEP: Deleting the pod 03/27/23 14:54:22.922
    STEP: Ensuring resource quota status released the pod usage 03/27/23 14:54:22.937
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 27 14:54:24.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6672" for this suite. 03/27/23 14:54:24.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:54:24.956
Mar 27 14:54:24.956: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename statefulset 03/27/23 14:54:24.957
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:54:24.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:54:24.982
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6191 03/27/23 14:54:24.986
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 03/27/23 14:54:24.993
STEP: Creating pod with conflicting port in namespace statefulset-6191 03/27/23 14:54:24.999
STEP: Waiting until pod test-pod will start running in namespace statefulset-6191 03/27/23 14:54:25.01
Mar 27 14:54:25.010: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-6191" to be "running"
Mar 27 14:54:25.017: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.264519ms
Mar 27 14:54:27.023: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012546475s
Mar 27 14:54:27.023: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-6191 03/27/23 14:54:27.023
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6191 03/27/23 14:54:27.031
Mar 27 14:54:27.046: INFO: Observed stateful pod in namespace: statefulset-6191, name: ss-0, uid: 51e33715-0b9c-4fba-8d1f-9265627f9102, status phase: Pending. Waiting for statefulset controller to delete.
Mar 27 14:54:27.068: INFO: Observed stateful pod in namespace: statefulset-6191, name: ss-0, uid: 51e33715-0b9c-4fba-8d1f-9265627f9102, status phase: Failed. Waiting for statefulset controller to delete.
Mar 27 14:54:27.089: INFO: Observed stateful pod in namespace: statefulset-6191, name: ss-0, uid: 51e33715-0b9c-4fba-8d1f-9265627f9102, status phase: Failed. Waiting for statefulset controller to delete.
Mar 27 14:54:27.093: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6191
STEP: Removing pod with conflicting port in namespace statefulset-6191 03/27/23 14:54:27.093
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6191 and will be in running state 03/27/23 14:54:27.115
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 27 14:54:29.128: INFO: Deleting all statefulset in ns statefulset-6191
Mar 27 14:54:29.133: INFO: Scaling statefulset ss to 0
Mar 27 14:54:39.155: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 14:54:39.160: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 27 14:54:39.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6191" for this suite. 03/27/23 14:54:39.182
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":170,"skipped":3049,"failed":0}
------------------------------
• [SLOW TEST] [14.235 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:54:24.956
    Mar 27 14:54:24.956: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename statefulset 03/27/23 14:54:24.957
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:54:24.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:54:24.982
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6191 03/27/23 14:54:24.986
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 03/27/23 14:54:24.993
    STEP: Creating pod with conflicting port in namespace statefulset-6191 03/27/23 14:54:24.999
    STEP: Waiting until pod test-pod will start running in namespace statefulset-6191 03/27/23 14:54:25.01
    Mar 27 14:54:25.010: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-6191" to be "running"
    Mar 27 14:54:25.017: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.264519ms
    Mar 27 14:54:27.023: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012546475s
    Mar 27 14:54:27.023: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-6191 03/27/23 14:54:27.023
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6191 03/27/23 14:54:27.031
    Mar 27 14:54:27.046: INFO: Observed stateful pod in namespace: statefulset-6191, name: ss-0, uid: 51e33715-0b9c-4fba-8d1f-9265627f9102, status phase: Pending. Waiting for statefulset controller to delete.
    Mar 27 14:54:27.068: INFO: Observed stateful pod in namespace: statefulset-6191, name: ss-0, uid: 51e33715-0b9c-4fba-8d1f-9265627f9102, status phase: Failed. Waiting for statefulset controller to delete.
    Mar 27 14:54:27.089: INFO: Observed stateful pod in namespace: statefulset-6191, name: ss-0, uid: 51e33715-0b9c-4fba-8d1f-9265627f9102, status phase: Failed. Waiting for statefulset controller to delete.
    Mar 27 14:54:27.093: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6191
    STEP: Removing pod with conflicting port in namespace statefulset-6191 03/27/23 14:54:27.093
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6191 and will be in running state 03/27/23 14:54:27.115
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 27 14:54:29.128: INFO: Deleting all statefulset in ns statefulset-6191
    Mar 27 14:54:29.133: INFO: Scaling statefulset ss to 0
    Mar 27 14:54:39.155: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 14:54:39.160: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 27 14:54:39.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6191" for this suite. 03/27/23 14:54:39.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:54:39.193
Mar 27 14:54:39.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename container-lifecycle-hook 03/27/23 14:54:39.194
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:54:39.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:54:39.215
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/27/23 14:54:39.224
Mar 27 14:54:39.232: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3144" to be "running and ready"
Mar 27 14:54:39.236: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.10823ms
Mar 27 14:54:39.236: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:54:41.241: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008901598s
Mar 27 14:54:41.241: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 27 14:54:41.241: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 03/27/23 14:54:41.245
Mar 27 14:54:41.252: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-3144" to be "running and ready"
Mar 27 14:54:41.257: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.464151ms
Mar 27 14:54:41.257: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 27 14:54:43.262: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009665107s
Mar 27 14:54:43.262: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Mar 27 14:54:43.262: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/27/23 14:54:43.266
STEP: delete the pod with lifecycle hook 03/27/23 14:54:43.279
Mar 27 14:54:43.289: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 27 14:54:43.294: INFO: Pod pod-with-poststart-http-hook still exists
Mar 27 14:54:45.294: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 27 14:54:45.299: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar 27 14:54:45.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3144" for this suite. 03/27/23 14:54:45.304
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":171,"skipped":3087,"failed":0}
------------------------------
• [SLOW TEST] [6.118 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:54:39.193
    Mar 27 14:54:39.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/27/23 14:54:39.194
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:54:39.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:54:39.215
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/27/23 14:54:39.224
    Mar 27 14:54:39.232: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3144" to be "running and ready"
    Mar 27 14:54:39.236: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.10823ms
    Mar 27 14:54:39.236: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:54:41.241: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008901598s
    Mar 27 14:54:41.241: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 27 14:54:41.241: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 03/27/23 14:54:41.245
    Mar 27 14:54:41.252: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-3144" to be "running and ready"
    Mar 27 14:54:41.257: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.464151ms
    Mar 27 14:54:41.257: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 14:54:43.262: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009665107s
    Mar 27 14:54:43.262: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Mar 27 14:54:43.262: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/27/23 14:54:43.266
    STEP: delete the pod with lifecycle hook 03/27/23 14:54:43.279
    Mar 27 14:54:43.289: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 27 14:54:43.294: INFO: Pod pod-with-poststart-http-hook still exists
    Mar 27 14:54:45.294: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 27 14:54:45.299: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar 27 14:54:45.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-3144" for this suite. 03/27/23 14:54:45.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:54:45.314
Mar 27 14:54:45.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename configmap 03/27/23 14:54:45.316
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:54:45.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:54:45.34
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-9ffcde4a-403f-496b-82b3-4d8821481e78 03/27/23 14:54:45.343
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar 27 14:54:45.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4037" for this suite. 03/27/23 14:54:45.354
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":172,"skipped":3105,"failed":0}
------------------------------
• [0.050 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:54:45.314
    Mar 27 14:54:45.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename configmap 03/27/23 14:54:45.316
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:54:45.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:54:45.34
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-9ffcde4a-403f-496b-82b3-4d8821481e78 03/27/23 14:54:45.343
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 27 14:54:45.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4037" for this suite. 03/27/23 14:54:45.354
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:54:45.364
Mar 27 14:54:45.364: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename webhook 03/27/23 14:54:45.365
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:54:45.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:54:45.385
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/27/23 14:54:45.405
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 14:54:45.641
STEP: Deploying the webhook pod 03/27/23 14:54:45.654
STEP: Wait for the deployment to be ready 03/27/23 14:54:45.669
Mar 27 14:54:45.676: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 03/27/23 14:54:47.69
STEP: Verifying the service has paired with the endpoint 03/27/23 14:54:47.701
Mar 27 14:54:48.701: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/27/23 14:54:48.705
STEP: create a pod that should be updated by the webhook 03/27/23 14:54:48.743
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:54:48.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3483" for this suite. 03/27/23 14:54:48.787
STEP: Destroying namespace "webhook-3483-markers" for this suite. 03/27/23 14:54:48.793
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":173,"skipped":3106,"failed":0}
------------------------------
• [3.478 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:54:45.364
    Mar 27 14:54:45.364: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename webhook 03/27/23 14:54:45.365
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:54:45.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:54:45.385
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/27/23 14:54:45.405
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 14:54:45.641
    STEP: Deploying the webhook pod 03/27/23 14:54:45.654
    STEP: Wait for the deployment to be ready 03/27/23 14:54:45.669
    Mar 27 14:54:45.676: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 03/27/23 14:54:47.69
    STEP: Verifying the service has paired with the endpoint 03/27/23 14:54:47.701
    Mar 27 14:54:48.701: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/27/23 14:54:48.705
    STEP: create a pod that should be updated by the webhook 03/27/23 14:54:48.743
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:54:48.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3483" for this suite. 03/27/23 14:54:48.787
    STEP: Destroying namespace "webhook-3483-markers" for this suite. 03/27/23 14:54:48.793
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:54:48.849
Mar 27 14:54:48.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename container-probe 03/27/23 14:54:48.85
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:54:48.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:54:48.883
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-23bc2db2-9118-4198-8b3a-2535e2ffc250 in namespace container-probe-309 03/27/23 14:54:48.886
Mar 27 14:54:48.894: INFO: Waiting up to 5m0s for pod "test-webserver-23bc2db2-9118-4198-8b3a-2535e2ffc250" in namespace "container-probe-309" to be "not pending"
Mar 27 14:54:48.905: INFO: Pod "test-webserver-23bc2db2-9118-4198-8b3a-2535e2ffc250": Phase="Pending", Reason="", readiness=false. Elapsed: 10.403309ms
Mar 27 14:54:50.911: INFO: Pod "test-webserver-23bc2db2-9118-4198-8b3a-2535e2ffc250": Phase="Running", Reason="", readiness=true. Elapsed: 2.016848877s
Mar 27 14:54:50.911: INFO: Pod "test-webserver-23bc2db2-9118-4198-8b3a-2535e2ffc250" satisfied condition "not pending"
Mar 27 14:54:50.911: INFO: Started pod test-webserver-23bc2db2-9118-4198-8b3a-2535e2ffc250 in namespace container-probe-309
STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 14:54:50.911
Mar 27 14:54:50.914: INFO: Initial restart count of pod test-webserver-23bc2db2-9118-4198-8b3a-2535e2ffc250 is 0
STEP: deleting the pod 03/27/23 14:58:51.781
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 27 14:58:51.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-309" for this suite. 03/27/23 14:58:51.813
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":174,"skipped":3176,"failed":0}
------------------------------
• [SLOW TEST] [242.971 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:54:48.849
    Mar 27 14:54:48.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename container-probe 03/27/23 14:54:48.85
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:54:48.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:54:48.883
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-23bc2db2-9118-4198-8b3a-2535e2ffc250 in namespace container-probe-309 03/27/23 14:54:48.886
    Mar 27 14:54:48.894: INFO: Waiting up to 5m0s for pod "test-webserver-23bc2db2-9118-4198-8b3a-2535e2ffc250" in namespace "container-probe-309" to be "not pending"
    Mar 27 14:54:48.905: INFO: Pod "test-webserver-23bc2db2-9118-4198-8b3a-2535e2ffc250": Phase="Pending", Reason="", readiness=false. Elapsed: 10.403309ms
    Mar 27 14:54:50.911: INFO: Pod "test-webserver-23bc2db2-9118-4198-8b3a-2535e2ffc250": Phase="Running", Reason="", readiness=true. Elapsed: 2.016848877s
    Mar 27 14:54:50.911: INFO: Pod "test-webserver-23bc2db2-9118-4198-8b3a-2535e2ffc250" satisfied condition "not pending"
    Mar 27 14:54:50.911: INFO: Started pod test-webserver-23bc2db2-9118-4198-8b3a-2535e2ffc250 in namespace container-probe-309
    STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 14:54:50.911
    Mar 27 14:54:50.914: INFO: Initial restart count of pod test-webserver-23bc2db2-9118-4198-8b3a-2535e2ffc250 is 0
    STEP: deleting the pod 03/27/23 14:58:51.781
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 27 14:58:51.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-309" for this suite. 03/27/23 14:58:51.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:58:51.821
Mar 27 14:58:51.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename webhook 03/27/23 14:58:51.823
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:58:51.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:58:51.846
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/27/23 14:58:51.867
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 14:58:52.417
STEP: Deploying the webhook pod 03/27/23 14:58:52.43
STEP: Wait for the deployment to be ready 03/27/23 14:58:52.451
Mar 27 14:58:52.458: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 14:58:54.472
STEP: Verifying the service has paired with the endpoint 03/27/23 14:58:54.49
Mar 27 14:58:55.490: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Mar 27 14:58:55.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1752-crds.webhook.example.com via the AdmissionRegistration API 03/27/23 14:58:56.017
STEP: Creating a custom resource while v1 is storage version 03/27/23 14:58:56.047
STEP: Patching Custom Resource Definition to set v2 as storage 03/27/23 14:58:58.201
STEP: Patching the custom resource while v2 is storage version 03/27/23 14:58:58.225
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:58:58.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2804" for this suite. 03/27/23 14:58:58.83
STEP: Destroying namespace "webhook-2804-markers" for this suite. 03/27/23 14:58:58.839
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":175,"skipped":3191,"failed":0}
------------------------------
• [SLOW TEST] [7.074 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:58:51.821
    Mar 27 14:58:51.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename webhook 03/27/23 14:58:51.823
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:58:51.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:58:51.846
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/27/23 14:58:51.867
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 14:58:52.417
    STEP: Deploying the webhook pod 03/27/23 14:58:52.43
    STEP: Wait for the deployment to be ready 03/27/23 14:58:52.451
    Mar 27 14:58:52.458: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 14:58:54.472
    STEP: Verifying the service has paired with the endpoint 03/27/23 14:58:54.49
    Mar 27 14:58:55.490: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Mar 27 14:58:55.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1752-crds.webhook.example.com via the AdmissionRegistration API 03/27/23 14:58:56.017
    STEP: Creating a custom resource while v1 is storage version 03/27/23 14:58:56.047
    STEP: Patching Custom Resource Definition to set v2 as storage 03/27/23 14:58:58.201
    STEP: Patching the custom resource while v2 is storage version 03/27/23 14:58:58.225
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:58:58.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2804" for this suite. 03/27/23 14:58:58.83
    STEP: Destroying namespace "webhook-2804-markers" for this suite. 03/27/23 14:58:58.839
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:58:58.897
Mar 27 14:58:58.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename crd-webhook 03/27/23 14:58:58.898
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:58:58.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:58:58.926
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/27/23 14:58:58.929
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/27/23 14:58:59.266
STEP: Deploying the custom resource conversion webhook pod 03/27/23 14:58:59.272
STEP: Wait for the deployment to be ready 03/27/23 14:58:59.29
Mar 27 14:58:59.298: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 14:59:01.312
STEP: Verifying the service has paired with the endpoint 03/27/23 14:59:01.322
Mar 27 14:59:02.322: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Mar 27 14:59:02.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Creating a v1 custom resource 03/27/23 14:59:04.946
STEP: Create a v2 custom resource 03/27/23 14:59:04.968
STEP: List CRs in v1 03/27/23 14:59:05.064
STEP: List CRs in v2 03/27/23 14:59:05.081
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 14:59:05.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5835" for this suite. 03/27/23 14:59:05.617
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":176,"skipped":3201,"failed":0}
------------------------------
• [SLOW TEST] [6.783 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:58:58.897
    Mar 27 14:58:58.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename crd-webhook 03/27/23 14:58:58.898
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:58:58.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:58:58.926
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/27/23 14:58:58.929
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/27/23 14:58:59.266
    STEP: Deploying the custom resource conversion webhook pod 03/27/23 14:58:59.272
    STEP: Wait for the deployment to be ready 03/27/23 14:58:59.29
    Mar 27 14:58:59.298: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 14:59:01.312
    STEP: Verifying the service has paired with the endpoint 03/27/23 14:59:01.322
    Mar 27 14:59:02.322: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Mar 27 14:59:02.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Creating a v1 custom resource 03/27/23 14:59:04.946
    STEP: Create a v2 custom resource 03/27/23 14:59:04.968
    STEP: List CRs in v1 03/27/23 14:59:05.064
    STEP: List CRs in v2 03/27/23 14:59:05.081
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 14:59:05.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-5835" for this suite. 03/27/23 14:59:05.617
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 14:59:05.68
Mar 27 14:59:05.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename taint-single-pod 03/27/23 14:59:05.681
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:59:05.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:59:05.729
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Mar 27 14:59:05.735: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 27 15:00:05.777: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Mar 27 15:00:05.782: INFO: Starting informer...
STEP: Starting pod... 03/27/23 15:00:05.782
Mar 27 15:00:06.004: INFO: Pod is running on k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc. Tainting Node
STEP: Trying to apply a taint on the Node 03/27/23 15:00:06.004
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/27/23 15:00:06.043
STEP: Waiting short time to make sure Pod is queued for deletion 03/27/23 15:00:06.06
Mar 27 15:00:06.061: INFO: Pod wasn't evicted. Proceeding
Mar 27 15:00:06.061: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/27/23 15:00:06.137
STEP: Waiting some time to make sure that toleration time passed. 03/27/23 15:00:06.151
Mar 27 15:01:21.152: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Mar 27 15:01:21.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-1612" for this suite. 03/27/23 15:01:21.162
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":177,"skipped":3208,"failed":0}
------------------------------
• [SLOW TEST] [135.489 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 14:59:05.68
    Mar 27 14:59:05.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename taint-single-pod 03/27/23 14:59:05.681
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 14:59:05.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 14:59:05.729
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Mar 27 14:59:05.735: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 27 15:00:05.777: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Mar 27 15:00:05.782: INFO: Starting informer...
    STEP: Starting pod... 03/27/23 15:00:05.782
    Mar 27 15:00:06.004: INFO: Pod is running on k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc. Tainting Node
    STEP: Trying to apply a taint on the Node 03/27/23 15:00:06.004
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/27/23 15:00:06.043
    STEP: Waiting short time to make sure Pod is queued for deletion 03/27/23 15:00:06.06
    Mar 27 15:00:06.061: INFO: Pod wasn't evicted. Proceeding
    Mar 27 15:00:06.061: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/27/23 15:00:06.137
    STEP: Waiting some time to make sure that toleration time passed. 03/27/23 15:00:06.151
    Mar 27 15:01:21.152: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 15:01:21.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-1612" for this suite. 03/27/23 15:01:21.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:01:21.171
Mar 27 15:01:21.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 15:01:21.172
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:01:21.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:01:21.253
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/27/23 15:01:21.257
Mar 27 15:01:21.257: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 15:01:23.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 15:01:50.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2104" for this suite. 03/27/23 15:01:50.101
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":178,"skipped":3223,"failed":0}
------------------------------
• [SLOW TEST] [28.953 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:01:21.171
    Mar 27 15:01:21.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 15:01:21.172
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:01:21.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:01:21.253
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/27/23 15:01:21.257
    Mar 27 15:01:21.257: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 15:01:23.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 15:01:50.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2104" for this suite. 03/27/23 15:01:50.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:01:50.125
Mar 27 15:01:50.125: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename namespaces 03/27/23 15:01:50.126
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:01:50.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:01:50.152
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 03/27/23 15:01:50.155
Mar 27 15:01:50.158: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 03/27/23 15:01:50.158
Mar 27 15:01:50.169: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 03/27/23 15:01:50.169
Mar 27 15:01:50.178: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar 27 15:01:50.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9644" for this suite. 03/27/23 15:01:50.182
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":179,"skipped":3246,"failed":0}
------------------------------
• [0.069 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:01:50.125
    Mar 27 15:01:50.125: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename namespaces 03/27/23 15:01:50.126
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:01:50.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:01:50.152
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 03/27/23 15:01:50.155
    Mar 27 15:01:50.158: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 03/27/23 15:01:50.158
    Mar 27 15:01:50.169: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 03/27/23 15:01:50.169
    Mar 27 15:01:50.178: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 15:01:50.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9644" for this suite. 03/27/23 15:01:50.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:01:50.195
Mar 27 15:01:50.195: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename resourcequota 03/27/23 15:01:50.195
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:01:50.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:01:50.223
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 03/27/23 15:01:50.226
STEP: Getting a ResourceQuota 03/27/23 15:01:50.238
STEP: Listing all ResourceQuotas with LabelSelector 03/27/23 15:01:50.242
STEP: Patching the ResourceQuota 03/27/23 15:01:50.245
STEP: Deleting a Collection of ResourceQuotas 03/27/23 15:01:50.258
STEP: Verifying the deleted ResourceQuota 03/27/23 15:01:50.27
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 27 15:01:50.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3637" for this suite. 03/27/23 15:01:50.279
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":180,"skipped":3269,"failed":0}
------------------------------
• [0.092 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:01:50.195
    Mar 27 15:01:50.195: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename resourcequota 03/27/23 15:01:50.195
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:01:50.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:01:50.223
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 03/27/23 15:01:50.226
    STEP: Getting a ResourceQuota 03/27/23 15:01:50.238
    STEP: Listing all ResourceQuotas with LabelSelector 03/27/23 15:01:50.242
    STEP: Patching the ResourceQuota 03/27/23 15:01:50.245
    STEP: Deleting a Collection of ResourceQuotas 03/27/23 15:01:50.258
    STEP: Verifying the deleted ResourceQuota 03/27/23 15:01:50.27
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 27 15:01:50.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3637" for this suite. 03/27/23 15:01:50.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:01:50.29
Mar 27 15:01:50.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename replication-controller 03/27/23 15:01:50.291
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:01:50.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:01:50.316
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934 03/27/23 15:01:50.319
Mar 27 15:01:50.330: INFO: Pod name my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934: Found 0 pods out of 1
Mar 27 15:01:55.335: INFO: Pod name my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934: Found 1 pods out of 1
Mar 27 15:01:55.335: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934" are running
Mar 27 15:01:55.335: INFO: Waiting up to 5m0s for pod "my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934-gf64r" in namespace "replication-controller-1959" to be "running"
Mar 27 15:01:55.338: INFO: Pod "my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934-gf64r": Phase="Running", Reason="", readiness=true. Elapsed: 2.835028ms
Mar 27 15:01:55.338: INFO: Pod "my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934-gf64r" satisfied condition "running"
Mar 27 15:01:55.338: INFO: Pod "my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934-gf64r" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 15:01:50 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 15:01:51 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 15:01:51 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 15:01:50 +0000 UTC Reason: Message:}])
Mar 27 15:01:55.338: INFO: Trying to dial the pod
Mar 27 15:02:00.409: INFO: Controller my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934: Got expected result from replica 1 [my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934-gf64r]: "my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934-gf64r", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar 27 15:02:00.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1959" for this suite. 03/27/23 15:02:00.414
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":181,"skipped":3309,"failed":0}
------------------------------
• [SLOW TEST] [10.132 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:01:50.29
    Mar 27 15:01:50.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename replication-controller 03/27/23 15:01:50.291
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:01:50.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:01:50.316
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934 03/27/23 15:01:50.319
    Mar 27 15:01:50.330: INFO: Pod name my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934: Found 0 pods out of 1
    Mar 27 15:01:55.335: INFO: Pod name my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934: Found 1 pods out of 1
    Mar 27 15:01:55.335: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934" are running
    Mar 27 15:01:55.335: INFO: Waiting up to 5m0s for pod "my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934-gf64r" in namespace "replication-controller-1959" to be "running"
    Mar 27 15:01:55.338: INFO: Pod "my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934-gf64r": Phase="Running", Reason="", readiness=true. Elapsed: 2.835028ms
    Mar 27 15:01:55.338: INFO: Pod "my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934-gf64r" satisfied condition "running"
    Mar 27 15:01:55.338: INFO: Pod "my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934-gf64r" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 15:01:50 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 15:01:51 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 15:01:51 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 15:01:50 +0000 UTC Reason: Message:}])
    Mar 27 15:01:55.338: INFO: Trying to dial the pod
    Mar 27 15:02:00.409: INFO: Controller my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934: Got expected result from replica 1 [my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934-gf64r]: "my-hostname-basic-b9e5b90c-8f6d-49a2-bcea-60a9fbce7934-gf64r", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar 27 15:02:00.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1959" for this suite. 03/27/23 15:02:00.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:02:00.424
Mar 27 15:02:00.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename pod-network-test 03/27/23 15:02:00.425
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:02:00.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:02:00.448
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-5054 03/27/23 15:02:00.451
STEP: creating a selector 03/27/23 15:02:00.451
STEP: Creating the service pods in kubernetes 03/27/23 15:02:00.451
Mar 27 15:02:00.451: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 27 15:02:00.486: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5054" to be "running and ready"
Mar 27 15:02:00.489: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.361921ms
Mar 27 15:02:00.489: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:02:02.498: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011610162s
Mar 27 15:02:02.498: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 15:02:04.578: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.09178595s
Mar 27 15:02:04.578: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 15:02:06.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006142211s
Mar 27 15:02:06.492: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 15:02:08.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.007449031s
Mar 27 15:02:08.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 15:02:10.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.006804897s
Mar 27 15:02:10.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 15:02:12.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.007060567s
Mar 27 15:02:12.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 15:02:14.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.007045405s
Mar 27 15:02:14.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 15:02:16.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.008181664s
Mar 27 15:02:16.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 15:02:18.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.007759486s
Mar 27 15:02:18.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 15:02:20.496: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.009801544s
Mar 27 15:02:20.496: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 15:02:22.495: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008749353s
Mar 27 15:02:22.495: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 27 15:02:22.495: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 27 15:02:22.501: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5054" to be "running and ready"
Mar 27 15:02:22.503: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.533938ms
Mar 27 15:02:22.503: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 27 15:02:22.503: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 27 15:02:22.506: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5054" to be "running and ready"
Mar 27 15:02:22.516: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 9.718217ms
Mar 27 15:02:22.516: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 27 15:02:22.516: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/27/23 15:02:22.52
Mar 27 15:02:22.568: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5054" to be "running"
Mar 27 15:02:22.574: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.961609ms
Mar 27 15:02:24.578: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010337309s
Mar 27 15:02:26.581: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012710721s
Mar 27 15:02:26.581: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 27 15:02:26.585: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 27 15:02:26.585: INFO: Breadth first check of 172.25.1.153 on host 192.168.1.3...
Mar 27 15:02:26.589: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.2.224:9080/dial?request=hostname&protocol=http&host=172.25.1.153&port=8083&tries=1'] Namespace:pod-network-test-5054 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 15:02:26.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 15:02:26.589: INFO: ExecWithOptions: Clientset creation
Mar 27 15:02:26.589: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-5054/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.2.224%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.1.153%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 27 15:02:26.693: INFO: Waiting for responses: map[]
Mar 27 15:02:26.693: INFO: reached 172.25.1.153 after 0/1 tries
Mar 27 15:02:26.693: INFO: Breadth first check of 172.25.2.223 on host 192.168.1.4...
Mar 27 15:02:26.699: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.2.224:9080/dial?request=hostname&protocol=http&host=172.25.2.223&port=8083&tries=1'] Namespace:pod-network-test-5054 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 15:02:26.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 15:02:26.700: INFO: ExecWithOptions: Clientset creation
Mar 27 15:02:26.700: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-5054/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.2.224%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.2.223%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 27 15:02:26.810: INFO: Waiting for responses: map[]
Mar 27 15:02:26.810: INFO: reached 172.25.2.223 after 0/1 tries
Mar 27 15:02:26.810: INFO: Breadth first check of 172.25.0.113 on host 192.168.1.7...
Mar 27 15:02:26.814: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.2.224:9080/dial?request=hostname&protocol=http&host=172.25.0.113&port=8083&tries=1'] Namespace:pod-network-test-5054 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 15:02:26.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 15:02:26.815: INFO: ExecWithOptions: Clientset creation
Mar 27 15:02:26.815: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-5054/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.2.224%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.0.113%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 27 15:02:26.935: INFO: Waiting for responses: map[]
Mar 27 15:02:26.935: INFO: reached 172.25.0.113 after 0/1 tries
Mar 27 15:02:26.935: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar 27 15:02:26.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5054" for this suite. 03/27/23 15:02:26.94
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":182,"skipped":3375,"failed":0}
------------------------------
• [SLOW TEST] [26.524 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:02:00.424
    Mar 27 15:02:00.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename pod-network-test 03/27/23 15:02:00.425
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:02:00.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:02:00.448
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-5054 03/27/23 15:02:00.451
    STEP: creating a selector 03/27/23 15:02:00.451
    STEP: Creating the service pods in kubernetes 03/27/23 15:02:00.451
    Mar 27 15:02:00.451: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 27 15:02:00.486: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5054" to be "running and ready"
    Mar 27 15:02:00.489: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.361921ms
    Mar 27 15:02:00.489: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:02:02.498: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011610162s
    Mar 27 15:02:02.498: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 15:02:04.578: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.09178595s
    Mar 27 15:02:04.578: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 15:02:06.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006142211s
    Mar 27 15:02:06.492: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 15:02:08.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.007449031s
    Mar 27 15:02:08.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 15:02:10.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.006804897s
    Mar 27 15:02:10.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 15:02:12.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.007060567s
    Mar 27 15:02:12.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 15:02:14.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.007045405s
    Mar 27 15:02:14.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 15:02:16.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.008181664s
    Mar 27 15:02:16.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 15:02:18.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.007759486s
    Mar 27 15:02:18.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 15:02:20.496: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.009801544s
    Mar 27 15:02:20.496: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 15:02:22.495: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008749353s
    Mar 27 15:02:22.495: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 27 15:02:22.495: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 27 15:02:22.501: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5054" to be "running and ready"
    Mar 27 15:02:22.503: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.533938ms
    Mar 27 15:02:22.503: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 27 15:02:22.503: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 27 15:02:22.506: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5054" to be "running and ready"
    Mar 27 15:02:22.516: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 9.718217ms
    Mar 27 15:02:22.516: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 27 15:02:22.516: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/27/23 15:02:22.52
    Mar 27 15:02:22.568: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5054" to be "running"
    Mar 27 15:02:22.574: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.961609ms
    Mar 27 15:02:24.578: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010337309s
    Mar 27 15:02:26.581: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012710721s
    Mar 27 15:02:26.581: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 27 15:02:26.585: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 27 15:02:26.585: INFO: Breadth first check of 172.25.1.153 on host 192.168.1.3...
    Mar 27 15:02:26.589: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.2.224:9080/dial?request=hostname&protocol=http&host=172.25.1.153&port=8083&tries=1'] Namespace:pod-network-test-5054 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 15:02:26.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 15:02:26.589: INFO: ExecWithOptions: Clientset creation
    Mar 27 15:02:26.589: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-5054/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.2.224%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.1.153%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 27 15:02:26.693: INFO: Waiting for responses: map[]
    Mar 27 15:02:26.693: INFO: reached 172.25.1.153 after 0/1 tries
    Mar 27 15:02:26.693: INFO: Breadth first check of 172.25.2.223 on host 192.168.1.4...
    Mar 27 15:02:26.699: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.2.224:9080/dial?request=hostname&protocol=http&host=172.25.2.223&port=8083&tries=1'] Namespace:pod-network-test-5054 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 15:02:26.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 15:02:26.700: INFO: ExecWithOptions: Clientset creation
    Mar 27 15:02:26.700: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-5054/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.2.224%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.2.223%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 27 15:02:26.810: INFO: Waiting for responses: map[]
    Mar 27 15:02:26.810: INFO: reached 172.25.2.223 after 0/1 tries
    Mar 27 15:02:26.810: INFO: Breadth first check of 172.25.0.113 on host 192.168.1.7...
    Mar 27 15:02:26.814: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.2.224:9080/dial?request=hostname&protocol=http&host=172.25.0.113&port=8083&tries=1'] Namespace:pod-network-test-5054 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 15:02:26.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 15:02:26.815: INFO: ExecWithOptions: Clientset creation
    Mar 27 15:02:26.815: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-5054/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.2.224%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.0.113%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 27 15:02:26.935: INFO: Waiting for responses: map[]
    Mar 27 15:02:26.935: INFO: reached 172.25.0.113 after 0/1 tries
    Mar 27 15:02:26.935: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar 27 15:02:26.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-5054" for this suite. 03/27/23 15:02:26.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:02:26.951
Mar 27 15:02:26.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename namespaces 03/27/23 15:02:26.952
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:02:26.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:02:26.978
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 03/27/23 15:02:26.983
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:02:27.095
STEP: Creating a pod in the namespace 03/27/23 15:02:27.099
STEP: Waiting for the pod to have running status 03/27/23 15:02:27.106
Mar 27 15:02:27.106: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-7110" to be "running"
Mar 27 15:02:27.109: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.528568ms
Mar 27 15:02:29.112: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006387212s
Mar 27 15:02:29.112: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 03/27/23 15:02:29.112
STEP: Waiting for the namespace to be removed. 03/27/23 15:02:29.118
STEP: Recreating the namespace 03/27/23 15:02:40.121
STEP: Verifying there are no pods in the namespace 03/27/23 15:02:40.138
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar 27 15:02:40.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-700" for this suite. 03/27/23 15:02:40.393
STEP: Destroying namespace "nsdeletetest-7110" for this suite. 03/27/23 15:02:40.481
Mar 27 15:02:40.485: INFO: Namespace nsdeletetest-7110 was already deleted
STEP: Destroying namespace "nsdeletetest-3339" for this suite. 03/27/23 15:02:40.485
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":183,"skipped":3385,"failed":0}
------------------------------
• [SLOW TEST] [13.790 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:02:26.951
    Mar 27 15:02:26.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename namespaces 03/27/23 15:02:26.952
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:02:26.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:02:26.978
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 03/27/23 15:02:26.983
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:02:27.095
    STEP: Creating a pod in the namespace 03/27/23 15:02:27.099
    STEP: Waiting for the pod to have running status 03/27/23 15:02:27.106
    Mar 27 15:02:27.106: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-7110" to be "running"
    Mar 27 15:02:27.109: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.528568ms
    Mar 27 15:02:29.112: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006387212s
    Mar 27 15:02:29.112: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 03/27/23 15:02:29.112
    STEP: Waiting for the namespace to be removed. 03/27/23 15:02:29.118
    STEP: Recreating the namespace 03/27/23 15:02:40.121
    STEP: Verifying there are no pods in the namespace 03/27/23 15:02:40.138
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 15:02:40.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-700" for this suite. 03/27/23 15:02:40.393
    STEP: Destroying namespace "nsdeletetest-7110" for this suite. 03/27/23 15:02:40.481
    Mar 27 15:02:40.485: INFO: Namespace nsdeletetest-7110 was already deleted
    STEP: Destroying namespace "nsdeletetest-3339" for this suite. 03/27/23 15:02:40.485
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:02:40.744
Mar 27 15:02:40.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename sched-preemption 03/27/23 15:02:40.744
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:02:40.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:02:40.948
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar 27 15:02:41.299: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 27 15:03:41.335: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 03/27/23 15:03:41.338
Mar 27 15:03:41.355: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 27 15:03:41.363: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 27 15:03:41.389: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 27 15:03:41.396: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar 27 15:03:41.420: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar 27 15:03:41.429: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/27/23 15:03:41.429
Mar 27 15:03:41.429: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8378" to be "running"
Mar 27 15:03:41.437: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.802459ms
Mar 27 15:03:43.443: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.013039905s
Mar 27 15:03:43.443: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar 27 15:03:43.443: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8378" to be "running"
Mar 27 15:03:43.447: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.196187ms
Mar 27 15:03:43.447: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 27 15:03:43.447: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8378" to be "running"
Mar 27 15:03:43.449: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.536372ms
Mar 27 15:03:43.449: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 27 15:03:43.449: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8378" to be "running"
Mar 27 15:03:43.452: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.536026ms
Mar 27 15:03:45.457: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.007079039s
Mar 27 15:03:45.457: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 27 15:03:45.457: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8378" to be "running"
Mar 27 15:03:45.460: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.971809ms
Mar 27 15:03:45.460: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 27 15:03:45.460: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8378" to be "running"
Mar 27 15:03:45.463: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.227866ms
Mar 27 15:03:45.463: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/27/23 15:03:45.463
Mar 27 15:03:45.468: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8378" to be "running"
Mar 27 15:03:45.470: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.734031ms
Mar 27 15:03:47.578: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.110760947s
Mar 27 15:03:49.476: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00832931s
Mar 27 15:03:51.477: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009155923s
Mar 27 15:03:53.499: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.031195179s
Mar 27 15:03:53.499: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar 27 15:03:53.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8378" for this suite. 03/27/23 15:03:53.524
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":184,"skipped":3452,"failed":0}
------------------------------
• [SLOW TEST] [73.539 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:02:40.744
    Mar 27 15:02:40.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename sched-preemption 03/27/23 15:02:40.744
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:02:40.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:02:40.948
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar 27 15:02:41.299: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 27 15:03:41.335: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 03/27/23 15:03:41.338
    Mar 27 15:03:41.355: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar 27 15:03:41.363: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar 27 15:03:41.389: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar 27 15:03:41.396: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar 27 15:03:41.420: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar 27 15:03:41.429: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/27/23 15:03:41.429
    Mar 27 15:03:41.429: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8378" to be "running"
    Mar 27 15:03:41.437: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.802459ms
    Mar 27 15:03:43.443: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.013039905s
    Mar 27 15:03:43.443: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar 27 15:03:43.443: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8378" to be "running"
    Mar 27 15:03:43.447: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.196187ms
    Mar 27 15:03:43.447: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 27 15:03:43.447: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8378" to be "running"
    Mar 27 15:03:43.449: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.536372ms
    Mar 27 15:03:43.449: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 27 15:03:43.449: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8378" to be "running"
    Mar 27 15:03:43.452: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.536026ms
    Mar 27 15:03:45.457: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.007079039s
    Mar 27 15:03:45.457: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 27 15:03:45.457: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8378" to be "running"
    Mar 27 15:03:45.460: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.971809ms
    Mar 27 15:03:45.460: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 27 15:03:45.460: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8378" to be "running"
    Mar 27 15:03:45.463: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.227866ms
    Mar 27 15:03:45.463: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/27/23 15:03:45.463
    Mar 27 15:03:45.468: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8378" to be "running"
    Mar 27 15:03:45.470: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.734031ms
    Mar 27 15:03:47.578: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.110760947s
    Mar 27 15:03:49.476: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00832931s
    Mar 27 15:03:51.477: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009155923s
    Mar 27 15:03:53.499: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.031195179s
    Mar 27 15:03:53.499: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 15:03:53.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-8378" for this suite. 03/27/23 15:03:53.524
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:03:54.285
Mar 27 15:03:54.285: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubectl 03/27/23 15:03:54.286
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:03:54.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:03:54.34
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 03/27/23 15:03:54.405
Mar 27 15:03:54.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 create -f -'
Mar 27 15:03:55.016: INFO: stderr: ""
Mar 27 15:03:55.016: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/27/23 15:03:55.016
Mar 27 15:03:55.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 27 15:03:55.131: INFO: stderr: ""
Mar 27 15:03:55.131: INFO: stdout: "update-demo-nautilus-ck4cr "
STEP: Replicas for name=update-demo: expected=2 actual=1 03/27/23 15:03:55.131
Mar 27 15:04:00.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 27 15:04:00.270: INFO: stderr: ""
Mar 27 15:04:00.270: INFO: stdout: "update-demo-nautilus-ck4cr update-demo-nautilus-g9c5c "
Mar 27 15:04:00.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-ck4cr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 15:04:00.335: INFO: stderr: ""
Mar 27 15:04:00.335: INFO: stdout: "true"
Mar 27 15:04:00.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-ck4cr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 27 15:04:00.396: INFO: stderr: ""
Mar 27 15:04:00.396: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 27 15:04:00.396: INFO: validating pod update-demo-nautilus-ck4cr
Mar 27 15:04:00.409: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 27 15:04:00.409: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 27 15:04:00.409: INFO: update-demo-nautilus-ck4cr is verified up and running
Mar 27 15:04:00.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-g9c5c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 15:04:00.479: INFO: stderr: ""
Mar 27 15:04:00.479: INFO: stdout: "true"
Mar 27 15:04:00.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-g9c5c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 27 15:04:00.550: INFO: stderr: ""
Mar 27 15:04:00.550: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 27 15:04:00.550: INFO: validating pod update-demo-nautilus-g9c5c
Mar 27 15:04:00.560: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 27 15:04:00.560: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 27 15:04:00.560: INFO: update-demo-nautilus-g9c5c is verified up and running
STEP: scaling down the replication controller 03/27/23 15:04:00.56
Mar 27 15:04:00.561: INFO: scanned /root for discovery docs: <nil>
Mar 27 15:04:00.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar 27 15:04:01.649: INFO: stderr: ""
Mar 27 15:04:01.649: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/27/23 15:04:01.649
Mar 27 15:04:01.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 27 15:04:01.718: INFO: stderr: ""
Mar 27 15:04:01.718: INFO: stdout: "update-demo-nautilus-ck4cr update-demo-nautilus-g9c5c "
STEP: Replicas for name=update-demo: expected=1 actual=2 03/27/23 15:04:01.718
Mar 27 15:04:06.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 27 15:04:06.788: INFO: stderr: ""
Mar 27 15:04:06.788: INFO: stdout: "update-demo-nautilus-g9c5c "
Mar 27 15:04:06.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-g9c5c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 15:04:06.860: INFO: stderr: ""
Mar 27 15:04:06.860: INFO: stdout: "true"
Mar 27 15:04:06.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-g9c5c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 27 15:04:06.929: INFO: stderr: ""
Mar 27 15:04:06.929: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 27 15:04:06.929: INFO: validating pod update-demo-nautilus-g9c5c
Mar 27 15:04:06.935: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 27 15:04:06.935: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 27 15:04:06.935: INFO: update-demo-nautilus-g9c5c is verified up and running
STEP: scaling up the replication controller 03/27/23 15:04:06.935
Mar 27 15:04:06.937: INFO: scanned /root for discovery docs: <nil>
Mar 27 15:04:06.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar 27 15:04:08.026: INFO: stderr: ""
Mar 27 15:04:08.026: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/27/23 15:04:08.026
Mar 27 15:04:08.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 27 15:04:08.101: INFO: stderr: ""
Mar 27 15:04:08.101: INFO: stdout: "update-demo-nautilus-g9c5c update-demo-nautilus-rw482 "
Mar 27 15:04:08.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-g9c5c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 15:04:08.172: INFO: stderr: ""
Mar 27 15:04:08.172: INFO: stdout: "true"
Mar 27 15:04:08.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-g9c5c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 27 15:04:08.242: INFO: stderr: ""
Mar 27 15:04:08.242: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 27 15:04:08.242: INFO: validating pod update-demo-nautilus-g9c5c
Mar 27 15:04:08.247: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 27 15:04:08.247: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 27 15:04:08.247: INFO: update-demo-nautilus-g9c5c is verified up and running
Mar 27 15:04:08.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-rw482 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 15:04:08.310: INFO: stderr: ""
Mar 27 15:04:08.310: INFO: stdout: ""
Mar 27 15:04:08.310: INFO: update-demo-nautilus-rw482 is created but not running
Mar 27 15:04:13.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 27 15:04:13.383: INFO: stderr: ""
Mar 27 15:04:13.383: INFO: stdout: "update-demo-nautilus-g9c5c update-demo-nautilus-rw482 "
Mar 27 15:04:13.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-g9c5c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 15:04:13.448: INFO: stderr: ""
Mar 27 15:04:13.448: INFO: stdout: "true"
Mar 27 15:04:13.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-g9c5c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 27 15:04:13.516: INFO: stderr: ""
Mar 27 15:04:13.516: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 27 15:04:13.516: INFO: validating pod update-demo-nautilus-g9c5c
Mar 27 15:04:13.521: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 27 15:04:13.521: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 27 15:04:13.521: INFO: update-demo-nautilus-g9c5c is verified up and running
Mar 27 15:04:13.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-rw482 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 15:04:13.583: INFO: stderr: ""
Mar 27 15:04:13.583: INFO: stdout: "true"
Mar 27 15:04:13.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-rw482 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 27 15:04:13.643: INFO: stderr: ""
Mar 27 15:04:13.643: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 27 15:04:13.643: INFO: validating pod update-demo-nautilus-rw482
Mar 27 15:04:13.652: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 27 15:04:13.652: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 27 15:04:13.652: INFO: update-demo-nautilus-rw482 is verified up and running
STEP: using delete to clean up resources 03/27/23 15:04:13.652
Mar 27 15:04:13.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 delete --grace-period=0 --force -f -'
Mar 27 15:04:13.726: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 27 15:04:13.726: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 27 15:04:13.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get rc,svc -l name=update-demo --no-headers'
Mar 27 15:04:13.810: INFO: stderr: "No resources found in kubectl-4107 namespace.\n"
Mar 27 15:04:13.810: INFO: stdout: ""
Mar 27 15:04:13.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 27 15:04:13.885: INFO: stderr: ""
Mar 27 15:04:13.885: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 27 15:04:13.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4107" for this suite. 03/27/23 15:04:13.889
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":185,"skipped":3464,"failed":0}
------------------------------
• [SLOW TEST] [19.610 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:03:54.285
    Mar 27 15:03:54.285: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubectl 03/27/23 15:03:54.286
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:03:54.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:03:54.34
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 03/27/23 15:03:54.405
    Mar 27 15:03:54.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 create -f -'
    Mar 27 15:03:55.016: INFO: stderr: ""
    Mar 27 15:03:55.016: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/27/23 15:03:55.016
    Mar 27 15:03:55.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 27 15:03:55.131: INFO: stderr: ""
    Mar 27 15:03:55.131: INFO: stdout: "update-demo-nautilus-ck4cr "
    STEP: Replicas for name=update-demo: expected=2 actual=1 03/27/23 15:03:55.131
    Mar 27 15:04:00.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 27 15:04:00.270: INFO: stderr: ""
    Mar 27 15:04:00.270: INFO: stdout: "update-demo-nautilus-ck4cr update-demo-nautilus-g9c5c "
    Mar 27 15:04:00.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-ck4cr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 15:04:00.335: INFO: stderr: ""
    Mar 27 15:04:00.335: INFO: stdout: "true"
    Mar 27 15:04:00.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-ck4cr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 27 15:04:00.396: INFO: stderr: ""
    Mar 27 15:04:00.396: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 27 15:04:00.396: INFO: validating pod update-demo-nautilus-ck4cr
    Mar 27 15:04:00.409: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 27 15:04:00.409: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 27 15:04:00.409: INFO: update-demo-nautilus-ck4cr is verified up and running
    Mar 27 15:04:00.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-g9c5c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 15:04:00.479: INFO: stderr: ""
    Mar 27 15:04:00.479: INFO: stdout: "true"
    Mar 27 15:04:00.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-g9c5c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 27 15:04:00.550: INFO: stderr: ""
    Mar 27 15:04:00.550: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 27 15:04:00.550: INFO: validating pod update-demo-nautilus-g9c5c
    Mar 27 15:04:00.560: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 27 15:04:00.560: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 27 15:04:00.560: INFO: update-demo-nautilus-g9c5c is verified up and running
    STEP: scaling down the replication controller 03/27/23 15:04:00.56
    Mar 27 15:04:00.561: INFO: scanned /root for discovery docs: <nil>
    Mar 27 15:04:00.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Mar 27 15:04:01.649: INFO: stderr: ""
    Mar 27 15:04:01.649: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/27/23 15:04:01.649
    Mar 27 15:04:01.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 27 15:04:01.718: INFO: stderr: ""
    Mar 27 15:04:01.718: INFO: stdout: "update-demo-nautilus-ck4cr update-demo-nautilus-g9c5c "
    STEP: Replicas for name=update-demo: expected=1 actual=2 03/27/23 15:04:01.718
    Mar 27 15:04:06.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 27 15:04:06.788: INFO: stderr: ""
    Mar 27 15:04:06.788: INFO: stdout: "update-demo-nautilus-g9c5c "
    Mar 27 15:04:06.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-g9c5c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 15:04:06.860: INFO: stderr: ""
    Mar 27 15:04:06.860: INFO: stdout: "true"
    Mar 27 15:04:06.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-g9c5c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 27 15:04:06.929: INFO: stderr: ""
    Mar 27 15:04:06.929: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 27 15:04:06.929: INFO: validating pod update-demo-nautilus-g9c5c
    Mar 27 15:04:06.935: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 27 15:04:06.935: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 27 15:04:06.935: INFO: update-demo-nautilus-g9c5c is verified up and running
    STEP: scaling up the replication controller 03/27/23 15:04:06.935
    Mar 27 15:04:06.937: INFO: scanned /root for discovery docs: <nil>
    Mar 27 15:04:06.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Mar 27 15:04:08.026: INFO: stderr: ""
    Mar 27 15:04:08.026: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/27/23 15:04:08.026
    Mar 27 15:04:08.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 27 15:04:08.101: INFO: stderr: ""
    Mar 27 15:04:08.101: INFO: stdout: "update-demo-nautilus-g9c5c update-demo-nautilus-rw482 "
    Mar 27 15:04:08.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-g9c5c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 15:04:08.172: INFO: stderr: ""
    Mar 27 15:04:08.172: INFO: stdout: "true"
    Mar 27 15:04:08.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-g9c5c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 27 15:04:08.242: INFO: stderr: ""
    Mar 27 15:04:08.242: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 27 15:04:08.242: INFO: validating pod update-demo-nautilus-g9c5c
    Mar 27 15:04:08.247: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 27 15:04:08.247: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 27 15:04:08.247: INFO: update-demo-nautilus-g9c5c is verified up and running
    Mar 27 15:04:08.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-rw482 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 15:04:08.310: INFO: stderr: ""
    Mar 27 15:04:08.310: INFO: stdout: ""
    Mar 27 15:04:08.310: INFO: update-demo-nautilus-rw482 is created but not running
    Mar 27 15:04:13.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 27 15:04:13.383: INFO: stderr: ""
    Mar 27 15:04:13.383: INFO: stdout: "update-demo-nautilus-g9c5c update-demo-nautilus-rw482 "
    Mar 27 15:04:13.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-g9c5c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 15:04:13.448: INFO: stderr: ""
    Mar 27 15:04:13.448: INFO: stdout: "true"
    Mar 27 15:04:13.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-g9c5c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 27 15:04:13.516: INFO: stderr: ""
    Mar 27 15:04:13.516: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 27 15:04:13.516: INFO: validating pod update-demo-nautilus-g9c5c
    Mar 27 15:04:13.521: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 27 15:04:13.521: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 27 15:04:13.521: INFO: update-demo-nautilus-g9c5c is verified up and running
    Mar 27 15:04:13.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-rw482 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 15:04:13.583: INFO: stderr: ""
    Mar 27 15:04:13.583: INFO: stdout: "true"
    Mar 27 15:04:13.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods update-demo-nautilus-rw482 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 27 15:04:13.643: INFO: stderr: ""
    Mar 27 15:04:13.643: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 27 15:04:13.643: INFO: validating pod update-demo-nautilus-rw482
    Mar 27 15:04:13.652: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 27 15:04:13.652: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 27 15:04:13.652: INFO: update-demo-nautilus-rw482 is verified up and running
    STEP: using delete to clean up resources 03/27/23 15:04:13.652
    Mar 27 15:04:13.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 delete --grace-period=0 --force -f -'
    Mar 27 15:04:13.726: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 27 15:04:13.726: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar 27 15:04:13.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get rc,svc -l name=update-demo --no-headers'
    Mar 27 15:04:13.810: INFO: stderr: "No resources found in kubectl-4107 namespace.\n"
    Mar 27 15:04:13.810: INFO: stdout: ""
    Mar 27 15:04:13.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4107 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 27 15:04:13.885: INFO: stderr: ""
    Mar 27 15:04:13.885: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 27 15:04:13.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4107" for this suite. 03/27/23 15:04:13.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:04:13.896
Mar 27 15:04:13.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename ingress 03/27/23 15:04:13.897
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:04:13.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:04:13.916
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 03/27/23 15:04:13.919
STEP: getting /apis/networking.k8s.io 03/27/23 15:04:13.921
STEP: getting /apis/networking.k8s.iov1 03/27/23 15:04:13.923
STEP: creating 03/27/23 15:04:13.924
STEP: getting 03/27/23 15:04:13.948
STEP: listing 03/27/23 15:04:13.951
STEP: watching 03/27/23 15:04:13.953
Mar 27 15:04:13.953: INFO: starting watch
STEP: cluster-wide listing 03/27/23 15:04:13.954
STEP: cluster-wide watching 03/27/23 15:04:13.96
Mar 27 15:04:13.960: INFO: starting watch
STEP: patching 03/27/23 15:04:13.962
STEP: updating 03/27/23 15:04:13.968
Mar 27 15:04:13.975: INFO: waiting for watch events with expected annotations
Mar 27 15:04:13.975: INFO: saw patched and updated annotations
STEP: patching /status 03/27/23 15:04:13.975
STEP: updating /status 03/27/23 15:04:13.981
STEP: get /status 03/27/23 15:04:13.988
STEP: deleting 03/27/23 15:04:13.991
STEP: deleting a collection 03/27/23 15:04:14.001
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Mar 27 15:04:14.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-2947" for this suite. 03/27/23 15:04:14.02
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":186,"skipped":3476,"failed":0}
------------------------------
• [0.129 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:04:13.896
    Mar 27 15:04:13.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename ingress 03/27/23 15:04:13.897
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:04:13.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:04:13.916
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 03/27/23 15:04:13.919
    STEP: getting /apis/networking.k8s.io 03/27/23 15:04:13.921
    STEP: getting /apis/networking.k8s.iov1 03/27/23 15:04:13.923
    STEP: creating 03/27/23 15:04:13.924
    STEP: getting 03/27/23 15:04:13.948
    STEP: listing 03/27/23 15:04:13.951
    STEP: watching 03/27/23 15:04:13.953
    Mar 27 15:04:13.953: INFO: starting watch
    STEP: cluster-wide listing 03/27/23 15:04:13.954
    STEP: cluster-wide watching 03/27/23 15:04:13.96
    Mar 27 15:04:13.960: INFO: starting watch
    STEP: patching 03/27/23 15:04:13.962
    STEP: updating 03/27/23 15:04:13.968
    Mar 27 15:04:13.975: INFO: waiting for watch events with expected annotations
    Mar 27 15:04:13.975: INFO: saw patched and updated annotations
    STEP: patching /status 03/27/23 15:04:13.975
    STEP: updating /status 03/27/23 15:04:13.981
    STEP: get /status 03/27/23 15:04:13.988
    STEP: deleting 03/27/23 15:04:13.991
    STEP: deleting a collection 03/27/23 15:04:14.001
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Mar 27 15:04:14.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-2947" for this suite. 03/27/23 15:04:14.02
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:04:14.025
Mar 27 15:04:14.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename watch 03/27/23 15:04:14.027
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:04:14.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:04:14.059
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 03/27/23 15:04:14.062
STEP: creating a new configmap 03/27/23 15:04:14.063
STEP: modifying the configmap once 03/27/23 15:04:14.067
STEP: closing the watch once it receives two notifications 03/27/23 15:04:14.076
Mar 27 15:04:14.076: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-540  f2fed902-6ed7-4241-8f2f-e08abb96ab8f 36795 0 2023-03-27 15:04:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-27 15:04:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 15:04:14.076: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-540  f2fed902-6ed7-4241-8f2f-e08abb96ab8f 36796 0 2023-03-27 15:04:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-27 15:04:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 03/27/23 15:04:14.076
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/27/23 15:04:14.084
STEP: deleting the configmap 03/27/23 15:04:14.085
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/27/23 15:04:14.093
Mar 27 15:04:14.094: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-540  f2fed902-6ed7-4241-8f2f-e08abb96ab8f 36797 0 2023-03-27 15:04:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-27 15:04:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 15:04:14.094: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-540  f2fed902-6ed7-4241-8f2f-e08abb96ab8f 36798 0 2023-03-27 15:04:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-27 15:04:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar 27 15:04:14.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-540" for this suite. 03/27/23 15:04:14.1
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":187,"skipped":3479,"failed":0}
------------------------------
• [0.080 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:04:14.025
    Mar 27 15:04:14.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename watch 03/27/23 15:04:14.027
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:04:14.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:04:14.059
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 03/27/23 15:04:14.062
    STEP: creating a new configmap 03/27/23 15:04:14.063
    STEP: modifying the configmap once 03/27/23 15:04:14.067
    STEP: closing the watch once it receives two notifications 03/27/23 15:04:14.076
    Mar 27 15:04:14.076: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-540  f2fed902-6ed7-4241-8f2f-e08abb96ab8f 36795 0 2023-03-27 15:04:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-27 15:04:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 15:04:14.076: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-540  f2fed902-6ed7-4241-8f2f-e08abb96ab8f 36796 0 2023-03-27 15:04:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-27 15:04:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 03/27/23 15:04:14.076
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/27/23 15:04:14.084
    STEP: deleting the configmap 03/27/23 15:04:14.085
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/27/23 15:04:14.093
    Mar 27 15:04:14.094: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-540  f2fed902-6ed7-4241-8f2f-e08abb96ab8f 36797 0 2023-03-27 15:04:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-27 15:04:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 15:04:14.094: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-540  f2fed902-6ed7-4241-8f2f-e08abb96ab8f 36798 0 2023-03-27 15:04:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-27 15:04:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar 27 15:04:14.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-540" for this suite. 03/27/23 15:04:14.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:04:14.108
Mar 27 15:04:14.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename downward-api 03/27/23 15:04:14.109
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:04:14.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:04:14.136
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 03/27/23 15:04:14.139
Mar 27 15:04:14.151: INFO: Waiting up to 5m0s for pod "downwardapi-volume-526bc738-9a27-44a5-9e58-14bc6a1b6b3a" in namespace "downward-api-7643" to be "Succeeded or Failed"
Mar 27 15:04:14.154: INFO: Pod "downwardapi-volume-526bc738-9a27-44a5-9e58-14bc6a1b6b3a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.344334ms
Mar 27 15:04:16.158: INFO: Pod "downwardapi-volume-526bc738-9a27-44a5-9e58-14bc6a1b6b3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007174681s
Mar 27 15:04:18.160: INFO: Pod "downwardapi-volume-526bc738-9a27-44a5-9e58-14bc6a1b6b3a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009137392s
Mar 27 15:04:20.158: INFO: Pod "downwardapi-volume-526bc738-9a27-44a5-9e58-14bc6a1b6b3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007190027s
STEP: Saw pod success 03/27/23 15:04:20.158
Mar 27 15:04:20.158: INFO: Pod "downwardapi-volume-526bc738-9a27-44a5-9e58-14bc6a1b6b3a" satisfied condition "Succeeded or Failed"
Mar 27 15:04:20.161: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-526bc738-9a27-44a5-9e58-14bc6a1b6b3a container client-container: <nil>
STEP: delete the pod 03/27/23 15:04:20.175
Mar 27 15:04:20.186: INFO: Waiting for pod downwardapi-volume-526bc738-9a27-44a5-9e58-14bc6a1b6b3a to disappear
Mar 27 15:04:20.188: INFO: Pod downwardapi-volume-526bc738-9a27-44a5-9e58-14bc6a1b6b3a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 27 15:04:20.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7643" for this suite. 03/27/23 15:04:20.193
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":188,"skipped":3504,"failed":0}
------------------------------
• [SLOW TEST] [6.090 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:04:14.108
    Mar 27 15:04:14.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename downward-api 03/27/23 15:04:14.109
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:04:14.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:04:14.136
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 03/27/23 15:04:14.139
    Mar 27 15:04:14.151: INFO: Waiting up to 5m0s for pod "downwardapi-volume-526bc738-9a27-44a5-9e58-14bc6a1b6b3a" in namespace "downward-api-7643" to be "Succeeded or Failed"
    Mar 27 15:04:14.154: INFO: Pod "downwardapi-volume-526bc738-9a27-44a5-9e58-14bc6a1b6b3a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.344334ms
    Mar 27 15:04:16.158: INFO: Pod "downwardapi-volume-526bc738-9a27-44a5-9e58-14bc6a1b6b3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007174681s
    Mar 27 15:04:18.160: INFO: Pod "downwardapi-volume-526bc738-9a27-44a5-9e58-14bc6a1b6b3a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009137392s
    Mar 27 15:04:20.158: INFO: Pod "downwardapi-volume-526bc738-9a27-44a5-9e58-14bc6a1b6b3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007190027s
    STEP: Saw pod success 03/27/23 15:04:20.158
    Mar 27 15:04:20.158: INFO: Pod "downwardapi-volume-526bc738-9a27-44a5-9e58-14bc6a1b6b3a" satisfied condition "Succeeded or Failed"
    Mar 27 15:04:20.161: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-526bc738-9a27-44a5-9e58-14bc6a1b6b3a container client-container: <nil>
    STEP: delete the pod 03/27/23 15:04:20.175
    Mar 27 15:04:20.186: INFO: Waiting for pod downwardapi-volume-526bc738-9a27-44a5-9e58-14bc6a1b6b3a to disappear
    Mar 27 15:04:20.188: INFO: Pod downwardapi-volume-526bc738-9a27-44a5-9e58-14bc6a1b6b3a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 27 15:04:20.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7643" for this suite. 03/27/23 15:04:20.193
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:04:20.198
Mar 27 15:04:20.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename emptydir 03/27/23 15:04:20.2
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:04:20.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:04:20.223
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 03/27/23 15:04:20.227
Mar 27 15:04:20.241: INFO: Waiting up to 5m0s for pod "pod-8b82df2e-4b6f-41e7-b778-a97315cbb592" in namespace "emptydir-4427" to be "Succeeded or Failed"
Mar 27 15:04:20.244: INFO: Pod "pod-8b82df2e-4b6f-41e7-b778-a97315cbb592": Phase="Pending", Reason="", readiness=false. Elapsed: 2.930453ms
Mar 27 15:04:22.249: INFO: Pod "pod-8b82df2e-4b6f-41e7-b778-a97315cbb592": Phase="Running", Reason="", readiness=true. Elapsed: 2.007892718s
Mar 27 15:04:24.249: INFO: Pod "pod-8b82df2e-4b6f-41e7-b778-a97315cbb592": Phase="Running", Reason="", readiness=false. Elapsed: 4.008271437s
Mar 27 15:04:26.250: INFO: Pod "pod-8b82df2e-4b6f-41e7-b778-a97315cbb592": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008764853s
STEP: Saw pod success 03/27/23 15:04:26.25
Mar 27 15:04:26.250: INFO: Pod "pod-8b82df2e-4b6f-41e7-b778-a97315cbb592" satisfied condition "Succeeded or Failed"
Mar 27 15:04:26.253: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-8b82df2e-4b6f-41e7-b778-a97315cbb592 container test-container: <nil>
STEP: delete the pod 03/27/23 15:04:26.26
Mar 27 15:04:26.273: INFO: Waiting for pod pod-8b82df2e-4b6f-41e7-b778-a97315cbb592 to disappear
Mar 27 15:04:26.276: INFO: Pod pod-8b82df2e-4b6f-41e7-b778-a97315cbb592 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 27 15:04:26.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4427" for this suite. 03/27/23 15:04:26.282
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":189,"skipped":3508,"failed":0}
------------------------------
• [SLOW TEST] [6.092 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:04:20.198
    Mar 27 15:04:20.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename emptydir 03/27/23 15:04:20.2
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:04:20.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:04:20.223
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/27/23 15:04:20.227
    Mar 27 15:04:20.241: INFO: Waiting up to 5m0s for pod "pod-8b82df2e-4b6f-41e7-b778-a97315cbb592" in namespace "emptydir-4427" to be "Succeeded or Failed"
    Mar 27 15:04:20.244: INFO: Pod "pod-8b82df2e-4b6f-41e7-b778-a97315cbb592": Phase="Pending", Reason="", readiness=false. Elapsed: 2.930453ms
    Mar 27 15:04:22.249: INFO: Pod "pod-8b82df2e-4b6f-41e7-b778-a97315cbb592": Phase="Running", Reason="", readiness=true. Elapsed: 2.007892718s
    Mar 27 15:04:24.249: INFO: Pod "pod-8b82df2e-4b6f-41e7-b778-a97315cbb592": Phase="Running", Reason="", readiness=false. Elapsed: 4.008271437s
    Mar 27 15:04:26.250: INFO: Pod "pod-8b82df2e-4b6f-41e7-b778-a97315cbb592": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008764853s
    STEP: Saw pod success 03/27/23 15:04:26.25
    Mar 27 15:04:26.250: INFO: Pod "pod-8b82df2e-4b6f-41e7-b778-a97315cbb592" satisfied condition "Succeeded or Failed"
    Mar 27 15:04:26.253: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-8b82df2e-4b6f-41e7-b778-a97315cbb592 container test-container: <nil>
    STEP: delete the pod 03/27/23 15:04:26.26
    Mar 27 15:04:26.273: INFO: Waiting for pod pod-8b82df2e-4b6f-41e7-b778-a97315cbb592 to disappear
    Mar 27 15:04:26.276: INFO: Pod pod-8b82df2e-4b6f-41e7-b778-a97315cbb592 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 27 15:04:26.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4427" for this suite. 03/27/23 15:04:26.282
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:04:26.293
Mar 27 15:04:26.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 15:04:26.294
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:04:26.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:04:26.317
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 03/27/23 15:04:26.32
Mar 27 15:04:26.327: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15123c92-859e-48e7-892a-07b777f49a7f" in namespace "projected-8271" to be "Succeeded or Failed"
Mar 27 15:04:26.331: INFO: Pod "downwardapi-volume-15123c92-859e-48e7-892a-07b777f49a7f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.558933ms
Mar 27 15:04:28.334: INFO: Pod "downwardapi-volume-15123c92-859e-48e7-892a-07b777f49a7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007069995s
Mar 27 15:04:30.335: INFO: Pod "downwardapi-volume-15123c92-859e-48e7-892a-07b777f49a7f": Phase="Running", Reason="", readiness=false. Elapsed: 4.008322651s
Mar 27 15:04:32.334: INFO: Pod "downwardapi-volume-15123c92-859e-48e7-892a-07b777f49a7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007386687s
STEP: Saw pod success 03/27/23 15:04:32.334
Mar 27 15:04:32.335: INFO: Pod "downwardapi-volume-15123c92-859e-48e7-892a-07b777f49a7f" satisfied condition "Succeeded or Failed"
Mar 27 15:04:32.338: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-15123c92-859e-48e7-892a-07b777f49a7f container client-container: <nil>
STEP: delete the pod 03/27/23 15:04:32.35
Mar 27 15:04:32.369: INFO: Waiting for pod downwardapi-volume-15123c92-859e-48e7-892a-07b777f49a7f to disappear
Mar 27 15:04:32.372: INFO: Pod downwardapi-volume-15123c92-859e-48e7-892a-07b777f49a7f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 27 15:04:32.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8271" for this suite. 03/27/23 15:04:32.376
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":190,"skipped":3510,"failed":0}
------------------------------
• [SLOW TEST] [6.096 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:04:26.293
    Mar 27 15:04:26.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 15:04:26.294
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:04:26.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:04:26.317
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 03/27/23 15:04:26.32
    Mar 27 15:04:26.327: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15123c92-859e-48e7-892a-07b777f49a7f" in namespace "projected-8271" to be "Succeeded or Failed"
    Mar 27 15:04:26.331: INFO: Pod "downwardapi-volume-15123c92-859e-48e7-892a-07b777f49a7f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.558933ms
    Mar 27 15:04:28.334: INFO: Pod "downwardapi-volume-15123c92-859e-48e7-892a-07b777f49a7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007069995s
    Mar 27 15:04:30.335: INFO: Pod "downwardapi-volume-15123c92-859e-48e7-892a-07b777f49a7f": Phase="Running", Reason="", readiness=false. Elapsed: 4.008322651s
    Mar 27 15:04:32.334: INFO: Pod "downwardapi-volume-15123c92-859e-48e7-892a-07b777f49a7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007386687s
    STEP: Saw pod success 03/27/23 15:04:32.334
    Mar 27 15:04:32.335: INFO: Pod "downwardapi-volume-15123c92-859e-48e7-892a-07b777f49a7f" satisfied condition "Succeeded or Failed"
    Mar 27 15:04:32.338: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-15123c92-859e-48e7-892a-07b777f49a7f container client-container: <nil>
    STEP: delete the pod 03/27/23 15:04:32.35
    Mar 27 15:04:32.369: INFO: Waiting for pod downwardapi-volume-15123c92-859e-48e7-892a-07b777f49a7f to disappear
    Mar 27 15:04:32.372: INFO: Pod downwardapi-volume-15123c92-859e-48e7-892a-07b777f49a7f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 27 15:04:32.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8271" for this suite. 03/27/23 15:04:32.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:04:32.392
Mar 27 15:04:32.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 15:04:32.393
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:04:32.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:04:32.415
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-fb7a12a7-8863-46eb-b596-9d5dc6ca661d 03/27/23 15:04:32.418
STEP: Creating a pod to test consume configMaps 03/27/23 15:04:32.426
Mar 27 15:04:32.435: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3" in namespace "projected-5255" to be "Succeeded or Failed"
Mar 27 15:04:32.444: INFO: Pod "pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.076535ms
Mar 27 15:04:39.148: INFO: Pod "pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3": Phase="Running", Reason="", readiness=false. Elapsed: 6.713528834s
Mar 27 15:04:40.450: INFO: Pod "pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3": Phase="Running", Reason="", readiness=false. Elapsed: 8.015600425s
Mar 27 15:04:43.776: INFO: Pod "pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3": Phase="Running", Reason="", readiness=false. Elapsed: 11.341789046s
Mar 27 15:04:44.448: INFO: Pod "pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.013444505s
STEP: Saw pod success 03/27/23 15:04:44.448
Mar 27 15:04:44.448: INFO: Pod "pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3" satisfied condition "Succeeded or Failed"
Mar 27 15:04:44.658: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 15:04:44.669
Mar 27 15:04:44.962: INFO: Waiting for pod pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3 to disappear
Mar 27 15:04:44.965: INFO: Pod pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 27 15:04:44.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5255" for this suite. 03/27/23 15:04:45.04
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":191,"skipped":3568,"failed":0}
------------------------------
• [SLOW TEST] [12.680 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:04:32.392
    Mar 27 15:04:32.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 15:04:32.393
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:04:32.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:04:32.415
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-fb7a12a7-8863-46eb-b596-9d5dc6ca661d 03/27/23 15:04:32.418
    STEP: Creating a pod to test consume configMaps 03/27/23 15:04:32.426
    Mar 27 15:04:32.435: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3" in namespace "projected-5255" to be "Succeeded or Failed"
    Mar 27 15:04:32.444: INFO: Pod "pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.076535ms
    Mar 27 15:04:39.148: INFO: Pod "pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3": Phase="Running", Reason="", readiness=false. Elapsed: 6.713528834s
    Mar 27 15:04:40.450: INFO: Pod "pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3": Phase="Running", Reason="", readiness=false. Elapsed: 8.015600425s
    Mar 27 15:04:43.776: INFO: Pod "pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3": Phase="Running", Reason="", readiness=false. Elapsed: 11.341789046s
    Mar 27 15:04:44.448: INFO: Pod "pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.013444505s
    STEP: Saw pod success 03/27/23 15:04:44.448
    Mar 27 15:04:44.448: INFO: Pod "pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3" satisfied condition "Succeeded or Failed"
    Mar 27 15:04:44.658: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 15:04:44.669
    Mar 27 15:04:44.962: INFO: Waiting for pod pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3 to disappear
    Mar 27 15:04:44.965: INFO: Pod pod-projected-configmaps-ab99d1f2-2968-41e3-bd97-509511b81be3 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 27 15:04:44.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5255" for this suite. 03/27/23 15:04:45.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:04:45.078
Mar 27 15:04:45.078: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename sched-pred 03/27/23 15:04:45.079
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:04:45.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:04:45.095
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar 27 15:04:45.103: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 27 15:04:45.111: INFO: Waiting for terminating namespaces to be deleted...
Mar 27 15:04:45.117: INFO: 
Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq before test
Mar 27 15:04:45.362: INFO: canal-vhmnn from kube-system started at 2023-03-27 14:03:29 +0000 UTC (2 container statuses recorded)
Mar 27 15:04:45.362: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 15:04:45.362: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 27 15:04:45.362: INFO: csi-cinder-nodeplugin-lhmqp from kube-system started at 2023-03-27 14:03:29 +0000 UTC (3 container statuses recorded)
Mar 27 15:04:45.362: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 27 15:04:45.362: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 27 15:04:45.362: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 27 15:04:45.362: INFO: kube-proxy-4njm5 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.362: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 27 15:04:45.362: INFO: node-exporter-x8df2 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.362: INFO: 	Container node-exporter ready: true, restart count 0
Mar 27 15:04:45.362: INFO: node-local-dns-zvl8z from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.362: INFO: 	Container node-cache ready: true, restart count 0
Mar 27 15:04:45.362: INFO: syseleven-node-problem-detector-2x2wt from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.362: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 27 15:04:45.362: INFO: user-ssh-keys-agent-65hx9 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.362: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Mar 27 15:04:45.362: INFO: sonobuoy-e2e-job-267544c30cdc4a58 from sonobuoy started at 2023-03-27 14:05:45 +0000 UTC (2 container statuses recorded)
Mar 27 15:04:45.362: INFO: 	Container e2e ready: true, restart count 0
Mar 27 15:04:45.362: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 15:04:45.362: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-bg2pk from sonobuoy started at 2023-03-27 14:05:46 +0000 UTC (2 container statuses recorded)
Mar 27 15:04:45.362: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 15:04:45.362: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 27 15:04:45.362: INFO: 
Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc before test
Mar 27 15:04:45.379: INFO: canal-8sfms from kube-system started at 2023-03-27 14:03:34 +0000 UTC (2 container statuses recorded)
Mar 27 15:04:45.379: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 15:04:45.379: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 27 15:04:45.379: INFO: csi-cinder-nodeplugin-lmbcb from kube-system started at 2023-03-27 14:03:34 +0000 UTC (3 container statuses recorded)
Mar 27 15:04:45.379: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 27 15:04:45.379: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 27 15:04:45.379: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 27 15:04:45.379: INFO: kube-proxy-gj9h9 from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.379: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 27 15:04:45.379: INFO: node-exporter-fgptx from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.379: INFO: 	Container node-exporter ready: true, restart count 0
Mar 27 15:04:45.379: INFO: node-local-dns-tmg5z from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.379: INFO: 	Container node-cache ready: true, restart count 0
Mar 27 15:04:45.379: INFO: syseleven-node-problem-detector-xh9cq from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.380: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 27 15:04:45.380: INFO: user-ssh-keys-agent-x7kpb from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.380: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Mar 27 15:04:45.380: INFO: sonobuoy from sonobuoy started at 2023-03-27 14:05:39 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.380: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 27 15:04:45.380: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-4ztlm from sonobuoy started at 2023-03-27 14:05:46 +0000 UTC (2 container statuses recorded)
Mar 27 15:04:45.380: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 15:04:45.380: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 27 15:04:45.380: INFO: 
Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv before test
Mar 27 15:04:45.388: INFO: calico-kube-controllers-659979cbcb-rxz7c from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.389: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 27 15:04:45.389: INFO: canal-llxzv from kube-system started at 2023-03-27 14:02:28 +0000 UTC (2 container statuses recorded)
Mar 27 15:04:45.389: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 15:04:45.389: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 27 15:04:45.389: INFO: cluster-autoscaler-855c4bc847-b7mwh from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.389: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Mar 27 15:04:45.389: INFO: coredns-5954c46f46-k6bxj from kube-system started at 2023-03-27 14:03:06 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.389: INFO: 	Container coredns ready: true, restart count 0
Mar 27 15:04:45.389: INFO: coredns-5954c46f46-qg6wb from kube-system started at 2023-03-27 14:03:06 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.389: INFO: 	Container coredns ready: true, restart count 0
Mar 27 15:04:45.389: INFO: csi-cinder-nodeplugin-4gffp from kube-system started at 2023-03-27 14:02:28 +0000 UTC (3 container statuses recorded)
Mar 27 15:04:45.389: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 27 15:04:45.390: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 27 15:04:45.390: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 27 15:04:45.390: INFO: dns-autoscaler-69d8768b6-qg5fp from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.390: INFO: 	Container autoscaler ready: true, restart count 0
Mar 27 15:04:45.390: INFO: konnectivity-agent-7c486cb7b8-7wxj6 from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.390: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 15:04:45.390: INFO: konnectivity-agent-7c486cb7b8-ns5pt from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.390: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 15:04:45.391: INFO: kube-proxy-4w8t9 from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.391: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 27 15:04:45.391: INFO: metrics-server-67b4667644-d4vmh from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.391: INFO: 	Container metrics-server ready: true, restart count 0
Mar 27 15:04:45.391: INFO: metrics-server-67b4667644-t5ftq from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.391: INFO: 	Container metrics-server ready: true, restart count 0
Mar 27 15:04:45.391: INFO: node-exporter-8hspl from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.391: INFO: 	Container node-exporter ready: true, restart count 0
Mar 27 15:04:45.391: INFO: node-local-dns-zgn95 from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.391: INFO: 	Container node-cache ready: true, restart count 0
Mar 27 15:04:45.391: INFO: syseleven-node-problem-detector-nvvwp from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.391: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 27 15:04:45.391: INFO: user-ssh-keys-agent-svptx from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 15:04:45.391: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Mar 27 15:04:45.391: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-dgx79 from sonobuoy started at 2023-03-27 14:05:45 +0000 UTC (2 container statuses recorded)
Mar 27 15:04:45.391: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 15:04:45.391: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/27/23 15:04:45.391
Mar 27 15:04:45.427: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2209" to be "running"
Mar 27 15:04:45.430: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.76477ms
Mar 27 15:04:47.435: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.0076701s
Mar 27 15:04:47.435: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/27/23 15:04:47.439
STEP: Trying to apply a random label on the found node. 03/27/23 15:04:47.457
STEP: verifying the node has the label kubernetes.io/e2e-1e786e9e-4df1-4c9f-afe0-0c4f1ec16884 95 03/27/23 15:04:47.471
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/27/23 15:04:47.474
Mar 27 15:04:47.500: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2209" to be "not pending"
Mar 27 15:04:47.505: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.156843ms
Mar 27 15:04:49.508: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008322089s
Mar 27 15:04:49.508: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.1.4 on the node which pod4 resides and expect not scheduled 03/27/23 15:04:49.508
Mar 27 15:04:49.515: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2209" to be "not pending"
Mar 27 15:04:49.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 354.001308ms
Mar 27 15:04:51.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.358107301s
Mar 27 15:04:53.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.358402929s
Mar 27 15:04:55.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.3600527s
Mar 27 15:04:57.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.360007333s
Mar 27 15:04:59.877: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.361259995s
Mar 27 15:05:01.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.358705023s
Mar 27 15:05:10.912: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 21.396166513s
Mar 27 15:05:11.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.357900747s
Mar 27 15:05:13.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.359124198s
Mar 27 15:05:15.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.358642326s
Mar 27 15:05:22.978: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 33.462746227s
Mar 27 15:05:27.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 37.488802223s
Mar 27 15:05:27.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.358739468s
Mar 27 15:05:29.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.35838142s
Mar 27 15:05:31.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.358826729s
Mar 27 15:05:33.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.358779924s
Mar 27 15:05:35.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.358148163s
Mar 27 15:05:37.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.3582795s
Mar 27 15:05:39.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.360164916s
Mar 27 15:05:41.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.357911989s
Mar 27 15:05:43.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.358378626s
Mar 27 15:05:45.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.360910607s
Mar 27 15:05:47.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.36061279s
Mar 27 15:05:49.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.358638338s
Mar 27 15:05:51.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.359341142s
Mar 27 15:05:53.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.359219557s
Mar 27 15:05:55.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.358275462s
Mar 27 15:05:57.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.360055197s
Mar 27 15:05:59.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.359239142s
Mar 27 15:06:01.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.359591086s
Mar 27 15:06:03.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.358813632s
Mar 27 15:06:05.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.359955982s
Mar 27 15:06:07.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.358512481s
Mar 27 15:06:09.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.357643741s
Mar 27 15:06:11.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.358243235s
Mar 27 15:06:13.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.360081593s
Mar 27 15:06:15.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.359308169s
Mar 27 15:06:17.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.360780833s
Mar 27 15:06:19.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.358296181s
Mar 27 15:06:21.878: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.362206482s
Mar 27 15:06:23.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.359659429s
Mar 27 15:06:25.879: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.363518666s
Mar 27 15:06:27.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.360529916s
Mar 27 15:06:29.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.358477392s
Mar 27 15:06:31.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.358093017s
Mar 27 15:06:33.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.358496416s
Mar 27 15:06:35.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.36009941s
Mar 27 15:06:37.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.359144904s
Mar 27 15:06:39.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.358190516s
Mar 27 15:06:41.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.360247841s
Mar 27 15:06:43.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.358498112s
Mar 27 15:06:45.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.358889244s
Mar 27 15:06:47.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.359150762s
Mar 27 15:06:49.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.359682344s
Mar 27 15:06:51.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.358959138s
Mar 27 15:06:53.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.35925053s
Mar 27 15:06:55.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.358430351s
Mar 27 15:06:57.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.358473216s
Mar 27 15:06:59.882: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.366445773s
Mar 27 15:07:01.880: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.364306677s
Mar 27 15:07:03.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.359915767s
Mar 27 15:07:05.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.357990902s
Mar 27 15:07:07.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.358829471s
Mar 27 15:07:09.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.359366971s
Mar 27 15:07:11.879: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.364008244s
Mar 27 15:07:13.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.359008635s
Mar 27 15:07:15.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.357929368s
Mar 27 15:07:17.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.358990443s
Mar 27 15:07:19.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.358678706s
Mar 27 15:07:21.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.359992748s
Mar 27 15:07:23.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.359749636s
Mar 27 15:07:25.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.359848058s
Mar 27 15:07:27.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.360790654s
Mar 27 15:07:29.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.358124883s
Mar 27 15:07:31.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.359818537s
Mar 27 15:07:33.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.359190975s
Mar 27 15:07:35.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.358994288s
Mar 27 15:07:37.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.359544066s
Mar 27 15:07:39.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.358216261s
Mar 27 15:07:41.878: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.362337946s
Mar 27 15:07:43.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.35792118s
Mar 27 15:07:45.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.359343111s
Mar 27 15:07:47.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.358556895s
Mar 27 15:07:49.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.358858239s
Mar 27 15:07:51.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.35816152s
Mar 27 15:07:53.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.360471758s
Mar 27 15:07:55.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.358421312s
Mar 27 15:07:57.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.358203217s
Mar 27 15:07:59.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.358887186s
Mar 27 15:08:01.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.357741214s
Mar 27 15:08:03.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.359081911s
Mar 27 15:08:05.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.358760853s
Mar 27 15:08:07.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.357607379s
Mar 27 15:08:09.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.358426897s
Mar 27 15:08:11.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.35841211s
Mar 27 15:08:13.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.360327806s
Mar 27 15:08:15.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.359096304s
Mar 27 15:08:17.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.358442492s
Mar 27 15:08:19.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.360965936s
Mar 27 15:08:21.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.359288095s
Mar 27 15:08:23.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.358562096s
Mar 27 15:08:25.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.359116712s
Mar 27 15:08:27.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.360056348s
Mar 27 15:08:29.908: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.392873035s
Mar 27 15:08:31.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.359133109s
Mar 27 15:08:33.877: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.361753076s
Mar 27 15:08:35.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.359873108s
Mar 27 15:08:37.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.358263062s
Mar 27 15:08:39.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.360395853s
Mar 27 15:08:41.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.359024487s
Mar 27 15:08:43.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.360298457s
Mar 27 15:08:45.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.359149435s
Mar 27 15:08:47.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.357908951s
Mar 27 15:08:49.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.358113669s
Mar 27 15:08:51.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.359806646s
Mar 27 15:08:53.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.417791726s
Mar 27 15:08:55.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.359022397s
Mar 27 15:08:57.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.358505249s
Mar 27 15:08:59.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.358203404s
Mar 27 15:09:01.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.359681649s
Mar 27 15:09:03.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.361020889s
Mar 27 15:09:05.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.358850373s
Mar 27 15:09:07.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.359980034s
Mar 27 15:09:09.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.357421429s
Mar 27 15:09:11.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.35900635s
Mar 27 15:09:13.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.360886522s
Mar 27 15:09:15.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.359467995s
Mar 27 15:09:17.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.358717527s
Mar 27 15:09:19.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.358830936s
Mar 27 15:09:21.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.358499665s
Mar 27 15:09:23.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.359183776s
Mar 27 15:09:25.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.35926068s
Mar 27 15:09:27.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.358860952s
Mar 27 15:09:29.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.35956882s
Mar 27 15:09:31.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.359433313s
Mar 27 15:09:33.878: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.362488501s
Mar 27 15:09:35.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.359288263s
Mar 27 15:09:37.877: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.361760997s
Mar 27 15:09:39.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.358480417s
Mar 27 15:09:41.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.358494802s
Mar 27 15:09:43.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.359591048s
Mar 27 15:09:45.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.358270664s
Mar 27 15:09:47.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.358703917s
Mar 27 15:09:49.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.359761441s
Mar 27 15:09:49.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.367568812s
STEP: removing the label kubernetes.io/e2e-1e786e9e-4df1-4c9f-afe0-0c4f1ec16884 off the node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc 03/27/23 15:09:49.883
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1e786e9e-4df1-4c9f-afe0-0c4f1ec16884 03/27/23 15:09:49.9
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar 27 15:09:49.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2209" for this suite. 03/27/23 15:09:49.91
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":192,"skipped":3635,"failed":0}
------------------------------
• [SLOW TEST] [304.840 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:04:45.078
    Mar 27 15:04:45.078: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename sched-pred 03/27/23 15:04:45.079
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:04:45.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:04:45.095
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar 27 15:04:45.103: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 27 15:04:45.111: INFO: Waiting for terminating namespaces to be deleted...
    Mar 27 15:04:45.117: INFO: 
    Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq before test
    Mar 27 15:04:45.362: INFO: canal-vhmnn from kube-system started at 2023-03-27 14:03:29 +0000 UTC (2 container statuses recorded)
    Mar 27 15:04:45.362: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 15:04:45.362: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 27 15:04:45.362: INFO: csi-cinder-nodeplugin-lhmqp from kube-system started at 2023-03-27 14:03:29 +0000 UTC (3 container statuses recorded)
    Mar 27 15:04:45.362: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar 27 15:04:45.362: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 27 15:04:45.362: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 27 15:04:45.362: INFO: kube-proxy-4njm5 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.362: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 27 15:04:45.362: INFO: node-exporter-x8df2 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.362: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 27 15:04:45.362: INFO: node-local-dns-zvl8z from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.362: INFO: 	Container node-cache ready: true, restart count 0
    Mar 27 15:04:45.362: INFO: syseleven-node-problem-detector-2x2wt from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.362: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 27 15:04:45.362: INFO: user-ssh-keys-agent-65hx9 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.362: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Mar 27 15:04:45.362: INFO: sonobuoy-e2e-job-267544c30cdc4a58 from sonobuoy started at 2023-03-27 14:05:45 +0000 UTC (2 container statuses recorded)
    Mar 27 15:04:45.362: INFO: 	Container e2e ready: true, restart count 0
    Mar 27 15:04:45.362: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 15:04:45.362: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-bg2pk from sonobuoy started at 2023-03-27 14:05:46 +0000 UTC (2 container statuses recorded)
    Mar 27 15:04:45.362: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 15:04:45.362: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 27 15:04:45.362: INFO: 
    Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc before test
    Mar 27 15:04:45.379: INFO: canal-8sfms from kube-system started at 2023-03-27 14:03:34 +0000 UTC (2 container statuses recorded)
    Mar 27 15:04:45.379: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 15:04:45.379: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 27 15:04:45.379: INFO: csi-cinder-nodeplugin-lmbcb from kube-system started at 2023-03-27 14:03:34 +0000 UTC (3 container statuses recorded)
    Mar 27 15:04:45.379: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar 27 15:04:45.379: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 27 15:04:45.379: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 27 15:04:45.379: INFO: kube-proxy-gj9h9 from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.379: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 27 15:04:45.379: INFO: node-exporter-fgptx from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.379: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 27 15:04:45.379: INFO: node-local-dns-tmg5z from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.379: INFO: 	Container node-cache ready: true, restart count 0
    Mar 27 15:04:45.379: INFO: syseleven-node-problem-detector-xh9cq from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.380: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 27 15:04:45.380: INFO: user-ssh-keys-agent-x7kpb from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.380: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Mar 27 15:04:45.380: INFO: sonobuoy from sonobuoy started at 2023-03-27 14:05:39 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.380: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 27 15:04:45.380: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-4ztlm from sonobuoy started at 2023-03-27 14:05:46 +0000 UTC (2 container statuses recorded)
    Mar 27 15:04:45.380: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 15:04:45.380: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 27 15:04:45.380: INFO: 
    Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv before test
    Mar 27 15:04:45.388: INFO: calico-kube-controllers-659979cbcb-rxz7c from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.389: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar 27 15:04:45.389: INFO: canal-llxzv from kube-system started at 2023-03-27 14:02:28 +0000 UTC (2 container statuses recorded)
    Mar 27 15:04:45.389: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 15:04:45.389: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 27 15:04:45.389: INFO: cluster-autoscaler-855c4bc847-b7mwh from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.389: INFO: 	Container cluster-autoscaler ready: true, restart count 0
    Mar 27 15:04:45.389: INFO: coredns-5954c46f46-k6bxj from kube-system started at 2023-03-27 14:03:06 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.389: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 15:04:45.389: INFO: coredns-5954c46f46-qg6wb from kube-system started at 2023-03-27 14:03:06 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.389: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 15:04:45.389: INFO: csi-cinder-nodeplugin-4gffp from kube-system started at 2023-03-27 14:02:28 +0000 UTC (3 container statuses recorded)
    Mar 27 15:04:45.389: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar 27 15:04:45.390: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 27 15:04:45.390: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 27 15:04:45.390: INFO: dns-autoscaler-69d8768b6-qg5fp from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.390: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 27 15:04:45.390: INFO: konnectivity-agent-7c486cb7b8-7wxj6 from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.390: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 15:04:45.390: INFO: konnectivity-agent-7c486cb7b8-ns5pt from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.390: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 15:04:45.391: INFO: kube-proxy-4w8t9 from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.391: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 27 15:04:45.391: INFO: metrics-server-67b4667644-d4vmh from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.391: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 27 15:04:45.391: INFO: metrics-server-67b4667644-t5ftq from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.391: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 27 15:04:45.391: INFO: node-exporter-8hspl from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.391: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 27 15:04:45.391: INFO: node-local-dns-zgn95 from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.391: INFO: 	Container node-cache ready: true, restart count 0
    Mar 27 15:04:45.391: INFO: syseleven-node-problem-detector-nvvwp from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.391: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 27 15:04:45.391: INFO: user-ssh-keys-agent-svptx from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 15:04:45.391: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Mar 27 15:04:45.391: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-dgx79 from sonobuoy started at 2023-03-27 14:05:45 +0000 UTC (2 container statuses recorded)
    Mar 27 15:04:45.391: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 15:04:45.391: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/27/23 15:04:45.391
    Mar 27 15:04:45.427: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2209" to be "running"
    Mar 27 15:04:45.430: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.76477ms
    Mar 27 15:04:47.435: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.0076701s
    Mar 27 15:04:47.435: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/27/23 15:04:47.439
    STEP: Trying to apply a random label on the found node. 03/27/23 15:04:47.457
    STEP: verifying the node has the label kubernetes.io/e2e-1e786e9e-4df1-4c9f-afe0-0c4f1ec16884 95 03/27/23 15:04:47.471
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/27/23 15:04:47.474
    Mar 27 15:04:47.500: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2209" to be "not pending"
    Mar 27 15:04:47.505: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.156843ms
    Mar 27 15:04:49.508: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008322089s
    Mar 27 15:04:49.508: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.1.4 on the node which pod4 resides and expect not scheduled 03/27/23 15:04:49.508
    Mar 27 15:04:49.515: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2209" to be "not pending"
    Mar 27 15:04:49.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 354.001308ms
    Mar 27 15:04:51.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.358107301s
    Mar 27 15:04:53.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.358402929s
    Mar 27 15:04:55.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.3600527s
    Mar 27 15:04:57.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.360007333s
    Mar 27 15:04:59.877: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.361259995s
    Mar 27 15:05:01.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.358705023s
    Mar 27 15:05:10.912: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 21.396166513s
    Mar 27 15:05:11.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.357900747s
    Mar 27 15:05:13.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.359124198s
    Mar 27 15:05:15.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.358642326s
    Mar 27 15:05:22.978: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 33.462746227s
    Mar 27 15:05:27.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 37.488802223s
    Mar 27 15:05:27.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.358739468s
    Mar 27 15:05:29.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.35838142s
    Mar 27 15:05:31.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.358826729s
    Mar 27 15:05:33.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.358779924s
    Mar 27 15:05:35.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.358148163s
    Mar 27 15:05:37.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.3582795s
    Mar 27 15:05:39.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.360164916s
    Mar 27 15:05:41.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.357911989s
    Mar 27 15:05:43.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.358378626s
    Mar 27 15:05:45.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.360910607s
    Mar 27 15:05:47.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.36061279s
    Mar 27 15:05:49.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.358638338s
    Mar 27 15:05:51.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.359341142s
    Mar 27 15:05:53.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.359219557s
    Mar 27 15:05:55.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.358275462s
    Mar 27 15:05:57.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.360055197s
    Mar 27 15:05:59.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.359239142s
    Mar 27 15:06:01.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.359591086s
    Mar 27 15:06:03.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.358813632s
    Mar 27 15:06:05.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.359955982s
    Mar 27 15:06:07.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.358512481s
    Mar 27 15:06:09.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.357643741s
    Mar 27 15:06:11.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.358243235s
    Mar 27 15:06:13.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.360081593s
    Mar 27 15:06:15.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.359308169s
    Mar 27 15:06:17.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.360780833s
    Mar 27 15:06:19.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.358296181s
    Mar 27 15:06:21.878: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.362206482s
    Mar 27 15:06:23.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.359659429s
    Mar 27 15:06:25.879: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.363518666s
    Mar 27 15:06:27.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.360529916s
    Mar 27 15:06:29.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.358477392s
    Mar 27 15:06:31.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.358093017s
    Mar 27 15:06:33.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.358496416s
    Mar 27 15:06:35.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.36009941s
    Mar 27 15:06:37.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.359144904s
    Mar 27 15:06:39.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.358190516s
    Mar 27 15:06:41.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.360247841s
    Mar 27 15:06:43.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.358498112s
    Mar 27 15:06:45.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.358889244s
    Mar 27 15:06:47.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.359150762s
    Mar 27 15:06:49.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.359682344s
    Mar 27 15:06:51.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.358959138s
    Mar 27 15:06:53.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.35925053s
    Mar 27 15:06:55.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.358430351s
    Mar 27 15:06:57.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.358473216s
    Mar 27 15:06:59.882: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.366445773s
    Mar 27 15:07:01.880: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.364306677s
    Mar 27 15:07:03.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.359915767s
    Mar 27 15:07:05.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.357990902s
    Mar 27 15:07:07.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.358829471s
    Mar 27 15:07:09.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.359366971s
    Mar 27 15:07:11.879: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.364008244s
    Mar 27 15:07:13.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.359008635s
    Mar 27 15:07:15.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.357929368s
    Mar 27 15:07:17.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.358990443s
    Mar 27 15:07:19.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.358678706s
    Mar 27 15:07:21.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.359992748s
    Mar 27 15:07:23.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.359749636s
    Mar 27 15:07:25.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.359848058s
    Mar 27 15:07:27.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.360790654s
    Mar 27 15:07:29.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.358124883s
    Mar 27 15:07:31.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.359818537s
    Mar 27 15:07:33.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.359190975s
    Mar 27 15:07:35.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.358994288s
    Mar 27 15:07:37.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.359544066s
    Mar 27 15:07:39.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.358216261s
    Mar 27 15:07:41.878: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.362337946s
    Mar 27 15:07:43.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.35792118s
    Mar 27 15:07:45.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.359343111s
    Mar 27 15:07:47.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.358556895s
    Mar 27 15:07:49.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.358858239s
    Mar 27 15:07:51.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.35816152s
    Mar 27 15:07:53.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.360471758s
    Mar 27 15:07:55.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.358421312s
    Mar 27 15:07:57.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.358203217s
    Mar 27 15:07:59.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.358887186s
    Mar 27 15:08:01.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.357741214s
    Mar 27 15:08:03.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.359081911s
    Mar 27 15:08:05.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.358760853s
    Mar 27 15:08:07.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.357607379s
    Mar 27 15:08:09.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.358426897s
    Mar 27 15:08:11.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.35841211s
    Mar 27 15:08:13.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.360327806s
    Mar 27 15:08:15.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.359096304s
    Mar 27 15:08:17.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.358442492s
    Mar 27 15:08:19.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.360965936s
    Mar 27 15:08:21.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.359288095s
    Mar 27 15:08:23.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.358562096s
    Mar 27 15:08:25.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.359116712s
    Mar 27 15:08:27.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.360056348s
    Mar 27 15:08:29.908: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.392873035s
    Mar 27 15:08:31.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.359133109s
    Mar 27 15:08:33.877: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.361753076s
    Mar 27 15:08:35.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.359873108s
    Mar 27 15:08:37.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.358263062s
    Mar 27 15:08:39.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.360395853s
    Mar 27 15:08:41.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.359024487s
    Mar 27 15:08:43.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.360298457s
    Mar 27 15:08:45.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.359149435s
    Mar 27 15:08:47.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.357908951s
    Mar 27 15:08:49.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.358113669s
    Mar 27 15:08:51.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.359806646s
    Mar 27 15:08:53.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.417791726s
    Mar 27 15:08:55.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.359022397s
    Mar 27 15:08:57.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.358505249s
    Mar 27 15:08:59.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.358203404s
    Mar 27 15:09:01.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.359681649s
    Mar 27 15:09:03.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.361020889s
    Mar 27 15:09:05.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.358850373s
    Mar 27 15:09:07.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.359980034s
    Mar 27 15:09:09.873: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.357421429s
    Mar 27 15:09:11.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.35900635s
    Mar 27 15:09:13.876: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.360886522s
    Mar 27 15:09:15.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.359467995s
    Mar 27 15:09:17.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.358717527s
    Mar 27 15:09:19.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.358830936s
    Mar 27 15:09:21.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.358499665s
    Mar 27 15:09:23.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.359183776s
    Mar 27 15:09:25.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.35926068s
    Mar 27 15:09:27.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.358860952s
    Mar 27 15:09:29.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.35956882s
    Mar 27 15:09:31.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.359433313s
    Mar 27 15:09:33.878: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.362488501s
    Mar 27 15:09:35.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.359288263s
    Mar 27 15:09:37.877: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.361760997s
    Mar 27 15:09:39.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.358480417s
    Mar 27 15:09:41.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.358494802s
    Mar 27 15:09:43.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.359591048s
    Mar 27 15:09:45.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.358270664s
    Mar 27 15:09:47.874: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.358703917s
    Mar 27 15:09:49.875: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.359761441s
    Mar 27 15:09:49.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.367568812s
    STEP: removing the label kubernetes.io/e2e-1e786e9e-4df1-4c9f-afe0-0c4f1ec16884 off the node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc 03/27/23 15:09:49.883
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-1e786e9e-4df1-4c9f-afe0-0c4f1ec16884 03/27/23 15:09:49.9
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 15:09:49.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-2209" for this suite. 03/27/23 15:09:49.91
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:09:49.919
Mar 27 15:09:49.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename configmap 03/27/23 15:09:49.92
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:09:49.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:09:49.947
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-9b38f699-f791-40dd-9d50-0be2781ed2b6 03/27/23 15:09:49.954
STEP: Creating configMap with name cm-test-opt-upd-d652dcac-0ede-435c-a4d6-fb388a8f6b7e 03/27/23 15:09:49.96
STEP: Creating the pod 03/27/23 15:09:49.965
Mar 27 15:09:49.978: INFO: Waiting up to 5m0s for pod "pod-configmaps-8a602efc-d588-43e0-bcb3-d153e9235355" in namespace "configmap-8753" to be "running and ready"
Mar 27 15:09:49.980: INFO: Pod "pod-configmaps-8a602efc-d588-43e0-bcb3-d153e9235355": Phase="Pending", Reason="", readiness=false. Elapsed: 2.576237ms
Mar 27 15:09:49.980: INFO: The phase of Pod pod-configmaps-8a602efc-d588-43e0-bcb3-d153e9235355 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:09:51.986: INFO: Pod "pod-configmaps-8a602efc-d588-43e0-bcb3-d153e9235355": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008570654s
Mar 27 15:09:51.986: INFO: The phase of Pod pod-configmaps-8a602efc-d588-43e0-bcb3-d153e9235355 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:09:53.985: INFO: Pod "pod-configmaps-8a602efc-d588-43e0-bcb3-d153e9235355": Phase="Running", Reason="", readiness=true. Elapsed: 4.007400375s
Mar 27 15:09:53.985: INFO: The phase of Pod pod-configmaps-8a602efc-d588-43e0-bcb3-d153e9235355 is Running (Ready = true)
Mar 27 15:09:53.985: INFO: Pod "pod-configmaps-8a602efc-d588-43e0-bcb3-d153e9235355" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-9b38f699-f791-40dd-9d50-0be2781ed2b6 03/27/23 15:09:54.018
STEP: Updating configmap cm-test-opt-upd-d652dcac-0ede-435c-a4d6-fb388a8f6b7e 03/27/23 15:09:54.025
STEP: Creating configMap with name cm-test-opt-create-b7b4839c-72cf-4b1d-9d90-8117a7d54bf8 03/27/23 15:09:54.03
STEP: waiting to observe update in volume 03/27/23 15:09:54.036
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 27 15:11:14.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8753" for this suite. 03/27/23 15:11:14.497
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":193,"skipped":3649,"failed":0}
------------------------------
• [SLOW TEST] [84.586 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:09:49.919
    Mar 27 15:09:49.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename configmap 03/27/23 15:09:49.92
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:09:49.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:09:49.947
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-9b38f699-f791-40dd-9d50-0be2781ed2b6 03/27/23 15:09:49.954
    STEP: Creating configMap with name cm-test-opt-upd-d652dcac-0ede-435c-a4d6-fb388a8f6b7e 03/27/23 15:09:49.96
    STEP: Creating the pod 03/27/23 15:09:49.965
    Mar 27 15:09:49.978: INFO: Waiting up to 5m0s for pod "pod-configmaps-8a602efc-d588-43e0-bcb3-d153e9235355" in namespace "configmap-8753" to be "running and ready"
    Mar 27 15:09:49.980: INFO: Pod "pod-configmaps-8a602efc-d588-43e0-bcb3-d153e9235355": Phase="Pending", Reason="", readiness=false. Elapsed: 2.576237ms
    Mar 27 15:09:49.980: INFO: The phase of Pod pod-configmaps-8a602efc-d588-43e0-bcb3-d153e9235355 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:09:51.986: INFO: Pod "pod-configmaps-8a602efc-d588-43e0-bcb3-d153e9235355": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008570654s
    Mar 27 15:09:51.986: INFO: The phase of Pod pod-configmaps-8a602efc-d588-43e0-bcb3-d153e9235355 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:09:53.985: INFO: Pod "pod-configmaps-8a602efc-d588-43e0-bcb3-d153e9235355": Phase="Running", Reason="", readiness=true. Elapsed: 4.007400375s
    Mar 27 15:09:53.985: INFO: The phase of Pod pod-configmaps-8a602efc-d588-43e0-bcb3-d153e9235355 is Running (Ready = true)
    Mar 27 15:09:53.985: INFO: Pod "pod-configmaps-8a602efc-d588-43e0-bcb3-d153e9235355" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-9b38f699-f791-40dd-9d50-0be2781ed2b6 03/27/23 15:09:54.018
    STEP: Updating configmap cm-test-opt-upd-d652dcac-0ede-435c-a4d6-fb388a8f6b7e 03/27/23 15:09:54.025
    STEP: Creating configMap with name cm-test-opt-create-b7b4839c-72cf-4b1d-9d90-8117a7d54bf8 03/27/23 15:09:54.03
    STEP: waiting to observe update in volume 03/27/23 15:09:54.036
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 27 15:11:14.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8753" for this suite. 03/27/23 15:11:14.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:11:14.513
Mar 27 15:11:14.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename endpointslice 03/27/23 15:11:14.515
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:11:14.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:11:14.55
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 03/27/23 15:11:14.556
STEP: getting /apis/discovery.k8s.io 03/27/23 15:11:14.559
STEP: getting /apis/discovery.k8s.iov1 03/27/23 15:11:14.56
STEP: creating 03/27/23 15:11:14.561
STEP: getting 03/27/23 15:11:14.575
STEP: listing 03/27/23 15:11:14.578
STEP: watching 03/27/23 15:11:14.582
Mar 27 15:11:14.582: INFO: starting watch
STEP: cluster-wide listing 03/27/23 15:11:14.583
STEP: cluster-wide watching 03/27/23 15:11:14.586
Mar 27 15:11:14.586: INFO: starting watch
STEP: patching 03/27/23 15:11:14.587
STEP: updating 03/27/23 15:11:14.592
Mar 27 15:11:14.603: INFO: waiting for watch events with expected annotations
Mar 27 15:11:14.603: INFO: saw patched and updated annotations
STEP: deleting 03/27/23 15:11:14.604
STEP: deleting a collection 03/27/23 15:11:14.624
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar 27 15:11:14.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-117" for this suite. 03/27/23 15:11:14.647
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":194,"skipped":3694,"failed":0}
------------------------------
• [0.141 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:11:14.513
    Mar 27 15:11:14.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename endpointslice 03/27/23 15:11:14.515
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:11:14.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:11:14.55
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 03/27/23 15:11:14.556
    STEP: getting /apis/discovery.k8s.io 03/27/23 15:11:14.559
    STEP: getting /apis/discovery.k8s.iov1 03/27/23 15:11:14.56
    STEP: creating 03/27/23 15:11:14.561
    STEP: getting 03/27/23 15:11:14.575
    STEP: listing 03/27/23 15:11:14.578
    STEP: watching 03/27/23 15:11:14.582
    Mar 27 15:11:14.582: INFO: starting watch
    STEP: cluster-wide listing 03/27/23 15:11:14.583
    STEP: cluster-wide watching 03/27/23 15:11:14.586
    Mar 27 15:11:14.586: INFO: starting watch
    STEP: patching 03/27/23 15:11:14.587
    STEP: updating 03/27/23 15:11:14.592
    Mar 27 15:11:14.603: INFO: waiting for watch events with expected annotations
    Mar 27 15:11:14.603: INFO: saw patched and updated annotations
    STEP: deleting 03/27/23 15:11:14.604
    STEP: deleting a collection 03/27/23 15:11:14.624
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar 27 15:11:14.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-117" for this suite. 03/27/23 15:11:14.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:11:14.655
Mar 27 15:11:14.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename sysctl 03/27/23 15:11:14.656
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:11:14.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:11:14.671
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 03/27/23 15:11:14.674
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 27 15:11:14.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9272" for this suite. 03/27/23 15:11:14.684
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":195,"skipped":3699,"failed":0}
------------------------------
• [0.036 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:11:14.655
    Mar 27 15:11:14.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename sysctl 03/27/23 15:11:14.656
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:11:14.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:11:14.671
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 03/27/23 15:11:14.674
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 27 15:11:14.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-9272" for this suite. 03/27/23 15:11:14.684
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:11:14.693
Mar 27 15:11:14.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename replicaset 03/27/23 15:11:14.694
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:11:14.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:11:14.714
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 03/27/23 15:11:14.721
STEP: Verify that the required pods have come up. 03/27/23 15:11:14.728
Mar 27 15:11:14.732: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 27 15:11:19.736: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/27/23 15:11:19.736
STEP: Getting /status 03/27/23 15:11:19.736
Mar 27 15:11:19.742: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 03/27/23 15:11:19.742
Mar 27 15:11:19.754: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 03/27/23 15:11:19.754
Mar 27 15:11:19.755: INFO: Observed &ReplicaSet event: ADDED
Mar 27 15:11:19.756: INFO: Observed &ReplicaSet event: MODIFIED
Mar 27 15:11:19.756: INFO: Observed &ReplicaSet event: MODIFIED
Mar 27 15:11:19.756: INFO: Observed &ReplicaSet event: MODIFIED
Mar 27 15:11:19.756: INFO: Found replicaset test-rs in namespace replicaset-4534 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 27 15:11:19.756: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 03/27/23 15:11:19.756
Mar 27 15:11:19.757: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 27 15:11:19.769: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 03/27/23 15:11:19.769
Mar 27 15:11:19.771: INFO: Observed &ReplicaSet event: ADDED
Mar 27 15:11:19.771: INFO: Observed &ReplicaSet event: MODIFIED
Mar 27 15:11:19.771: INFO: Observed &ReplicaSet event: MODIFIED
Mar 27 15:11:19.771: INFO: Observed &ReplicaSet event: MODIFIED
Mar 27 15:11:19.771: INFO: Observed replicaset test-rs in namespace replicaset-4534 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 27 15:11:19.771: INFO: Observed &ReplicaSet event: MODIFIED
Mar 27 15:11:19.772: INFO: Found replicaset test-rs in namespace replicaset-4534 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Mar 27 15:11:19.772: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar 27 15:11:19.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4534" for this suite. 03/27/23 15:11:19.777
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":196,"skipped":3730,"failed":0}
------------------------------
• [SLOW TEST] [5.094 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:11:14.693
    Mar 27 15:11:14.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename replicaset 03/27/23 15:11:14.694
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:11:14.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:11:14.714
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 03/27/23 15:11:14.721
    STEP: Verify that the required pods have come up. 03/27/23 15:11:14.728
    Mar 27 15:11:14.732: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 27 15:11:19.736: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/27/23 15:11:19.736
    STEP: Getting /status 03/27/23 15:11:19.736
    Mar 27 15:11:19.742: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 03/27/23 15:11:19.742
    Mar 27 15:11:19.754: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 03/27/23 15:11:19.754
    Mar 27 15:11:19.755: INFO: Observed &ReplicaSet event: ADDED
    Mar 27 15:11:19.756: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 27 15:11:19.756: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 27 15:11:19.756: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 27 15:11:19.756: INFO: Found replicaset test-rs in namespace replicaset-4534 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 27 15:11:19.756: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 03/27/23 15:11:19.756
    Mar 27 15:11:19.757: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 27 15:11:19.769: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 03/27/23 15:11:19.769
    Mar 27 15:11:19.771: INFO: Observed &ReplicaSet event: ADDED
    Mar 27 15:11:19.771: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 27 15:11:19.771: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 27 15:11:19.771: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 27 15:11:19.771: INFO: Observed replicaset test-rs in namespace replicaset-4534 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 27 15:11:19.771: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 27 15:11:19.772: INFO: Found replicaset test-rs in namespace replicaset-4534 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Mar 27 15:11:19.772: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar 27 15:11:19.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4534" for this suite. 03/27/23 15:11:19.777
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:11:19.787
Mar 27 15:11:19.787: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 15:11:19.788
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:11:19.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:11:19.805
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Mar 27 15:11:19.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 15:11:25.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3547" for this suite. 03/27/23 15:11:25.59
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":197,"skipped":3731,"failed":0}
------------------------------
• [SLOW TEST] [5.811 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:11:19.787
    Mar 27 15:11:19.787: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 15:11:19.788
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:11:19.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:11:19.805
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Mar 27 15:11:19.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 15:11:25.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-3547" for this suite. 03/27/23 15:11:25.59
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:11:25.601
Mar 27 15:11:25.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename sched-preemption 03/27/23 15:11:25.603
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:11:25.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:11:25.627
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar 27 15:11:25.651: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 27 15:12:25.683: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:12:25.686
Mar 27 15:12:25.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename sched-preemption-path 03/27/23 15:12:25.687
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:12:25.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:12:25.702
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 03/27/23 15:12:25.705
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/27/23 15:12:25.705
Mar 27 15:12:25.716: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6066" to be "running"
Mar 27 15:12:25.719: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.192303ms
Mar 27 15:12:27.724: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007844523s
Mar 27 15:12:27.724: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/27/23 15:12:27.726
Mar 27 15:12:27.739: INFO: found a healthy node: k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Mar 27 15:12:37.819: INFO: pods created so far: [1 1 1]
Mar 27 15:12:37.819: INFO: length of pods created so far: 3
Mar 27 15:12:39.833: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Mar 27 15:12:46.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6066" for this suite. 03/27/23 15:12:46.839
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar 27 15:12:46.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2864" for this suite. 03/27/23 15:12:46.883
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":198,"skipped":3815,"failed":0}
------------------------------
• [SLOW TEST] [81.332 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:11:25.601
    Mar 27 15:11:25.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename sched-preemption 03/27/23 15:11:25.603
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:11:25.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:11:25.627
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar 27 15:11:25.651: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 27 15:12:25.683: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:12:25.686
    Mar 27 15:12:25.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename sched-preemption-path 03/27/23 15:12:25.687
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:12:25.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:12:25.702
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 03/27/23 15:12:25.705
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/27/23 15:12:25.705
    Mar 27 15:12:25.716: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6066" to be "running"
    Mar 27 15:12:25.719: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.192303ms
    Mar 27 15:12:27.724: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007844523s
    Mar 27 15:12:27.724: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/27/23 15:12:27.726
    Mar 27 15:12:27.739: INFO: found a healthy node: k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Mar 27 15:12:37.819: INFO: pods created so far: [1 1 1]
    Mar 27 15:12:37.819: INFO: length of pods created so far: 3
    Mar 27 15:12:39.833: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Mar 27 15:12:46.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-6066" for this suite. 03/27/23 15:12:46.839
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 15:12:46.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-2864" for this suite. 03/27/23 15:12:46.883
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:12:46.934
Mar 27 15:12:46.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename pods 03/27/23 15:12:46.935
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:12:46.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:12:46.954
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 03/27/23 15:12:46.956
STEP: setting up watch 03/27/23 15:12:46.956
STEP: submitting the pod to kubernetes 03/27/23 15:12:47.062
STEP: verifying the pod is in kubernetes 03/27/23 15:12:47.07
STEP: verifying pod creation was observed 03/27/23 15:12:47.076
Mar 27 15:12:47.076: INFO: Waiting up to 5m0s for pod "pod-submit-remove-6da9e223-54ae-483b-a9d1-b069a09e7b08" in namespace "pods-5492" to be "running"
Mar 27 15:12:47.081: INFO: Pod "pod-submit-remove-6da9e223-54ae-483b-a9d1-b069a09e7b08": Phase="Pending", Reason="", readiness=false. Elapsed: 4.516664ms
Mar 27 15:12:49.085: INFO: Pod "pod-submit-remove-6da9e223-54ae-483b-a9d1-b069a09e7b08": Phase="Running", Reason="", readiness=true. Elapsed: 2.008985123s
Mar 27 15:12:49.085: INFO: Pod "pod-submit-remove-6da9e223-54ae-483b-a9d1-b069a09e7b08" satisfied condition "running"
STEP: deleting the pod gracefully 03/27/23 15:12:49.089
STEP: verifying pod deletion was observed 03/27/23 15:12:49.096
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 27 15:12:51.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5492" for this suite. 03/27/23 15:12:51.414
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":199,"skipped":3835,"failed":0}
------------------------------
• [4.485 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:12:46.934
    Mar 27 15:12:46.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename pods 03/27/23 15:12:46.935
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:12:46.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:12:46.954
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 03/27/23 15:12:46.956
    STEP: setting up watch 03/27/23 15:12:46.956
    STEP: submitting the pod to kubernetes 03/27/23 15:12:47.062
    STEP: verifying the pod is in kubernetes 03/27/23 15:12:47.07
    STEP: verifying pod creation was observed 03/27/23 15:12:47.076
    Mar 27 15:12:47.076: INFO: Waiting up to 5m0s for pod "pod-submit-remove-6da9e223-54ae-483b-a9d1-b069a09e7b08" in namespace "pods-5492" to be "running"
    Mar 27 15:12:47.081: INFO: Pod "pod-submit-remove-6da9e223-54ae-483b-a9d1-b069a09e7b08": Phase="Pending", Reason="", readiness=false. Elapsed: 4.516664ms
    Mar 27 15:12:49.085: INFO: Pod "pod-submit-remove-6da9e223-54ae-483b-a9d1-b069a09e7b08": Phase="Running", Reason="", readiness=true. Elapsed: 2.008985123s
    Mar 27 15:12:49.085: INFO: Pod "pod-submit-remove-6da9e223-54ae-483b-a9d1-b069a09e7b08" satisfied condition "running"
    STEP: deleting the pod gracefully 03/27/23 15:12:49.089
    STEP: verifying pod deletion was observed 03/27/23 15:12:49.096
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 27 15:12:51.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5492" for this suite. 03/27/23 15:12:51.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:12:51.424
Mar 27 15:12:51.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename secrets 03/27/23 15:12:51.425
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:12:51.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:12:51.442
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 03/27/23 15:12:51.444
STEP: listing secrets in all namespaces to ensure that there are more than zero 03/27/23 15:12:51.448
STEP: patching the secret 03/27/23 15:12:51.452
STEP: deleting the secret using a LabelSelector 03/27/23 15:12:51.461
STEP: listing secrets in all namespaces, searching for label name and value in patch 03/27/23 15:12:51.473
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar 27 15:12:51.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6441" for this suite. 03/27/23 15:12:51.485
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":200,"skipped":3897,"failed":0}
------------------------------
• [0.070 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:12:51.424
    Mar 27 15:12:51.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename secrets 03/27/23 15:12:51.425
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:12:51.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:12:51.442
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 03/27/23 15:12:51.444
    STEP: listing secrets in all namespaces to ensure that there are more than zero 03/27/23 15:12:51.448
    STEP: patching the secret 03/27/23 15:12:51.452
    STEP: deleting the secret using a LabelSelector 03/27/23 15:12:51.461
    STEP: listing secrets in all namespaces, searching for label name and value in patch 03/27/23 15:12:51.473
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar 27 15:12:51.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6441" for this suite. 03/27/23 15:12:51.485
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:12:51.494
Mar 27 15:12:51.495: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename webhook 03/27/23 15:12:51.496
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:12:51.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:12:51.514
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/27/23 15:12:51.532
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:12:52.008
STEP: Deploying the webhook pod 03/27/23 15:12:52.019
STEP: Wait for the deployment to be ready 03/27/23 15:12:52.048
Mar 27 15:12:52.054: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 03/27/23 15:12:54.064
STEP: Verifying the service has paired with the endpoint 03/27/23 15:12:54.076
Mar 27 15:12:55.076: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 03/27/23 15:12:55.154
STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 15:12:55.192
STEP: Deleting the collection of validation webhooks 03/27/23 15:12:55.222
STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 15:12:55.311
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 15:12:55.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6750" for this suite. 03/27/23 15:12:55.352
STEP: Destroying namespace "webhook-6750-markers" for this suite. 03/27/23 15:12:55.367
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":201,"skipped":3897,"failed":0}
------------------------------
• [4.256 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:12:51.494
    Mar 27 15:12:51.495: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename webhook 03/27/23 15:12:51.496
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:12:51.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:12:51.514
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/27/23 15:12:51.532
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:12:52.008
    STEP: Deploying the webhook pod 03/27/23 15:12:52.019
    STEP: Wait for the deployment to be ready 03/27/23 15:12:52.048
    Mar 27 15:12:52.054: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 03/27/23 15:12:54.064
    STEP: Verifying the service has paired with the endpoint 03/27/23 15:12:54.076
    Mar 27 15:12:55.076: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 03/27/23 15:12:55.154
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 15:12:55.192
    STEP: Deleting the collection of validation webhooks 03/27/23 15:12:55.222
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 15:12:55.311
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 15:12:55.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6750" for this suite. 03/27/23 15:12:55.352
    STEP: Destroying namespace "webhook-6750-markers" for this suite. 03/27/23 15:12:55.367
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:12:55.751
Mar 27 15:12:55.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename container-lifecycle-hook 03/27/23 15:12:55.752
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:12:55.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:12:55.821
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/27/23 15:12:55.877
Mar 27 15:12:55.890: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1644" to be "running and ready"
Mar 27 15:12:55.899: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.14217ms
Mar 27 15:12:55.899: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:12:57.913: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.023070964s
Mar 27 15:12:57.913: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 27 15:12:57.913: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 03/27/23 15:12:58.005
Mar 27 15:12:58.310: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-1644" to be "running and ready"
Mar 27 15:12:58.313: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.131152ms
Mar 27 15:12:58.313: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:13:00.320: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009531224s
Mar 27 15:13:00.320: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Mar 27 15:13:00.320: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/27/23 15:13:00.325
Mar 27 15:13:00.336: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 27 15:13:00.340: INFO: Pod pod-with-prestop-http-hook still exists
Mar 27 15:13:02.341: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 27 15:13:02.346: INFO: Pod pod-with-prestop-http-hook still exists
Mar 27 15:13:04.341: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 27 15:13:04.351: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 03/27/23 15:13:04.351
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar 27 15:13:04.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1644" for this suite. 03/27/23 15:13:04.367
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":202,"skipped":3898,"failed":0}
------------------------------
• [SLOW TEST] [8.624 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:12:55.751
    Mar 27 15:12:55.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/27/23 15:12:55.752
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:12:55.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:12:55.821
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/27/23 15:12:55.877
    Mar 27 15:12:55.890: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1644" to be "running and ready"
    Mar 27 15:12:55.899: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.14217ms
    Mar 27 15:12:55.899: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:12:57.913: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.023070964s
    Mar 27 15:12:57.913: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 27 15:12:57.913: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 03/27/23 15:12:58.005
    Mar 27 15:12:58.310: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-1644" to be "running and ready"
    Mar 27 15:12:58.313: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.131152ms
    Mar 27 15:12:58.313: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:13:00.320: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009531224s
    Mar 27 15:13:00.320: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Mar 27 15:13:00.320: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/27/23 15:13:00.325
    Mar 27 15:13:00.336: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 27 15:13:00.340: INFO: Pod pod-with-prestop-http-hook still exists
    Mar 27 15:13:02.341: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 27 15:13:02.346: INFO: Pod pod-with-prestop-http-hook still exists
    Mar 27 15:13:04.341: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 27 15:13:04.351: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 03/27/23 15:13:04.351
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar 27 15:13:04.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-1644" for this suite. 03/27/23 15:13:04.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:13:04.376
Mar 27 15:13:04.376: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 15:13:04.377
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:13:04.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:13:04.393
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 03/27/23 15:13:04.4
Mar 27 15:13:04.407: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9d1780f-8614-4aca-8126-17288dfafc16" in namespace "projected-7548" to be "Succeeded or Failed"
Mar 27 15:13:04.410: INFO: Pod "downwardapi-volume-d9d1780f-8614-4aca-8126-17288dfafc16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.778115ms
Mar 27 15:13:06.414: INFO: Pod "downwardapi-volume-d9d1780f-8614-4aca-8126-17288dfafc16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006932555s
Mar 27 15:13:08.416: INFO: Pod "downwardapi-volume-d9d1780f-8614-4aca-8126-17288dfafc16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008671819s
STEP: Saw pod success 03/27/23 15:13:08.416
Mar 27 15:13:08.416: INFO: Pod "downwardapi-volume-d9d1780f-8614-4aca-8126-17288dfafc16" satisfied condition "Succeeded or Failed"
Mar 27 15:13:08.422: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-d9d1780f-8614-4aca-8126-17288dfafc16 container client-container: <nil>
STEP: delete the pod 03/27/23 15:13:08.429
Mar 27 15:13:08.444: INFO: Waiting for pod downwardapi-volume-d9d1780f-8614-4aca-8126-17288dfafc16 to disappear
Mar 27 15:13:08.446: INFO: Pod downwardapi-volume-d9d1780f-8614-4aca-8126-17288dfafc16 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 27 15:13:08.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7548" for this suite. 03/27/23 15:13:08.451
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":203,"skipped":3911,"failed":0}
------------------------------
• [4.084 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:13:04.376
    Mar 27 15:13:04.376: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 15:13:04.377
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:13:04.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:13:04.393
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 03/27/23 15:13:04.4
    Mar 27 15:13:04.407: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9d1780f-8614-4aca-8126-17288dfafc16" in namespace "projected-7548" to be "Succeeded or Failed"
    Mar 27 15:13:04.410: INFO: Pod "downwardapi-volume-d9d1780f-8614-4aca-8126-17288dfafc16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.778115ms
    Mar 27 15:13:06.414: INFO: Pod "downwardapi-volume-d9d1780f-8614-4aca-8126-17288dfafc16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006932555s
    Mar 27 15:13:08.416: INFO: Pod "downwardapi-volume-d9d1780f-8614-4aca-8126-17288dfafc16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008671819s
    STEP: Saw pod success 03/27/23 15:13:08.416
    Mar 27 15:13:08.416: INFO: Pod "downwardapi-volume-d9d1780f-8614-4aca-8126-17288dfafc16" satisfied condition "Succeeded or Failed"
    Mar 27 15:13:08.422: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-d9d1780f-8614-4aca-8126-17288dfafc16 container client-container: <nil>
    STEP: delete the pod 03/27/23 15:13:08.429
    Mar 27 15:13:08.444: INFO: Waiting for pod downwardapi-volume-d9d1780f-8614-4aca-8126-17288dfafc16 to disappear
    Mar 27 15:13:08.446: INFO: Pod downwardapi-volume-d9d1780f-8614-4aca-8126-17288dfafc16 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 27 15:13:08.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7548" for this suite. 03/27/23 15:13:08.451
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:13:08.46
Mar 27 15:13:08.460: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename configmap 03/27/23 15:13:08.461
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:13:08.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:13:08.483
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-6083/configmap-test-df21f843-1aef-467a-b1f0-6c07996c279e 03/27/23 15:13:08.485
STEP: Creating a pod to test consume configMaps 03/27/23 15:13:08.491
Mar 27 15:13:08.503: INFO: Waiting up to 5m0s for pod "pod-configmaps-bfb70262-4f62-46ec-933e-eb8f19e5ee17" in namespace "configmap-6083" to be "Succeeded or Failed"
Mar 27 15:13:08.507: INFO: Pod "pod-configmaps-bfb70262-4f62-46ec-933e-eb8f19e5ee17": Phase="Pending", Reason="", readiness=false. Elapsed: 3.376605ms
Mar 27 15:13:10.511: INFO: Pod "pod-configmaps-bfb70262-4f62-46ec-933e-eb8f19e5ee17": Phase="Running", Reason="", readiness=true. Elapsed: 2.007228512s
Mar 27 15:13:12.512: INFO: Pod "pod-configmaps-bfb70262-4f62-46ec-933e-eb8f19e5ee17": Phase="Running", Reason="", readiness=false. Elapsed: 4.008944961s
Mar 27 15:13:14.512: INFO: Pod "pod-configmaps-bfb70262-4f62-46ec-933e-eb8f19e5ee17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008281274s
STEP: Saw pod success 03/27/23 15:13:14.512
Mar 27 15:13:14.512: INFO: Pod "pod-configmaps-bfb70262-4f62-46ec-933e-eb8f19e5ee17" satisfied condition "Succeeded or Failed"
Mar 27 15:13:14.515: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-configmaps-bfb70262-4f62-46ec-933e-eb8f19e5ee17 container env-test: <nil>
STEP: delete the pod 03/27/23 15:13:14.522
Mar 27 15:13:14.537: INFO: Waiting for pod pod-configmaps-bfb70262-4f62-46ec-933e-eb8f19e5ee17 to disappear
Mar 27 15:13:14.539: INFO: Pod pod-configmaps-bfb70262-4f62-46ec-933e-eb8f19e5ee17 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar 27 15:13:14.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6083" for this suite. 03/27/23 15:13:14.543
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":204,"skipped":3913,"failed":0}
------------------------------
• [SLOW TEST] [6.089 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:13:08.46
    Mar 27 15:13:08.460: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename configmap 03/27/23 15:13:08.461
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:13:08.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:13:08.483
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-6083/configmap-test-df21f843-1aef-467a-b1f0-6c07996c279e 03/27/23 15:13:08.485
    STEP: Creating a pod to test consume configMaps 03/27/23 15:13:08.491
    Mar 27 15:13:08.503: INFO: Waiting up to 5m0s for pod "pod-configmaps-bfb70262-4f62-46ec-933e-eb8f19e5ee17" in namespace "configmap-6083" to be "Succeeded or Failed"
    Mar 27 15:13:08.507: INFO: Pod "pod-configmaps-bfb70262-4f62-46ec-933e-eb8f19e5ee17": Phase="Pending", Reason="", readiness=false. Elapsed: 3.376605ms
    Mar 27 15:13:10.511: INFO: Pod "pod-configmaps-bfb70262-4f62-46ec-933e-eb8f19e5ee17": Phase="Running", Reason="", readiness=true. Elapsed: 2.007228512s
    Mar 27 15:13:12.512: INFO: Pod "pod-configmaps-bfb70262-4f62-46ec-933e-eb8f19e5ee17": Phase="Running", Reason="", readiness=false. Elapsed: 4.008944961s
    Mar 27 15:13:14.512: INFO: Pod "pod-configmaps-bfb70262-4f62-46ec-933e-eb8f19e5ee17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008281274s
    STEP: Saw pod success 03/27/23 15:13:14.512
    Mar 27 15:13:14.512: INFO: Pod "pod-configmaps-bfb70262-4f62-46ec-933e-eb8f19e5ee17" satisfied condition "Succeeded or Failed"
    Mar 27 15:13:14.515: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-configmaps-bfb70262-4f62-46ec-933e-eb8f19e5ee17 container env-test: <nil>
    STEP: delete the pod 03/27/23 15:13:14.522
    Mar 27 15:13:14.537: INFO: Waiting for pod pod-configmaps-bfb70262-4f62-46ec-933e-eb8f19e5ee17 to disappear
    Mar 27 15:13:14.539: INFO: Pod pod-configmaps-bfb70262-4f62-46ec-933e-eb8f19e5ee17 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 27 15:13:14.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6083" for this suite. 03/27/23 15:13:14.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:13:14.552
Mar 27 15:13:14.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename endpointslice 03/27/23 15:13:14.554
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:13:14.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:13:14.575
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Mar 27 15:13:14.587: INFO: Endpoints addresses: [195.192.148.238] , ports: [30518]
Mar 27 15:13:14.587: INFO: EndpointSlices addresses: [195.192.148.238] , ports: [30518]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar 27 15:13:14.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5103" for this suite. 03/27/23 15:13:14.592
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":205,"skipped":3939,"failed":0}
------------------------------
• [0.046 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:13:14.552
    Mar 27 15:13:14.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename endpointslice 03/27/23 15:13:14.554
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:13:14.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:13:14.575
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Mar 27 15:13:14.587: INFO: Endpoints addresses: [195.192.148.238] , ports: [30518]
    Mar 27 15:13:14.587: INFO: EndpointSlices addresses: [195.192.148.238] , ports: [30518]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar 27 15:13:14.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-5103" for this suite. 03/27/23 15:13:14.592
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:13:14.599
Mar 27 15:13:14.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename secrets 03/27/23 15:13:14.6
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:13:14.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:13:14.62
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-66b88f60-288d-49b5-ad15-662ad26d6562 03/27/23 15:13:14.626
STEP: Creating a pod to test consume secrets 03/27/23 15:13:14.633
Mar 27 15:13:14.641: INFO: Waiting up to 5m0s for pod "pod-secrets-72a53458-e5a1-439c-8938-0a6693569e28" in namespace "secrets-7365" to be "Succeeded or Failed"
Mar 27 15:13:14.645: INFO: Pod "pod-secrets-72a53458-e5a1-439c-8938-0a6693569e28": Phase="Pending", Reason="", readiness=false. Elapsed: 4.616904ms
Mar 27 15:13:16.649: INFO: Pod "pod-secrets-72a53458-e5a1-439c-8938-0a6693569e28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008732194s
Mar 27 15:13:18.651: INFO: Pod "pod-secrets-72a53458-e5a1-439c-8938-0a6693569e28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010286232s
STEP: Saw pod success 03/27/23 15:13:18.651
Mar 27 15:13:18.651: INFO: Pod "pod-secrets-72a53458-e5a1-439c-8938-0a6693569e28" satisfied condition "Succeeded or Failed"
Mar 27 15:13:18.655: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-secrets-72a53458-e5a1-439c-8938-0a6693569e28 container secret-env-test: <nil>
STEP: delete the pod 03/27/23 15:13:18.667
Mar 27 15:13:18.685: INFO: Waiting for pod pod-secrets-72a53458-e5a1-439c-8938-0a6693569e28 to disappear
Mar 27 15:13:18.687: INFO: Pod pod-secrets-72a53458-e5a1-439c-8938-0a6693569e28 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar 27 15:13:18.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7365" for this suite. 03/27/23 15:13:18.692
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":206,"skipped":3942,"failed":0}
------------------------------
• [4.101 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:13:14.599
    Mar 27 15:13:14.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename secrets 03/27/23 15:13:14.6
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:13:14.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:13:14.62
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-66b88f60-288d-49b5-ad15-662ad26d6562 03/27/23 15:13:14.626
    STEP: Creating a pod to test consume secrets 03/27/23 15:13:14.633
    Mar 27 15:13:14.641: INFO: Waiting up to 5m0s for pod "pod-secrets-72a53458-e5a1-439c-8938-0a6693569e28" in namespace "secrets-7365" to be "Succeeded or Failed"
    Mar 27 15:13:14.645: INFO: Pod "pod-secrets-72a53458-e5a1-439c-8938-0a6693569e28": Phase="Pending", Reason="", readiness=false. Elapsed: 4.616904ms
    Mar 27 15:13:16.649: INFO: Pod "pod-secrets-72a53458-e5a1-439c-8938-0a6693569e28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008732194s
    Mar 27 15:13:18.651: INFO: Pod "pod-secrets-72a53458-e5a1-439c-8938-0a6693569e28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010286232s
    STEP: Saw pod success 03/27/23 15:13:18.651
    Mar 27 15:13:18.651: INFO: Pod "pod-secrets-72a53458-e5a1-439c-8938-0a6693569e28" satisfied condition "Succeeded or Failed"
    Mar 27 15:13:18.655: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-secrets-72a53458-e5a1-439c-8938-0a6693569e28 container secret-env-test: <nil>
    STEP: delete the pod 03/27/23 15:13:18.667
    Mar 27 15:13:18.685: INFO: Waiting for pod pod-secrets-72a53458-e5a1-439c-8938-0a6693569e28 to disappear
    Mar 27 15:13:18.687: INFO: Pod pod-secrets-72a53458-e5a1-439c-8938-0a6693569e28 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar 27 15:13:18.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7365" for this suite. 03/27/23 15:13:18.692
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:13:18.7
Mar 27 15:13:18.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename container-lifecycle-hook 03/27/23 15:13:18.701
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:13:18.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:13:18.717
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/27/23 15:13:18.723
Mar 27 15:13:18.730: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1792" to be "running and ready"
Mar 27 15:13:18.739: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.727021ms
Mar 27 15:13:18.739: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:13:20.744: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013417773s
Mar 27 15:13:20.744: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 27 15:13:20.744: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 03/27/23 15:13:20.747
Mar 27 15:13:20.752: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1792" to be "running and ready"
Mar 27 15:13:20.755: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.91294ms
Mar 27 15:13:20.756: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:13:22.761: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007990015s
Mar 27 15:13:22.761: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Mar 27 15:13:22.761: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/27/23 15:13:22.764
STEP: delete the pod with lifecycle hook 03/27/23 15:13:22.772
Mar 27 15:13:22.778: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 27 15:13:22.782: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 27 15:13:24.783: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 27 15:13:24.787: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 27 15:13:26.782: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 27 15:13:26.787: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar 27 15:13:26.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1792" for this suite. 03/27/23 15:13:26.792
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":207,"skipped":3943,"failed":0}
------------------------------
• [SLOW TEST] [8.099 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:13:18.7
    Mar 27 15:13:18.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/27/23 15:13:18.701
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:13:18.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:13:18.717
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/27/23 15:13:18.723
    Mar 27 15:13:18.730: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1792" to be "running and ready"
    Mar 27 15:13:18.739: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.727021ms
    Mar 27 15:13:18.739: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:13:20.744: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013417773s
    Mar 27 15:13:20.744: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 27 15:13:20.744: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 03/27/23 15:13:20.747
    Mar 27 15:13:20.752: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1792" to be "running and ready"
    Mar 27 15:13:20.755: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.91294ms
    Mar 27 15:13:20.756: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:13:22.761: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007990015s
    Mar 27 15:13:22.761: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Mar 27 15:13:22.761: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/27/23 15:13:22.764
    STEP: delete the pod with lifecycle hook 03/27/23 15:13:22.772
    Mar 27 15:13:22.778: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 27 15:13:22.782: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar 27 15:13:24.783: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 27 15:13:24.787: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar 27 15:13:26.782: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 27 15:13:26.787: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar 27 15:13:26.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-1792" for this suite. 03/27/23 15:13:26.792
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:13:26.801
Mar 27 15:13:26.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename services 03/27/23 15:13:26.802
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:13:26.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:13:26.825
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 03/27/23 15:13:26.829
Mar 27 15:13:26.829: INFO: Creating e2e-svc-a-9hnzl
Mar 27 15:13:26.841: INFO: Creating e2e-svc-b-tbb64
Mar 27 15:13:26.850: INFO: Creating e2e-svc-c-kv868
STEP: deleting service collection 03/27/23 15:13:26.865
Mar 27 15:13:26.891: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 27 15:13:26.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4092" for this suite. 03/27/23 15:13:26.896
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":208,"skipped":3945,"failed":0}
------------------------------
• [0.101 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:13:26.801
    Mar 27 15:13:26.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename services 03/27/23 15:13:26.802
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:13:26.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:13:26.825
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 03/27/23 15:13:26.829
    Mar 27 15:13:26.829: INFO: Creating e2e-svc-a-9hnzl
    Mar 27 15:13:26.841: INFO: Creating e2e-svc-b-tbb64
    Mar 27 15:13:26.850: INFO: Creating e2e-svc-c-kv868
    STEP: deleting service collection 03/27/23 15:13:26.865
    Mar 27 15:13:26.891: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 27 15:13:26.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4092" for this suite. 03/27/23 15:13:26.896
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:13:26.905
Mar 27 15:13:26.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename configmap 03/27/23 15:13:26.906
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:13:26.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:13:26.924
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-547e38ec-5737-446f-b64e-b409ee5d96c5 03/27/23 15:13:26.926
STEP: Creating a pod to test consume configMaps 03/27/23 15:13:26.933
Mar 27 15:13:26.942: INFO: Waiting up to 5m0s for pod "pod-configmaps-723ce599-355a-4557-88e8-b855facfc453" in namespace "configmap-9963" to be "Succeeded or Failed"
Mar 27 15:13:26.945: INFO: Pod "pod-configmaps-723ce599-355a-4557-88e8-b855facfc453": Phase="Pending", Reason="", readiness=false. Elapsed: 2.814876ms
Mar 27 15:13:28.950: INFO: Pod "pod-configmaps-723ce599-355a-4557-88e8-b855facfc453": Phase="Running", Reason="", readiness=true. Elapsed: 2.007531086s
Mar 27 15:13:30.951: INFO: Pod "pod-configmaps-723ce599-355a-4557-88e8-b855facfc453": Phase="Running", Reason="", readiness=false. Elapsed: 4.008451512s
Mar 27 15:13:32.950: INFO: Pod "pod-configmaps-723ce599-355a-4557-88e8-b855facfc453": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007820942s
STEP: Saw pod success 03/27/23 15:13:32.95
Mar 27 15:13:32.950: INFO: Pod "pod-configmaps-723ce599-355a-4557-88e8-b855facfc453" satisfied condition "Succeeded or Failed"
Mar 27 15:13:32.953: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-configmaps-723ce599-355a-4557-88e8-b855facfc453 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 15:13:32.965
Mar 27 15:13:32.982: INFO: Waiting for pod pod-configmaps-723ce599-355a-4557-88e8-b855facfc453 to disappear
Mar 27 15:13:32.984: INFO: Pod pod-configmaps-723ce599-355a-4557-88e8-b855facfc453 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 27 15:13:32.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9963" for this suite. 03/27/23 15:13:32.989
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":209,"skipped":3953,"failed":0}
------------------------------
• [SLOW TEST] [6.090 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:13:26.905
    Mar 27 15:13:26.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename configmap 03/27/23 15:13:26.906
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:13:26.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:13:26.924
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-547e38ec-5737-446f-b64e-b409ee5d96c5 03/27/23 15:13:26.926
    STEP: Creating a pod to test consume configMaps 03/27/23 15:13:26.933
    Mar 27 15:13:26.942: INFO: Waiting up to 5m0s for pod "pod-configmaps-723ce599-355a-4557-88e8-b855facfc453" in namespace "configmap-9963" to be "Succeeded or Failed"
    Mar 27 15:13:26.945: INFO: Pod "pod-configmaps-723ce599-355a-4557-88e8-b855facfc453": Phase="Pending", Reason="", readiness=false. Elapsed: 2.814876ms
    Mar 27 15:13:28.950: INFO: Pod "pod-configmaps-723ce599-355a-4557-88e8-b855facfc453": Phase="Running", Reason="", readiness=true. Elapsed: 2.007531086s
    Mar 27 15:13:30.951: INFO: Pod "pod-configmaps-723ce599-355a-4557-88e8-b855facfc453": Phase="Running", Reason="", readiness=false. Elapsed: 4.008451512s
    Mar 27 15:13:32.950: INFO: Pod "pod-configmaps-723ce599-355a-4557-88e8-b855facfc453": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007820942s
    STEP: Saw pod success 03/27/23 15:13:32.95
    Mar 27 15:13:32.950: INFO: Pod "pod-configmaps-723ce599-355a-4557-88e8-b855facfc453" satisfied condition "Succeeded or Failed"
    Mar 27 15:13:32.953: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-configmaps-723ce599-355a-4557-88e8-b855facfc453 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 15:13:32.965
    Mar 27 15:13:32.982: INFO: Waiting for pod pod-configmaps-723ce599-355a-4557-88e8-b855facfc453 to disappear
    Mar 27 15:13:32.984: INFO: Pod pod-configmaps-723ce599-355a-4557-88e8-b855facfc453 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 27 15:13:32.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9963" for this suite. 03/27/23 15:13:32.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:13:32.998
Mar 27 15:13:32.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename daemonsets 03/27/23 15:13:32.999
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:13:33.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:13:33.017
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Mar 27 15:13:33.040: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 03/27/23 15:13:33.052
Mar 27 15:13:33.055: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 15:13:33.055: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 03/27/23 15:13:33.055
Mar 27 15:13:33.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 15:13:33.077: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc is running 0 daemon pod, expected 1
Mar 27 15:13:34.081: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 15:13:34.081: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc is running 0 daemon pod, expected 1
Mar 27 15:13:35.082: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 27 15:13:35.082: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 03/27/23 15:13:35.085
Mar 27 15:13:35.107: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 27 15:13:35.107: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Mar 27 15:13:36.113: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 15:13:36.113: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/27/23 15:13:36.113
Mar 27 15:13:36.123: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 15:13:36.123: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc is running 0 daemon pod, expected 1
Mar 27 15:13:37.128: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 15:13:37.128: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc is running 0 daemon pod, expected 1
Mar 27 15:13:38.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 15:13:38.127: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc is running 0 daemon pod, expected 1
Mar 27 15:13:39.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 15:13:39.127: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc is running 0 daemon pod, expected 1
Mar 27 15:13:40.128: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 15:13:40.128: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc is running 0 daemon pod, expected 1
Mar 27 15:13:41.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 27 15:13:41.127: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/27/23 15:13:41.133
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1209, will wait for the garbage collector to delete the pods 03/27/23 15:13:41.133
Mar 27 15:13:41.193: INFO: Deleting DaemonSet.extensions daemon-set took: 6.387637ms
Mar 27 15:13:41.294: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.709507ms
Mar 27 15:13:44.398: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 15:13:44.398: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 27 15:13:44.401: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40858"},"items":null}

Mar 27 15:13:44.406: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40858"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 27 15:13:44.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1209" for this suite. 03/27/23 15:13:44.435
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":210,"skipped":4018,"failed":0}
------------------------------
• [SLOW TEST] [11.443 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:13:32.998
    Mar 27 15:13:32.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename daemonsets 03/27/23 15:13:32.999
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:13:33.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:13:33.017
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Mar 27 15:13:33.040: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 03/27/23 15:13:33.052
    Mar 27 15:13:33.055: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 15:13:33.055: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 03/27/23 15:13:33.055
    Mar 27 15:13:33.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 15:13:33.077: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc is running 0 daemon pod, expected 1
    Mar 27 15:13:34.081: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 15:13:34.081: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc is running 0 daemon pod, expected 1
    Mar 27 15:13:35.082: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 27 15:13:35.082: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 03/27/23 15:13:35.085
    Mar 27 15:13:35.107: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 27 15:13:35.107: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Mar 27 15:13:36.113: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 15:13:36.113: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/27/23 15:13:36.113
    Mar 27 15:13:36.123: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 15:13:36.123: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc is running 0 daemon pod, expected 1
    Mar 27 15:13:37.128: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 15:13:37.128: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc is running 0 daemon pod, expected 1
    Mar 27 15:13:38.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 15:13:38.127: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc is running 0 daemon pod, expected 1
    Mar 27 15:13:39.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 15:13:39.127: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc is running 0 daemon pod, expected 1
    Mar 27 15:13:40.128: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 15:13:40.128: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc is running 0 daemon pod, expected 1
    Mar 27 15:13:41.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 27 15:13:41.127: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/27/23 15:13:41.133
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1209, will wait for the garbage collector to delete the pods 03/27/23 15:13:41.133
    Mar 27 15:13:41.193: INFO: Deleting DaemonSet.extensions daemon-set took: 6.387637ms
    Mar 27 15:13:41.294: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.709507ms
    Mar 27 15:13:44.398: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 15:13:44.398: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 27 15:13:44.401: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40858"},"items":null}

    Mar 27 15:13:44.406: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40858"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 15:13:44.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1209" for this suite. 03/27/23 15:13:44.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:13:44.442
Mar 27 15:13:44.442: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename webhook 03/27/23 15:13:44.443
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:13:44.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:13:44.463
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/27/23 15:13:44.476
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:13:44.828
STEP: Deploying the webhook pod 03/27/23 15:13:44.836
STEP: Wait for the deployment to be ready 03/27/23 15:13:44.846
Mar 27 15:13:44.858: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 15:13:46.868
STEP: Verifying the service has paired with the endpoint 03/27/23 15:13:46.877
Mar 27 15:13:47.877: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/27/23 15:13:47.881
STEP: Registering slow webhook via the AdmissionRegistration API 03/27/23 15:13:47.881
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/27/23 15:13:47.905
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/27/23 15:13:48.917
STEP: Registering slow webhook via the AdmissionRegistration API 03/27/23 15:13:48.917
STEP: Having no error when timeout is longer than webhook latency 03/27/23 15:13:49.954
STEP: Registering slow webhook via the AdmissionRegistration API 03/27/23 15:13:49.954
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/27/23 15:13:55.003
STEP: Registering slow webhook via the AdmissionRegistration API 03/27/23 15:13:55.003
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 15:14:00.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-222" for this suite. 03/27/23 15:14:00.04
STEP: Destroying namespace "webhook-222-markers" for this suite. 03/27/23 15:14:00.046
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":211,"skipped":4034,"failed":0}
------------------------------
• [SLOW TEST] [15.655 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:13:44.442
    Mar 27 15:13:44.442: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename webhook 03/27/23 15:13:44.443
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:13:44.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:13:44.463
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/27/23 15:13:44.476
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:13:44.828
    STEP: Deploying the webhook pod 03/27/23 15:13:44.836
    STEP: Wait for the deployment to be ready 03/27/23 15:13:44.846
    Mar 27 15:13:44.858: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 15:13:46.868
    STEP: Verifying the service has paired with the endpoint 03/27/23 15:13:46.877
    Mar 27 15:13:47.877: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/27/23 15:13:47.881
    STEP: Registering slow webhook via the AdmissionRegistration API 03/27/23 15:13:47.881
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/27/23 15:13:47.905
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/27/23 15:13:48.917
    STEP: Registering slow webhook via the AdmissionRegistration API 03/27/23 15:13:48.917
    STEP: Having no error when timeout is longer than webhook latency 03/27/23 15:13:49.954
    STEP: Registering slow webhook via the AdmissionRegistration API 03/27/23 15:13:49.954
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/27/23 15:13:55.003
    STEP: Registering slow webhook via the AdmissionRegistration API 03/27/23 15:13:55.003
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 15:14:00.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-222" for this suite. 03/27/23 15:14:00.04
    STEP: Destroying namespace "webhook-222-markers" for this suite. 03/27/23 15:14:00.046
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:14:00.1
Mar 27 15:14:00.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename replicaset 03/27/23 15:14:00.106
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:14:00.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:14:00.129
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/27/23 15:14:00.132
Mar 27 15:14:00.143: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 27 15:14:05.154: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/27/23 15:14:05.154
STEP: getting scale subresource 03/27/23 15:14:05.154
STEP: updating a scale subresource 03/27/23 15:14:05.158
STEP: verifying the replicaset Spec.Replicas was modified 03/27/23 15:14:05.165
STEP: Patch a scale subresource 03/27/23 15:14:05.167
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar 27 15:14:05.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9328" for this suite. 03/27/23 15:14:05.188
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":212,"skipped":4041,"failed":0}
------------------------------
• [SLOW TEST] [5.098 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:14:00.1
    Mar 27 15:14:00.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename replicaset 03/27/23 15:14:00.106
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:14:00.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:14:00.129
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/27/23 15:14:00.132
    Mar 27 15:14:00.143: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 27 15:14:05.154: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/27/23 15:14:05.154
    STEP: getting scale subresource 03/27/23 15:14:05.154
    STEP: updating a scale subresource 03/27/23 15:14:05.158
    STEP: verifying the replicaset Spec.Replicas was modified 03/27/23 15:14:05.165
    STEP: Patch a scale subresource 03/27/23 15:14:05.167
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar 27 15:14:05.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9328" for this suite. 03/27/23 15:14:05.188
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:14:05.198
Mar 27 15:14:05.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename job 03/27/23 15:14:05.199
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:14:05.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:14:05.215
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 03/27/23 15:14:05.218
STEP: Ensuring job reaches completions 03/27/23 15:14:05.228
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar 27 15:14:17.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5540" for this suite. 03/27/23 15:14:17.237
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":213,"skipped":4045,"failed":0}
------------------------------
• [SLOW TEST] [12.045 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:14:05.198
    Mar 27 15:14:05.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename job 03/27/23 15:14:05.199
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:14:05.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:14:05.215
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 03/27/23 15:14:05.218
    STEP: Ensuring job reaches completions 03/27/23 15:14:05.228
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar 27 15:14:17.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5540" for this suite. 03/27/23 15:14:17.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:14:17.244
Mar 27 15:14:17.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename configmap 03/27/23 15:14:17.245
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:14:17.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:14:17.265
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-8091/configmap-test-2f32f570-d5ee-45e7-980e-be7823815314 03/27/23 15:14:17.267
STEP: Creating a pod to test consume configMaps 03/27/23 15:14:17.274
Mar 27 15:14:17.289: INFO: Waiting up to 5m0s for pod "pod-configmaps-2cf70d5f-d9eb-44eb-9d6e-16d0582fa00a" in namespace "configmap-8091" to be "Succeeded or Failed"
Mar 27 15:14:17.296: INFO: Pod "pod-configmaps-2cf70d5f-d9eb-44eb-9d6e-16d0582fa00a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.692965ms
Mar 27 15:14:19.304: INFO: Pod "pod-configmaps-2cf70d5f-d9eb-44eb-9d6e-16d0582fa00a": Phase="Running", Reason="", readiness=true. Elapsed: 2.01519896s
Mar 27 15:14:21.302: INFO: Pod "pod-configmaps-2cf70d5f-d9eb-44eb-9d6e-16d0582fa00a": Phase="Running", Reason="", readiness=false. Elapsed: 4.012912169s
Mar 27 15:14:23.300: INFO: Pod "pod-configmaps-2cf70d5f-d9eb-44eb-9d6e-16d0582fa00a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010884731s
STEP: Saw pod success 03/27/23 15:14:23.3
Mar 27 15:14:23.300: INFO: Pod "pod-configmaps-2cf70d5f-d9eb-44eb-9d6e-16d0582fa00a" satisfied condition "Succeeded or Failed"
Mar 27 15:14:23.303: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-configmaps-2cf70d5f-d9eb-44eb-9d6e-16d0582fa00a container env-test: <nil>
STEP: delete the pod 03/27/23 15:14:23.311
Mar 27 15:14:23.327: INFO: Waiting for pod pod-configmaps-2cf70d5f-d9eb-44eb-9d6e-16d0582fa00a to disappear
Mar 27 15:14:23.330: INFO: Pod pod-configmaps-2cf70d5f-d9eb-44eb-9d6e-16d0582fa00a no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar 27 15:14:23.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8091" for this suite. 03/27/23 15:14:23.334
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":214,"skipped":4052,"failed":0}
------------------------------
• [SLOW TEST] [6.096 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:14:17.244
    Mar 27 15:14:17.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename configmap 03/27/23 15:14:17.245
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:14:17.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:14:17.265
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-8091/configmap-test-2f32f570-d5ee-45e7-980e-be7823815314 03/27/23 15:14:17.267
    STEP: Creating a pod to test consume configMaps 03/27/23 15:14:17.274
    Mar 27 15:14:17.289: INFO: Waiting up to 5m0s for pod "pod-configmaps-2cf70d5f-d9eb-44eb-9d6e-16d0582fa00a" in namespace "configmap-8091" to be "Succeeded or Failed"
    Mar 27 15:14:17.296: INFO: Pod "pod-configmaps-2cf70d5f-d9eb-44eb-9d6e-16d0582fa00a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.692965ms
    Mar 27 15:14:19.304: INFO: Pod "pod-configmaps-2cf70d5f-d9eb-44eb-9d6e-16d0582fa00a": Phase="Running", Reason="", readiness=true. Elapsed: 2.01519896s
    Mar 27 15:14:21.302: INFO: Pod "pod-configmaps-2cf70d5f-d9eb-44eb-9d6e-16d0582fa00a": Phase="Running", Reason="", readiness=false. Elapsed: 4.012912169s
    Mar 27 15:14:23.300: INFO: Pod "pod-configmaps-2cf70d5f-d9eb-44eb-9d6e-16d0582fa00a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010884731s
    STEP: Saw pod success 03/27/23 15:14:23.3
    Mar 27 15:14:23.300: INFO: Pod "pod-configmaps-2cf70d5f-d9eb-44eb-9d6e-16d0582fa00a" satisfied condition "Succeeded or Failed"
    Mar 27 15:14:23.303: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-configmaps-2cf70d5f-d9eb-44eb-9d6e-16d0582fa00a container env-test: <nil>
    STEP: delete the pod 03/27/23 15:14:23.311
    Mar 27 15:14:23.327: INFO: Waiting for pod pod-configmaps-2cf70d5f-d9eb-44eb-9d6e-16d0582fa00a to disappear
    Mar 27 15:14:23.330: INFO: Pod pod-configmaps-2cf70d5f-d9eb-44eb-9d6e-16d0582fa00a no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 27 15:14:23.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8091" for this suite. 03/27/23 15:14:23.334
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:14:23.341
Mar 27 15:14:23.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 15:14:23.342
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:14:23.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:14:23.363
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 03/27/23 15:14:23.366
Mar 27 15:14:23.378: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec446b2c-0f58-43c1-8eb5-0a5c81dc3edd" in namespace "projected-820" to be "Succeeded or Failed"
Mar 27 15:14:23.384: INFO: Pod "downwardapi-volume-ec446b2c-0f58-43c1-8eb5-0a5c81dc3edd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.220066ms
Mar 27 15:14:25.389: INFO: Pod "downwardapi-volume-ec446b2c-0f58-43c1-8eb5-0a5c81dc3edd": Phase="Running", Reason="", readiness=true. Elapsed: 2.010760632s
Mar 27 15:14:27.388: INFO: Pod "downwardapi-volume-ec446b2c-0f58-43c1-8eb5-0a5c81dc3edd": Phase="Running", Reason="", readiness=false. Elapsed: 4.00984736s
Mar 27 15:14:29.389: INFO: Pod "downwardapi-volume-ec446b2c-0f58-43c1-8eb5-0a5c81dc3edd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010478947s
STEP: Saw pod success 03/27/23 15:14:29.389
Mar 27 15:14:29.389: INFO: Pod "downwardapi-volume-ec446b2c-0f58-43c1-8eb5-0a5c81dc3edd" satisfied condition "Succeeded or Failed"
Mar 27 15:14:29.392: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-ec446b2c-0f58-43c1-8eb5-0a5c81dc3edd container client-container: <nil>
STEP: delete the pod 03/27/23 15:14:29.4
Mar 27 15:14:29.414: INFO: Waiting for pod downwardapi-volume-ec446b2c-0f58-43c1-8eb5-0a5c81dc3edd to disappear
Mar 27 15:14:29.417: INFO: Pod downwardapi-volume-ec446b2c-0f58-43c1-8eb5-0a5c81dc3edd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 27 15:14:29.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-820" for this suite. 03/27/23 15:14:29.422
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":215,"skipped":4066,"failed":0}
------------------------------
• [SLOW TEST] [6.090 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:14:23.341
    Mar 27 15:14:23.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 15:14:23.342
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:14:23.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:14:23.363
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 03/27/23 15:14:23.366
    Mar 27 15:14:23.378: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec446b2c-0f58-43c1-8eb5-0a5c81dc3edd" in namespace "projected-820" to be "Succeeded or Failed"
    Mar 27 15:14:23.384: INFO: Pod "downwardapi-volume-ec446b2c-0f58-43c1-8eb5-0a5c81dc3edd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.220066ms
    Mar 27 15:14:25.389: INFO: Pod "downwardapi-volume-ec446b2c-0f58-43c1-8eb5-0a5c81dc3edd": Phase="Running", Reason="", readiness=true. Elapsed: 2.010760632s
    Mar 27 15:14:27.388: INFO: Pod "downwardapi-volume-ec446b2c-0f58-43c1-8eb5-0a5c81dc3edd": Phase="Running", Reason="", readiness=false. Elapsed: 4.00984736s
    Mar 27 15:14:29.389: INFO: Pod "downwardapi-volume-ec446b2c-0f58-43c1-8eb5-0a5c81dc3edd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010478947s
    STEP: Saw pod success 03/27/23 15:14:29.389
    Mar 27 15:14:29.389: INFO: Pod "downwardapi-volume-ec446b2c-0f58-43c1-8eb5-0a5c81dc3edd" satisfied condition "Succeeded or Failed"
    Mar 27 15:14:29.392: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-ec446b2c-0f58-43c1-8eb5-0a5c81dc3edd container client-container: <nil>
    STEP: delete the pod 03/27/23 15:14:29.4
    Mar 27 15:14:29.414: INFO: Waiting for pod downwardapi-volume-ec446b2c-0f58-43c1-8eb5-0a5c81dc3edd to disappear
    Mar 27 15:14:29.417: INFO: Pod downwardapi-volume-ec446b2c-0f58-43c1-8eb5-0a5c81dc3edd no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 27 15:14:29.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-820" for this suite. 03/27/23 15:14:29.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:14:29.431
Mar 27 15:14:29.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename var-expansion 03/27/23 15:14:29.432
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:14:29.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:14:29.453
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 03/27/23 15:14:29.456
STEP: waiting for pod running 03/27/23 15:14:29.466
Mar 27 15:14:29.466: INFO: Waiting up to 2m0s for pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e" in namespace "var-expansion-206" to be "running"
Mar 27 15:14:29.478: INFO: Pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.148308ms
Mar 27 15:14:31.487: INFO: Pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020523957s
Mar 27 15:14:33.484: INFO: Pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e": Phase="Running", Reason="", readiness=true. Elapsed: 4.017165421s
Mar 27 15:14:33.484: INFO: Pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e" satisfied condition "running"
STEP: creating a file in subpath 03/27/23 15:14:33.484
Mar 27 15:14:33.486: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-206 PodName:var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 15:14:33.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 15:14:33.487: INFO: ExecWithOptions: Clientset creation
Mar 27 15:14:33.487: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/var-expansion-206/pods/var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 03/27/23 15:14:33.567
Mar 27 15:14:33.571: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-206 PodName:var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 15:14:33.571: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 15:14:33.572: INFO: ExecWithOptions: Clientset creation
Mar 27 15:14:33.572: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/var-expansion-206/pods/var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 03/27/23 15:14:33.662
Mar 27 15:14:34.176: INFO: Successfully updated pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e"
STEP: waiting for annotated pod running 03/27/23 15:14:34.176
Mar 27 15:14:34.176: INFO: Waiting up to 2m0s for pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e" in namespace "var-expansion-206" to be "running"
Mar 27 15:14:34.179: INFO: Pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e": Phase="Running", Reason="", readiness=true. Elapsed: 2.987669ms
Mar 27 15:14:34.179: INFO: Pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e" satisfied condition "running"
STEP: deleting the pod gracefully 03/27/23 15:14:34.179
Mar 27 15:14:34.179: INFO: Deleting pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e" in namespace "var-expansion-206"
Mar 27 15:14:34.187: INFO: Wait up to 5m0s for pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 27 15:15:06.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-206" for this suite. 03/27/23 15:15:06.2
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":216,"skipped":4071,"failed":0}
------------------------------
• [SLOW TEST] [36.776 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:14:29.431
    Mar 27 15:14:29.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename var-expansion 03/27/23 15:14:29.432
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:14:29.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:14:29.453
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 03/27/23 15:14:29.456
    STEP: waiting for pod running 03/27/23 15:14:29.466
    Mar 27 15:14:29.466: INFO: Waiting up to 2m0s for pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e" in namespace "var-expansion-206" to be "running"
    Mar 27 15:14:29.478: INFO: Pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.148308ms
    Mar 27 15:14:31.487: INFO: Pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020523957s
    Mar 27 15:14:33.484: INFO: Pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e": Phase="Running", Reason="", readiness=true. Elapsed: 4.017165421s
    Mar 27 15:14:33.484: INFO: Pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e" satisfied condition "running"
    STEP: creating a file in subpath 03/27/23 15:14:33.484
    Mar 27 15:14:33.486: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-206 PodName:var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 15:14:33.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 15:14:33.487: INFO: ExecWithOptions: Clientset creation
    Mar 27 15:14:33.487: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/var-expansion-206/pods/var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 03/27/23 15:14:33.567
    Mar 27 15:14:33.571: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-206 PodName:var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 15:14:33.571: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 15:14:33.572: INFO: ExecWithOptions: Clientset creation
    Mar 27 15:14:33.572: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/var-expansion-206/pods/var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 03/27/23 15:14:33.662
    Mar 27 15:14:34.176: INFO: Successfully updated pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e"
    STEP: waiting for annotated pod running 03/27/23 15:14:34.176
    Mar 27 15:14:34.176: INFO: Waiting up to 2m0s for pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e" in namespace "var-expansion-206" to be "running"
    Mar 27 15:14:34.179: INFO: Pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e": Phase="Running", Reason="", readiness=true. Elapsed: 2.987669ms
    Mar 27 15:14:34.179: INFO: Pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e" satisfied condition "running"
    STEP: deleting the pod gracefully 03/27/23 15:14:34.179
    Mar 27 15:14:34.179: INFO: Deleting pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e" in namespace "var-expansion-206"
    Mar 27 15:14:34.187: INFO: Wait up to 5m0s for pod "var-expansion-c7009155-2e5b-490b-b6a4-9214ac21c08e" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 27 15:15:06.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-206" for this suite. 03/27/23 15:15:06.2
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:15:06.209
Mar 27 15:15:06.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename dns 03/27/23 15:15:06.21
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:15:06.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:15:06.23
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 03/27/23 15:15:06.239
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6510.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6510.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6510.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6510.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6510.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6510.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 196.19.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.19.196_udp@PTR;check="$$(dig +tcp +noall +answer +search 196.19.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.19.196_tcp@PTR;sleep 1; done
 03/27/23 15:15:06.262
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6510.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6510.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6510.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6510.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6510.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6510.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 196.19.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.19.196_udp@PTR;check="$$(dig +tcp +noall +answer +search 196.19.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.19.196_tcp@PTR;sleep 1; done
 03/27/23 15:15:06.262
STEP: creating a pod to probe DNS 03/27/23 15:15:06.263
STEP: submitting the pod to kubernetes 03/27/23 15:15:06.263
Mar 27 15:15:06.275: INFO: Waiting up to 15m0s for pod "dns-test-10b81a19-4b7d-490f-81c2-1603ada678be" in namespace "dns-6510" to be "running"
Mar 27 15:15:06.284: INFO: Pod "dns-test-10b81a19-4b7d-490f-81c2-1603ada678be": Phase="Pending", Reason="", readiness=false. Elapsed: 8.425339ms
Mar 27 15:15:08.288: INFO: Pod "dns-test-10b81a19-4b7d-490f-81c2-1603ada678be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01272167s
Mar 27 15:15:10.288: INFO: Pod "dns-test-10b81a19-4b7d-490f-81c2-1603ada678be": Phase="Running", Reason="", readiness=true. Elapsed: 4.012416848s
Mar 27 15:15:10.288: INFO: Pod "dns-test-10b81a19-4b7d-490f-81c2-1603ada678be" satisfied condition "running"
STEP: retrieving the pod 03/27/23 15:15:10.288
STEP: looking for the results for each expected name from probers 03/27/23 15:15:10.291
Mar 27 15:15:10.300: INFO: Unable to read wheezy_udp@dns-test-service.dns-6510.svc.cluster.local from pod dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be: the server could not find the requested resource (get pods dns-test-10b81a19-4b7d-490f-81c2-1603ada678be)
Mar 27 15:15:10.305: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6510.svc.cluster.local from pod dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be: the server could not find the requested resource (get pods dns-test-10b81a19-4b7d-490f-81c2-1603ada678be)
Mar 27 15:15:10.310: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local from pod dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be: the server could not find the requested resource (get pods dns-test-10b81a19-4b7d-490f-81c2-1603ada678be)
Mar 27 15:15:10.314: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local from pod dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be: the server could not find the requested resource (get pods dns-test-10b81a19-4b7d-490f-81c2-1603ada678be)
Mar 27 15:15:10.336: INFO: Unable to read jessie_udp@dns-test-service.dns-6510.svc.cluster.local from pod dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be: the server could not find the requested resource (get pods dns-test-10b81a19-4b7d-490f-81c2-1603ada678be)
Mar 27 15:15:10.340: INFO: Unable to read jessie_tcp@dns-test-service.dns-6510.svc.cluster.local from pod dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be: the server could not find the requested resource (get pods dns-test-10b81a19-4b7d-490f-81c2-1603ada678be)
Mar 27 15:15:10.344: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local from pod dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be: the server could not find the requested resource (get pods dns-test-10b81a19-4b7d-490f-81c2-1603ada678be)
Mar 27 15:15:10.348: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local from pod dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be: the server could not find the requested resource (get pods dns-test-10b81a19-4b7d-490f-81c2-1603ada678be)
Mar 27 15:15:10.371: INFO: Lookups using dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be failed for: [wheezy_udp@dns-test-service.dns-6510.svc.cluster.local wheezy_tcp@dns-test-service.dns-6510.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local jessie_udp@dns-test-service.dns-6510.svc.cluster.local jessie_tcp@dns-test-service.dns-6510.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local]

Mar 27 15:15:15.442: INFO: DNS probes using dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be succeeded

STEP: deleting the pod 03/27/23 15:15:15.442
STEP: deleting the test service 03/27/23 15:15:15.458
STEP: deleting the test headless service 03/27/23 15:15:15.502
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 27 15:15:15.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6510" for this suite. 03/27/23 15:15:15.515
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":217,"skipped":4074,"failed":0}
------------------------------
• [SLOW TEST] [9.311 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:15:06.209
    Mar 27 15:15:06.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename dns 03/27/23 15:15:06.21
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:15:06.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:15:06.23
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 03/27/23 15:15:06.239
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6510.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6510.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6510.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6510.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6510.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6510.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 196.19.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.19.196_udp@PTR;check="$$(dig +tcp +noall +answer +search 196.19.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.19.196_tcp@PTR;sleep 1; done
     03/27/23 15:15:06.262
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6510.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6510.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6510.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6510.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6510.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6510.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 196.19.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.19.196_udp@PTR;check="$$(dig +tcp +noall +answer +search 196.19.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.19.196_tcp@PTR;sleep 1; done
     03/27/23 15:15:06.262
    STEP: creating a pod to probe DNS 03/27/23 15:15:06.263
    STEP: submitting the pod to kubernetes 03/27/23 15:15:06.263
    Mar 27 15:15:06.275: INFO: Waiting up to 15m0s for pod "dns-test-10b81a19-4b7d-490f-81c2-1603ada678be" in namespace "dns-6510" to be "running"
    Mar 27 15:15:06.284: INFO: Pod "dns-test-10b81a19-4b7d-490f-81c2-1603ada678be": Phase="Pending", Reason="", readiness=false. Elapsed: 8.425339ms
    Mar 27 15:15:08.288: INFO: Pod "dns-test-10b81a19-4b7d-490f-81c2-1603ada678be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01272167s
    Mar 27 15:15:10.288: INFO: Pod "dns-test-10b81a19-4b7d-490f-81c2-1603ada678be": Phase="Running", Reason="", readiness=true. Elapsed: 4.012416848s
    Mar 27 15:15:10.288: INFO: Pod "dns-test-10b81a19-4b7d-490f-81c2-1603ada678be" satisfied condition "running"
    STEP: retrieving the pod 03/27/23 15:15:10.288
    STEP: looking for the results for each expected name from probers 03/27/23 15:15:10.291
    Mar 27 15:15:10.300: INFO: Unable to read wheezy_udp@dns-test-service.dns-6510.svc.cluster.local from pod dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be: the server could not find the requested resource (get pods dns-test-10b81a19-4b7d-490f-81c2-1603ada678be)
    Mar 27 15:15:10.305: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6510.svc.cluster.local from pod dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be: the server could not find the requested resource (get pods dns-test-10b81a19-4b7d-490f-81c2-1603ada678be)
    Mar 27 15:15:10.310: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local from pod dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be: the server could not find the requested resource (get pods dns-test-10b81a19-4b7d-490f-81c2-1603ada678be)
    Mar 27 15:15:10.314: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local from pod dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be: the server could not find the requested resource (get pods dns-test-10b81a19-4b7d-490f-81c2-1603ada678be)
    Mar 27 15:15:10.336: INFO: Unable to read jessie_udp@dns-test-service.dns-6510.svc.cluster.local from pod dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be: the server could not find the requested resource (get pods dns-test-10b81a19-4b7d-490f-81c2-1603ada678be)
    Mar 27 15:15:10.340: INFO: Unable to read jessie_tcp@dns-test-service.dns-6510.svc.cluster.local from pod dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be: the server could not find the requested resource (get pods dns-test-10b81a19-4b7d-490f-81c2-1603ada678be)
    Mar 27 15:15:10.344: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local from pod dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be: the server could not find the requested resource (get pods dns-test-10b81a19-4b7d-490f-81c2-1603ada678be)
    Mar 27 15:15:10.348: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local from pod dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be: the server could not find the requested resource (get pods dns-test-10b81a19-4b7d-490f-81c2-1603ada678be)
    Mar 27 15:15:10.371: INFO: Lookups using dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be failed for: [wheezy_udp@dns-test-service.dns-6510.svc.cluster.local wheezy_tcp@dns-test-service.dns-6510.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local jessie_udp@dns-test-service.dns-6510.svc.cluster.local jessie_tcp@dns-test-service.dns-6510.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6510.svc.cluster.local]

    Mar 27 15:15:15.442: INFO: DNS probes using dns-6510/dns-test-10b81a19-4b7d-490f-81c2-1603ada678be succeeded

    STEP: deleting the pod 03/27/23 15:15:15.442
    STEP: deleting the test service 03/27/23 15:15:15.458
    STEP: deleting the test headless service 03/27/23 15:15:15.502
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 27 15:15:15.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6510" for this suite. 03/27/23 15:15:15.515
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:15:15.521
Mar 27 15:15:15.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename services 03/27/23 15:15:15.523
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:15:15.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:15:15.549
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 03/27/23 15:15:15.555
STEP: waiting for available Endpoint 03/27/23 15:15:15.561
STEP: listing all Endpoints 03/27/23 15:15:15.564
STEP: updating the Endpoint 03/27/23 15:15:15.567
STEP: fetching the Endpoint 03/27/23 15:15:15.573
STEP: patching the Endpoint 03/27/23 15:15:15.577
STEP: fetching the Endpoint 03/27/23 15:15:15.584
STEP: deleting the Endpoint by Collection 03/27/23 15:15:15.588
STEP: waiting for Endpoint deletion 03/27/23 15:15:15.595
STEP: fetching the Endpoint 03/27/23 15:15:15.596
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 27 15:15:15.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3397" for this suite. 03/27/23 15:15:15.604
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":218,"skipped":4089,"failed":0}
------------------------------
• [0.092 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:15:15.521
    Mar 27 15:15:15.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename services 03/27/23 15:15:15.523
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:15:15.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:15:15.549
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 03/27/23 15:15:15.555
    STEP: waiting for available Endpoint 03/27/23 15:15:15.561
    STEP: listing all Endpoints 03/27/23 15:15:15.564
    STEP: updating the Endpoint 03/27/23 15:15:15.567
    STEP: fetching the Endpoint 03/27/23 15:15:15.573
    STEP: patching the Endpoint 03/27/23 15:15:15.577
    STEP: fetching the Endpoint 03/27/23 15:15:15.584
    STEP: deleting the Endpoint by Collection 03/27/23 15:15:15.588
    STEP: waiting for Endpoint deletion 03/27/23 15:15:15.595
    STEP: fetching the Endpoint 03/27/23 15:15:15.596
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 27 15:15:15.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3397" for this suite. 03/27/23 15:15:15.604
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:15:15.618
Mar 27 15:15:15.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename job 03/27/23 15:15:15.619
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:15:15.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:15:15.637
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 03/27/23 15:15:15.645
STEP: Patching the Job 03/27/23 15:15:15.651
STEP: Watching for Job to be patched 03/27/23 15:15:15.668
Mar 27 15:15:15.671: INFO: Event ADDED observed for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar 27 15:15:15.671: INFO: Event MODIFIED observed for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar 27 15:15:15.672: INFO: Event MODIFIED found for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 03/27/23 15:15:15.672
STEP: Watching for Job to be updated 03/27/23 15:15:15.684
Mar 27 15:15:15.686: INFO: Event MODIFIED found for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 27 15:15:15.686: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 03/27/23 15:15:15.686
Mar 27 15:15:15.689: INFO: Job: e2e-vjddp as labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched]
STEP: Waiting for job to complete 03/27/23 15:15:15.689
STEP: Delete a job collection with a labelselector 03/27/23 15:15:25.694
STEP: Watching for Job to be deleted 03/27/23 15:15:25.702
Mar 27 15:15:25.704: INFO: Event MODIFIED observed for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 27 15:15:25.704: INFO: Event MODIFIED observed for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 27 15:15:25.704: INFO: Event MODIFIED observed for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 27 15:15:25.704: INFO: Event MODIFIED observed for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 27 15:15:25.704: INFO: Event MODIFIED observed for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 27 15:15:25.704: INFO: Event MODIFIED observed for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 27 15:15:25.704: INFO: Event MODIFIED observed for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 27 15:15:25.704: INFO: Event DELETED found for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 03/27/23 15:15:25.704
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar 27 15:15:25.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1917" for this suite. 03/27/23 15:15:25.715
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":219,"skipped":4135,"failed":0}
------------------------------
• [SLOW TEST] [10.115 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:15:15.618
    Mar 27 15:15:15.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename job 03/27/23 15:15:15.619
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:15:15.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:15:15.637
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 03/27/23 15:15:15.645
    STEP: Patching the Job 03/27/23 15:15:15.651
    STEP: Watching for Job to be patched 03/27/23 15:15:15.668
    Mar 27 15:15:15.671: INFO: Event ADDED observed for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar 27 15:15:15.671: INFO: Event MODIFIED observed for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar 27 15:15:15.672: INFO: Event MODIFIED found for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 03/27/23 15:15:15.672
    STEP: Watching for Job to be updated 03/27/23 15:15:15.684
    Mar 27 15:15:15.686: INFO: Event MODIFIED found for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 27 15:15:15.686: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 03/27/23 15:15:15.686
    Mar 27 15:15:15.689: INFO: Job: e2e-vjddp as labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched]
    STEP: Waiting for job to complete 03/27/23 15:15:15.689
    STEP: Delete a job collection with a labelselector 03/27/23 15:15:25.694
    STEP: Watching for Job to be deleted 03/27/23 15:15:25.702
    Mar 27 15:15:25.704: INFO: Event MODIFIED observed for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 27 15:15:25.704: INFO: Event MODIFIED observed for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 27 15:15:25.704: INFO: Event MODIFIED observed for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 27 15:15:25.704: INFO: Event MODIFIED observed for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 27 15:15:25.704: INFO: Event MODIFIED observed for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 27 15:15:25.704: INFO: Event MODIFIED observed for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 27 15:15:25.704: INFO: Event MODIFIED observed for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 27 15:15:25.704: INFO: Event DELETED found for Job e2e-vjddp in namespace job-1917 with labels: map[e2e-job-label:e2e-vjddp e2e-vjddp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 03/27/23 15:15:25.704
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar 27 15:15:25.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1917" for this suite. 03/27/23 15:15:25.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:15:25.733
Mar 27 15:15:25.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename subpath 03/27/23 15:15:25.734
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:15:25.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:15:25.751
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/27/23 15:15:25.754
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-nxmp 03/27/23 15:15:25.766
STEP: Creating a pod to test atomic-volume-subpath 03/27/23 15:15:25.766
Mar 27 15:15:25.774: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-nxmp" in namespace "subpath-7001" to be "Succeeded or Failed"
Mar 27 15:15:25.777: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.668087ms
Mar 27 15:15:27.782: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 2.00745594s
Mar 27 15:15:29.781: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 4.006940743s
Mar 27 15:15:31.784: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 6.009246812s
Mar 27 15:15:33.785: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 8.010682922s
Mar 27 15:15:35.782: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 10.008000713s
Mar 27 15:15:37.781: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 12.006848212s
Mar 27 15:15:39.783: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 14.008561678s
Mar 27 15:15:41.781: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 16.006914473s
Mar 27 15:15:43.783: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 18.008399287s
Mar 27 15:15:45.784: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 20.009506584s
Mar 27 15:15:47.783: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=false. Elapsed: 22.008837308s
Mar 27 15:15:49.781: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007013445s
STEP: Saw pod success 03/27/23 15:15:49.781
Mar 27 15:15:49.782: INFO: Pod "pod-subpath-test-configmap-nxmp" satisfied condition "Succeeded or Failed"
Mar 27 15:15:49.785: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-subpath-test-configmap-nxmp container test-container-subpath-configmap-nxmp: <nil>
STEP: delete the pod 03/27/23 15:15:49.796
Mar 27 15:15:49.808: INFO: Waiting for pod pod-subpath-test-configmap-nxmp to disappear
Mar 27 15:15:49.810: INFO: Pod pod-subpath-test-configmap-nxmp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-nxmp 03/27/23 15:15:49.81
Mar 27 15:15:49.810: INFO: Deleting pod "pod-subpath-test-configmap-nxmp" in namespace "subpath-7001"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar 27 15:15:49.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7001" for this suite. 03/27/23 15:15:49.818
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":220,"skipped":4147,"failed":0}
------------------------------
• [SLOW TEST] [24.091 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:15:25.733
    Mar 27 15:15:25.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename subpath 03/27/23 15:15:25.734
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:15:25.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:15:25.751
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/27/23 15:15:25.754
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-nxmp 03/27/23 15:15:25.766
    STEP: Creating a pod to test atomic-volume-subpath 03/27/23 15:15:25.766
    Mar 27 15:15:25.774: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-nxmp" in namespace "subpath-7001" to be "Succeeded or Failed"
    Mar 27 15:15:25.777: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.668087ms
    Mar 27 15:15:27.782: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 2.00745594s
    Mar 27 15:15:29.781: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 4.006940743s
    Mar 27 15:15:31.784: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 6.009246812s
    Mar 27 15:15:33.785: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 8.010682922s
    Mar 27 15:15:35.782: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 10.008000713s
    Mar 27 15:15:37.781: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 12.006848212s
    Mar 27 15:15:39.783: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 14.008561678s
    Mar 27 15:15:41.781: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 16.006914473s
    Mar 27 15:15:43.783: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 18.008399287s
    Mar 27 15:15:45.784: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=true. Elapsed: 20.009506584s
    Mar 27 15:15:47.783: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Running", Reason="", readiness=false. Elapsed: 22.008837308s
    Mar 27 15:15:49.781: INFO: Pod "pod-subpath-test-configmap-nxmp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007013445s
    STEP: Saw pod success 03/27/23 15:15:49.781
    Mar 27 15:15:49.782: INFO: Pod "pod-subpath-test-configmap-nxmp" satisfied condition "Succeeded or Failed"
    Mar 27 15:15:49.785: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-subpath-test-configmap-nxmp container test-container-subpath-configmap-nxmp: <nil>
    STEP: delete the pod 03/27/23 15:15:49.796
    Mar 27 15:15:49.808: INFO: Waiting for pod pod-subpath-test-configmap-nxmp to disappear
    Mar 27 15:15:49.810: INFO: Pod pod-subpath-test-configmap-nxmp no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-nxmp 03/27/23 15:15:49.81
    Mar 27 15:15:49.810: INFO: Deleting pod "pod-subpath-test-configmap-nxmp" in namespace "subpath-7001"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar 27 15:15:49.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7001" for this suite. 03/27/23 15:15:49.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:15:49.826
Mar 27 15:15:49.826: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename container-runtime 03/27/23 15:15:49.826
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:15:49.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:15:49.848
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 03/27/23 15:15:49.851
STEP: wait for the container to reach Failed 03/27/23 15:15:49.869
STEP: get the container status 03/27/23 15:15:53.889
STEP: the container should be terminated 03/27/23 15:15:53.892
STEP: the termination message should be set 03/27/23 15:15:53.892
Mar 27 15:15:53.892: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/27/23 15:15:53.892
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar 27 15:15:53.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3272" for this suite. 03/27/23 15:15:53.918
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":221,"skipped":4165,"failed":0}
------------------------------
• [4.100 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:15:49.826
    Mar 27 15:15:49.826: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename container-runtime 03/27/23 15:15:49.826
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:15:49.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:15:49.848
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 03/27/23 15:15:49.851
    STEP: wait for the container to reach Failed 03/27/23 15:15:49.869
    STEP: get the container status 03/27/23 15:15:53.889
    STEP: the container should be terminated 03/27/23 15:15:53.892
    STEP: the termination message should be set 03/27/23 15:15:53.892
    Mar 27 15:15:53.892: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/27/23 15:15:53.892
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar 27 15:15:53.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3272" for this suite. 03/27/23 15:15:53.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:15:53.925
Mar 27 15:15:53.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename svcaccounts 03/27/23 15:15:53.926
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:15:53.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:15:53.952
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Mar 27 15:15:53.970: INFO: Waiting up to 5m0s for pod "pod-service-account-c46e93c8-b46f-4311-860f-c5fde648291a" in namespace "svcaccounts-3304" to be "running"
Mar 27 15:15:53.972: INFO: Pod "pod-service-account-c46e93c8-b46f-4311-860f-c5fde648291a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.382312ms
Mar 27 15:15:55.977: INFO: Pod "pod-service-account-c46e93c8-b46f-4311-860f-c5fde648291a": Phase="Running", Reason="", readiness=true. Elapsed: 2.007750352s
Mar 27 15:15:55.977: INFO: Pod "pod-service-account-c46e93c8-b46f-4311-860f-c5fde648291a" satisfied condition "running"
STEP: reading a file in the container 03/27/23 15:15:55.977
Mar 27 15:15:55.978: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3304 pod-service-account-c46e93c8-b46f-4311-860f-c5fde648291a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 03/27/23 15:15:56.124
Mar 27 15:15:56.124: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3304 pod-service-account-c46e93c8-b46f-4311-860f-c5fde648291a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 03/27/23 15:15:56.267
Mar 27 15:15:56.267: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3304 pod-service-account-c46e93c8-b46f-4311-860f-c5fde648291a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Mar 27 15:15:56.419: INFO: Got root ca configmap in namespace "svcaccounts-3304"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar 27 15:15:56.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3304" for this suite. 03/27/23 15:15:56.427
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":222,"skipped":4171,"failed":0}
------------------------------
• [2.507 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:15:53.925
    Mar 27 15:15:53.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename svcaccounts 03/27/23 15:15:53.926
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:15:53.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:15:53.952
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Mar 27 15:15:53.970: INFO: Waiting up to 5m0s for pod "pod-service-account-c46e93c8-b46f-4311-860f-c5fde648291a" in namespace "svcaccounts-3304" to be "running"
    Mar 27 15:15:53.972: INFO: Pod "pod-service-account-c46e93c8-b46f-4311-860f-c5fde648291a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.382312ms
    Mar 27 15:15:55.977: INFO: Pod "pod-service-account-c46e93c8-b46f-4311-860f-c5fde648291a": Phase="Running", Reason="", readiness=true. Elapsed: 2.007750352s
    Mar 27 15:15:55.977: INFO: Pod "pod-service-account-c46e93c8-b46f-4311-860f-c5fde648291a" satisfied condition "running"
    STEP: reading a file in the container 03/27/23 15:15:55.977
    Mar 27 15:15:55.978: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3304 pod-service-account-c46e93c8-b46f-4311-860f-c5fde648291a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 03/27/23 15:15:56.124
    Mar 27 15:15:56.124: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3304 pod-service-account-c46e93c8-b46f-4311-860f-c5fde648291a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 03/27/23 15:15:56.267
    Mar 27 15:15:56.267: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3304 pod-service-account-c46e93c8-b46f-4311-860f-c5fde648291a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Mar 27 15:15:56.419: INFO: Got root ca configmap in namespace "svcaccounts-3304"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar 27 15:15:56.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-3304" for this suite. 03/27/23 15:15:56.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:15:56.435
Mar 27 15:15:56.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename gc 03/27/23 15:15:56.436
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:15:56.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:15:56.453
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 03/27/23 15:15:56.455
STEP: delete the rc 03/27/23 15:16:01.475
STEP: wait for all pods to be garbage collected 03/27/23 15:16:01.481
STEP: Gathering metrics 03/27/23 15:16:06.489
W0327 15:16:06.499053      24 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 27 15:16:06.499: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 27 15:16:06.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2143" for this suite. 03/27/23 15:16:06.505
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":223,"skipped":4194,"failed":0}
------------------------------
• [SLOW TEST] [10.077 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:15:56.435
    Mar 27 15:15:56.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename gc 03/27/23 15:15:56.436
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:15:56.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:15:56.453
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 03/27/23 15:15:56.455
    STEP: delete the rc 03/27/23 15:16:01.475
    STEP: wait for all pods to be garbage collected 03/27/23 15:16:01.481
    STEP: Gathering metrics 03/27/23 15:16:06.489
    W0327 15:16:06.499053      24 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 27 15:16:06.499: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 27 15:16:06.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2143" for this suite. 03/27/23 15:16:06.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:16:06.516
Mar 27 15:16:06.516: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename configmap 03/27/23 15:16:06.518
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:16:06.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:16:06.537
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-1e88c980-a1c8-44ac-a05e-cd7aa28cc1f5 03/27/23 15:16:06.546
STEP: Creating the pod 03/27/23 15:16:06.551
Mar 27 15:16:06.560: INFO: Waiting up to 5m0s for pod "pod-configmaps-d5290bcc-2482-4e82-8b1d-826ce46811bf" in namespace "configmap-4845" to be "running and ready"
Mar 27 15:16:06.569: INFO: Pod "pod-configmaps-d5290bcc-2482-4e82-8b1d-826ce46811bf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.967882ms
Mar 27 15:16:06.569: INFO: The phase of Pod pod-configmaps-d5290bcc-2482-4e82-8b1d-826ce46811bf is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:16:08.574: INFO: Pod "pod-configmaps-d5290bcc-2482-4e82-8b1d-826ce46811bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014540867s
Mar 27 15:16:08.574: INFO: The phase of Pod pod-configmaps-d5290bcc-2482-4e82-8b1d-826ce46811bf is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:16:10.574: INFO: Pod "pod-configmaps-d5290bcc-2482-4e82-8b1d-826ce46811bf": Phase="Running", Reason="", readiness=true. Elapsed: 4.014419461s
Mar 27 15:16:10.574: INFO: The phase of Pod pod-configmaps-d5290bcc-2482-4e82-8b1d-826ce46811bf is Running (Ready = true)
Mar 27 15:16:10.574: INFO: Pod "pod-configmaps-d5290bcc-2482-4e82-8b1d-826ce46811bf" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-1e88c980-a1c8-44ac-a05e-cd7aa28cc1f5 03/27/23 15:16:10.587
STEP: waiting to observe update in volume 03/27/23 15:16:10.592
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 27 15:17:31.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4845" for this suite. 03/27/23 15:17:31.146
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":224,"skipped":4268,"failed":0}
------------------------------
• [SLOW TEST] [84.638 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:16:06.516
    Mar 27 15:16:06.516: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename configmap 03/27/23 15:16:06.518
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:16:06.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:16:06.537
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-1e88c980-a1c8-44ac-a05e-cd7aa28cc1f5 03/27/23 15:16:06.546
    STEP: Creating the pod 03/27/23 15:16:06.551
    Mar 27 15:16:06.560: INFO: Waiting up to 5m0s for pod "pod-configmaps-d5290bcc-2482-4e82-8b1d-826ce46811bf" in namespace "configmap-4845" to be "running and ready"
    Mar 27 15:16:06.569: INFO: Pod "pod-configmaps-d5290bcc-2482-4e82-8b1d-826ce46811bf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.967882ms
    Mar 27 15:16:06.569: INFO: The phase of Pod pod-configmaps-d5290bcc-2482-4e82-8b1d-826ce46811bf is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:16:08.574: INFO: Pod "pod-configmaps-d5290bcc-2482-4e82-8b1d-826ce46811bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014540867s
    Mar 27 15:16:08.574: INFO: The phase of Pod pod-configmaps-d5290bcc-2482-4e82-8b1d-826ce46811bf is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:16:10.574: INFO: Pod "pod-configmaps-d5290bcc-2482-4e82-8b1d-826ce46811bf": Phase="Running", Reason="", readiness=true. Elapsed: 4.014419461s
    Mar 27 15:16:10.574: INFO: The phase of Pod pod-configmaps-d5290bcc-2482-4e82-8b1d-826ce46811bf is Running (Ready = true)
    Mar 27 15:16:10.574: INFO: Pod "pod-configmaps-d5290bcc-2482-4e82-8b1d-826ce46811bf" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-1e88c980-a1c8-44ac-a05e-cd7aa28cc1f5 03/27/23 15:16:10.587
    STEP: waiting to observe update in volume 03/27/23 15:16:10.592
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 27 15:17:31.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4845" for this suite. 03/27/23 15:17:31.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:17:31.157
Mar 27 15:17:31.157: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename configmap 03/27/23 15:17:31.158
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:17:31.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:17:31.177
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 03/27/23 15:17:31.18
STEP: fetching the ConfigMap 03/27/23 15:17:31.188
STEP: patching the ConfigMap 03/27/23 15:17:31.191
STEP: listing all ConfigMaps in all namespaces with a label selector 03/27/23 15:17:31.199
STEP: deleting the ConfigMap by collection with a label selector 03/27/23 15:17:31.204
STEP: listing all ConfigMaps in test namespace 03/27/23 15:17:31.211
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar 27 15:17:31.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6023" for this suite. 03/27/23 15:17:31.221
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":225,"skipped":4294,"failed":0}
------------------------------
• [0.075 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:17:31.157
    Mar 27 15:17:31.157: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename configmap 03/27/23 15:17:31.158
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:17:31.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:17:31.177
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 03/27/23 15:17:31.18
    STEP: fetching the ConfigMap 03/27/23 15:17:31.188
    STEP: patching the ConfigMap 03/27/23 15:17:31.191
    STEP: listing all ConfigMaps in all namespaces with a label selector 03/27/23 15:17:31.199
    STEP: deleting the ConfigMap by collection with a label selector 03/27/23 15:17:31.204
    STEP: listing all ConfigMaps in test namespace 03/27/23 15:17:31.211
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 27 15:17:31.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6023" for this suite. 03/27/23 15:17:31.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:17:31.233
Mar 27 15:17:31.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename events 03/27/23 15:17:31.234
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:17:31.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:17:31.255
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 03/27/23 15:17:31.257
STEP: listing events in all namespaces 03/27/23 15:17:31.266
STEP: listing events in test namespace 03/27/23 15:17:31.27
STEP: listing events with field selection filtering on source 03/27/23 15:17:31.272
STEP: listing events with field selection filtering on reportingController 03/27/23 15:17:31.275
STEP: getting the test event 03/27/23 15:17:31.278
STEP: patching the test event 03/27/23 15:17:31.28
STEP: getting the test event 03/27/23 15:17:31.287
STEP: updating the test event 03/27/23 15:17:31.289
STEP: getting the test event 03/27/23 15:17:31.295
STEP: deleting the test event 03/27/23 15:17:31.299
STEP: listing events in all namespaces 03/27/23 15:17:31.311
STEP: listing events in test namespace 03/27/23 15:17:31.315
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Mar 27 15:17:31.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4718" for this suite. 03/27/23 15:17:31.323
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":226,"skipped":4308,"failed":0}
------------------------------
• [0.096 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:17:31.233
    Mar 27 15:17:31.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename events 03/27/23 15:17:31.234
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:17:31.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:17:31.255
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 03/27/23 15:17:31.257
    STEP: listing events in all namespaces 03/27/23 15:17:31.266
    STEP: listing events in test namespace 03/27/23 15:17:31.27
    STEP: listing events with field selection filtering on source 03/27/23 15:17:31.272
    STEP: listing events with field selection filtering on reportingController 03/27/23 15:17:31.275
    STEP: getting the test event 03/27/23 15:17:31.278
    STEP: patching the test event 03/27/23 15:17:31.28
    STEP: getting the test event 03/27/23 15:17:31.287
    STEP: updating the test event 03/27/23 15:17:31.289
    STEP: getting the test event 03/27/23 15:17:31.295
    STEP: deleting the test event 03/27/23 15:17:31.299
    STEP: listing events in all namespaces 03/27/23 15:17:31.311
    STEP: listing events in test namespace 03/27/23 15:17:31.315
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Mar 27 15:17:31.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4718" for this suite. 03/27/23 15:17:31.323
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:17:31.329
Mar 27 15:17:31.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename webhook 03/27/23 15:17:31.33
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:17:31.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:17:31.348
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/27/23 15:17:31.369
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:17:31.971
STEP: Deploying the webhook pod 03/27/23 15:17:31.98
STEP: Wait for the deployment to be ready 03/27/23 15:17:31.994
Mar 27 15:17:32.008: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 15:17:34.022
STEP: Verifying the service has paired with the endpoint 03/27/23 15:17:34.037
Mar 27 15:17:35.038: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Mar 27 15:17:35.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1216-crds.webhook.example.com via the AdmissionRegistration API 03/27/23 15:17:35.555
STEP: Creating a custom resource that should be mutated by the webhook 03/27/23 15:17:35.576
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 15:17:38.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4347" for this suite. 03/27/23 15:17:38.163
STEP: Destroying namespace "webhook-4347-markers" for this suite. 03/27/23 15:17:38.169
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":227,"skipped":4311,"failed":0}
------------------------------
• [SLOW TEST] [6.887 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:17:31.329
    Mar 27 15:17:31.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename webhook 03/27/23 15:17:31.33
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:17:31.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:17:31.348
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/27/23 15:17:31.369
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:17:31.971
    STEP: Deploying the webhook pod 03/27/23 15:17:31.98
    STEP: Wait for the deployment to be ready 03/27/23 15:17:31.994
    Mar 27 15:17:32.008: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 15:17:34.022
    STEP: Verifying the service has paired with the endpoint 03/27/23 15:17:34.037
    Mar 27 15:17:35.038: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Mar 27 15:17:35.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1216-crds.webhook.example.com via the AdmissionRegistration API 03/27/23 15:17:35.555
    STEP: Creating a custom resource that should be mutated by the webhook 03/27/23 15:17:35.576
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 15:17:38.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4347" for this suite. 03/27/23 15:17:38.163
    STEP: Destroying namespace "webhook-4347-markers" for this suite. 03/27/23 15:17:38.169
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:17:38.226
Mar 27 15:17:38.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename replicaset 03/27/23 15:17:38.227
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:17:38.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:17:38.244
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 03/27/23 15:17:38.248
STEP: Verify that the required pods have come up 03/27/23 15:17:38.254
Mar 27 15:17:38.259: INFO: Pod name sample-pod: Found 0 pods out of 3
Mar 27 15:17:43.263: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 03/27/23 15:17:43.263
Mar 27 15:17:43.266: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 03/27/23 15:17:43.266
STEP: DeleteCollection of the ReplicaSets 03/27/23 15:17:43.27
STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/27/23 15:17:43.277
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar 27 15:17:43.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2711" for this suite. 03/27/23 15:17:43.284
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":228,"skipped":4389,"failed":0}
------------------------------
• [SLOW TEST] [5.064 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:17:38.226
    Mar 27 15:17:38.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename replicaset 03/27/23 15:17:38.227
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:17:38.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:17:38.244
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 03/27/23 15:17:38.248
    STEP: Verify that the required pods have come up 03/27/23 15:17:38.254
    Mar 27 15:17:38.259: INFO: Pod name sample-pod: Found 0 pods out of 3
    Mar 27 15:17:43.263: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 03/27/23 15:17:43.263
    Mar 27 15:17:43.266: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 03/27/23 15:17:43.266
    STEP: DeleteCollection of the ReplicaSets 03/27/23 15:17:43.27
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/27/23 15:17:43.277
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar 27 15:17:43.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2711" for this suite. 03/27/23 15:17:43.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:17:43.291
Mar 27 15:17:43.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename statefulset 03/27/23 15:17:43.292
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:17:43.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:17:43.304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5944 03/27/23 15:17:43.309
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 03/27/23 15:17:43.323
Mar 27 15:17:43.343: INFO: Found 0 stateful pods, waiting for 3
Mar 27 15:17:53.350: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 15:17:53.350: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 15:17:53.350: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/27/23 15:17:53.359
Mar 27 15:17:53.379: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/27/23 15:17:53.379
STEP: Not applying an update when the partition is greater than the number of replicas 03/27/23 15:18:03.394
STEP: Performing a canary update 03/27/23 15:18:03.394
Mar 27 15:18:03.413: INFO: Updating stateful set ss2
Mar 27 15:18:03.419: INFO: Waiting for Pod statefulset-5944/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 03/27/23 15:18:13.433
Mar 27 15:18:13.473: INFO: Found 1 stateful pods, waiting for 3
Mar 27 15:18:23.479: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 15:18:23.480: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 15:18:23.480: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 03/27/23 15:18:23.499
Mar 27 15:18:23.523: INFO: Updating stateful set ss2
Mar 27 15:18:23.529: INFO: Waiting for Pod statefulset-5944/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Mar 27 15:18:33.561: INFO: Updating stateful set ss2
Mar 27 15:18:33.577: INFO: Waiting for StatefulSet statefulset-5944/ss2 to complete update
Mar 27 15:18:33.577: INFO: Waiting for Pod statefulset-5944/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 27 15:18:43.583: INFO: Deleting all statefulset in ns statefulset-5944
Mar 27 15:18:43.586: INFO: Scaling statefulset ss2 to 0
Mar 27 15:18:53.603: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 15:18:53.606: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 27 15:18:53.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5944" for this suite. 03/27/23 15:18:53.622
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":229,"skipped":4411,"failed":0}
------------------------------
• [SLOW TEST] [70.339 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:17:43.291
    Mar 27 15:17:43.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename statefulset 03/27/23 15:17:43.292
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:17:43.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:17:43.304
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5944 03/27/23 15:17:43.309
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 03/27/23 15:17:43.323
    Mar 27 15:17:43.343: INFO: Found 0 stateful pods, waiting for 3
    Mar 27 15:17:53.350: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 15:17:53.350: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 15:17:53.350: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/27/23 15:17:53.359
    Mar 27 15:17:53.379: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/27/23 15:17:53.379
    STEP: Not applying an update when the partition is greater than the number of replicas 03/27/23 15:18:03.394
    STEP: Performing a canary update 03/27/23 15:18:03.394
    Mar 27 15:18:03.413: INFO: Updating stateful set ss2
    Mar 27 15:18:03.419: INFO: Waiting for Pod statefulset-5944/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 03/27/23 15:18:13.433
    Mar 27 15:18:13.473: INFO: Found 1 stateful pods, waiting for 3
    Mar 27 15:18:23.479: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 15:18:23.480: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 15:18:23.480: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 03/27/23 15:18:23.499
    Mar 27 15:18:23.523: INFO: Updating stateful set ss2
    Mar 27 15:18:23.529: INFO: Waiting for Pod statefulset-5944/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Mar 27 15:18:33.561: INFO: Updating stateful set ss2
    Mar 27 15:18:33.577: INFO: Waiting for StatefulSet statefulset-5944/ss2 to complete update
    Mar 27 15:18:33.577: INFO: Waiting for Pod statefulset-5944/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 27 15:18:43.583: INFO: Deleting all statefulset in ns statefulset-5944
    Mar 27 15:18:43.586: INFO: Scaling statefulset ss2 to 0
    Mar 27 15:18:53.603: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 15:18:53.606: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 27 15:18:53.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5944" for this suite. 03/27/23 15:18:53.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:18:53.631
Mar 27 15:18:53.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename watch 03/27/23 15:18:53.633
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:18:53.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:18:53.651
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 03/27/23 15:18:53.655
STEP: creating a watch on configmaps with label B 03/27/23 15:18:53.657
STEP: creating a watch on configmaps with label A or B 03/27/23 15:18:53.658
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/27/23 15:18:53.659
Mar 27 15:18:53.665: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2244  13e5f03a-aaed-48cd-88cb-101ef66228d9 43600 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 15:18:53.665: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2244  13e5f03a-aaed-48cd-88cb-101ef66228d9 43600 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/27/23 15:18:53.665
Mar 27 15:18:53.673: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2244  13e5f03a-aaed-48cd-88cb-101ef66228d9 43601 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 15:18:53.673: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2244  13e5f03a-aaed-48cd-88cb-101ef66228d9 43601 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/27/23 15:18:53.673
Mar 27 15:18:53.686: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2244  13e5f03a-aaed-48cd-88cb-101ef66228d9 43603 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 15:18:53.686: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2244  13e5f03a-aaed-48cd-88cb-101ef66228d9 43603 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/27/23 15:18:53.686
Mar 27 15:18:53.694: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2244  13e5f03a-aaed-48cd-88cb-101ef66228d9 43604 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 15:18:53.694: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2244  13e5f03a-aaed-48cd-88cb-101ef66228d9 43604 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/27/23 15:18:53.694
Mar 27 15:18:53.700: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2244  59718963-2ef8-4fcf-a411-18fff7eff552 43605 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 15:18:53.700: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2244  59718963-2ef8-4fcf-a411-18fff7eff552 43605 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/27/23 15:19:03.701
Mar 27 15:19:03.727: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2244  59718963-2ef8-4fcf-a411-18fff7eff552 43714 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 15:19:03.727: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2244  59718963-2ef8-4fcf-a411-18fff7eff552 43714 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar 27 15:19:13.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2244" for this suite. 03/27/23 15:19:13.737
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":230,"skipped":4448,"failed":0}
------------------------------
• [SLOW TEST] [20.113 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:18:53.631
    Mar 27 15:18:53.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename watch 03/27/23 15:18:53.633
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:18:53.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:18:53.651
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 03/27/23 15:18:53.655
    STEP: creating a watch on configmaps with label B 03/27/23 15:18:53.657
    STEP: creating a watch on configmaps with label A or B 03/27/23 15:18:53.658
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/27/23 15:18:53.659
    Mar 27 15:18:53.665: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2244  13e5f03a-aaed-48cd-88cb-101ef66228d9 43600 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 15:18:53.665: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2244  13e5f03a-aaed-48cd-88cb-101ef66228d9 43600 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/27/23 15:18:53.665
    Mar 27 15:18:53.673: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2244  13e5f03a-aaed-48cd-88cb-101ef66228d9 43601 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 15:18:53.673: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2244  13e5f03a-aaed-48cd-88cb-101ef66228d9 43601 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/27/23 15:18:53.673
    Mar 27 15:18:53.686: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2244  13e5f03a-aaed-48cd-88cb-101ef66228d9 43603 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 15:18:53.686: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2244  13e5f03a-aaed-48cd-88cb-101ef66228d9 43603 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/27/23 15:18:53.686
    Mar 27 15:18:53.694: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2244  13e5f03a-aaed-48cd-88cb-101ef66228d9 43604 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 15:18:53.694: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2244  13e5f03a-aaed-48cd-88cb-101ef66228d9 43604 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/27/23 15:18:53.694
    Mar 27 15:18:53.700: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2244  59718963-2ef8-4fcf-a411-18fff7eff552 43605 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 15:18:53.700: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2244  59718963-2ef8-4fcf-a411-18fff7eff552 43605 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/27/23 15:19:03.701
    Mar 27 15:19:03.727: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2244  59718963-2ef8-4fcf-a411-18fff7eff552 43714 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 15:19:03.727: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2244  59718963-2ef8-4fcf-a411-18fff7eff552 43714 0 2023-03-27 15:18:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-27 15:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar 27 15:19:13.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2244" for this suite. 03/27/23 15:19:13.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:19:13.746
Mar 27 15:19:13.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename var-expansion 03/27/23 15:19:13.747
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:19:13.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:19:13.773
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 03/27/23 15:19:13.784
Mar 27 15:19:13.794: INFO: Waiting up to 5m0s for pod "var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80" in namespace "var-expansion-5823" to be "Succeeded or Failed"
Mar 27 15:19:13.807: INFO: Pod "var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80": Phase="Pending", Reason="", readiness=false. Elapsed: 13.66767ms
Mar 27 15:19:15.812: INFO: Pod "var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018089936s
Mar 27 15:19:17.813: INFO: Pod "var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80": Phase="Running", Reason="", readiness=true. Elapsed: 4.019531534s
Mar 27 15:19:19.813: INFO: Pod "var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80": Phase="Running", Reason="", readiness=false. Elapsed: 6.018789788s
Mar 27 15:19:21.823: INFO: Pod "var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.029670084s
STEP: Saw pod success 03/27/23 15:19:21.824
Mar 27 15:19:21.824: INFO: Pod "var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80" satisfied condition "Succeeded or Failed"
Mar 27 15:19:21.827: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80 container dapi-container: <nil>
STEP: delete the pod 03/27/23 15:19:21.836
Mar 27 15:19:21.853: INFO: Waiting for pod var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80 to disappear
Mar 27 15:19:21.859: INFO: Pod var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 27 15:19:21.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5823" for this suite. 03/27/23 15:19:21.864
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":231,"skipped":4470,"failed":0}
------------------------------
• [SLOW TEST] [8.127 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:19:13.746
    Mar 27 15:19:13.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename var-expansion 03/27/23 15:19:13.747
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:19:13.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:19:13.773
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 03/27/23 15:19:13.784
    Mar 27 15:19:13.794: INFO: Waiting up to 5m0s for pod "var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80" in namespace "var-expansion-5823" to be "Succeeded or Failed"
    Mar 27 15:19:13.807: INFO: Pod "var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80": Phase="Pending", Reason="", readiness=false. Elapsed: 13.66767ms
    Mar 27 15:19:15.812: INFO: Pod "var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018089936s
    Mar 27 15:19:17.813: INFO: Pod "var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80": Phase="Running", Reason="", readiness=true. Elapsed: 4.019531534s
    Mar 27 15:19:19.813: INFO: Pod "var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80": Phase="Running", Reason="", readiness=false. Elapsed: 6.018789788s
    Mar 27 15:19:21.823: INFO: Pod "var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.029670084s
    STEP: Saw pod success 03/27/23 15:19:21.824
    Mar 27 15:19:21.824: INFO: Pod "var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80" satisfied condition "Succeeded or Failed"
    Mar 27 15:19:21.827: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80 container dapi-container: <nil>
    STEP: delete the pod 03/27/23 15:19:21.836
    Mar 27 15:19:21.853: INFO: Waiting for pod var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80 to disappear
    Mar 27 15:19:21.859: INFO: Pod var-expansion-7a832bc6-598b-4893-afeb-17f7cd810d80 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 27 15:19:21.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5823" for this suite. 03/27/23 15:19:21.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:19:21.875
Mar 27 15:19:21.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename security-context-test 03/27/23 15:19:21.876
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:19:21.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:19:21.894
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Mar 27 15:19:21.908: INFO: Waiting up to 5m0s for pod "busybox-user-65534-05609fc3-8170-4072-927a-57b462320420" in namespace "security-context-test-7930" to be "Succeeded or Failed"
Mar 27 15:19:21.915: INFO: Pod "busybox-user-65534-05609fc3-8170-4072-927a-57b462320420": Phase="Pending", Reason="", readiness=false. Elapsed: 7.038359ms
Mar 27 15:19:23.920: INFO: Pod "busybox-user-65534-05609fc3-8170-4072-927a-57b462320420": Phase="Running", Reason="", readiness=true. Elapsed: 2.012396612s
Mar 27 15:19:25.921: INFO: Pod "busybox-user-65534-05609fc3-8170-4072-927a-57b462320420": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013515151s
Mar 27 15:19:25.921: INFO: Pod "busybox-user-65534-05609fc3-8170-4072-927a-57b462320420" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar 27 15:19:25.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7930" for this suite. 03/27/23 15:19:25.927
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":232,"skipped":4502,"failed":0}
------------------------------
• [4.058 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:19:21.875
    Mar 27 15:19:21.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename security-context-test 03/27/23 15:19:21.876
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:19:21.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:19:21.894
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Mar 27 15:19:21.908: INFO: Waiting up to 5m0s for pod "busybox-user-65534-05609fc3-8170-4072-927a-57b462320420" in namespace "security-context-test-7930" to be "Succeeded or Failed"
    Mar 27 15:19:21.915: INFO: Pod "busybox-user-65534-05609fc3-8170-4072-927a-57b462320420": Phase="Pending", Reason="", readiness=false. Elapsed: 7.038359ms
    Mar 27 15:19:23.920: INFO: Pod "busybox-user-65534-05609fc3-8170-4072-927a-57b462320420": Phase="Running", Reason="", readiness=true. Elapsed: 2.012396612s
    Mar 27 15:19:25.921: INFO: Pod "busybox-user-65534-05609fc3-8170-4072-927a-57b462320420": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013515151s
    Mar 27 15:19:25.921: INFO: Pod "busybox-user-65534-05609fc3-8170-4072-927a-57b462320420" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar 27 15:19:25.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-7930" for this suite. 03/27/23 15:19:25.927
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:19:25.936
Mar 27 15:19:25.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename hostport 03/27/23 15:19:25.937
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:19:25.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:19:25.957
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/27/23 15:19:25.965
Mar 27 15:19:25.972: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-6658" to be "running and ready"
Mar 27 15:19:25.975: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.610629ms
Mar 27 15:19:25.975: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:19:27.979: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006933289s
Mar 27 15:19:27.979: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 27 15:19:27.979: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.1.4 on the node which pod1 resides and expect scheduled 03/27/23 15:19:27.979
Mar 27 15:19:27.986: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-6658" to be "running and ready"
Mar 27 15:19:27.988: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.834427ms
Mar 27 15:19:27.988: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:19:29.993: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007476559s
Mar 27 15:19:29.993: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 27 15:19:29.993: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.1.4 but use UDP protocol on the node which pod2 resides 03/27/23 15:19:29.993
Mar 27 15:19:30.000: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-6658" to be "running and ready"
Mar 27 15:19:30.003: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.940028ms
Mar 27 15:19:30.003: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:19:32.013: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.012907069s
Mar 27 15:19:32.013: INFO: The phase of Pod pod3 is Running (Ready = true)
Mar 27 15:19:32.013: INFO: Pod "pod3" satisfied condition "running and ready"
Mar 27 15:19:32.019: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-6658" to be "running and ready"
Mar 27 15:19:32.023: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.64959ms
Mar 27 15:19:32.023: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:19:34.028: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.008790575s
Mar 27 15:19:34.028: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Mar 27 15:19:34.028: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/27/23 15:19:34.033
Mar 27 15:19:34.033: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.1.4 http://127.0.0.1:54323/hostname] Namespace:hostport-6658 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 15:19:34.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 15:19:34.033: INFO: ExecWithOptions: Clientset creation
Mar 27 15:19:34.033: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-6658/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.1.4+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.1.4, port: 54323 03/27/23 15:19:34.139
Mar 27 15:19:34.139: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.1.4:54323/hostname] Namespace:hostport-6658 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 15:19:34.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 15:19:34.139: INFO: ExecWithOptions: Clientset creation
Mar 27 15:19:34.139: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-6658/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.1.4%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.1.4, port: 54323 UDP 03/27/23 15:19:34.237
Mar 27 15:19:34.237: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.1.4 54323] Namespace:hostport-6658 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 15:19:34.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 15:19:34.238: INFO: ExecWithOptions: Clientset creation
Mar 27 15:19:34.238: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-6658/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.1.4+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Mar 27 15:19:39.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-6658" for this suite. 03/27/23 15:19:39.357
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":233,"skipped":4510,"failed":0}
------------------------------
• [SLOW TEST] [13.435 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:19:25.936
    Mar 27 15:19:25.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename hostport 03/27/23 15:19:25.937
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:19:25.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:19:25.957
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/27/23 15:19:25.965
    Mar 27 15:19:25.972: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-6658" to be "running and ready"
    Mar 27 15:19:25.975: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.610629ms
    Mar 27 15:19:25.975: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:19:27.979: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006933289s
    Mar 27 15:19:27.979: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 27 15:19:27.979: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.1.4 on the node which pod1 resides and expect scheduled 03/27/23 15:19:27.979
    Mar 27 15:19:27.986: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-6658" to be "running and ready"
    Mar 27 15:19:27.988: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.834427ms
    Mar 27 15:19:27.988: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:19:29.993: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007476559s
    Mar 27 15:19:29.993: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 27 15:19:29.993: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.1.4 but use UDP protocol on the node which pod2 resides 03/27/23 15:19:29.993
    Mar 27 15:19:30.000: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-6658" to be "running and ready"
    Mar 27 15:19:30.003: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.940028ms
    Mar 27 15:19:30.003: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:19:32.013: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.012907069s
    Mar 27 15:19:32.013: INFO: The phase of Pod pod3 is Running (Ready = true)
    Mar 27 15:19:32.013: INFO: Pod "pod3" satisfied condition "running and ready"
    Mar 27 15:19:32.019: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-6658" to be "running and ready"
    Mar 27 15:19:32.023: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.64959ms
    Mar 27 15:19:32.023: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:19:34.028: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.008790575s
    Mar 27 15:19:34.028: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Mar 27 15:19:34.028: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/27/23 15:19:34.033
    Mar 27 15:19:34.033: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.1.4 http://127.0.0.1:54323/hostname] Namespace:hostport-6658 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 15:19:34.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 15:19:34.033: INFO: ExecWithOptions: Clientset creation
    Mar 27 15:19:34.033: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-6658/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.1.4+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.1.4, port: 54323 03/27/23 15:19:34.139
    Mar 27 15:19:34.139: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.1.4:54323/hostname] Namespace:hostport-6658 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 15:19:34.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 15:19:34.139: INFO: ExecWithOptions: Clientset creation
    Mar 27 15:19:34.139: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-6658/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.1.4%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.1.4, port: 54323 UDP 03/27/23 15:19:34.237
    Mar 27 15:19:34.237: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.1.4 54323] Namespace:hostport-6658 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 15:19:34.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 15:19:34.238: INFO: ExecWithOptions: Clientset creation
    Mar 27 15:19:34.238: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-6658/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.1.4+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Mar 27 15:19:39.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-6658" for this suite. 03/27/23 15:19:39.357
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:19:39.371
Mar 27 15:19:39.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename secrets 03/27/23 15:19:39.373
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:19:39.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:19:39.39
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-40dc9b79-ea12-410f-80cb-47bd27f0d650 03/27/23 15:19:39.394
STEP: Creating a pod to test consume secrets 03/27/23 15:19:39.403
Mar 27 15:19:39.410: INFO: Waiting up to 5m0s for pod "pod-secrets-4d6f1f92-b41e-4b39-9e9a-5247e86d9091" in namespace "secrets-2000" to be "Succeeded or Failed"
Mar 27 15:19:39.418: INFO: Pod "pod-secrets-4d6f1f92-b41e-4b39-9e9a-5247e86d9091": Phase="Pending", Reason="", readiness=false. Elapsed: 7.697049ms
Mar 27 15:19:41.423: INFO: Pod "pod-secrets-4d6f1f92-b41e-4b39-9e9a-5247e86d9091": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012275887s
Mar 27 15:19:43.423: INFO: Pod "pod-secrets-4d6f1f92-b41e-4b39-9e9a-5247e86d9091": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012608549s
STEP: Saw pod success 03/27/23 15:19:43.423
Mar 27 15:19:43.424: INFO: Pod "pod-secrets-4d6f1f92-b41e-4b39-9e9a-5247e86d9091" satisfied condition "Succeeded or Failed"
Mar 27 15:19:43.426: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-secrets-4d6f1f92-b41e-4b39-9e9a-5247e86d9091 container secret-volume-test: <nil>
STEP: delete the pod 03/27/23 15:19:43.435
Mar 27 15:19:43.454: INFO: Waiting for pod pod-secrets-4d6f1f92-b41e-4b39-9e9a-5247e86d9091 to disappear
Mar 27 15:19:43.458: INFO: Pod pod-secrets-4d6f1f92-b41e-4b39-9e9a-5247e86d9091 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 27 15:19:43.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2000" for this suite. 03/27/23 15:19:43.463
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":234,"skipped":4514,"failed":0}
------------------------------
• [4.099 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:19:39.371
    Mar 27 15:19:39.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename secrets 03/27/23 15:19:39.373
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:19:39.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:19:39.39
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-40dc9b79-ea12-410f-80cb-47bd27f0d650 03/27/23 15:19:39.394
    STEP: Creating a pod to test consume secrets 03/27/23 15:19:39.403
    Mar 27 15:19:39.410: INFO: Waiting up to 5m0s for pod "pod-secrets-4d6f1f92-b41e-4b39-9e9a-5247e86d9091" in namespace "secrets-2000" to be "Succeeded or Failed"
    Mar 27 15:19:39.418: INFO: Pod "pod-secrets-4d6f1f92-b41e-4b39-9e9a-5247e86d9091": Phase="Pending", Reason="", readiness=false. Elapsed: 7.697049ms
    Mar 27 15:19:41.423: INFO: Pod "pod-secrets-4d6f1f92-b41e-4b39-9e9a-5247e86d9091": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012275887s
    Mar 27 15:19:43.423: INFO: Pod "pod-secrets-4d6f1f92-b41e-4b39-9e9a-5247e86d9091": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012608549s
    STEP: Saw pod success 03/27/23 15:19:43.423
    Mar 27 15:19:43.424: INFO: Pod "pod-secrets-4d6f1f92-b41e-4b39-9e9a-5247e86d9091" satisfied condition "Succeeded or Failed"
    Mar 27 15:19:43.426: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-secrets-4d6f1f92-b41e-4b39-9e9a-5247e86d9091 container secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 15:19:43.435
    Mar 27 15:19:43.454: INFO: Waiting for pod pod-secrets-4d6f1f92-b41e-4b39-9e9a-5247e86d9091 to disappear
    Mar 27 15:19:43.458: INFO: Pod pod-secrets-4d6f1f92-b41e-4b39-9e9a-5247e86d9091 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 27 15:19:43.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2000" for this suite. 03/27/23 15:19:43.463
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:19:43.472
Mar 27 15:19:43.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename webhook 03/27/23 15:19:43.472
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:19:43.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:19:43.49
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/27/23 15:19:43.506
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:19:43.935
STEP: Deploying the webhook pod 03/27/23 15:19:43.943
STEP: Wait for the deployment to be ready 03/27/23 15:19:43.957
Mar 27 15:19:43.966: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 15:19:45.975
STEP: Verifying the service has paired with the endpoint 03/27/23 15:19:45.986
Mar 27 15:19:46.986: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 03/27/23 15:19:46.99
STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/27/23 15:19:47.411
STEP: Creating a configMap that should not be mutated 03/27/23 15:19:47.418
STEP: Patching a mutating webhook configuration's rules to include the create operation 03/27/23 15:19:48.256
STEP: Creating a configMap that should be mutated 03/27/23 15:19:48.523
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 15:19:48.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-942" for this suite. 03/27/23 15:19:48.71
STEP: Destroying namespace "webhook-942-markers" for this suite. 03/27/23 15:19:48.95
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":235,"skipped":4517,"failed":0}
------------------------------
• [SLOW TEST] [6.795 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:19:43.472
    Mar 27 15:19:43.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename webhook 03/27/23 15:19:43.472
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:19:43.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:19:43.49
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/27/23 15:19:43.506
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:19:43.935
    STEP: Deploying the webhook pod 03/27/23 15:19:43.943
    STEP: Wait for the deployment to be ready 03/27/23 15:19:43.957
    Mar 27 15:19:43.966: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 15:19:45.975
    STEP: Verifying the service has paired with the endpoint 03/27/23 15:19:45.986
    Mar 27 15:19:46.986: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 03/27/23 15:19:46.99
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/27/23 15:19:47.411
    STEP: Creating a configMap that should not be mutated 03/27/23 15:19:47.418
    STEP: Patching a mutating webhook configuration's rules to include the create operation 03/27/23 15:19:48.256
    STEP: Creating a configMap that should be mutated 03/27/23 15:19:48.523
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 15:19:48.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-942" for this suite. 03/27/23 15:19:48.71
    STEP: Destroying namespace "webhook-942-markers" for this suite. 03/27/23 15:19:48.95
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:19:50.267
Mar 27 15:19:50.267: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename cronjob 03/27/23 15:19:50.268
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:19:50.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:19:50.378
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 03/27/23 15:19:50.557
STEP: Ensuring a job is scheduled 03/27/23 15:19:50.573
STEP: Ensuring exactly one is scheduled 03/27/23 15:20:00.579
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/27/23 15:20:00.582
STEP: Ensuring the job is replaced with a new one 03/27/23 15:20:00.586
STEP: Removing cronjob 03/27/23 15:21:00.593
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar 27 15:21:00.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2252" for this suite. 03/27/23 15:21:00.606
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":236,"skipped":4526,"failed":0}
------------------------------
• [SLOW TEST] [70.346 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:19:50.267
    Mar 27 15:19:50.267: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename cronjob 03/27/23 15:19:50.268
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:19:50.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:19:50.378
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 03/27/23 15:19:50.557
    STEP: Ensuring a job is scheduled 03/27/23 15:19:50.573
    STEP: Ensuring exactly one is scheduled 03/27/23 15:20:00.579
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/27/23 15:20:00.582
    STEP: Ensuring the job is replaced with a new one 03/27/23 15:20:00.586
    STEP: Removing cronjob 03/27/23 15:21:00.593
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar 27 15:21:00.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2252" for this suite. 03/27/23 15:21:00.606
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:21:00.614
Mar 27 15:21:00.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename webhook 03/27/23 15:21:00.615
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:21:00.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:21:00.649
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/27/23 15:21:00.675
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:21:01.211
STEP: Deploying the webhook pod 03/27/23 15:21:01.22
STEP: Wait for the deployment to be ready 03/27/23 15:21:01.234
Mar 27 15:21:01.241: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 03/27/23 15:21:03.252
STEP: Verifying the service has paired with the endpoint 03/27/23 15:21:03.265
Mar 27 15:21:04.265: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 03/27/23 15:21:04.269
STEP: create a pod that should be denied by the webhook 03/27/23 15:21:04.292
STEP: create a pod that causes the webhook to hang 03/27/23 15:21:04.306
STEP: create a configmap that should be denied by the webhook 03/27/23 15:21:14.315
STEP: create a configmap that should be admitted by the webhook 03/27/23 15:21:14.342
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/27/23 15:21:14.358
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/27/23 15:21:14.366
STEP: create a namespace that bypass the webhook 03/27/23 15:21:14.371
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/27/23 15:21:14.377
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 15:21:14.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9630" for this suite. 03/27/23 15:21:14.459
STEP: Destroying namespace "webhook-9630-markers" for this suite. 03/27/23 15:21:14.464
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":237,"skipped":4526,"failed":0}
------------------------------
• [SLOW TEST] [13.899 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:21:00.614
    Mar 27 15:21:00.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename webhook 03/27/23 15:21:00.615
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:21:00.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:21:00.649
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/27/23 15:21:00.675
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:21:01.211
    STEP: Deploying the webhook pod 03/27/23 15:21:01.22
    STEP: Wait for the deployment to be ready 03/27/23 15:21:01.234
    Mar 27 15:21:01.241: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 03/27/23 15:21:03.252
    STEP: Verifying the service has paired with the endpoint 03/27/23 15:21:03.265
    Mar 27 15:21:04.265: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 03/27/23 15:21:04.269
    STEP: create a pod that should be denied by the webhook 03/27/23 15:21:04.292
    STEP: create a pod that causes the webhook to hang 03/27/23 15:21:04.306
    STEP: create a configmap that should be denied by the webhook 03/27/23 15:21:14.315
    STEP: create a configmap that should be admitted by the webhook 03/27/23 15:21:14.342
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/27/23 15:21:14.358
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/27/23 15:21:14.366
    STEP: create a namespace that bypass the webhook 03/27/23 15:21:14.371
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/27/23 15:21:14.377
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 15:21:14.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9630" for this suite. 03/27/23 15:21:14.459
    STEP: Destroying namespace "webhook-9630-markers" for this suite. 03/27/23 15:21:14.464
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:21:14.515
Mar 27 15:21:14.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename daemonsets 03/27/23 15:21:14.516
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:21:14.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:21:14.546
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 03/27/23 15:21:14.584
STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 15:21:14.592
Mar 27 15:21:14.600: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 15:21:14.600: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 15:21:15.610: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 27 15:21:15.610: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 15:21:16.610: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 27 15:21:16.610: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 03/27/23 15:21:16.615
Mar 27 15:21:16.633: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 15:21:16.633: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 15:21:17.643: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 15:21:17.643: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 15:21:18.643: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 15:21:18.644: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 15:21:19.644: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 15:21:19.644: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 15:21:20.641: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 15:21:20.641: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 15:21:21.642: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 27 15:21:21.642: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/27/23 15:21:21.646
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9254, will wait for the garbage collector to delete the pods 03/27/23 15:21:21.646
Mar 27 15:21:21.706: INFO: Deleting DaemonSet.extensions daemon-set took: 7.156274ms
Mar 27 15:21:21.807: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.564317ms
Mar 27 15:21:24.512: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 15:21:24.512: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 27 15:21:24.514: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"44893"},"items":null}

Mar 27 15:21:24.516: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"44893"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 27 15:21:24.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9254" for this suite. 03/27/23 15:21:24.545
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":238,"skipped":4551,"failed":0}
------------------------------
• [SLOW TEST] [10.037 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:21:14.515
    Mar 27 15:21:14.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename daemonsets 03/27/23 15:21:14.516
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:21:14.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:21:14.546
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 03/27/23 15:21:14.584
    STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 15:21:14.592
    Mar 27 15:21:14.600: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 15:21:14.600: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 15:21:15.610: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 27 15:21:15.610: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 15:21:16.610: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 27 15:21:16.610: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 03/27/23 15:21:16.615
    Mar 27 15:21:16.633: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 15:21:16.633: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 15:21:17.643: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 15:21:17.643: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 15:21:18.643: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 15:21:18.644: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 15:21:19.644: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 15:21:19.644: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 15:21:20.641: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 15:21:20.641: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 15:21:21.642: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 27 15:21:21.642: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/27/23 15:21:21.646
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9254, will wait for the garbage collector to delete the pods 03/27/23 15:21:21.646
    Mar 27 15:21:21.706: INFO: Deleting DaemonSet.extensions daemon-set took: 7.156274ms
    Mar 27 15:21:21.807: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.564317ms
    Mar 27 15:21:24.512: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 15:21:24.512: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 27 15:21:24.514: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"44893"},"items":null}

    Mar 27 15:21:24.516: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"44893"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 15:21:24.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9254" for this suite. 03/27/23 15:21:24.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:21:24.554
Mar 27 15:21:24.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename disruption 03/27/23 15:21:24.555
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:21:24.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:21:24.571
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 03/27/23 15:21:24.574
STEP: Waiting for the pdb to be processed 03/27/23 15:21:24.58
STEP: First trying to evict a pod which shouldn't be evictable 03/27/23 15:21:26.595
STEP: Waiting for all pods to be running 03/27/23 15:21:26.595
Mar 27 15:21:26.598: INFO: pods: 0 < 3
Mar 27 15:21:28.608: INFO: running pods: 2 < 3
STEP: locating a running pod 03/27/23 15:21:30.604
STEP: Updating the pdb to allow a pod to be evicted 03/27/23 15:21:30.614
STEP: Waiting for the pdb to be processed 03/27/23 15:21:31.308
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/27/23 15:21:31.311
STEP: Waiting for all pods to be running 03/27/23 15:21:31.311
STEP: Waiting for the pdb to observed all healthy pods 03/27/23 15:21:31.325
STEP: Patching the pdb to disallow a pod to be evicted 03/27/23 15:21:31.408
STEP: Waiting for the pdb to be processed 03/27/23 15:21:31.419
STEP: Waiting for all pods to be running 03/27/23 15:21:31.512
Mar 27 15:21:31.517: INFO: running pods: 2 < 3
Mar 27 15:21:33.522: INFO: running pods: 2 < 3
STEP: locating a running pod 03/27/23 15:21:35.52
STEP: Deleting the pdb to allow a pod to be evicted 03/27/23 15:21:35.53
STEP: Waiting for the pdb to be deleted 03/27/23 15:21:35.587
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/27/23 15:21:35.591
STEP: Waiting for all pods to be running 03/27/23 15:21:35.591
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar 27 15:21:35.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-656" for this suite. 03/27/23 15:21:36.18
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":239,"skipped":4557,"failed":0}
------------------------------
• [SLOW TEST] [11.820 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:21:24.554
    Mar 27 15:21:24.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename disruption 03/27/23 15:21:24.555
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:21:24.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:21:24.571
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 03/27/23 15:21:24.574
    STEP: Waiting for the pdb to be processed 03/27/23 15:21:24.58
    STEP: First trying to evict a pod which shouldn't be evictable 03/27/23 15:21:26.595
    STEP: Waiting for all pods to be running 03/27/23 15:21:26.595
    Mar 27 15:21:26.598: INFO: pods: 0 < 3
    Mar 27 15:21:28.608: INFO: running pods: 2 < 3
    STEP: locating a running pod 03/27/23 15:21:30.604
    STEP: Updating the pdb to allow a pod to be evicted 03/27/23 15:21:30.614
    STEP: Waiting for the pdb to be processed 03/27/23 15:21:31.308
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/27/23 15:21:31.311
    STEP: Waiting for all pods to be running 03/27/23 15:21:31.311
    STEP: Waiting for the pdb to observed all healthy pods 03/27/23 15:21:31.325
    STEP: Patching the pdb to disallow a pod to be evicted 03/27/23 15:21:31.408
    STEP: Waiting for the pdb to be processed 03/27/23 15:21:31.419
    STEP: Waiting for all pods to be running 03/27/23 15:21:31.512
    Mar 27 15:21:31.517: INFO: running pods: 2 < 3
    Mar 27 15:21:33.522: INFO: running pods: 2 < 3
    STEP: locating a running pod 03/27/23 15:21:35.52
    STEP: Deleting the pdb to allow a pod to be evicted 03/27/23 15:21:35.53
    STEP: Waiting for the pdb to be deleted 03/27/23 15:21:35.587
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/27/23 15:21:35.591
    STEP: Waiting for all pods to be running 03/27/23 15:21:35.591
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar 27 15:21:35.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-656" for this suite. 03/27/23 15:21:36.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:21:36.375
Mar 27 15:21:36.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename webhook 03/27/23 15:21:36.376
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:21:37.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:21:37.662
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/27/23 15:21:38.087
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:21:38.89
STEP: Deploying the webhook pod 03/27/23 15:21:38.898
STEP: Wait for the deployment to be ready 03/27/23 15:21:38.913
Mar 27 15:21:38.920: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 15:21:41.005
STEP: Verifying the service has paired with the endpoint 03/27/23 15:21:41.136
Mar 27 15:21:42.136: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/27/23 15:21:42.14
STEP: create a namespace for the webhook 03/27/23 15:21:42.16
STEP: create a configmap should be unconditionally rejected by the webhook 03/27/23 15:21:42.169
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 15:21:42.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8134" for this suite. 03/27/23 15:21:42.208
STEP: Destroying namespace "webhook-8134-markers" for this suite. 03/27/23 15:21:42.217
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":240,"skipped":4574,"failed":0}
------------------------------
• [SLOW TEST] [5.891 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:21:36.375
    Mar 27 15:21:36.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename webhook 03/27/23 15:21:36.376
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:21:37.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:21:37.662
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/27/23 15:21:38.087
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:21:38.89
    STEP: Deploying the webhook pod 03/27/23 15:21:38.898
    STEP: Wait for the deployment to be ready 03/27/23 15:21:38.913
    Mar 27 15:21:38.920: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 15:21:41.005
    STEP: Verifying the service has paired with the endpoint 03/27/23 15:21:41.136
    Mar 27 15:21:42.136: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/27/23 15:21:42.14
    STEP: create a namespace for the webhook 03/27/23 15:21:42.16
    STEP: create a configmap should be unconditionally rejected by the webhook 03/27/23 15:21:42.169
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 15:21:42.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8134" for this suite. 03/27/23 15:21:42.208
    STEP: Destroying namespace "webhook-8134-markers" for this suite. 03/27/23 15:21:42.217
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:21:42.269
Mar 27 15:21:42.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename configmap 03/27/23 15:21:42.27
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:21:42.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:21:42.295
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-61c03ef3-cddb-4a8a-8403-f4d066a1346f 03/27/23 15:21:42.3
STEP: Creating a pod to test consume configMaps 03/27/23 15:21:42.309
Mar 27 15:21:42.317: INFO: Waiting up to 5m0s for pod "pod-configmaps-83c026d9-a4f6-437c-9dcc-6c11f6a585b4" in namespace "configmap-1325" to be "Succeeded or Failed"
Mar 27 15:21:42.320: INFO: Pod "pod-configmaps-83c026d9-a4f6-437c-9dcc-6c11f6a585b4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.064908ms
Mar 27 15:21:44.325: INFO: Pod "pod-configmaps-83c026d9-a4f6-437c-9dcc-6c11f6a585b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008334841s
Mar 27 15:21:46.325: INFO: Pod "pod-configmaps-83c026d9-a4f6-437c-9dcc-6c11f6a585b4": Phase="Running", Reason="", readiness=false. Elapsed: 4.008496997s
Mar 27 15:21:48.325: INFO: Pod "pod-configmaps-83c026d9-a4f6-437c-9dcc-6c11f6a585b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008315475s
STEP: Saw pod success 03/27/23 15:21:48.325
Mar 27 15:21:48.325: INFO: Pod "pod-configmaps-83c026d9-a4f6-437c-9dcc-6c11f6a585b4" satisfied condition "Succeeded or Failed"
Mar 27 15:21:48.328: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-configmaps-83c026d9-a4f6-437c-9dcc-6c11f6a585b4 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 15:21:48.339
Mar 27 15:21:48.353: INFO: Waiting for pod pod-configmaps-83c026d9-a4f6-437c-9dcc-6c11f6a585b4 to disappear
Mar 27 15:21:48.358: INFO: Pod pod-configmaps-83c026d9-a4f6-437c-9dcc-6c11f6a585b4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 27 15:21:48.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1325" for this suite. 03/27/23 15:21:48.362
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":241,"skipped":4613,"failed":0}
------------------------------
• [SLOW TEST] [6.101 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:21:42.269
    Mar 27 15:21:42.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename configmap 03/27/23 15:21:42.27
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:21:42.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:21:42.295
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-61c03ef3-cddb-4a8a-8403-f4d066a1346f 03/27/23 15:21:42.3
    STEP: Creating a pod to test consume configMaps 03/27/23 15:21:42.309
    Mar 27 15:21:42.317: INFO: Waiting up to 5m0s for pod "pod-configmaps-83c026d9-a4f6-437c-9dcc-6c11f6a585b4" in namespace "configmap-1325" to be "Succeeded or Failed"
    Mar 27 15:21:42.320: INFO: Pod "pod-configmaps-83c026d9-a4f6-437c-9dcc-6c11f6a585b4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.064908ms
    Mar 27 15:21:44.325: INFO: Pod "pod-configmaps-83c026d9-a4f6-437c-9dcc-6c11f6a585b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008334841s
    Mar 27 15:21:46.325: INFO: Pod "pod-configmaps-83c026d9-a4f6-437c-9dcc-6c11f6a585b4": Phase="Running", Reason="", readiness=false. Elapsed: 4.008496997s
    Mar 27 15:21:48.325: INFO: Pod "pod-configmaps-83c026d9-a4f6-437c-9dcc-6c11f6a585b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008315475s
    STEP: Saw pod success 03/27/23 15:21:48.325
    Mar 27 15:21:48.325: INFO: Pod "pod-configmaps-83c026d9-a4f6-437c-9dcc-6c11f6a585b4" satisfied condition "Succeeded or Failed"
    Mar 27 15:21:48.328: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-configmaps-83c026d9-a4f6-437c-9dcc-6c11f6a585b4 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 15:21:48.339
    Mar 27 15:21:48.353: INFO: Waiting for pod pod-configmaps-83c026d9-a4f6-437c-9dcc-6c11f6a585b4 to disappear
    Mar 27 15:21:48.358: INFO: Pod pod-configmaps-83c026d9-a4f6-437c-9dcc-6c11f6a585b4 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 27 15:21:48.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1325" for this suite. 03/27/23 15:21:48.362
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:21:48.371
Mar 27 15:21:48.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename ingressclass 03/27/23 15:21:48.372
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:21:48.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:21:48.39
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 03/27/23 15:21:48.393
STEP: getting /apis/networking.k8s.io 03/27/23 15:21:48.395
STEP: getting /apis/networking.k8s.iov1 03/27/23 15:21:48.397
STEP: creating 03/27/23 15:21:48.398
STEP: getting 03/27/23 15:21:48.412
STEP: listing 03/27/23 15:21:48.415
STEP: watching 03/27/23 15:21:48.418
Mar 27 15:21:48.419: INFO: starting watch
STEP: patching 03/27/23 15:21:48.42
STEP: updating 03/27/23 15:21:48.425
Mar 27 15:21:48.431: INFO: waiting for watch events with expected annotations
Mar 27 15:21:48.431: INFO: saw patched and updated annotations
STEP: deleting 03/27/23 15:21:48.431
STEP: deleting a collection 03/27/23 15:21:48.441
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Mar 27 15:21:48.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-2988" for this suite. 03/27/23 15:21:48.466
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":242,"skipped":4619,"failed":0}
------------------------------
• [0.103 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:21:48.371
    Mar 27 15:21:48.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename ingressclass 03/27/23 15:21:48.372
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:21:48.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:21:48.39
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 03/27/23 15:21:48.393
    STEP: getting /apis/networking.k8s.io 03/27/23 15:21:48.395
    STEP: getting /apis/networking.k8s.iov1 03/27/23 15:21:48.397
    STEP: creating 03/27/23 15:21:48.398
    STEP: getting 03/27/23 15:21:48.412
    STEP: listing 03/27/23 15:21:48.415
    STEP: watching 03/27/23 15:21:48.418
    Mar 27 15:21:48.419: INFO: starting watch
    STEP: patching 03/27/23 15:21:48.42
    STEP: updating 03/27/23 15:21:48.425
    Mar 27 15:21:48.431: INFO: waiting for watch events with expected annotations
    Mar 27 15:21:48.431: INFO: saw patched and updated annotations
    STEP: deleting 03/27/23 15:21:48.431
    STEP: deleting a collection 03/27/23 15:21:48.441
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Mar 27 15:21:48.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-2988" for this suite. 03/27/23 15:21:48.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:21:48.475
Mar 27 15:21:48.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename services 03/27/23 15:21:48.476
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:21:48.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:21:48.494
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-3005 03/27/23 15:21:48.496
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3005 to expose endpoints map[] 03/27/23 15:21:48.51
Mar 27 15:21:48.534: INFO: successfully validated that service endpoint-test2 in namespace services-3005 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3005 03/27/23 15:21:48.534
Mar 27 15:21:48.544: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3005" to be "running and ready"
Mar 27 15:21:48.549: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.857514ms
Mar 27 15:21:48.549: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:21:50.739: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.195473252s
Mar 27 15:21:50.740: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 27 15:21:50.740: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3005 to expose endpoints map[pod1:[80]] 03/27/23 15:21:50.749
Mar 27 15:21:50.781: INFO: successfully validated that service endpoint-test2 in namespace services-3005 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 03/27/23 15:21:50.781
Mar 27 15:21:50.782: INFO: Creating new exec pod
Mar 27 15:21:50.793: INFO: Waiting up to 5m0s for pod "execpodgfwc9" in namespace "services-3005" to be "running"
Mar 27 15:21:50.805: INFO: Pod "execpodgfwc9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.551893ms
Mar 27 15:21:52.809: INFO: Pod "execpodgfwc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.016295364s
Mar 27 15:21:52.809: INFO: Pod "execpodgfwc9" satisfied condition "running"
Mar 27 15:21:53.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-3005 exec execpodgfwc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar 27 15:21:53.965: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 27 15:21:53.965: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 15:21:53.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-3005 exec execpodgfwc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.26.99 80'
Mar 27 15:21:54.108: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.26.99 80\nConnection to 10.240.26.99 80 port [tcp/http] succeeded!\n"
Mar 27 15:21:54.108: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-3005 03/27/23 15:21:54.108
Mar 27 15:21:54.115: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3005" to be "running and ready"
Mar 27 15:21:54.118: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.462935ms
Mar 27 15:21:54.118: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:21:56.122: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006872977s
Mar 27 15:21:56.122: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 27 15:21:56.122: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3005 to expose endpoints map[pod1:[80] pod2:[80]] 03/27/23 15:21:56.125
Mar 27 15:21:56.139: INFO: successfully validated that service endpoint-test2 in namespace services-3005 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 03/27/23 15:21:56.139
Mar 27 15:21:57.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-3005 exec execpodgfwc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar 27 15:21:57.283: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 27 15:21:57.283: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 15:21:57.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-3005 exec execpodgfwc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.26.99 80'
Mar 27 15:21:57.433: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.26.99 80\nConnection to 10.240.26.99 80 port [tcp/http] succeeded!\n"
Mar 27 15:21:57.433: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3005 03/27/23 15:21:57.433
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3005 to expose endpoints map[pod2:[80]] 03/27/23 15:21:57.449
Mar 27 15:21:58.475: INFO: successfully validated that service endpoint-test2 in namespace services-3005 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 03/27/23 15:21:58.475
Mar 27 15:21:59.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-3005 exec execpodgfwc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar 27 15:21:59.640: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 27 15:21:59.640: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 15:21:59.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-3005 exec execpodgfwc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.26.99 80'
Mar 27 15:21:59.779: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.26.99 80\nConnection to 10.240.26.99 80 port [tcp/http] succeeded!\n"
Mar 27 15:21:59.779: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-3005 03/27/23 15:21:59.779
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3005 to expose endpoints map[] 03/27/23 15:21:59.799
Mar 27 15:22:00.814: INFO: successfully validated that service endpoint-test2 in namespace services-3005 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 27 15:22:00.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3005" for this suite. 03/27/23 15:22:00.834
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":243,"skipped":4650,"failed":0}
------------------------------
• [SLOW TEST] [12.365 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:21:48.475
    Mar 27 15:21:48.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename services 03/27/23 15:21:48.476
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:21:48.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:21:48.494
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-3005 03/27/23 15:21:48.496
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3005 to expose endpoints map[] 03/27/23 15:21:48.51
    Mar 27 15:21:48.534: INFO: successfully validated that service endpoint-test2 in namespace services-3005 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3005 03/27/23 15:21:48.534
    Mar 27 15:21:48.544: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3005" to be "running and ready"
    Mar 27 15:21:48.549: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.857514ms
    Mar 27 15:21:48.549: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:21:50.739: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.195473252s
    Mar 27 15:21:50.740: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 27 15:21:50.740: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3005 to expose endpoints map[pod1:[80]] 03/27/23 15:21:50.749
    Mar 27 15:21:50.781: INFO: successfully validated that service endpoint-test2 in namespace services-3005 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 03/27/23 15:21:50.781
    Mar 27 15:21:50.782: INFO: Creating new exec pod
    Mar 27 15:21:50.793: INFO: Waiting up to 5m0s for pod "execpodgfwc9" in namespace "services-3005" to be "running"
    Mar 27 15:21:50.805: INFO: Pod "execpodgfwc9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.551893ms
    Mar 27 15:21:52.809: INFO: Pod "execpodgfwc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.016295364s
    Mar 27 15:21:52.809: INFO: Pod "execpodgfwc9" satisfied condition "running"
    Mar 27 15:21:53.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-3005 exec execpodgfwc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar 27 15:21:53.965: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 27 15:21:53.965: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 15:21:53.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-3005 exec execpodgfwc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.26.99 80'
    Mar 27 15:21:54.108: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.26.99 80\nConnection to 10.240.26.99 80 port [tcp/http] succeeded!\n"
    Mar 27 15:21:54.108: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-3005 03/27/23 15:21:54.108
    Mar 27 15:21:54.115: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3005" to be "running and ready"
    Mar 27 15:21:54.118: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.462935ms
    Mar 27 15:21:54.118: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:21:56.122: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006872977s
    Mar 27 15:21:56.122: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 27 15:21:56.122: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3005 to expose endpoints map[pod1:[80] pod2:[80]] 03/27/23 15:21:56.125
    Mar 27 15:21:56.139: INFO: successfully validated that service endpoint-test2 in namespace services-3005 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 03/27/23 15:21:56.139
    Mar 27 15:21:57.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-3005 exec execpodgfwc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar 27 15:21:57.283: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 27 15:21:57.283: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 15:21:57.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-3005 exec execpodgfwc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.26.99 80'
    Mar 27 15:21:57.433: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.26.99 80\nConnection to 10.240.26.99 80 port [tcp/http] succeeded!\n"
    Mar 27 15:21:57.433: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-3005 03/27/23 15:21:57.433
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3005 to expose endpoints map[pod2:[80]] 03/27/23 15:21:57.449
    Mar 27 15:21:58.475: INFO: successfully validated that service endpoint-test2 in namespace services-3005 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 03/27/23 15:21:58.475
    Mar 27 15:21:59.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-3005 exec execpodgfwc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar 27 15:21:59.640: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 27 15:21:59.640: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 15:21:59.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-3005 exec execpodgfwc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.26.99 80'
    Mar 27 15:21:59.779: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.26.99 80\nConnection to 10.240.26.99 80 port [tcp/http] succeeded!\n"
    Mar 27 15:21:59.779: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-3005 03/27/23 15:21:59.779
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3005 to expose endpoints map[] 03/27/23 15:21:59.799
    Mar 27 15:22:00.814: INFO: successfully validated that service endpoint-test2 in namespace services-3005 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 27 15:22:00.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3005" for this suite. 03/27/23 15:22:00.834
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:22:00.84
Mar 27 15:22:00.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubectl 03/27/23 15:22:00.841
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:00.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:00.859
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 03/27/23 15:22:00.862
Mar 27 15:22:00.864: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-7442 proxy --unix-socket=/tmp/kubectl-proxy-unix437975134/test'
STEP: retrieving proxy /api/ output 03/27/23 15:22:00.918
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 27 15:22:00.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7442" for this suite. 03/27/23 15:22:00.925
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":244,"skipped":4656,"failed":0}
------------------------------
• [0.090 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:22:00.84
    Mar 27 15:22:00.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubectl 03/27/23 15:22:00.841
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:00.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:00.859
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 03/27/23 15:22:00.862
    Mar 27 15:22:00.864: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-7442 proxy --unix-socket=/tmp/kubectl-proxy-unix437975134/test'
    STEP: retrieving proxy /api/ output 03/27/23 15:22:00.918
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 27 15:22:00.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7442" for this suite. 03/27/23 15:22:00.925
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:22:00.932
Mar 27 15:22:00.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename security-context 03/27/23 15:22:00.933
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:00.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:00.959
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/27/23 15:22:00.962
Mar 27 15:22:00.969: INFO: Waiting up to 5m0s for pod "security-context-6f5eb1b2-a8c6-4249-b832-ffc39f0d3362" in namespace "security-context-8815" to be "Succeeded or Failed"
Mar 27 15:22:00.975: INFO: Pod "security-context-6f5eb1b2-a8c6-4249-b832-ffc39f0d3362": Phase="Pending", Reason="", readiness=false. Elapsed: 6.206666ms
Mar 27 15:22:03.297: INFO: Pod "security-context-6f5eb1b2-a8c6-4249-b832-ffc39f0d3362": Phase="Pending", Reason="", readiness=false. Elapsed: 2.328222746s
Mar 27 15:22:04.980: INFO: Pod "security-context-6f5eb1b2-a8c6-4249-b832-ffc39f0d3362": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011169985s
Mar 27 15:22:06.979: INFO: Pod "security-context-6f5eb1b2-a8c6-4249-b832-ffc39f0d3362": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010689788s
STEP: Saw pod success 03/27/23 15:22:06.979
Mar 27 15:22:06.980: INFO: Pod "security-context-6f5eb1b2-a8c6-4249-b832-ffc39f0d3362" satisfied condition "Succeeded or Failed"
Mar 27 15:22:06.983: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod security-context-6f5eb1b2-a8c6-4249-b832-ffc39f0d3362 container test-container: <nil>
STEP: delete the pod 03/27/23 15:22:06.994
Mar 27 15:22:07.008: INFO: Waiting for pod security-context-6f5eb1b2-a8c6-4249-b832-ffc39f0d3362 to disappear
Mar 27 15:22:07.010: INFO: Pod security-context-6f5eb1b2-a8c6-4249-b832-ffc39f0d3362 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar 27 15:22:07.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-8815" for this suite. 03/27/23 15:22:07.017
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":245,"skipped":4657,"failed":0}
------------------------------
• [SLOW TEST] [6.091 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:22:00.932
    Mar 27 15:22:00.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename security-context 03/27/23 15:22:00.933
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:00.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:00.959
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/27/23 15:22:00.962
    Mar 27 15:22:00.969: INFO: Waiting up to 5m0s for pod "security-context-6f5eb1b2-a8c6-4249-b832-ffc39f0d3362" in namespace "security-context-8815" to be "Succeeded or Failed"
    Mar 27 15:22:00.975: INFO: Pod "security-context-6f5eb1b2-a8c6-4249-b832-ffc39f0d3362": Phase="Pending", Reason="", readiness=false. Elapsed: 6.206666ms
    Mar 27 15:22:03.297: INFO: Pod "security-context-6f5eb1b2-a8c6-4249-b832-ffc39f0d3362": Phase="Pending", Reason="", readiness=false. Elapsed: 2.328222746s
    Mar 27 15:22:04.980: INFO: Pod "security-context-6f5eb1b2-a8c6-4249-b832-ffc39f0d3362": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011169985s
    Mar 27 15:22:06.979: INFO: Pod "security-context-6f5eb1b2-a8c6-4249-b832-ffc39f0d3362": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010689788s
    STEP: Saw pod success 03/27/23 15:22:06.979
    Mar 27 15:22:06.980: INFO: Pod "security-context-6f5eb1b2-a8c6-4249-b832-ffc39f0d3362" satisfied condition "Succeeded or Failed"
    Mar 27 15:22:06.983: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod security-context-6f5eb1b2-a8c6-4249-b832-ffc39f0d3362 container test-container: <nil>
    STEP: delete the pod 03/27/23 15:22:06.994
    Mar 27 15:22:07.008: INFO: Waiting for pod security-context-6f5eb1b2-a8c6-4249-b832-ffc39f0d3362 to disappear
    Mar 27 15:22:07.010: INFO: Pod security-context-6f5eb1b2-a8c6-4249-b832-ffc39f0d3362 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar 27 15:22:07.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-8815" for this suite. 03/27/23 15:22:07.017
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:22:07.025
Mar 27 15:22:07.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename configmap 03/27/23 15:22:07.026
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:07.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:07.041
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-ac19acf7-d30f-4bb5-8b71-e0c9ce3fb1cc 03/27/23 15:22:07.044
STEP: Creating a pod to test consume configMaps 03/27/23 15:22:07.048
Mar 27 15:22:07.058: INFO: Waiting up to 5m0s for pod "pod-configmaps-ca546e6b-7b2d-4950-9ee4-bd914bf18274" in namespace "configmap-7001" to be "Succeeded or Failed"
Mar 27 15:22:07.062: INFO: Pod "pod-configmaps-ca546e6b-7b2d-4950-9ee4-bd914bf18274": Phase="Pending", Reason="", readiness=false. Elapsed: 4.220344ms
Mar 27 15:22:09.067: INFO: Pod "pod-configmaps-ca546e6b-7b2d-4950-9ee4-bd914bf18274": Phase="Running", Reason="", readiness=true. Elapsed: 2.008878824s
Mar 27 15:22:11.067: INFO: Pod "pod-configmaps-ca546e6b-7b2d-4950-9ee4-bd914bf18274": Phase="Running", Reason="", readiness=false. Elapsed: 4.008910466s
Mar 27 15:22:13.351: INFO: Pod "pod-configmaps-ca546e6b-7b2d-4950-9ee4-bd914bf18274": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.292797562s
STEP: Saw pod success 03/27/23 15:22:13.351
Mar 27 15:22:13.351: INFO: Pod "pod-configmaps-ca546e6b-7b2d-4950-9ee4-bd914bf18274" satisfied condition "Succeeded or Failed"
Mar 27 15:22:13.355: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-configmaps-ca546e6b-7b2d-4950-9ee4-bd914bf18274 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 15:22:13.587
Mar 27 15:22:13.832: INFO: Waiting for pod pod-configmaps-ca546e6b-7b2d-4950-9ee4-bd914bf18274 to disappear
Mar 27 15:22:13.835: INFO: Pod pod-configmaps-ca546e6b-7b2d-4950-9ee4-bd914bf18274 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 27 15:22:13.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7001" for this suite. 03/27/23 15:22:13.897
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":246,"skipped":4669,"failed":0}
------------------------------
• [SLOW TEST] [6.880 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:22:07.025
    Mar 27 15:22:07.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename configmap 03/27/23 15:22:07.026
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:07.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:07.041
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-ac19acf7-d30f-4bb5-8b71-e0c9ce3fb1cc 03/27/23 15:22:07.044
    STEP: Creating a pod to test consume configMaps 03/27/23 15:22:07.048
    Mar 27 15:22:07.058: INFO: Waiting up to 5m0s for pod "pod-configmaps-ca546e6b-7b2d-4950-9ee4-bd914bf18274" in namespace "configmap-7001" to be "Succeeded or Failed"
    Mar 27 15:22:07.062: INFO: Pod "pod-configmaps-ca546e6b-7b2d-4950-9ee4-bd914bf18274": Phase="Pending", Reason="", readiness=false. Elapsed: 4.220344ms
    Mar 27 15:22:09.067: INFO: Pod "pod-configmaps-ca546e6b-7b2d-4950-9ee4-bd914bf18274": Phase="Running", Reason="", readiness=true. Elapsed: 2.008878824s
    Mar 27 15:22:11.067: INFO: Pod "pod-configmaps-ca546e6b-7b2d-4950-9ee4-bd914bf18274": Phase="Running", Reason="", readiness=false. Elapsed: 4.008910466s
    Mar 27 15:22:13.351: INFO: Pod "pod-configmaps-ca546e6b-7b2d-4950-9ee4-bd914bf18274": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.292797562s
    STEP: Saw pod success 03/27/23 15:22:13.351
    Mar 27 15:22:13.351: INFO: Pod "pod-configmaps-ca546e6b-7b2d-4950-9ee4-bd914bf18274" satisfied condition "Succeeded or Failed"
    Mar 27 15:22:13.355: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-configmaps-ca546e6b-7b2d-4950-9ee4-bd914bf18274 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 15:22:13.587
    Mar 27 15:22:13.832: INFO: Waiting for pod pod-configmaps-ca546e6b-7b2d-4950-9ee4-bd914bf18274 to disappear
    Mar 27 15:22:13.835: INFO: Pod pod-configmaps-ca546e6b-7b2d-4950-9ee4-bd914bf18274 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 27 15:22:13.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7001" for this suite. 03/27/23 15:22:13.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:22:13.906
Mar 27 15:22:13.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename emptydir 03/27/23 15:22:13.907
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:13.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:13.93
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 03/27/23 15:22:13.941
Mar 27 15:22:13.971: INFO: Waiting up to 5m0s for pod "pod-64457b17-58ec-4f95-b8d7-c8dfaec5db9a" in namespace "emptydir-4265" to be "Succeeded or Failed"
Mar 27 15:22:13.995: INFO: Pod "pod-64457b17-58ec-4f95-b8d7-c8dfaec5db9a": Phase="Pending", Reason="", readiness=false. Elapsed: 23.533464ms
Mar 27 15:22:15.999: INFO: Pod "pod-64457b17-58ec-4f95-b8d7-c8dfaec5db9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027750602s
Mar 27 15:22:17.998: INFO: Pod "pod-64457b17-58ec-4f95-b8d7-c8dfaec5db9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027408581s
STEP: Saw pod success 03/27/23 15:22:17.999
Mar 27 15:22:17.999: INFO: Pod "pod-64457b17-58ec-4f95-b8d7-c8dfaec5db9a" satisfied condition "Succeeded or Failed"
Mar 27 15:22:18.001: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-64457b17-58ec-4f95-b8d7-c8dfaec5db9a container test-container: <nil>
STEP: delete the pod 03/27/23 15:22:18.009
Mar 27 15:22:18.029: INFO: Waiting for pod pod-64457b17-58ec-4f95-b8d7-c8dfaec5db9a to disappear
Mar 27 15:22:18.032: INFO: Pod pod-64457b17-58ec-4f95-b8d7-c8dfaec5db9a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 27 15:22:18.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4265" for this suite. 03/27/23 15:22:18.037
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":247,"skipped":4707,"failed":0}
------------------------------
• [4.140 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:22:13.906
    Mar 27 15:22:13.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename emptydir 03/27/23 15:22:13.907
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:13.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:13.93
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/27/23 15:22:13.941
    Mar 27 15:22:13.971: INFO: Waiting up to 5m0s for pod "pod-64457b17-58ec-4f95-b8d7-c8dfaec5db9a" in namespace "emptydir-4265" to be "Succeeded or Failed"
    Mar 27 15:22:13.995: INFO: Pod "pod-64457b17-58ec-4f95-b8d7-c8dfaec5db9a": Phase="Pending", Reason="", readiness=false. Elapsed: 23.533464ms
    Mar 27 15:22:15.999: INFO: Pod "pod-64457b17-58ec-4f95-b8d7-c8dfaec5db9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027750602s
    Mar 27 15:22:17.998: INFO: Pod "pod-64457b17-58ec-4f95-b8d7-c8dfaec5db9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027408581s
    STEP: Saw pod success 03/27/23 15:22:17.999
    Mar 27 15:22:17.999: INFO: Pod "pod-64457b17-58ec-4f95-b8d7-c8dfaec5db9a" satisfied condition "Succeeded or Failed"
    Mar 27 15:22:18.001: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-64457b17-58ec-4f95-b8d7-c8dfaec5db9a container test-container: <nil>
    STEP: delete the pod 03/27/23 15:22:18.009
    Mar 27 15:22:18.029: INFO: Waiting for pod pod-64457b17-58ec-4f95-b8d7-c8dfaec5db9a to disappear
    Mar 27 15:22:18.032: INFO: Pod pod-64457b17-58ec-4f95-b8d7-c8dfaec5db9a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 27 15:22:18.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4265" for this suite. 03/27/23 15:22:18.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:22:18.048
Mar 27 15:22:18.048: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename emptydir 03/27/23 15:22:18.049
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:18.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:18.07
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/27/23 15:22:18.08
Mar 27 15:22:18.095: INFO: Waiting up to 5m0s for pod "pod-ef84610a-1a24-48e8-b76f-5c2fc5b1385d" in namespace "emptydir-5006" to be "Succeeded or Failed"
Mar 27 15:22:18.102: INFO: Pod "pod-ef84610a-1a24-48e8-b76f-5c2fc5b1385d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.532522ms
Mar 27 15:22:20.106: INFO: Pod "pod-ef84610a-1a24-48e8-b76f-5c2fc5b1385d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011184722s
Mar 27 15:22:22.113: INFO: Pod "pod-ef84610a-1a24-48e8-b76f-5c2fc5b1385d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018119035s
STEP: Saw pod success 03/27/23 15:22:22.113
Mar 27 15:22:22.113: INFO: Pod "pod-ef84610a-1a24-48e8-b76f-5c2fc5b1385d" satisfied condition "Succeeded or Failed"
Mar 27 15:22:22.123: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-ef84610a-1a24-48e8-b76f-5c2fc5b1385d container test-container: <nil>
STEP: delete the pod 03/27/23 15:22:22.137
Mar 27 15:22:22.150: INFO: Waiting for pod pod-ef84610a-1a24-48e8-b76f-5c2fc5b1385d to disappear
Mar 27 15:22:22.153: INFO: Pod pod-ef84610a-1a24-48e8-b76f-5c2fc5b1385d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 27 15:22:22.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5006" for this suite. 03/27/23 15:22:22.158
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":248,"skipped":4742,"failed":0}
------------------------------
• [4.117 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:22:18.048
    Mar 27 15:22:18.048: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename emptydir 03/27/23 15:22:18.049
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:18.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:18.07
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/27/23 15:22:18.08
    Mar 27 15:22:18.095: INFO: Waiting up to 5m0s for pod "pod-ef84610a-1a24-48e8-b76f-5c2fc5b1385d" in namespace "emptydir-5006" to be "Succeeded or Failed"
    Mar 27 15:22:18.102: INFO: Pod "pod-ef84610a-1a24-48e8-b76f-5c2fc5b1385d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.532522ms
    Mar 27 15:22:20.106: INFO: Pod "pod-ef84610a-1a24-48e8-b76f-5c2fc5b1385d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011184722s
    Mar 27 15:22:22.113: INFO: Pod "pod-ef84610a-1a24-48e8-b76f-5c2fc5b1385d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018119035s
    STEP: Saw pod success 03/27/23 15:22:22.113
    Mar 27 15:22:22.113: INFO: Pod "pod-ef84610a-1a24-48e8-b76f-5c2fc5b1385d" satisfied condition "Succeeded or Failed"
    Mar 27 15:22:22.123: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-ef84610a-1a24-48e8-b76f-5c2fc5b1385d container test-container: <nil>
    STEP: delete the pod 03/27/23 15:22:22.137
    Mar 27 15:22:22.150: INFO: Waiting for pod pod-ef84610a-1a24-48e8-b76f-5c2fc5b1385d to disappear
    Mar 27 15:22:22.153: INFO: Pod pod-ef84610a-1a24-48e8-b76f-5c2fc5b1385d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 27 15:22:22.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5006" for this suite. 03/27/23 15:22:22.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:22:22.166
Mar 27 15:22:22.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename lease-test 03/27/23 15:22:22.167
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:22.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:22.188
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Mar 27 15:22:22.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-6540" for this suite. 03/27/23 15:22:22.261
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":249,"skipped":4788,"failed":0}
------------------------------
• [0.113 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:22:22.166
    Mar 27 15:22:22.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename lease-test 03/27/23 15:22:22.167
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:22.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:22.188
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Mar 27 15:22:22.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-6540" for this suite. 03/27/23 15:22:22.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:22:22.283
Mar 27 15:22:22.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 15:22:22.284
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:22.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:22.301
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-d2b71547-c5af-4be9-9e2b-4b0e05566840 03/27/23 15:22:22.304
STEP: Creating secret with name secret-projected-all-test-volume-aa9d4912-89a7-4512-b2bc-24f4d7c578b4 03/27/23 15:22:22.312
STEP: Creating a pod to test Check all projections for projected volume plugin 03/27/23 15:22:22.32
Mar 27 15:22:22.328: INFO: Waiting up to 5m0s for pod "projected-volume-c6980f23-9efe-4009-95be-f78e3f0f9a31" in namespace "projected-5224" to be "Succeeded or Failed"
Mar 27 15:22:22.332: INFO: Pod "projected-volume-c6980f23-9efe-4009-95be-f78e3f0f9a31": Phase="Pending", Reason="", readiness=false. Elapsed: 3.14162ms
Mar 27 15:22:24.337: INFO: Pod "projected-volume-c6980f23-9efe-4009-95be-f78e3f0f9a31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008190058s
Mar 27 15:22:26.336: INFO: Pod "projected-volume-c6980f23-9efe-4009-95be-f78e3f0f9a31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007865818s
STEP: Saw pod success 03/27/23 15:22:26.336
Mar 27 15:22:26.337: INFO: Pod "projected-volume-c6980f23-9efe-4009-95be-f78e3f0f9a31" satisfied condition "Succeeded or Failed"
Mar 27 15:22:26.339: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod projected-volume-c6980f23-9efe-4009-95be-f78e3f0f9a31 container projected-all-volume-test: <nil>
STEP: delete the pod 03/27/23 15:22:26.347
Mar 27 15:22:26.360: INFO: Waiting for pod projected-volume-c6980f23-9efe-4009-95be-f78e3f0f9a31 to disappear
Mar 27 15:22:26.362: INFO: Pod projected-volume-c6980f23-9efe-4009-95be-f78e3f0f9a31 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Mar 27 15:22:26.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5224" for this suite. 03/27/23 15:22:26.367
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":250,"skipped":4796,"failed":0}
------------------------------
• [4.093 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:22:22.283
    Mar 27 15:22:22.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 15:22:22.284
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:22.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:22.301
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-d2b71547-c5af-4be9-9e2b-4b0e05566840 03/27/23 15:22:22.304
    STEP: Creating secret with name secret-projected-all-test-volume-aa9d4912-89a7-4512-b2bc-24f4d7c578b4 03/27/23 15:22:22.312
    STEP: Creating a pod to test Check all projections for projected volume plugin 03/27/23 15:22:22.32
    Mar 27 15:22:22.328: INFO: Waiting up to 5m0s for pod "projected-volume-c6980f23-9efe-4009-95be-f78e3f0f9a31" in namespace "projected-5224" to be "Succeeded or Failed"
    Mar 27 15:22:22.332: INFO: Pod "projected-volume-c6980f23-9efe-4009-95be-f78e3f0f9a31": Phase="Pending", Reason="", readiness=false. Elapsed: 3.14162ms
    Mar 27 15:22:24.337: INFO: Pod "projected-volume-c6980f23-9efe-4009-95be-f78e3f0f9a31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008190058s
    Mar 27 15:22:26.336: INFO: Pod "projected-volume-c6980f23-9efe-4009-95be-f78e3f0f9a31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007865818s
    STEP: Saw pod success 03/27/23 15:22:26.336
    Mar 27 15:22:26.337: INFO: Pod "projected-volume-c6980f23-9efe-4009-95be-f78e3f0f9a31" satisfied condition "Succeeded or Failed"
    Mar 27 15:22:26.339: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod projected-volume-c6980f23-9efe-4009-95be-f78e3f0f9a31 container projected-all-volume-test: <nil>
    STEP: delete the pod 03/27/23 15:22:26.347
    Mar 27 15:22:26.360: INFO: Waiting for pod projected-volume-c6980f23-9efe-4009-95be-f78e3f0f9a31 to disappear
    Mar 27 15:22:26.362: INFO: Pod projected-volume-c6980f23-9efe-4009-95be-f78e3f0f9a31 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Mar 27 15:22:26.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5224" for this suite. 03/27/23 15:22:26.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:22:26.378
Mar 27 15:22:26.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename proxy 03/27/23 15:22:26.379
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:26.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:26.394
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 03/27/23 15:22:26.408
STEP: creating replication controller proxy-service-7hf67 in namespace proxy-4935 03/27/23 15:22:26.408
I0327 15:22:26.416181      24 runners.go:193] Created replication controller with name: proxy-service-7hf67, namespace: proxy-4935, replica count: 1
I0327 15:22:27.467797      24 runners.go:193] proxy-service-7hf67 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0327 15:22:28.468641      24 runners.go:193] proxy-service-7hf67 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0327 15:22:29.469350      24 runners.go:193] proxy-service-7hf67 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 15:22:29.473: INFO: setup took 3.075927079s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/27/23 15:22:29.473
Mar 27 15:22:29.484: INFO: (0) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 10.386544ms)
Mar 27 15:22:29.484: INFO: (0) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 10.607594ms)
Mar 27 15:22:29.484: INFO: (0) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 10.717286ms)
Mar 27 15:22:29.484: INFO: (0) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 11.044306ms)
Mar 27 15:22:29.484: INFO: (0) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 10.865199ms)
Mar 27 15:22:29.484: INFO: (0) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 11.238535ms)
Mar 27 15:22:29.484: INFO: (0) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 10.957836ms)
Mar 27 15:22:29.485: INFO: (0) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 11.35225ms)
Mar 27 15:22:29.486: INFO: (0) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 12.316099ms)
Mar 27 15:22:29.486: INFO: (0) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 12.842619ms)
Mar 27 15:22:29.488: INFO: (0) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 14.614217ms)
Mar 27 15:22:29.489: INFO: (0) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 15.855461ms)
Mar 27 15:22:29.489: INFO: (0) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 15.79828ms)
Mar 27 15:22:29.491: INFO: (0) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 17.447999ms)
Mar 27 15:22:29.491: INFO: (0) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 17.341666ms)
Mar 27 15:22:29.494: INFO: (0) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 20.013497ms)
Mar 27 15:22:29.498: INFO: (1) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 4.267371ms)
Mar 27 15:22:29.498: INFO: (1) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 4.561344ms)
Mar 27 15:22:29.498: INFO: (1) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 4.40439ms)
Mar 27 15:22:29.499: INFO: (1) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.115963ms)
Mar 27 15:22:29.499: INFO: (1) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 5.028901ms)
Mar 27 15:22:29.499: INFO: (1) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 5.341754ms)
Mar 27 15:22:29.499: INFO: (1) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 5.536372ms)
Mar 27 15:22:29.499: INFO: (1) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 5.383354ms)
Mar 27 15:22:29.499: INFO: (1) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 5.48041ms)
Mar 27 15:22:29.500: INFO: (1) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 5.835739ms)
Mar 27 15:22:29.500: INFO: (1) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 6.221668ms)
Mar 27 15:22:29.500: INFO: (1) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 6.058005ms)
Mar 27 15:22:29.500: INFO: (1) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 6.142042ms)
Mar 27 15:22:29.500: INFO: (1) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 5.991474ms)
Mar 27 15:22:29.500: INFO: (1) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 6.242175ms)
Mar 27 15:22:29.501: INFO: (1) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 6.974985ms)
Mar 27 15:22:29.505: INFO: (2) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 3.827378ms)
Mar 27 15:22:29.505: INFO: (2) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 4.021623ms)
Mar 27 15:22:29.505: INFO: (2) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 3.935882ms)
Mar 27 15:22:29.506: INFO: (2) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 4.507243ms)
Mar 27 15:22:29.506: INFO: (2) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 4.551906ms)
Mar 27 15:22:29.506: INFO: (2) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 4.750558ms)
Mar 27 15:22:29.506: INFO: (2) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 4.934377ms)
Mar 27 15:22:29.506: INFO: (2) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 4.886417ms)
Mar 27 15:22:29.506: INFO: (2) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 4.845564ms)
Mar 27 15:22:29.506: INFO: (2) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 5.033423ms)
Mar 27 15:22:29.506: INFO: (2) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 5.206341ms)
Mar 27 15:22:29.506: INFO: (2) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 5.046123ms)
Mar 27 15:22:29.507: INFO: (2) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 5.616576ms)
Mar 27 15:22:29.507: INFO: (2) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 5.715201ms)
Mar 27 15:22:29.507: INFO: (2) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 5.753572ms)
Mar 27 15:22:29.507: INFO: (2) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 6.399101ms)
Mar 27 15:22:29.513: INFO: (3) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.775145ms)
Mar 27 15:22:29.513: INFO: (3) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 5.748787ms)
Mar 27 15:22:29.513: INFO: (3) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.744527ms)
Mar 27 15:22:29.513: INFO: (3) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 5.626547ms)
Mar 27 15:22:29.513: INFO: (3) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 5.703802ms)
Mar 27 15:22:29.514: INFO: (3) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 5.839387ms)
Mar 27 15:22:29.514: INFO: (3) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 6.560369ms)
Mar 27 15:22:29.514: INFO: (3) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 6.506616ms)
Mar 27 15:22:29.514: INFO: (3) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 6.60646ms)
Mar 27 15:22:29.514: INFO: (3) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 6.620767ms)
Mar 27 15:22:29.515: INFO: (3) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 6.810717ms)
Mar 27 15:22:29.515: INFO: (3) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 6.84944ms)
Mar 27 15:22:29.516: INFO: (3) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 7.828022ms)
Mar 27 15:22:29.516: INFO: (3) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 8.007908ms)
Mar 27 15:22:29.516: INFO: (3) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 8.365065ms)
Mar 27 15:22:29.516: INFO: (3) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 8.322196ms)
Mar 27 15:22:29.520: INFO: (4) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 4.128233ms)
Mar 27 15:22:29.520: INFO: (4) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 4.110933ms)
Mar 27 15:22:29.521: INFO: (4) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 4.200289ms)
Mar 27 15:22:29.521: INFO: (4) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 4.321527ms)
Mar 27 15:22:29.521: INFO: (4) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 4.573098ms)
Mar 27 15:22:29.521: INFO: (4) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 5.066743ms)
Mar 27 15:22:29.521: INFO: (4) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 5.183051ms)
Mar 27 15:22:29.521: INFO: (4) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 5.117635ms)
Mar 27 15:22:29.522: INFO: (4) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 5.273431ms)
Mar 27 15:22:29.522: INFO: (4) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.62398ms)
Mar 27 15:22:29.523: INFO: (4) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 6.366619ms)
Mar 27 15:22:29.523: INFO: (4) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 6.769527ms)
Mar 27 15:22:29.523: INFO: (4) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 6.814457ms)
Mar 27 15:22:29.523: INFO: (4) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 6.881655ms)
Mar 27 15:22:29.523: INFO: (4) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 6.887564ms)
Mar 27 15:22:29.524: INFO: (4) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 7.868176ms)
Mar 27 15:22:29.527: INFO: (5) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 3.111205ms)
Mar 27 15:22:29.528: INFO: (5) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 3.123461ms)
Mar 27 15:22:29.528: INFO: (5) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 3.262453ms)
Mar 27 15:22:29.528: INFO: (5) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 3.084966ms)
Mar 27 15:22:29.529: INFO: (5) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 4.010356ms)
Mar 27 15:22:29.530: INFO: (5) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 5.24691ms)
Mar 27 15:22:29.530: INFO: (5) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 5.635649ms)
Mar 27 15:22:29.530: INFO: (5) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 5.855688ms)
Mar 27 15:22:29.531: INFO: (5) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 5.972038ms)
Mar 27 15:22:29.531: INFO: (5) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 6.133577ms)
Mar 27 15:22:29.531: INFO: (5) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 6.537437ms)
Mar 27 15:22:29.531: INFO: (5) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 6.673259ms)
Mar 27 15:22:29.531: INFO: (5) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 6.377473ms)
Mar 27 15:22:29.532: INFO: (5) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 7.434388ms)
Mar 27 15:22:29.532: INFO: (5) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 7.579318ms)
Mar 27 15:22:29.532: INFO: (5) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 7.892572ms)
Mar 27 15:22:29.538: INFO: (6) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.302039ms)
Mar 27 15:22:29.538: INFO: (6) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.585901ms)
Mar 27 15:22:29.538: INFO: (6) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 5.153779ms)
Mar 27 15:22:29.539: INFO: (6) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 5.892505ms)
Mar 27 15:22:29.539: INFO: (6) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 6.218597ms)
Mar 27 15:22:29.539: INFO: (6) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 6.144866ms)
Mar 27 15:22:29.539: INFO: (6) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 6.197423ms)
Mar 27 15:22:29.539: INFO: (6) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 6.044285ms)
Mar 27 15:22:29.539: INFO: (6) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 6.526644ms)
Mar 27 15:22:29.539: INFO: (6) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 7.2196ms)
Mar 27 15:22:29.539: INFO: (6) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 6.656102ms)
Mar 27 15:22:29.539: INFO: (6) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 6.961418ms)
Mar 27 15:22:29.540: INFO: (6) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 6.835258ms)
Mar 27 15:22:29.540: INFO: (6) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 7.038186ms)
Mar 27 15:22:29.540: INFO: (6) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 7.001661ms)
Mar 27 15:22:29.540: INFO: (6) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 7.675172ms)
Mar 27 15:22:29.550: INFO: (7) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 9.404647ms)
Mar 27 15:22:29.550: INFO: (7) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 9.638171ms)
Mar 27 15:22:29.550: INFO: (7) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 9.748174ms)
Mar 27 15:22:29.550: INFO: (7) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 9.199661ms)
Mar 27 15:22:29.550: INFO: (7) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 9.99632ms)
Mar 27 15:22:29.550: INFO: (7) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 9.630635ms)
Mar 27 15:22:29.551: INFO: (7) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 10.022439ms)
Mar 27 15:22:29.552: INFO: (7) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 10.806548ms)
Mar 27 15:22:29.552: INFO: (7) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 10.560697ms)
Mar 27 15:22:29.552: INFO: (7) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 10.702801ms)
Mar 27 15:22:29.552: INFO: (7) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 10.938927ms)
Mar 27 15:22:29.552: INFO: (7) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 10.504024ms)
Mar 27 15:22:29.553: INFO: (7) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 12.428775ms)
Mar 27 15:22:29.553: INFO: (7) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 12.333164ms)
Mar 27 15:22:29.554: INFO: (7) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 13.228166ms)
Mar 27 15:22:29.554: INFO: (7) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 13.336019ms)
Mar 27 15:22:29.562: INFO: (8) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 7.299533ms)
Mar 27 15:22:29.562: INFO: (8) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 7.821367ms)
Mar 27 15:22:29.562: INFO: (8) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 7.746265ms)
Mar 27 15:22:29.563: INFO: (8) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 7.942827ms)
Mar 27 15:22:29.563: INFO: (8) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 8.117048ms)
Mar 27 15:22:29.563: INFO: (8) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 8.835632ms)
Mar 27 15:22:29.563: INFO: (8) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 8.685053ms)
Mar 27 15:22:29.563: INFO: (8) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 8.69828ms)
Mar 27 15:22:29.563: INFO: (8) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 8.740322ms)
Mar 27 15:22:29.563: INFO: (8) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 8.696849ms)
Mar 27 15:22:29.564: INFO: (8) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 9.830675ms)
Mar 27 15:22:29.564: INFO: (8) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 9.721025ms)
Mar 27 15:22:29.564: INFO: (8) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 9.771733ms)
Mar 27 15:22:29.564: INFO: (8) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 9.769513ms)
Mar 27 15:22:29.564: INFO: (8) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 9.679701ms)
Mar 27 15:22:29.565: INFO: (8) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 9.836276ms)
Mar 27 15:22:29.571: INFO: (9) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.411565ms)
Mar 27 15:22:29.572: INFO: (9) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 6.720937ms)
Mar 27 15:22:29.572: INFO: (9) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 6.971478ms)
Mar 27 15:22:29.572: INFO: (9) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 6.937088ms)
Mar 27 15:22:29.573: INFO: (9) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 7.985068ms)
Mar 27 15:22:29.573: INFO: (9) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 7.876566ms)
Mar 27 15:22:29.573: INFO: (9) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 7.93852ms)
Mar 27 15:22:29.573: INFO: (9) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 7.924573ms)
Mar 27 15:22:29.574: INFO: (9) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 8.659767ms)
Mar 27 15:22:29.574: INFO: (9) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 8.620011ms)
Mar 27 15:22:29.574: INFO: (9) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 8.728768ms)
Mar 27 15:22:29.574: INFO: (9) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 8.597698ms)
Mar 27 15:22:29.577: INFO: (9) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 11.579729ms)
Mar 27 15:22:29.577: INFO: (9) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 11.735215ms)
Mar 27 15:22:29.582: INFO: (9) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 16.872277ms)
Mar 27 15:22:29.582: INFO: (9) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 16.887152ms)
Mar 27 15:22:29.587: INFO: (10) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 4.534872ms)
Mar 27 15:22:29.587: INFO: (10) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 4.62675ms)
Mar 27 15:22:29.587: INFO: (10) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 4.572374ms)
Mar 27 15:22:29.587: INFO: (10) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 4.657852ms)
Mar 27 15:22:29.587: INFO: (10) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 4.752897ms)
Mar 27 15:22:29.587: INFO: (10) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 4.850228ms)
Mar 27 15:22:29.587: INFO: (10) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 4.576397ms)
Mar 27 15:22:29.587: INFO: (10) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 5.054416ms)
Mar 27 15:22:29.587: INFO: (10) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 5.236909ms)
Mar 27 15:22:29.588: INFO: (10) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 5.241883ms)
Mar 27 15:22:29.588: INFO: (10) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.872104ms)
Mar 27 15:22:29.588: INFO: (10) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 5.802584ms)
Mar 27 15:22:29.589: INFO: (10) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 6.564591ms)
Mar 27 15:22:29.589: INFO: (10) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 6.721448ms)
Mar 27 15:22:29.590: INFO: (10) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 7.742521ms)
Mar 27 15:22:29.590: INFO: (10) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 7.882109ms)
Mar 27 15:22:29.596: INFO: (11) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 5.573707ms)
Mar 27 15:22:29.596: INFO: (11) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 5.67931ms)
Mar 27 15:22:29.596: INFO: (11) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 5.762866ms)
Mar 27 15:22:29.596: INFO: (11) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 5.943277ms)
Mar 27 15:22:29.597: INFO: (11) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 6.744335ms)
Mar 27 15:22:29.597: INFO: (11) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 6.831085ms)
Mar 27 15:22:29.597: INFO: (11) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 6.646377ms)
Mar 27 15:22:29.597: INFO: (11) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 6.839773ms)
Mar 27 15:22:29.597: INFO: (11) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 6.726017ms)
Mar 27 15:22:29.597: INFO: (11) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 6.974149ms)
Mar 27 15:22:29.601: INFO: (11) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 10.449287ms)
Mar 27 15:22:29.601: INFO: (11) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 10.452052ms)
Mar 27 15:22:29.601: INFO: (11) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 10.682188ms)
Mar 27 15:22:29.601: INFO: (11) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 10.575436ms)
Mar 27 15:22:29.601: INFO: (11) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 10.645847ms)
Mar 27 15:22:29.602: INFO: (11) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 11.185182ms)
Mar 27 15:22:29.606: INFO: (12) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 4.05152ms)
Mar 27 15:22:29.606: INFO: (12) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 3.947876ms)
Mar 27 15:22:29.606: INFO: (12) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 4.430372ms)
Mar 27 15:22:29.607: INFO: (12) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 4.829501ms)
Mar 27 15:22:29.607: INFO: (12) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 4.918429ms)
Mar 27 15:22:29.607: INFO: (12) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 4.888396ms)
Mar 27 15:22:29.607: INFO: (12) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 4.966405ms)
Mar 27 15:22:29.607: INFO: (12) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 5.066806ms)
Mar 27 15:22:29.607: INFO: (12) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 5.498665ms)
Mar 27 15:22:29.607: INFO: (12) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 5.686181ms)
Mar 27 15:22:29.608: INFO: (12) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 5.923083ms)
Mar 27 15:22:29.608: INFO: (12) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 6.097289ms)
Mar 27 15:22:29.608: INFO: (12) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 5.944478ms)
Mar 27 15:22:29.608: INFO: (12) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 6.047578ms)
Mar 27 15:22:29.609: INFO: (12) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 6.675191ms)
Mar 27 15:22:29.609: INFO: (12) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 6.784112ms)
Mar 27 15:22:29.614: INFO: (13) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 5.479926ms)
Mar 27 15:22:29.614: INFO: (13) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 5.461462ms)
Mar 27 15:22:29.614: INFO: (13) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.413225ms)
Mar 27 15:22:29.614: INFO: (13) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 5.73899ms)
Mar 27 15:22:29.614: INFO: (13) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.78744ms)
Mar 27 15:22:29.615: INFO: (13) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 5.846192ms)
Mar 27 15:22:29.615: INFO: (13) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 5.858835ms)
Mar 27 15:22:29.616: INFO: (13) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 6.777772ms)
Mar 27 15:22:29.616: INFO: (13) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 6.887045ms)
Mar 27 15:22:29.616: INFO: (13) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 7.080698ms)
Mar 27 15:22:29.616: INFO: (13) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 6.883441ms)
Mar 27 15:22:29.616: INFO: (13) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 7.188128ms)
Mar 27 15:22:29.616: INFO: (13) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 7.282149ms)
Mar 27 15:22:29.616: INFO: (13) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 7.325076ms)
Mar 27 15:22:29.616: INFO: (13) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 7.498595ms)
Mar 27 15:22:29.616: INFO: (13) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 7.545543ms)
Mar 27 15:22:29.620: INFO: (14) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 3.493733ms)
Mar 27 15:22:29.620: INFO: (14) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 3.60462ms)
Mar 27 15:22:29.620: INFO: (14) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 3.57072ms)
Mar 27 15:22:29.621: INFO: (14) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 4.866272ms)
Mar 27 15:22:29.621: INFO: (14) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 4.764845ms)
Mar 27 15:22:29.622: INFO: (14) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 5.047368ms)
Mar 27 15:22:29.623: INFO: (14) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 5.681373ms)
Mar 27 15:22:29.623: INFO: (14) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 5.642078ms)
Mar 27 15:22:29.623: INFO: (14) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 6.17749ms)
Mar 27 15:22:29.623: INFO: (14) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 6.130621ms)
Mar 27 15:22:29.623: INFO: (14) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 6.050235ms)
Mar 27 15:22:29.623: INFO: (14) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 6.145129ms)
Mar 27 15:22:29.623: INFO: (14) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.865621ms)
Mar 27 15:22:29.623: INFO: (14) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 6.245415ms)
Mar 27 15:22:29.624: INFO: (14) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 6.993847ms)
Mar 27 15:22:29.624: INFO: (14) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 7.174175ms)
Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.967616ms)
Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 5.997949ms)
Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 6.052954ms)
Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 6.203161ms)
Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 6.108473ms)
Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 6.198302ms)
Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 6.183572ms)
Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 6.34546ms)
Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 5.93442ms)
Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 6.25701ms)
Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 6.409519ms)
Mar 27 15:22:29.631: INFO: (15) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 7.231209ms)
Mar 27 15:22:29.631: INFO: (15) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 7.44146ms)
Mar 27 15:22:29.632: INFO: (15) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 7.623782ms)
Mar 27 15:22:29.632: INFO: (15) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 7.525762ms)
Mar 27 15:22:29.632: INFO: (15) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 7.687898ms)
Mar 27 15:22:29.637: INFO: (16) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 4.957078ms)
Mar 27 15:22:29.637: INFO: (16) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 5.219367ms)
Mar 27 15:22:29.637: INFO: (16) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 5.380222ms)
Mar 27 15:22:29.637: INFO: (16) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.429044ms)
Mar 27 15:22:29.639: INFO: (16) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 7.023654ms)
Mar 27 15:22:29.639: INFO: (16) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 6.969651ms)
Mar 27 15:22:29.639: INFO: (16) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 7.176055ms)
Mar 27 15:22:29.639: INFO: (16) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 7.138059ms)
Mar 27 15:22:29.639: INFO: (16) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 7.385794ms)
Mar 27 15:22:29.640: INFO: (16) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 7.715535ms)
Mar 27 15:22:29.640: INFO: (16) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 7.736279ms)
Mar 27 15:22:29.640: INFO: (16) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 7.773698ms)
Mar 27 15:22:29.640: INFO: (16) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 7.829606ms)
Mar 27 15:22:29.640: INFO: (16) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 7.935755ms)
Mar 27 15:22:29.640: INFO: (16) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 7.895655ms)
Mar 27 15:22:29.640: INFO: (16) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 8.443551ms)
Mar 27 15:22:29.644: INFO: (17) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 3.769145ms)
Mar 27 15:22:29.644: INFO: (17) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 3.839812ms)
Mar 27 15:22:29.645: INFO: (17) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 4.413386ms)
Mar 27 15:22:29.645: INFO: (17) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 4.408121ms)
Mar 27 15:22:29.645: INFO: (17) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 4.56851ms)
Mar 27 15:22:29.645: INFO: (17) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 4.73124ms)
Mar 27 15:22:29.646: INFO: (17) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 4.914873ms)
Mar 27 15:22:29.646: INFO: (17) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 5.178699ms)
Mar 27 15:22:29.649: INFO: (17) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 8.092224ms)
Mar 27 15:22:29.649: INFO: (17) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 8.084131ms)
Mar 27 15:22:29.649: INFO: (17) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 8.154397ms)
Mar 27 15:22:29.649: INFO: (17) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 8.27434ms)
Mar 27 15:22:29.649: INFO: (17) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 8.922758ms)
Mar 27 15:22:29.649: INFO: (17) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 8.860378ms)
Mar 27 15:22:29.650: INFO: (17) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 8.91691ms)
Mar 27 15:22:29.650: INFO: (17) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 8.985847ms)
Mar 27 15:22:29.655: INFO: (18) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 5.747941ms)
Mar 27 15:22:29.656: INFO: (18) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 6.163147ms)
Mar 27 15:22:29.656: INFO: (18) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 6.079273ms)
Mar 27 15:22:29.656: INFO: (18) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 6.36329ms)
Mar 27 15:22:29.656: INFO: (18) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 6.299784ms)
Mar 27 15:22:29.656: INFO: (18) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 6.51025ms)
Mar 27 15:22:29.657: INFO: (18) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 7.331612ms)
Mar 27 15:22:29.657: INFO: (18) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 7.311335ms)
Mar 27 15:22:29.657: INFO: (18) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 7.383511ms)
Mar 27 15:22:29.657: INFO: (18) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 7.32616ms)
Mar 27 15:22:29.658: INFO: (18) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 8.050818ms)
Mar 27 15:22:29.658: INFO: (18) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 8.517234ms)
Mar 27 15:22:29.658: INFO: (18) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 8.596396ms)
Mar 27 15:22:29.658: INFO: (18) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 8.714751ms)
Mar 27 15:22:29.659: INFO: (18) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 8.897162ms)
Mar 27 15:22:29.659: INFO: (18) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 9.083721ms)
Mar 27 15:22:29.663: INFO: (19) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 3.504451ms)
Mar 27 15:22:29.663: INFO: (19) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 3.62249ms)
Mar 27 15:22:29.663: INFO: (19) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 3.61283ms)
Mar 27 15:22:29.664: INFO: (19) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 4.276489ms)
Mar 27 15:22:29.664: INFO: (19) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 4.353868ms)
Mar 27 15:22:29.664: INFO: (19) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 4.683453ms)
Mar 27 15:22:29.664: INFO: (19) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 4.999838ms)
Mar 27 15:22:29.664: INFO: (19) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 4.873924ms)
Mar 27 15:22:29.664: INFO: (19) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 4.950791ms)
Mar 27 15:22:29.669: INFO: (19) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 10.437239ms)
Mar 27 15:22:29.669: INFO: (19) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 9.839064ms)
Mar 27 15:22:29.670: INFO: (19) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 9.994447ms)
Mar 27 15:22:29.670: INFO: (19) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 10.004297ms)
Mar 27 15:22:29.670: INFO: (19) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 9.998647ms)
Mar 27 15:22:29.670: INFO: (19) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 10.527183ms)
Mar 27 15:22:29.670: INFO: (19) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 10.73685ms)
STEP: deleting ReplicationController proxy-service-7hf67 in namespace proxy-4935, will wait for the garbage collector to delete the pods 03/27/23 15:22:29.67
Mar 27 15:22:29.736: INFO: Deleting ReplicationController proxy-service-7hf67 took: 11.533675ms
Mar 27 15:22:29.836: INFO: Terminating ReplicationController proxy-service-7hf67 pods took: 100.216267ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar 27 15:22:32.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4935" for this suite. 03/27/23 15:22:32.048
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":251,"skipped":4810,"failed":0}
------------------------------
• [SLOW TEST] [5.679 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:22:26.378
    Mar 27 15:22:26.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename proxy 03/27/23 15:22:26.379
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:26.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:26.394
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 03/27/23 15:22:26.408
    STEP: creating replication controller proxy-service-7hf67 in namespace proxy-4935 03/27/23 15:22:26.408
    I0327 15:22:26.416181      24 runners.go:193] Created replication controller with name: proxy-service-7hf67, namespace: proxy-4935, replica count: 1
    I0327 15:22:27.467797      24 runners.go:193] proxy-service-7hf67 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0327 15:22:28.468641      24 runners.go:193] proxy-service-7hf67 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0327 15:22:29.469350      24 runners.go:193] proxy-service-7hf67 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 15:22:29.473: INFO: setup took 3.075927079s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/27/23 15:22:29.473
    Mar 27 15:22:29.484: INFO: (0) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 10.386544ms)
    Mar 27 15:22:29.484: INFO: (0) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 10.607594ms)
    Mar 27 15:22:29.484: INFO: (0) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 10.717286ms)
    Mar 27 15:22:29.484: INFO: (0) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 11.044306ms)
    Mar 27 15:22:29.484: INFO: (0) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 10.865199ms)
    Mar 27 15:22:29.484: INFO: (0) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 11.238535ms)
    Mar 27 15:22:29.484: INFO: (0) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 10.957836ms)
    Mar 27 15:22:29.485: INFO: (0) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 11.35225ms)
    Mar 27 15:22:29.486: INFO: (0) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 12.316099ms)
    Mar 27 15:22:29.486: INFO: (0) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 12.842619ms)
    Mar 27 15:22:29.488: INFO: (0) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 14.614217ms)
    Mar 27 15:22:29.489: INFO: (0) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 15.855461ms)
    Mar 27 15:22:29.489: INFO: (0) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 15.79828ms)
    Mar 27 15:22:29.491: INFO: (0) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 17.447999ms)
    Mar 27 15:22:29.491: INFO: (0) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 17.341666ms)
    Mar 27 15:22:29.494: INFO: (0) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 20.013497ms)
    Mar 27 15:22:29.498: INFO: (1) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 4.267371ms)
    Mar 27 15:22:29.498: INFO: (1) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 4.561344ms)
    Mar 27 15:22:29.498: INFO: (1) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 4.40439ms)
    Mar 27 15:22:29.499: INFO: (1) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.115963ms)
    Mar 27 15:22:29.499: INFO: (1) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 5.028901ms)
    Mar 27 15:22:29.499: INFO: (1) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 5.341754ms)
    Mar 27 15:22:29.499: INFO: (1) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 5.536372ms)
    Mar 27 15:22:29.499: INFO: (1) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 5.383354ms)
    Mar 27 15:22:29.499: INFO: (1) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 5.48041ms)
    Mar 27 15:22:29.500: INFO: (1) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 5.835739ms)
    Mar 27 15:22:29.500: INFO: (1) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 6.221668ms)
    Mar 27 15:22:29.500: INFO: (1) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 6.058005ms)
    Mar 27 15:22:29.500: INFO: (1) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 6.142042ms)
    Mar 27 15:22:29.500: INFO: (1) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 5.991474ms)
    Mar 27 15:22:29.500: INFO: (1) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 6.242175ms)
    Mar 27 15:22:29.501: INFO: (1) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 6.974985ms)
    Mar 27 15:22:29.505: INFO: (2) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 3.827378ms)
    Mar 27 15:22:29.505: INFO: (2) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 4.021623ms)
    Mar 27 15:22:29.505: INFO: (2) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 3.935882ms)
    Mar 27 15:22:29.506: INFO: (2) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 4.507243ms)
    Mar 27 15:22:29.506: INFO: (2) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 4.551906ms)
    Mar 27 15:22:29.506: INFO: (2) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 4.750558ms)
    Mar 27 15:22:29.506: INFO: (2) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 4.934377ms)
    Mar 27 15:22:29.506: INFO: (2) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 4.886417ms)
    Mar 27 15:22:29.506: INFO: (2) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 4.845564ms)
    Mar 27 15:22:29.506: INFO: (2) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 5.033423ms)
    Mar 27 15:22:29.506: INFO: (2) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 5.206341ms)
    Mar 27 15:22:29.506: INFO: (2) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 5.046123ms)
    Mar 27 15:22:29.507: INFO: (2) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 5.616576ms)
    Mar 27 15:22:29.507: INFO: (2) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 5.715201ms)
    Mar 27 15:22:29.507: INFO: (2) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 5.753572ms)
    Mar 27 15:22:29.507: INFO: (2) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 6.399101ms)
    Mar 27 15:22:29.513: INFO: (3) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.775145ms)
    Mar 27 15:22:29.513: INFO: (3) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 5.748787ms)
    Mar 27 15:22:29.513: INFO: (3) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.744527ms)
    Mar 27 15:22:29.513: INFO: (3) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 5.626547ms)
    Mar 27 15:22:29.513: INFO: (3) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 5.703802ms)
    Mar 27 15:22:29.514: INFO: (3) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 5.839387ms)
    Mar 27 15:22:29.514: INFO: (3) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 6.560369ms)
    Mar 27 15:22:29.514: INFO: (3) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 6.506616ms)
    Mar 27 15:22:29.514: INFO: (3) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 6.60646ms)
    Mar 27 15:22:29.514: INFO: (3) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 6.620767ms)
    Mar 27 15:22:29.515: INFO: (3) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 6.810717ms)
    Mar 27 15:22:29.515: INFO: (3) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 6.84944ms)
    Mar 27 15:22:29.516: INFO: (3) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 7.828022ms)
    Mar 27 15:22:29.516: INFO: (3) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 8.007908ms)
    Mar 27 15:22:29.516: INFO: (3) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 8.365065ms)
    Mar 27 15:22:29.516: INFO: (3) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 8.322196ms)
    Mar 27 15:22:29.520: INFO: (4) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 4.128233ms)
    Mar 27 15:22:29.520: INFO: (4) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 4.110933ms)
    Mar 27 15:22:29.521: INFO: (4) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 4.200289ms)
    Mar 27 15:22:29.521: INFO: (4) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 4.321527ms)
    Mar 27 15:22:29.521: INFO: (4) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 4.573098ms)
    Mar 27 15:22:29.521: INFO: (4) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 5.066743ms)
    Mar 27 15:22:29.521: INFO: (4) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 5.183051ms)
    Mar 27 15:22:29.521: INFO: (4) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 5.117635ms)
    Mar 27 15:22:29.522: INFO: (4) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 5.273431ms)
    Mar 27 15:22:29.522: INFO: (4) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.62398ms)
    Mar 27 15:22:29.523: INFO: (4) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 6.366619ms)
    Mar 27 15:22:29.523: INFO: (4) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 6.769527ms)
    Mar 27 15:22:29.523: INFO: (4) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 6.814457ms)
    Mar 27 15:22:29.523: INFO: (4) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 6.881655ms)
    Mar 27 15:22:29.523: INFO: (4) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 6.887564ms)
    Mar 27 15:22:29.524: INFO: (4) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 7.868176ms)
    Mar 27 15:22:29.527: INFO: (5) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 3.111205ms)
    Mar 27 15:22:29.528: INFO: (5) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 3.123461ms)
    Mar 27 15:22:29.528: INFO: (5) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 3.262453ms)
    Mar 27 15:22:29.528: INFO: (5) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 3.084966ms)
    Mar 27 15:22:29.529: INFO: (5) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 4.010356ms)
    Mar 27 15:22:29.530: INFO: (5) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 5.24691ms)
    Mar 27 15:22:29.530: INFO: (5) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 5.635649ms)
    Mar 27 15:22:29.530: INFO: (5) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 5.855688ms)
    Mar 27 15:22:29.531: INFO: (5) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 5.972038ms)
    Mar 27 15:22:29.531: INFO: (5) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 6.133577ms)
    Mar 27 15:22:29.531: INFO: (5) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 6.537437ms)
    Mar 27 15:22:29.531: INFO: (5) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 6.673259ms)
    Mar 27 15:22:29.531: INFO: (5) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 6.377473ms)
    Mar 27 15:22:29.532: INFO: (5) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 7.434388ms)
    Mar 27 15:22:29.532: INFO: (5) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 7.579318ms)
    Mar 27 15:22:29.532: INFO: (5) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 7.892572ms)
    Mar 27 15:22:29.538: INFO: (6) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.302039ms)
    Mar 27 15:22:29.538: INFO: (6) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.585901ms)
    Mar 27 15:22:29.538: INFO: (6) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 5.153779ms)
    Mar 27 15:22:29.539: INFO: (6) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 5.892505ms)
    Mar 27 15:22:29.539: INFO: (6) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 6.218597ms)
    Mar 27 15:22:29.539: INFO: (6) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 6.144866ms)
    Mar 27 15:22:29.539: INFO: (6) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 6.197423ms)
    Mar 27 15:22:29.539: INFO: (6) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 6.044285ms)
    Mar 27 15:22:29.539: INFO: (6) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 6.526644ms)
    Mar 27 15:22:29.539: INFO: (6) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 7.2196ms)
    Mar 27 15:22:29.539: INFO: (6) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 6.656102ms)
    Mar 27 15:22:29.539: INFO: (6) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 6.961418ms)
    Mar 27 15:22:29.540: INFO: (6) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 6.835258ms)
    Mar 27 15:22:29.540: INFO: (6) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 7.038186ms)
    Mar 27 15:22:29.540: INFO: (6) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 7.001661ms)
    Mar 27 15:22:29.540: INFO: (6) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 7.675172ms)
    Mar 27 15:22:29.550: INFO: (7) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 9.404647ms)
    Mar 27 15:22:29.550: INFO: (7) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 9.638171ms)
    Mar 27 15:22:29.550: INFO: (7) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 9.748174ms)
    Mar 27 15:22:29.550: INFO: (7) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 9.199661ms)
    Mar 27 15:22:29.550: INFO: (7) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 9.99632ms)
    Mar 27 15:22:29.550: INFO: (7) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 9.630635ms)
    Mar 27 15:22:29.551: INFO: (7) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 10.022439ms)
    Mar 27 15:22:29.552: INFO: (7) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 10.806548ms)
    Mar 27 15:22:29.552: INFO: (7) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 10.560697ms)
    Mar 27 15:22:29.552: INFO: (7) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 10.702801ms)
    Mar 27 15:22:29.552: INFO: (7) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 10.938927ms)
    Mar 27 15:22:29.552: INFO: (7) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 10.504024ms)
    Mar 27 15:22:29.553: INFO: (7) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 12.428775ms)
    Mar 27 15:22:29.553: INFO: (7) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 12.333164ms)
    Mar 27 15:22:29.554: INFO: (7) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 13.228166ms)
    Mar 27 15:22:29.554: INFO: (7) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 13.336019ms)
    Mar 27 15:22:29.562: INFO: (8) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 7.299533ms)
    Mar 27 15:22:29.562: INFO: (8) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 7.821367ms)
    Mar 27 15:22:29.562: INFO: (8) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 7.746265ms)
    Mar 27 15:22:29.563: INFO: (8) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 7.942827ms)
    Mar 27 15:22:29.563: INFO: (8) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 8.117048ms)
    Mar 27 15:22:29.563: INFO: (8) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 8.835632ms)
    Mar 27 15:22:29.563: INFO: (8) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 8.685053ms)
    Mar 27 15:22:29.563: INFO: (8) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 8.69828ms)
    Mar 27 15:22:29.563: INFO: (8) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 8.740322ms)
    Mar 27 15:22:29.563: INFO: (8) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 8.696849ms)
    Mar 27 15:22:29.564: INFO: (8) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 9.830675ms)
    Mar 27 15:22:29.564: INFO: (8) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 9.721025ms)
    Mar 27 15:22:29.564: INFO: (8) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 9.771733ms)
    Mar 27 15:22:29.564: INFO: (8) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 9.769513ms)
    Mar 27 15:22:29.564: INFO: (8) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 9.679701ms)
    Mar 27 15:22:29.565: INFO: (8) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 9.836276ms)
    Mar 27 15:22:29.571: INFO: (9) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.411565ms)
    Mar 27 15:22:29.572: INFO: (9) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 6.720937ms)
    Mar 27 15:22:29.572: INFO: (9) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 6.971478ms)
    Mar 27 15:22:29.572: INFO: (9) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 6.937088ms)
    Mar 27 15:22:29.573: INFO: (9) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 7.985068ms)
    Mar 27 15:22:29.573: INFO: (9) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 7.876566ms)
    Mar 27 15:22:29.573: INFO: (9) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 7.93852ms)
    Mar 27 15:22:29.573: INFO: (9) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 7.924573ms)
    Mar 27 15:22:29.574: INFO: (9) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 8.659767ms)
    Mar 27 15:22:29.574: INFO: (9) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 8.620011ms)
    Mar 27 15:22:29.574: INFO: (9) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 8.728768ms)
    Mar 27 15:22:29.574: INFO: (9) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 8.597698ms)
    Mar 27 15:22:29.577: INFO: (9) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 11.579729ms)
    Mar 27 15:22:29.577: INFO: (9) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 11.735215ms)
    Mar 27 15:22:29.582: INFO: (9) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 16.872277ms)
    Mar 27 15:22:29.582: INFO: (9) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 16.887152ms)
    Mar 27 15:22:29.587: INFO: (10) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 4.534872ms)
    Mar 27 15:22:29.587: INFO: (10) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 4.62675ms)
    Mar 27 15:22:29.587: INFO: (10) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 4.572374ms)
    Mar 27 15:22:29.587: INFO: (10) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 4.657852ms)
    Mar 27 15:22:29.587: INFO: (10) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 4.752897ms)
    Mar 27 15:22:29.587: INFO: (10) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 4.850228ms)
    Mar 27 15:22:29.587: INFO: (10) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 4.576397ms)
    Mar 27 15:22:29.587: INFO: (10) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 5.054416ms)
    Mar 27 15:22:29.587: INFO: (10) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 5.236909ms)
    Mar 27 15:22:29.588: INFO: (10) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 5.241883ms)
    Mar 27 15:22:29.588: INFO: (10) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.872104ms)
    Mar 27 15:22:29.588: INFO: (10) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 5.802584ms)
    Mar 27 15:22:29.589: INFO: (10) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 6.564591ms)
    Mar 27 15:22:29.589: INFO: (10) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 6.721448ms)
    Mar 27 15:22:29.590: INFO: (10) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 7.742521ms)
    Mar 27 15:22:29.590: INFO: (10) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 7.882109ms)
    Mar 27 15:22:29.596: INFO: (11) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 5.573707ms)
    Mar 27 15:22:29.596: INFO: (11) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 5.67931ms)
    Mar 27 15:22:29.596: INFO: (11) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 5.762866ms)
    Mar 27 15:22:29.596: INFO: (11) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 5.943277ms)
    Mar 27 15:22:29.597: INFO: (11) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 6.744335ms)
    Mar 27 15:22:29.597: INFO: (11) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 6.831085ms)
    Mar 27 15:22:29.597: INFO: (11) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 6.646377ms)
    Mar 27 15:22:29.597: INFO: (11) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 6.839773ms)
    Mar 27 15:22:29.597: INFO: (11) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 6.726017ms)
    Mar 27 15:22:29.597: INFO: (11) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 6.974149ms)
    Mar 27 15:22:29.601: INFO: (11) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 10.449287ms)
    Mar 27 15:22:29.601: INFO: (11) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 10.452052ms)
    Mar 27 15:22:29.601: INFO: (11) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 10.682188ms)
    Mar 27 15:22:29.601: INFO: (11) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 10.575436ms)
    Mar 27 15:22:29.601: INFO: (11) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 10.645847ms)
    Mar 27 15:22:29.602: INFO: (11) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 11.185182ms)
    Mar 27 15:22:29.606: INFO: (12) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 4.05152ms)
    Mar 27 15:22:29.606: INFO: (12) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 3.947876ms)
    Mar 27 15:22:29.606: INFO: (12) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 4.430372ms)
    Mar 27 15:22:29.607: INFO: (12) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 4.829501ms)
    Mar 27 15:22:29.607: INFO: (12) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 4.918429ms)
    Mar 27 15:22:29.607: INFO: (12) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 4.888396ms)
    Mar 27 15:22:29.607: INFO: (12) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 4.966405ms)
    Mar 27 15:22:29.607: INFO: (12) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 5.066806ms)
    Mar 27 15:22:29.607: INFO: (12) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 5.498665ms)
    Mar 27 15:22:29.607: INFO: (12) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 5.686181ms)
    Mar 27 15:22:29.608: INFO: (12) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 5.923083ms)
    Mar 27 15:22:29.608: INFO: (12) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 6.097289ms)
    Mar 27 15:22:29.608: INFO: (12) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 5.944478ms)
    Mar 27 15:22:29.608: INFO: (12) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 6.047578ms)
    Mar 27 15:22:29.609: INFO: (12) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 6.675191ms)
    Mar 27 15:22:29.609: INFO: (12) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 6.784112ms)
    Mar 27 15:22:29.614: INFO: (13) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 5.479926ms)
    Mar 27 15:22:29.614: INFO: (13) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 5.461462ms)
    Mar 27 15:22:29.614: INFO: (13) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.413225ms)
    Mar 27 15:22:29.614: INFO: (13) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 5.73899ms)
    Mar 27 15:22:29.614: INFO: (13) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.78744ms)
    Mar 27 15:22:29.615: INFO: (13) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 5.846192ms)
    Mar 27 15:22:29.615: INFO: (13) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 5.858835ms)
    Mar 27 15:22:29.616: INFO: (13) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 6.777772ms)
    Mar 27 15:22:29.616: INFO: (13) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 6.887045ms)
    Mar 27 15:22:29.616: INFO: (13) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 7.080698ms)
    Mar 27 15:22:29.616: INFO: (13) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 6.883441ms)
    Mar 27 15:22:29.616: INFO: (13) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 7.188128ms)
    Mar 27 15:22:29.616: INFO: (13) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 7.282149ms)
    Mar 27 15:22:29.616: INFO: (13) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 7.325076ms)
    Mar 27 15:22:29.616: INFO: (13) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 7.498595ms)
    Mar 27 15:22:29.616: INFO: (13) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 7.545543ms)
    Mar 27 15:22:29.620: INFO: (14) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 3.493733ms)
    Mar 27 15:22:29.620: INFO: (14) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 3.60462ms)
    Mar 27 15:22:29.620: INFO: (14) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 3.57072ms)
    Mar 27 15:22:29.621: INFO: (14) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 4.866272ms)
    Mar 27 15:22:29.621: INFO: (14) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 4.764845ms)
    Mar 27 15:22:29.622: INFO: (14) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 5.047368ms)
    Mar 27 15:22:29.623: INFO: (14) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 5.681373ms)
    Mar 27 15:22:29.623: INFO: (14) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 5.642078ms)
    Mar 27 15:22:29.623: INFO: (14) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 6.17749ms)
    Mar 27 15:22:29.623: INFO: (14) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 6.130621ms)
    Mar 27 15:22:29.623: INFO: (14) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 6.050235ms)
    Mar 27 15:22:29.623: INFO: (14) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 6.145129ms)
    Mar 27 15:22:29.623: INFO: (14) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.865621ms)
    Mar 27 15:22:29.623: INFO: (14) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 6.245415ms)
    Mar 27 15:22:29.624: INFO: (14) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 6.993847ms)
    Mar 27 15:22:29.624: INFO: (14) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 7.174175ms)
    Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.967616ms)
    Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 5.997949ms)
    Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 6.052954ms)
    Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 6.203161ms)
    Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 6.108473ms)
    Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 6.198302ms)
    Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 6.183572ms)
    Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 6.34546ms)
    Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 5.93442ms)
    Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 6.25701ms)
    Mar 27 15:22:29.630: INFO: (15) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 6.409519ms)
    Mar 27 15:22:29.631: INFO: (15) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 7.231209ms)
    Mar 27 15:22:29.631: INFO: (15) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 7.44146ms)
    Mar 27 15:22:29.632: INFO: (15) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 7.623782ms)
    Mar 27 15:22:29.632: INFO: (15) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 7.525762ms)
    Mar 27 15:22:29.632: INFO: (15) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 7.687898ms)
    Mar 27 15:22:29.637: INFO: (16) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 4.957078ms)
    Mar 27 15:22:29.637: INFO: (16) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 5.219367ms)
    Mar 27 15:22:29.637: INFO: (16) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 5.380222ms)
    Mar 27 15:22:29.637: INFO: (16) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 5.429044ms)
    Mar 27 15:22:29.639: INFO: (16) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 7.023654ms)
    Mar 27 15:22:29.639: INFO: (16) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 6.969651ms)
    Mar 27 15:22:29.639: INFO: (16) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 7.176055ms)
    Mar 27 15:22:29.639: INFO: (16) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 7.138059ms)
    Mar 27 15:22:29.639: INFO: (16) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 7.385794ms)
    Mar 27 15:22:29.640: INFO: (16) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 7.715535ms)
    Mar 27 15:22:29.640: INFO: (16) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 7.736279ms)
    Mar 27 15:22:29.640: INFO: (16) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 7.773698ms)
    Mar 27 15:22:29.640: INFO: (16) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 7.829606ms)
    Mar 27 15:22:29.640: INFO: (16) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 7.935755ms)
    Mar 27 15:22:29.640: INFO: (16) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 7.895655ms)
    Mar 27 15:22:29.640: INFO: (16) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 8.443551ms)
    Mar 27 15:22:29.644: INFO: (17) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 3.769145ms)
    Mar 27 15:22:29.644: INFO: (17) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 3.839812ms)
    Mar 27 15:22:29.645: INFO: (17) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 4.413386ms)
    Mar 27 15:22:29.645: INFO: (17) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 4.408121ms)
    Mar 27 15:22:29.645: INFO: (17) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 4.56851ms)
    Mar 27 15:22:29.645: INFO: (17) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 4.73124ms)
    Mar 27 15:22:29.646: INFO: (17) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 4.914873ms)
    Mar 27 15:22:29.646: INFO: (17) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 5.178699ms)
    Mar 27 15:22:29.649: INFO: (17) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 8.092224ms)
    Mar 27 15:22:29.649: INFO: (17) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 8.084131ms)
    Mar 27 15:22:29.649: INFO: (17) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 8.154397ms)
    Mar 27 15:22:29.649: INFO: (17) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 8.27434ms)
    Mar 27 15:22:29.649: INFO: (17) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 8.922758ms)
    Mar 27 15:22:29.649: INFO: (17) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 8.860378ms)
    Mar 27 15:22:29.650: INFO: (17) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 8.91691ms)
    Mar 27 15:22:29.650: INFO: (17) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 8.985847ms)
    Mar 27 15:22:29.655: INFO: (18) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 5.747941ms)
    Mar 27 15:22:29.656: INFO: (18) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 6.163147ms)
    Mar 27 15:22:29.656: INFO: (18) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 6.079273ms)
    Mar 27 15:22:29.656: INFO: (18) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 6.36329ms)
    Mar 27 15:22:29.656: INFO: (18) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 6.299784ms)
    Mar 27 15:22:29.656: INFO: (18) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 6.51025ms)
    Mar 27 15:22:29.657: INFO: (18) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 7.331612ms)
    Mar 27 15:22:29.657: INFO: (18) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 7.311335ms)
    Mar 27 15:22:29.657: INFO: (18) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 7.383511ms)
    Mar 27 15:22:29.657: INFO: (18) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 7.32616ms)
    Mar 27 15:22:29.658: INFO: (18) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 8.050818ms)
    Mar 27 15:22:29.658: INFO: (18) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 8.517234ms)
    Mar 27 15:22:29.658: INFO: (18) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 8.596396ms)
    Mar 27 15:22:29.658: INFO: (18) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 8.714751ms)
    Mar 27 15:22:29.659: INFO: (18) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 8.897162ms)
    Mar 27 15:22:29.659: INFO: (18) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 9.083721ms)
    Mar 27 15:22:29.663: INFO: (19) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:1080/proxy/rewriteme">... (200; 3.504451ms)
    Mar 27 15:22:29.663: INFO: (19) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:162/proxy/: bar (200; 3.62249ms)
    Mar 27 15:22:29.663: INFO: (19) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:160/proxy/: foo (200; 3.61283ms)
    Mar 27 15:22:29.664: INFO: (19) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:460/proxy/: tls baz (200; 4.276489ms)
    Mar 27 15:22:29.664: INFO: (19) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:443/proxy/tlsrewritem... (200; 4.353868ms)
    Mar 27 15:22:29.664: INFO: (19) /api/v1/namespaces/proxy-4935/pods/https:proxy-service-7hf67-r842v:462/proxy/: tls qux (200; 4.683453ms)
    Mar 27 15:22:29.664: INFO: (19) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname2/proxy/: bar (200; 4.999838ms)
    Mar 27 15:22:29.664: INFO: (19) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname2/proxy/: tls qux (200; 4.873924ms)
    Mar 27 15:22:29.664: INFO: (19) /api/v1/namespaces/proxy-4935/services/proxy-service-7hf67:portname1/proxy/: foo (200; 4.950791ms)
    Mar 27 15:22:29.669: INFO: (19) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v/proxy/rewriteme">test</a> (200; 10.437239ms)
    Mar 27 15:22:29.669: INFO: (19) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:160/proxy/: foo (200; 9.839064ms)
    Mar 27 15:22:29.670: INFO: (19) /api/v1/namespaces/proxy-4935/pods/http:proxy-service-7hf67-r842v:162/proxy/: bar (200; 9.994447ms)
    Mar 27 15:22:29.670: INFO: (19) /api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/: <a href="/api/v1/namespaces/proxy-4935/pods/proxy-service-7hf67-r842v:1080/proxy/rewriteme">test<... (200; 10.004297ms)
    Mar 27 15:22:29.670: INFO: (19) /api/v1/namespaces/proxy-4935/services/https:proxy-service-7hf67:tlsportname1/proxy/: tls baz (200; 9.998647ms)
    Mar 27 15:22:29.670: INFO: (19) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname1/proxy/: foo (200; 10.527183ms)
    Mar 27 15:22:29.670: INFO: (19) /api/v1/namespaces/proxy-4935/services/http:proxy-service-7hf67:portname2/proxy/: bar (200; 10.73685ms)
    STEP: deleting ReplicationController proxy-service-7hf67 in namespace proxy-4935, will wait for the garbage collector to delete the pods 03/27/23 15:22:29.67
    Mar 27 15:22:29.736: INFO: Deleting ReplicationController proxy-service-7hf67 took: 11.533675ms
    Mar 27 15:22:29.836: INFO: Terminating ReplicationController proxy-service-7hf67 pods took: 100.216267ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar 27 15:22:32.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-4935" for this suite. 03/27/23 15:22:32.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:22:32.059
Mar 27 15:22:32.059: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename webhook 03/27/23 15:22:32.06
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:32.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:32.08
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/27/23 15:22:32.099
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:22:32.838
STEP: Deploying the webhook pod 03/27/23 15:22:32.845
STEP: Wait for the deployment to be ready 03/27/23 15:22:32.86
Mar 27 15:22:32.870: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 15:22:34.885
STEP: Verifying the service has paired with the endpoint 03/27/23 15:22:34.899
Mar 27 15:22:35.900: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/27/23 15:22:35.904
STEP: create a configmap that should be updated by the webhook 03/27/23 15:22:35.924
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 15:22:35.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8942" for this suite. 03/27/23 15:22:35.955
STEP: Destroying namespace "webhook-8942-markers" for this suite. 03/27/23 15:22:35.968
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":252,"skipped":4819,"failed":0}
------------------------------
• [3.963 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:22:32.059
    Mar 27 15:22:32.059: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename webhook 03/27/23 15:22:32.06
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:32.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:32.08
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/27/23 15:22:32.099
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:22:32.838
    STEP: Deploying the webhook pod 03/27/23 15:22:32.845
    STEP: Wait for the deployment to be ready 03/27/23 15:22:32.86
    Mar 27 15:22:32.870: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 15:22:34.885
    STEP: Verifying the service has paired with the endpoint 03/27/23 15:22:34.899
    Mar 27 15:22:35.900: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/27/23 15:22:35.904
    STEP: create a configmap that should be updated by the webhook 03/27/23 15:22:35.924
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 15:22:35.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8942" for this suite. 03/27/23 15:22:35.955
    STEP: Destroying namespace "webhook-8942-markers" for this suite. 03/27/23 15:22:35.968
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:22:36.023
Mar 27 15:22:36.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename dns 03/27/23 15:22:36.024
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:36.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:36.045
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3305.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3305.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 03/27/23 15:22:36.051
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3305.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3305.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 03/27/23 15:22:36.051
STEP: creating a pod to probe /etc/hosts 03/27/23 15:22:36.052
STEP: submitting the pod to kubernetes 03/27/23 15:22:36.052
Mar 27 15:22:36.067: INFO: Waiting up to 15m0s for pod "dns-test-7ecd6683-e0f2-41a0-a922-7ca99c310f10" in namespace "dns-3305" to be "running"
Mar 27 15:22:36.075: INFO: Pod "dns-test-7ecd6683-e0f2-41a0-a922-7ca99c310f10": Phase="Pending", Reason="", readiness=false. Elapsed: 7.925709ms
Mar 27 15:22:38.081: INFO: Pod "dns-test-7ecd6683-e0f2-41a0-a922-7ca99c310f10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014479093s
Mar 27 15:22:40.082: INFO: Pod "dns-test-7ecd6683-e0f2-41a0-a922-7ca99c310f10": Phase="Running", Reason="", readiness=true. Elapsed: 4.014895663s
Mar 27 15:22:40.082: INFO: Pod "dns-test-7ecd6683-e0f2-41a0-a922-7ca99c310f10" satisfied condition "running"
STEP: retrieving the pod 03/27/23 15:22:40.082
STEP: looking for the results for each expected name from probers 03/27/23 15:22:40.084
Mar 27 15:22:40.108: INFO: DNS probes using dns-3305/dns-test-7ecd6683-e0f2-41a0-a922-7ca99c310f10 succeeded

STEP: deleting the pod 03/27/23 15:22:40.108
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 27 15:22:40.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3305" for this suite. 03/27/23 15:22:40.123
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":253,"skipped":4845,"failed":0}
------------------------------
• [4.105 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:22:36.023
    Mar 27 15:22:36.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename dns 03/27/23 15:22:36.024
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:36.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:36.045
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3305.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3305.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     03/27/23 15:22:36.051
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3305.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3305.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     03/27/23 15:22:36.051
    STEP: creating a pod to probe /etc/hosts 03/27/23 15:22:36.052
    STEP: submitting the pod to kubernetes 03/27/23 15:22:36.052
    Mar 27 15:22:36.067: INFO: Waiting up to 15m0s for pod "dns-test-7ecd6683-e0f2-41a0-a922-7ca99c310f10" in namespace "dns-3305" to be "running"
    Mar 27 15:22:36.075: INFO: Pod "dns-test-7ecd6683-e0f2-41a0-a922-7ca99c310f10": Phase="Pending", Reason="", readiness=false. Elapsed: 7.925709ms
    Mar 27 15:22:38.081: INFO: Pod "dns-test-7ecd6683-e0f2-41a0-a922-7ca99c310f10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014479093s
    Mar 27 15:22:40.082: INFO: Pod "dns-test-7ecd6683-e0f2-41a0-a922-7ca99c310f10": Phase="Running", Reason="", readiness=true. Elapsed: 4.014895663s
    Mar 27 15:22:40.082: INFO: Pod "dns-test-7ecd6683-e0f2-41a0-a922-7ca99c310f10" satisfied condition "running"
    STEP: retrieving the pod 03/27/23 15:22:40.082
    STEP: looking for the results for each expected name from probers 03/27/23 15:22:40.084
    Mar 27 15:22:40.108: INFO: DNS probes using dns-3305/dns-test-7ecd6683-e0f2-41a0-a922-7ca99c310f10 succeeded

    STEP: deleting the pod 03/27/23 15:22:40.108
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 27 15:22:40.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3305" for this suite. 03/27/23 15:22:40.123
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:22:40.129
Mar 27 15:22:40.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename downward-api 03/27/23 15:22:40.131
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:40.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:40.146
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 03/27/23 15:22:40.149
Mar 27 15:22:40.157: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01858042-02fc-479d-832b-cae42060e4ef" in namespace "downward-api-592" to be "Succeeded or Failed"
Mar 27 15:22:40.161: INFO: Pod "downwardapi-volume-01858042-02fc-479d-832b-cae42060e4ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007776ms
Mar 27 15:22:42.165: INFO: Pod "downwardapi-volume-01858042-02fc-479d-832b-cae42060e4ef": Phase="Running", Reason="", readiness=true. Elapsed: 2.008110124s
Mar 27 15:22:44.166: INFO: Pod "downwardapi-volume-01858042-02fc-479d-832b-cae42060e4ef": Phase="Running", Reason="", readiness=false. Elapsed: 4.009420068s
Mar 27 15:22:46.165: INFO: Pod "downwardapi-volume-01858042-02fc-479d-832b-cae42060e4ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008486938s
STEP: Saw pod success 03/27/23 15:22:46.165
Mar 27 15:22:46.165: INFO: Pod "downwardapi-volume-01858042-02fc-479d-832b-cae42060e4ef" satisfied condition "Succeeded or Failed"
Mar 27 15:22:46.170: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-01858042-02fc-479d-832b-cae42060e4ef container client-container: <nil>
STEP: delete the pod 03/27/23 15:22:46.178
Mar 27 15:22:46.193: INFO: Waiting for pod downwardapi-volume-01858042-02fc-479d-832b-cae42060e4ef to disappear
Mar 27 15:22:46.195: INFO: Pod downwardapi-volume-01858042-02fc-479d-832b-cae42060e4ef no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 27 15:22:46.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-592" for this suite. 03/27/23 15:22:46.2
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":254,"skipped":4859,"failed":0}
------------------------------
• [SLOW TEST] [6.076 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:22:40.129
    Mar 27 15:22:40.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename downward-api 03/27/23 15:22:40.131
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:40.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:40.146
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 03/27/23 15:22:40.149
    Mar 27 15:22:40.157: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01858042-02fc-479d-832b-cae42060e4ef" in namespace "downward-api-592" to be "Succeeded or Failed"
    Mar 27 15:22:40.161: INFO: Pod "downwardapi-volume-01858042-02fc-479d-832b-cae42060e4ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007776ms
    Mar 27 15:22:42.165: INFO: Pod "downwardapi-volume-01858042-02fc-479d-832b-cae42060e4ef": Phase="Running", Reason="", readiness=true. Elapsed: 2.008110124s
    Mar 27 15:22:44.166: INFO: Pod "downwardapi-volume-01858042-02fc-479d-832b-cae42060e4ef": Phase="Running", Reason="", readiness=false. Elapsed: 4.009420068s
    Mar 27 15:22:46.165: INFO: Pod "downwardapi-volume-01858042-02fc-479d-832b-cae42060e4ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008486938s
    STEP: Saw pod success 03/27/23 15:22:46.165
    Mar 27 15:22:46.165: INFO: Pod "downwardapi-volume-01858042-02fc-479d-832b-cae42060e4ef" satisfied condition "Succeeded or Failed"
    Mar 27 15:22:46.170: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-01858042-02fc-479d-832b-cae42060e4ef container client-container: <nil>
    STEP: delete the pod 03/27/23 15:22:46.178
    Mar 27 15:22:46.193: INFO: Waiting for pod downwardapi-volume-01858042-02fc-479d-832b-cae42060e4ef to disappear
    Mar 27 15:22:46.195: INFO: Pod downwardapi-volume-01858042-02fc-479d-832b-cae42060e4ef no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 27 15:22:46.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-592" for this suite. 03/27/23 15:22:46.2
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:22:46.206
Mar 27 15:22:46.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename podtemplate 03/27/23 15:22:46.207
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:46.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:46.226
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 03/27/23 15:22:46.228
Mar 27 15:22:46.234: INFO: created test-podtemplate-1
Mar 27 15:22:46.240: INFO: created test-podtemplate-2
Mar 27 15:22:46.245: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 03/27/23 15:22:46.245
STEP: delete collection of pod templates 03/27/23 15:22:46.251
Mar 27 15:22:46.251: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 03/27/23 15:22:46.27
Mar 27 15:22:46.270: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar 27 15:22:46.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9837" for this suite. 03/27/23 15:22:46.275
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":255,"skipped":4859,"failed":0}
------------------------------
• [0.075 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:22:46.206
    Mar 27 15:22:46.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename podtemplate 03/27/23 15:22:46.207
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:46.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:46.226
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 03/27/23 15:22:46.228
    Mar 27 15:22:46.234: INFO: created test-podtemplate-1
    Mar 27 15:22:46.240: INFO: created test-podtemplate-2
    Mar 27 15:22:46.245: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 03/27/23 15:22:46.245
    STEP: delete collection of pod templates 03/27/23 15:22:46.251
    Mar 27 15:22:46.251: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 03/27/23 15:22:46.27
    Mar 27 15:22:46.270: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar 27 15:22:46.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-9837" for this suite. 03/27/23 15:22:46.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:22:46.282
Mar 27 15:22:46.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename downward-api 03/27/23 15:22:46.283
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:46.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:46.297
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 03/27/23 15:22:46.303
Mar 27 15:22:46.326: INFO: Waiting up to 5m0s for pod "labelsupdate744c69ee-f197-472c-9e9c-b74b21a66815" in namespace "downward-api-6284" to be "running and ready"
Mar 27 15:22:46.334: INFO: Pod "labelsupdate744c69ee-f197-472c-9e9c-b74b21a66815": Phase="Pending", Reason="", readiness=false. Elapsed: 7.796051ms
Mar 27 15:22:46.334: INFO: The phase of Pod labelsupdate744c69ee-f197-472c-9e9c-b74b21a66815 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:22:48.339: INFO: Pod "labelsupdate744c69ee-f197-472c-9e9c-b74b21a66815": Phase="Running", Reason="", readiness=true. Elapsed: 2.013004394s
Mar 27 15:22:48.339: INFO: The phase of Pod labelsupdate744c69ee-f197-472c-9e9c-b74b21a66815 is Running (Ready = true)
Mar 27 15:22:48.339: INFO: Pod "labelsupdate744c69ee-f197-472c-9e9c-b74b21a66815" satisfied condition "running and ready"
Mar 27 15:22:48.862: INFO: Successfully updated pod "labelsupdate744c69ee-f197-472c-9e9c-b74b21a66815"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 27 15:22:50.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6284" for this suite. 03/27/23 15:22:50.886
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":256,"skipped":4871,"failed":0}
------------------------------
• [4.612 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:22:46.282
    Mar 27 15:22:46.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename downward-api 03/27/23 15:22:46.283
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:46.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:46.297
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 03/27/23 15:22:46.303
    Mar 27 15:22:46.326: INFO: Waiting up to 5m0s for pod "labelsupdate744c69ee-f197-472c-9e9c-b74b21a66815" in namespace "downward-api-6284" to be "running and ready"
    Mar 27 15:22:46.334: INFO: Pod "labelsupdate744c69ee-f197-472c-9e9c-b74b21a66815": Phase="Pending", Reason="", readiness=false. Elapsed: 7.796051ms
    Mar 27 15:22:46.334: INFO: The phase of Pod labelsupdate744c69ee-f197-472c-9e9c-b74b21a66815 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:22:48.339: INFO: Pod "labelsupdate744c69ee-f197-472c-9e9c-b74b21a66815": Phase="Running", Reason="", readiness=true. Elapsed: 2.013004394s
    Mar 27 15:22:48.339: INFO: The phase of Pod labelsupdate744c69ee-f197-472c-9e9c-b74b21a66815 is Running (Ready = true)
    Mar 27 15:22:48.339: INFO: Pod "labelsupdate744c69ee-f197-472c-9e9c-b74b21a66815" satisfied condition "running and ready"
    Mar 27 15:22:48.862: INFO: Successfully updated pod "labelsupdate744c69ee-f197-472c-9e9c-b74b21a66815"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 27 15:22:50.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6284" for this suite. 03/27/23 15:22:50.886
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:22:50.894
Mar 27 15:22:50.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename secrets 03/27/23 15:22:50.895
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:50.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:50.909
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-4f55e119-fbd2-497b-9505-a40ca553d7ac 03/27/23 15:22:50.912
STEP: Creating a pod to test consume secrets 03/27/23 15:22:50.918
Mar 27 15:22:50.932: INFO: Waiting up to 5m0s for pod "pod-secrets-1af38ff1-3928-4a87-af99-bd81f4827664" in namespace "secrets-8932" to be "Succeeded or Failed"
Mar 27 15:22:50.936: INFO: Pod "pod-secrets-1af38ff1-3928-4a87-af99-bd81f4827664": Phase="Pending", Reason="", readiness=false. Elapsed: 3.335167ms
Mar 27 15:22:52.940: INFO: Pod "pod-secrets-1af38ff1-3928-4a87-af99-bd81f4827664": Phase="Running", Reason="", readiness=true. Elapsed: 2.007562713s
Mar 27 15:22:54.940: INFO: Pod "pod-secrets-1af38ff1-3928-4a87-af99-bd81f4827664": Phase="Running", Reason="", readiness=false. Elapsed: 4.008091366s
Mar 27 15:22:56.939: INFO: Pod "pod-secrets-1af38ff1-3928-4a87-af99-bd81f4827664": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006332704s
STEP: Saw pod success 03/27/23 15:22:56.939
Mar 27 15:22:56.939: INFO: Pod "pod-secrets-1af38ff1-3928-4a87-af99-bd81f4827664" satisfied condition "Succeeded or Failed"
Mar 27 15:22:56.943: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-secrets-1af38ff1-3928-4a87-af99-bd81f4827664 container secret-volume-test: <nil>
STEP: delete the pod 03/27/23 15:22:56.95
Mar 27 15:22:56.962: INFO: Waiting for pod pod-secrets-1af38ff1-3928-4a87-af99-bd81f4827664 to disappear
Mar 27 15:22:56.964: INFO: Pod pod-secrets-1af38ff1-3928-4a87-af99-bd81f4827664 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 27 15:22:56.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8932" for this suite. 03/27/23 15:22:56.969
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":257,"skipped":4871,"failed":0}
------------------------------
• [SLOW TEST] [6.080 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:22:50.894
    Mar 27 15:22:50.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename secrets 03/27/23 15:22:50.895
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:50.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:50.909
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-4f55e119-fbd2-497b-9505-a40ca553d7ac 03/27/23 15:22:50.912
    STEP: Creating a pod to test consume secrets 03/27/23 15:22:50.918
    Mar 27 15:22:50.932: INFO: Waiting up to 5m0s for pod "pod-secrets-1af38ff1-3928-4a87-af99-bd81f4827664" in namespace "secrets-8932" to be "Succeeded or Failed"
    Mar 27 15:22:50.936: INFO: Pod "pod-secrets-1af38ff1-3928-4a87-af99-bd81f4827664": Phase="Pending", Reason="", readiness=false. Elapsed: 3.335167ms
    Mar 27 15:22:52.940: INFO: Pod "pod-secrets-1af38ff1-3928-4a87-af99-bd81f4827664": Phase="Running", Reason="", readiness=true. Elapsed: 2.007562713s
    Mar 27 15:22:54.940: INFO: Pod "pod-secrets-1af38ff1-3928-4a87-af99-bd81f4827664": Phase="Running", Reason="", readiness=false. Elapsed: 4.008091366s
    Mar 27 15:22:56.939: INFO: Pod "pod-secrets-1af38ff1-3928-4a87-af99-bd81f4827664": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006332704s
    STEP: Saw pod success 03/27/23 15:22:56.939
    Mar 27 15:22:56.939: INFO: Pod "pod-secrets-1af38ff1-3928-4a87-af99-bd81f4827664" satisfied condition "Succeeded or Failed"
    Mar 27 15:22:56.943: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-secrets-1af38ff1-3928-4a87-af99-bd81f4827664 container secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 15:22:56.95
    Mar 27 15:22:56.962: INFO: Waiting for pod pod-secrets-1af38ff1-3928-4a87-af99-bd81f4827664 to disappear
    Mar 27 15:22:56.964: INFO: Pod pod-secrets-1af38ff1-3928-4a87-af99-bd81f4827664 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 27 15:22:56.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8932" for this suite. 03/27/23 15:22:56.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:22:56.979
Mar 27 15:22:56.979: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 15:22:56.98
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:56.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:57.001
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 03/27/23 15:22:57.004
Mar 27 15:22:57.012: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9c5520f-5fb2-4e8b-b80a-e5a6cf57776e" in namespace "projected-5253" to be "Succeeded or Failed"
Mar 27 15:22:57.016: INFO: Pod "downwardapi-volume-f9c5520f-5fb2-4e8b-b80a-e5a6cf57776e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.978223ms
Mar 27 15:22:59.022: INFO: Pod "downwardapi-volume-f9c5520f-5fb2-4e8b-b80a-e5a6cf57776e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009527288s
Mar 27 15:23:01.021: INFO: Pod "downwardapi-volume-f9c5520f-5fb2-4e8b-b80a-e5a6cf57776e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008493221s
STEP: Saw pod success 03/27/23 15:23:01.021
Mar 27 15:23:01.021: INFO: Pod "downwardapi-volume-f9c5520f-5fb2-4e8b-b80a-e5a6cf57776e" satisfied condition "Succeeded or Failed"
Mar 27 15:23:01.024: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod downwardapi-volume-f9c5520f-5fb2-4e8b-b80a-e5a6cf57776e container client-container: <nil>
STEP: delete the pod 03/27/23 15:23:01.03
Mar 27 15:23:01.049: INFO: Waiting for pod downwardapi-volume-f9c5520f-5fb2-4e8b-b80a-e5a6cf57776e to disappear
Mar 27 15:23:01.052: INFO: Pod downwardapi-volume-f9c5520f-5fb2-4e8b-b80a-e5a6cf57776e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 27 15:23:01.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5253" for this suite. 03/27/23 15:23:01.055
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":258,"skipped":4879,"failed":0}
------------------------------
• [4.084 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:22:56.979
    Mar 27 15:22:56.979: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 15:22:56.98
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:22:56.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:22:57.001
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 03/27/23 15:22:57.004
    Mar 27 15:22:57.012: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9c5520f-5fb2-4e8b-b80a-e5a6cf57776e" in namespace "projected-5253" to be "Succeeded or Failed"
    Mar 27 15:22:57.016: INFO: Pod "downwardapi-volume-f9c5520f-5fb2-4e8b-b80a-e5a6cf57776e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.978223ms
    Mar 27 15:22:59.022: INFO: Pod "downwardapi-volume-f9c5520f-5fb2-4e8b-b80a-e5a6cf57776e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009527288s
    Mar 27 15:23:01.021: INFO: Pod "downwardapi-volume-f9c5520f-5fb2-4e8b-b80a-e5a6cf57776e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008493221s
    STEP: Saw pod success 03/27/23 15:23:01.021
    Mar 27 15:23:01.021: INFO: Pod "downwardapi-volume-f9c5520f-5fb2-4e8b-b80a-e5a6cf57776e" satisfied condition "Succeeded or Failed"
    Mar 27 15:23:01.024: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod downwardapi-volume-f9c5520f-5fb2-4e8b-b80a-e5a6cf57776e container client-container: <nil>
    STEP: delete the pod 03/27/23 15:23:01.03
    Mar 27 15:23:01.049: INFO: Waiting for pod downwardapi-volume-f9c5520f-5fb2-4e8b-b80a-e5a6cf57776e to disappear
    Mar 27 15:23:01.052: INFO: Pod downwardapi-volume-f9c5520f-5fb2-4e8b-b80a-e5a6cf57776e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 27 15:23:01.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5253" for this suite. 03/27/23 15:23:01.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:23:01.066
Mar 27 15:23:01.066: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename services 03/27/23 15:23:01.067
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:23:01.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:23:01.08
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1788 03/27/23 15:23:01.083
STEP: changing the ExternalName service to type=NodePort 03/27/23 15:23:01.089
STEP: creating replication controller externalname-service in namespace services-1788 03/27/23 15:23:01.121
I0327 15:23:01.132341      24 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1788, replica count: 2
I0327 15:23:04.183427      24 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 15:23:04.183: INFO: Creating new exec pod
Mar 27 15:23:04.189: INFO: Waiting up to 5m0s for pod "execpod5tmwr" in namespace "services-1788" to be "running"
Mar 27 15:23:04.195: INFO: Pod "execpod5tmwr": Phase="Pending", Reason="", readiness=false. Elapsed: 5.742462ms
Mar 27 15:23:06.198: INFO: Pod "execpod5tmwr": Phase="Running", Reason="", readiness=true. Elapsed: 2.008560427s
Mar 27 15:23:06.198: INFO: Pod "execpod5tmwr" satisfied condition "running"
Mar 27 15:23:07.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1788 exec execpod5tmwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 27 15:23:07.454: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 27 15:23:07.454: INFO: stdout: "externalname-service-knkpt"
Mar 27 15:23:07.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1788 exec execpod5tmwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.23.151 80'
Mar 27 15:23:07.637: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.23.151 80\nConnection to 10.240.23.151 80 port [tcp/http] succeeded!\n"
Mar 27 15:23:07.637: INFO: stdout: "externalname-service-f99bv"
Mar 27 15:23:07.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1788 exec execpod5tmwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.7 30969'
Mar 27 15:23:07.786: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.7 30969\nConnection to 192.168.1.7 30969 port [tcp/*] succeeded!\n"
Mar 27 15:23:07.786: INFO: stdout: "externalname-service-knkpt"
Mar 27 15:23:07.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1788 exec execpod5tmwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.4 30969'
Mar 27 15:23:07.962: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.4 30969\nConnection to 192.168.1.4 30969 port [tcp/*] succeeded!\n"
Mar 27 15:23:07.962: INFO: stdout: ""
Mar 27 15:23:08.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1788 exec execpod5tmwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.4 30969'
Mar 27 15:23:09.120: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.4 30969\nConnection to 192.168.1.4 30969 port [tcp/*] succeeded!\n"
Mar 27 15:23:09.120: INFO: stdout: "externalname-service-f99bv"
Mar 27 15:23:09.120: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 27 15:23:09.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1788" for this suite. 03/27/23 15:23:09.147
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":259,"skipped":4916,"failed":0}
------------------------------
• [SLOW TEST] [8.094 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:23:01.066
    Mar 27 15:23:01.066: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename services 03/27/23 15:23:01.067
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:23:01.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:23:01.08
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-1788 03/27/23 15:23:01.083
    STEP: changing the ExternalName service to type=NodePort 03/27/23 15:23:01.089
    STEP: creating replication controller externalname-service in namespace services-1788 03/27/23 15:23:01.121
    I0327 15:23:01.132341      24 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1788, replica count: 2
    I0327 15:23:04.183427      24 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 15:23:04.183: INFO: Creating new exec pod
    Mar 27 15:23:04.189: INFO: Waiting up to 5m0s for pod "execpod5tmwr" in namespace "services-1788" to be "running"
    Mar 27 15:23:04.195: INFO: Pod "execpod5tmwr": Phase="Pending", Reason="", readiness=false. Elapsed: 5.742462ms
    Mar 27 15:23:06.198: INFO: Pod "execpod5tmwr": Phase="Running", Reason="", readiness=true. Elapsed: 2.008560427s
    Mar 27 15:23:06.198: INFO: Pod "execpod5tmwr" satisfied condition "running"
    Mar 27 15:23:07.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1788 exec execpod5tmwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar 27 15:23:07.454: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 27 15:23:07.454: INFO: stdout: "externalname-service-knkpt"
    Mar 27 15:23:07.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1788 exec execpod5tmwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.23.151 80'
    Mar 27 15:23:07.637: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.23.151 80\nConnection to 10.240.23.151 80 port [tcp/http] succeeded!\n"
    Mar 27 15:23:07.637: INFO: stdout: "externalname-service-f99bv"
    Mar 27 15:23:07.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1788 exec execpod5tmwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.7 30969'
    Mar 27 15:23:07.786: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.7 30969\nConnection to 192.168.1.7 30969 port [tcp/*] succeeded!\n"
    Mar 27 15:23:07.786: INFO: stdout: "externalname-service-knkpt"
    Mar 27 15:23:07.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1788 exec execpod5tmwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.4 30969'
    Mar 27 15:23:07.962: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.4 30969\nConnection to 192.168.1.4 30969 port [tcp/*] succeeded!\n"
    Mar 27 15:23:07.962: INFO: stdout: ""
    Mar 27 15:23:08.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1788 exec execpod5tmwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.4 30969'
    Mar 27 15:23:09.120: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.4 30969\nConnection to 192.168.1.4 30969 port [tcp/*] succeeded!\n"
    Mar 27 15:23:09.120: INFO: stdout: "externalname-service-f99bv"
    Mar 27 15:23:09.120: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 27 15:23:09.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1788" for this suite. 03/27/23 15:23:09.147
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:23:09.16
Mar 27 15:23:09.161: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename secrets 03/27/23 15:23:09.162
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:23:09.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:23:09.177
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-5843/secret-test-99d8af38-49af-4bb3-aa4f-83281ba20b3b 03/27/23 15:23:09.18
STEP: Creating a pod to test consume secrets 03/27/23 15:23:09.193
Mar 27 15:23:09.205: INFO: Waiting up to 5m0s for pod "pod-configmaps-ad41f002-3833-4c65-8eb4-b9cf1b1334da" in namespace "secrets-5843" to be "Succeeded or Failed"
Mar 27 15:23:09.213: INFO: Pod "pod-configmaps-ad41f002-3833-4c65-8eb4-b9cf1b1334da": Phase="Pending", Reason="", readiness=false. Elapsed: 8.344907ms
Mar 27 15:23:11.218: INFO: Pod "pod-configmaps-ad41f002-3833-4c65-8eb4-b9cf1b1334da": Phase="Running", Reason="", readiness=true. Elapsed: 2.013350258s
Mar 27 15:23:13.218: INFO: Pod "pod-configmaps-ad41f002-3833-4c65-8eb4-b9cf1b1334da": Phase="Running", Reason="", readiness=false. Elapsed: 4.013220194s
Mar 27 15:23:15.218: INFO: Pod "pod-configmaps-ad41f002-3833-4c65-8eb4-b9cf1b1334da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012932016s
STEP: Saw pod success 03/27/23 15:23:15.218
Mar 27 15:23:15.218: INFO: Pod "pod-configmaps-ad41f002-3833-4c65-8eb4-b9cf1b1334da" satisfied condition "Succeeded or Failed"
Mar 27 15:23:15.224: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-configmaps-ad41f002-3833-4c65-8eb4-b9cf1b1334da container env-test: <nil>
STEP: delete the pod 03/27/23 15:23:15.234
Mar 27 15:23:15.248: INFO: Waiting for pod pod-configmaps-ad41f002-3833-4c65-8eb4-b9cf1b1334da to disappear
Mar 27 15:23:15.250: INFO: Pod pod-configmaps-ad41f002-3833-4c65-8eb4-b9cf1b1334da no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar 27 15:23:15.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5843" for this suite. 03/27/23 15:23:15.255
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":260,"skipped":4917,"failed":0}
------------------------------
• [SLOW TEST] [6.100 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:23:09.16
    Mar 27 15:23:09.161: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename secrets 03/27/23 15:23:09.162
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:23:09.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:23:09.177
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-5843/secret-test-99d8af38-49af-4bb3-aa4f-83281ba20b3b 03/27/23 15:23:09.18
    STEP: Creating a pod to test consume secrets 03/27/23 15:23:09.193
    Mar 27 15:23:09.205: INFO: Waiting up to 5m0s for pod "pod-configmaps-ad41f002-3833-4c65-8eb4-b9cf1b1334da" in namespace "secrets-5843" to be "Succeeded or Failed"
    Mar 27 15:23:09.213: INFO: Pod "pod-configmaps-ad41f002-3833-4c65-8eb4-b9cf1b1334da": Phase="Pending", Reason="", readiness=false. Elapsed: 8.344907ms
    Mar 27 15:23:11.218: INFO: Pod "pod-configmaps-ad41f002-3833-4c65-8eb4-b9cf1b1334da": Phase="Running", Reason="", readiness=true. Elapsed: 2.013350258s
    Mar 27 15:23:13.218: INFO: Pod "pod-configmaps-ad41f002-3833-4c65-8eb4-b9cf1b1334da": Phase="Running", Reason="", readiness=false. Elapsed: 4.013220194s
    Mar 27 15:23:15.218: INFO: Pod "pod-configmaps-ad41f002-3833-4c65-8eb4-b9cf1b1334da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012932016s
    STEP: Saw pod success 03/27/23 15:23:15.218
    Mar 27 15:23:15.218: INFO: Pod "pod-configmaps-ad41f002-3833-4c65-8eb4-b9cf1b1334da" satisfied condition "Succeeded or Failed"
    Mar 27 15:23:15.224: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-configmaps-ad41f002-3833-4c65-8eb4-b9cf1b1334da container env-test: <nil>
    STEP: delete the pod 03/27/23 15:23:15.234
    Mar 27 15:23:15.248: INFO: Waiting for pod pod-configmaps-ad41f002-3833-4c65-8eb4-b9cf1b1334da to disappear
    Mar 27 15:23:15.250: INFO: Pod pod-configmaps-ad41f002-3833-4c65-8eb4-b9cf1b1334da no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar 27 15:23:15.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5843" for this suite. 03/27/23 15:23:15.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:23:15.262
Mar 27 15:23:15.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename namespaces 03/27/23 15:23:15.263
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:23:15.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:23:15.28
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 03/27/23 15:23:15.282
STEP: patching the Namespace 03/27/23 15:23:15.303
STEP: get the Namespace and ensuring it has the label 03/27/23 15:23:15.311
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar 27 15:23:15.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8577" for this suite. 03/27/23 15:23:15.318
STEP: Destroying namespace "nspatchtest-9c39f122-98d0-423c-bdd9-07e74fbea94c-7373" for this suite. 03/27/23 15:23:15.324
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":261,"skipped":4945,"failed":0}
------------------------------
• [0.068 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:23:15.262
    Mar 27 15:23:15.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename namespaces 03/27/23 15:23:15.263
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:23:15.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:23:15.28
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 03/27/23 15:23:15.282
    STEP: patching the Namespace 03/27/23 15:23:15.303
    STEP: get the Namespace and ensuring it has the label 03/27/23 15:23:15.311
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 15:23:15.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-8577" for this suite. 03/27/23 15:23:15.318
    STEP: Destroying namespace "nspatchtest-9c39f122-98d0-423c-bdd9-07e74fbea94c-7373" for this suite. 03/27/23 15:23:15.324
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:23:15.331
Mar 27 15:23:15.331: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename emptydir-wrapper 03/27/23 15:23:15.332
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:23:15.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:23:15.348
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 03/27/23 15:23:15.35
STEP: Creating RC which spawns configmap-volume pods 03/27/23 15:23:15.604
Mar 27 15:23:15.711: INFO: Pod name wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/27/23 15:23:15.711
Mar 27 15:23:15.711: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw" in namespace "emptydir-wrapper-8672" to be "running"
Mar 27 15:23:15.738: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw": Phase="Pending", Reason="", readiness=false. Elapsed: 27.228854ms
Mar 27 15:23:17.743: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031786393s
Mar 27 15:23:19.743: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031821332s
Mar 27 15:23:21.744: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032834775s
Mar 27 15:23:23.743: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032404803s
Mar 27 15:23:25.744: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw": Phase="Pending", Reason="", readiness=false. Elapsed: 10.032704972s
Mar 27 15:23:27.744: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw": Phase="Pending", Reason="", readiness=false. Elapsed: 12.033667671s
Mar 27 15:23:29.744: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw": Phase="Pending", Reason="", readiness=false. Elapsed: 14.032949324s
Mar 27 15:23:31.743: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw": Phase="Running", Reason="", readiness=true. Elapsed: 16.032671382s
Mar 27 15:23:31.744: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw" satisfied condition "running"
Mar 27 15:23:31.744: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-gz7zz" in namespace "emptydir-wrapper-8672" to be "running"
Mar 27 15:23:31.748: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-gz7zz": Phase="Running", Reason="", readiness=true. Elapsed: 4.846079ms
Mar 27 15:23:31.748: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-gz7zz" satisfied condition "running"
Mar 27 15:23:31.749: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-hztxm" in namespace "emptydir-wrapper-8672" to be "running"
Mar 27 15:23:31.752: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-hztxm": Phase="Running", Reason="", readiness=true. Elapsed: 3.852614ms
Mar 27 15:23:31.752: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-hztxm" satisfied condition "running"
Mar 27 15:23:31.753: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-zhwth" in namespace "emptydir-wrapper-8672" to be "running"
Mar 27 15:23:31.756: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-zhwth": Phase="Running", Reason="", readiness=true. Elapsed: 3.84941ms
Mar 27 15:23:31.756: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-zhwth" satisfied condition "running"
Mar 27 15:23:31.756: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-zwq6k" in namespace "emptydir-wrapper-8672" to be "running"
Mar 27 15:23:31.761: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-zwq6k": Phase="Running", Reason="", readiness=true. Elapsed: 4.464023ms
Mar 27 15:23:31.761: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-zwq6k" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550 in namespace emptydir-wrapper-8672, will wait for the garbage collector to delete the pods 03/27/23 15:23:31.761
Mar 27 15:23:31.824: INFO: Deleting ReplicationController wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550 took: 7.129357ms
Mar 27 15:23:31.925: INFO: Terminating ReplicationController wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550 pods took: 101.261206ms
STEP: Creating RC which spawns configmap-volume pods 03/27/23 15:23:35.131
Mar 27 15:23:35.144: INFO: Pod name wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1: Found 0 pods out of 5
Mar 27 15:23:40.152: INFO: Pod name wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/27/23 15:23:40.152
Mar 27 15:23:40.152: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-fvn8f" in namespace "emptydir-wrapper-8672" to be "running"
Mar 27 15:23:40.155: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-fvn8f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.438061ms
Mar 27 15:23:42.160: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-fvn8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008242214s
Mar 27 15:23:44.160: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-fvn8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008474964s
Mar 27 15:23:46.161: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-fvn8f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00865029s
Mar 27 15:23:48.161: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-fvn8f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009190671s
Mar 27 15:23:50.163: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-fvn8f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010596333s
Mar 27 15:23:52.161: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-fvn8f": Phase="Running", Reason="", readiness=true. Elapsed: 12.009449667s
Mar 27 15:23:52.161: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-fvn8f" satisfied condition "running"
Mar 27 15:23:52.161: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-kmwqf" in namespace "emptydir-wrapper-8672" to be "running"
Mar 27 15:23:52.166: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-kmwqf": Phase="Running", Reason="", readiness=true. Elapsed: 4.092896ms
Mar 27 15:23:52.166: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-kmwqf" satisfied condition "running"
Mar 27 15:23:52.166: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-lrb5c" in namespace "emptydir-wrapper-8672" to be "running"
Mar 27 15:23:52.169: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-lrb5c": Phase="Running", Reason="", readiness=true. Elapsed: 3.212484ms
Mar 27 15:23:52.169: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-lrb5c" satisfied condition "running"
Mar 27 15:23:52.169: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-mfglb" in namespace "emptydir-wrapper-8672" to be "running"
Mar 27 15:23:52.172: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-mfglb": Phase="Running", Reason="", readiness=true. Elapsed: 3.086352ms
Mar 27 15:23:52.172: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-mfglb" satisfied condition "running"
Mar 27 15:23:52.172: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-pg7rs" in namespace "emptydir-wrapper-8672" to be "running"
Mar 27 15:23:52.176: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-pg7rs": Phase="Running", Reason="", readiness=true. Elapsed: 3.646765ms
Mar 27 15:23:52.176: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-pg7rs" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1 in namespace emptydir-wrapper-8672, will wait for the garbage collector to delete the pods 03/27/23 15:23:52.176
Mar 27 15:23:52.237: INFO: Deleting ReplicationController wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1 took: 7.559164ms
Mar 27 15:23:52.337: INFO: Terminating ReplicationController wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1 pods took: 100.284906ms
STEP: Creating RC which spawns configmap-volume pods 03/27/23 15:23:55.842
Mar 27 15:23:55.856: INFO: Pod name wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428: Found 0 pods out of 5
Mar 27 15:24:00.865: INFO: Pod name wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/27/23 15:24:00.865
Mar 27 15:24:00.865: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-6pfvj" in namespace "emptydir-wrapper-8672" to be "running"
Mar 27 15:24:00.868: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-6pfvj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.23717ms
Mar 27 15:24:02.879: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-6pfvj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014116814s
Mar 27 15:24:04.874: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-6pfvj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008944361s
Mar 27 15:24:06.876: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-6pfvj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010762218s
Mar 27 15:24:08.876: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-6pfvj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011170566s
Mar 27 15:24:10.874: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-6pfvj": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009294304s
Mar 27 15:24:12.874: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-6pfvj": Phase="Running", Reason="", readiness=true. Elapsed: 12.009280072s
Mar 27 15:24:12.874: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-6pfvj" satisfied condition "running"
Mar 27 15:24:12.874: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-89cbj" in namespace "emptydir-wrapper-8672" to be "running"
Mar 27 15:24:12.878: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-89cbj": Phase="Running", Reason="", readiness=true. Elapsed: 3.897478ms
Mar 27 15:24:12.878: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-89cbj" satisfied condition "running"
Mar 27 15:24:12.878: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-dwnkq" in namespace "emptydir-wrapper-8672" to be "running"
Mar 27 15:24:12.882: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-dwnkq": Phase="Running", Reason="", readiness=true. Elapsed: 3.788062ms
Mar 27 15:24:12.882: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-dwnkq" satisfied condition "running"
Mar 27 15:24:12.882: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-gcsxj" in namespace "emptydir-wrapper-8672" to be "running"
Mar 27 15:24:12.885: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-gcsxj": Phase="Running", Reason="", readiness=true. Elapsed: 3.144301ms
Mar 27 15:24:12.886: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-gcsxj" satisfied condition "running"
Mar 27 15:24:12.886: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-nsb2s" in namespace "emptydir-wrapper-8672" to be "running"
Mar 27 15:24:12.889: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-nsb2s": Phase="Running", Reason="", readiness=true. Elapsed: 3.33742ms
Mar 27 15:24:12.889: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-nsb2s" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428 in namespace emptydir-wrapper-8672, will wait for the garbage collector to delete the pods 03/27/23 15:24:12.889
Mar 27 15:24:12.955: INFO: Deleting ReplicationController wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428 took: 10.899543ms
Mar 27 15:24:13.055: INFO: Terminating ReplicationController wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428 pods took: 100.364319ms
STEP: Cleaning up the configMaps 03/27/23 15:24:16.656
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Mar 27 15:24:16.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8672" for this suite. 03/27/23 15:24:16.987
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":262,"skipped":4953,"failed":0}
------------------------------
• [SLOW TEST] [61.664 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:23:15.331
    Mar 27 15:23:15.331: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename emptydir-wrapper 03/27/23 15:23:15.332
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:23:15.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:23:15.348
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 03/27/23 15:23:15.35
    STEP: Creating RC which spawns configmap-volume pods 03/27/23 15:23:15.604
    Mar 27 15:23:15.711: INFO: Pod name wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/27/23 15:23:15.711
    Mar 27 15:23:15.711: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw" in namespace "emptydir-wrapper-8672" to be "running"
    Mar 27 15:23:15.738: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw": Phase="Pending", Reason="", readiness=false. Elapsed: 27.228854ms
    Mar 27 15:23:17.743: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031786393s
    Mar 27 15:23:19.743: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031821332s
    Mar 27 15:23:21.744: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032834775s
    Mar 27 15:23:23.743: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032404803s
    Mar 27 15:23:25.744: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw": Phase="Pending", Reason="", readiness=false. Elapsed: 10.032704972s
    Mar 27 15:23:27.744: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw": Phase="Pending", Reason="", readiness=false. Elapsed: 12.033667671s
    Mar 27 15:23:29.744: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw": Phase="Pending", Reason="", readiness=false. Elapsed: 14.032949324s
    Mar 27 15:23:31.743: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw": Phase="Running", Reason="", readiness=true. Elapsed: 16.032671382s
    Mar 27 15:23:31.744: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-4fgzw" satisfied condition "running"
    Mar 27 15:23:31.744: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-gz7zz" in namespace "emptydir-wrapper-8672" to be "running"
    Mar 27 15:23:31.748: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-gz7zz": Phase="Running", Reason="", readiness=true. Elapsed: 4.846079ms
    Mar 27 15:23:31.748: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-gz7zz" satisfied condition "running"
    Mar 27 15:23:31.749: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-hztxm" in namespace "emptydir-wrapper-8672" to be "running"
    Mar 27 15:23:31.752: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-hztxm": Phase="Running", Reason="", readiness=true. Elapsed: 3.852614ms
    Mar 27 15:23:31.752: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-hztxm" satisfied condition "running"
    Mar 27 15:23:31.753: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-zhwth" in namespace "emptydir-wrapper-8672" to be "running"
    Mar 27 15:23:31.756: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-zhwth": Phase="Running", Reason="", readiness=true. Elapsed: 3.84941ms
    Mar 27 15:23:31.756: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-zhwth" satisfied condition "running"
    Mar 27 15:23:31.756: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-zwq6k" in namespace "emptydir-wrapper-8672" to be "running"
    Mar 27 15:23:31.761: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-zwq6k": Phase="Running", Reason="", readiness=true. Elapsed: 4.464023ms
    Mar 27 15:23:31.761: INFO: Pod "wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550-zwq6k" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550 in namespace emptydir-wrapper-8672, will wait for the garbage collector to delete the pods 03/27/23 15:23:31.761
    Mar 27 15:23:31.824: INFO: Deleting ReplicationController wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550 took: 7.129357ms
    Mar 27 15:23:31.925: INFO: Terminating ReplicationController wrapped-volume-race-f4479abb-66c1-480c-b886-bedd986ea550 pods took: 101.261206ms
    STEP: Creating RC which spawns configmap-volume pods 03/27/23 15:23:35.131
    Mar 27 15:23:35.144: INFO: Pod name wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1: Found 0 pods out of 5
    Mar 27 15:23:40.152: INFO: Pod name wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/27/23 15:23:40.152
    Mar 27 15:23:40.152: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-fvn8f" in namespace "emptydir-wrapper-8672" to be "running"
    Mar 27 15:23:40.155: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-fvn8f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.438061ms
    Mar 27 15:23:42.160: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-fvn8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008242214s
    Mar 27 15:23:44.160: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-fvn8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008474964s
    Mar 27 15:23:46.161: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-fvn8f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00865029s
    Mar 27 15:23:48.161: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-fvn8f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009190671s
    Mar 27 15:23:50.163: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-fvn8f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010596333s
    Mar 27 15:23:52.161: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-fvn8f": Phase="Running", Reason="", readiness=true. Elapsed: 12.009449667s
    Mar 27 15:23:52.161: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-fvn8f" satisfied condition "running"
    Mar 27 15:23:52.161: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-kmwqf" in namespace "emptydir-wrapper-8672" to be "running"
    Mar 27 15:23:52.166: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-kmwqf": Phase="Running", Reason="", readiness=true. Elapsed: 4.092896ms
    Mar 27 15:23:52.166: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-kmwqf" satisfied condition "running"
    Mar 27 15:23:52.166: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-lrb5c" in namespace "emptydir-wrapper-8672" to be "running"
    Mar 27 15:23:52.169: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-lrb5c": Phase="Running", Reason="", readiness=true. Elapsed: 3.212484ms
    Mar 27 15:23:52.169: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-lrb5c" satisfied condition "running"
    Mar 27 15:23:52.169: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-mfglb" in namespace "emptydir-wrapper-8672" to be "running"
    Mar 27 15:23:52.172: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-mfglb": Phase="Running", Reason="", readiness=true. Elapsed: 3.086352ms
    Mar 27 15:23:52.172: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-mfglb" satisfied condition "running"
    Mar 27 15:23:52.172: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-pg7rs" in namespace "emptydir-wrapper-8672" to be "running"
    Mar 27 15:23:52.176: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-pg7rs": Phase="Running", Reason="", readiness=true. Elapsed: 3.646765ms
    Mar 27 15:23:52.176: INFO: Pod "wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1-pg7rs" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1 in namespace emptydir-wrapper-8672, will wait for the garbage collector to delete the pods 03/27/23 15:23:52.176
    Mar 27 15:23:52.237: INFO: Deleting ReplicationController wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1 took: 7.559164ms
    Mar 27 15:23:52.337: INFO: Terminating ReplicationController wrapped-volume-race-a0a8bb91-2395-4876-bb2d-6d15c760cce1 pods took: 100.284906ms
    STEP: Creating RC which spawns configmap-volume pods 03/27/23 15:23:55.842
    Mar 27 15:23:55.856: INFO: Pod name wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428: Found 0 pods out of 5
    Mar 27 15:24:00.865: INFO: Pod name wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/27/23 15:24:00.865
    Mar 27 15:24:00.865: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-6pfvj" in namespace "emptydir-wrapper-8672" to be "running"
    Mar 27 15:24:00.868: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-6pfvj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.23717ms
    Mar 27 15:24:02.879: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-6pfvj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014116814s
    Mar 27 15:24:04.874: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-6pfvj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008944361s
    Mar 27 15:24:06.876: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-6pfvj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010762218s
    Mar 27 15:24:08.876: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-6pfvj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011170566s
    Mar 27 15:24:10.874: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-6pfvj": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009294304s
    Mar 27 15:24:12.874: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-6pfvj": Phase="Running", Reason="", readiness=true. Elapsed: 12.009280072s
    Mar 27 15:24:12.874: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-6pfvj" satisfied condition "running"
    Mar 27 15:24:12.874: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-89cbj" in namespace "emptydir-wrapper-8672" to be "running"
    Mar 27 15:24:12.878: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-89cbj": Phase="Running", Reason="", readiness=true. Elapsed: 3.897478ms
    Mar 27 15:24:12.878: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-89cbj" satisfied condition "running"
    Mar 27 15:24:12.878: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-dwnkq" in namespace "emptydir-wrapper-8672" to be "running"
    Mar 27 15:24:12.882: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-dwnkq": Phase="Running", Reason="", readiness=true. Elapsed: 3.788062ms
    Mar 27 15:24:12.882: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-dwnkq" satisfied condition "running"
    Mar 27 15:24:12.882: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-gcsxj" in namespace "emptydir-wrapper-8672" to be "running"
    Mar 27 15:24:12.885: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-gcsxj": Phase="Running", Reason="", readiness=true. Elapsed: 3.144301ms
    Mar 27 15:24:12.886: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-gcsxj" satisfied condition "running"
    Mar 27 15:24:12.886: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-nsb2s" in namespace "emptydir-wrapper-8672" to be "running"
    Mar 27 15:24:12.889: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-nsb2s": Phase="Running", Reason="", readiness=true. Elapsed: 3.33742ms
    Mar 27 15:24:12.889: INFO: Pod "wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428-nsb2s" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428 in namespace emptydir-wrapper-8672, will wait for the garbage collector to delete the pods 03/27/23 15:24:12.889
    Mar 27 15:24:12.955: INFO: Deleting ReplicationController wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428 took: 10.899543ms
    Mar 27 15:24:13.055: INFO: Terminating ReplicationController wrapped-volume-race-90b9aa77-584e-40ad-9a3a-15d78a4be428 pods took: 100.364319ms
    STEP: Cleaning up the configMaps 03/27/23 15:24:16.656
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Mar 27 15:24:16.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-8672" for this suite. 03/27/23 15:24:16.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:24:16.997
Mar 27 15:24:16.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename secrets 03/27/23 15:24:16.998
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:24:17.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:24:17.017
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 27 15:24:17.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8809" for this suite. 03/27/23 15:24:17.061
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":263,"skipped":4959,"failed":0}
------------------------------
• [0.071 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:24:16.997
    Mar 27 15:24:16.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename secrets 03/27/23 15:24:16.998
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:24:17.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:24:17.017
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 27 15:24:17.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8809" for this suite. 03/27/23 15:24:17.061
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:24:17.071
Mar 27 15:24:17.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename downward-api 03/27/23 15:24:17.072
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:24:17.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:24:17.09
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 03/27/23 15:24:17.093
Mar 27 15:24:17.107: INFO: Waiting up to 5m0s for pod "downwardapi-volume-852b4963-9416-41f2-bd6d-91091b40d39a" in namespace "downward-api-9955" to be "Succeeded or Failed"
Mar 27 15:24:17.112: INFO: Pod "downwardapi-volume-852b4963-9416-41f2-bd6d-91091b40d39a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.096859ms
Mar 27 15:24:19.118: INFO: Pod "downwardapi-volume-852b4963-9416-41f2-bd6d-91091b40d39a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011283988s
Mar 27 15:24:21.117: INFO: Pod "downwardapi-volume-852b4963-9416-41f2-bd6d-91091b40d39a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009782932s
Mar 27 15:24:23.123: INFO: Pod "downwardapi-volume-852b4963-9416-41f2-bd6d-91091b40d39a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016262149s
STEP: Saw pod success 03/27/23 15:24:23.123
Mar 27 15:24:23.123: INFO: Pod "downwardapi-volume-852b4963-9416-41f2-bd6d-91091b40d39a" satisfied condition "Succeeded or Failed"
Mar 27 15:24:23.132: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-852b4963-9416-41f2-bd6d-91091b40d39a container client-container: <nil>
STEP: delete the pod 03/27/23 15:24:23.142
Mar 27 15:24:23.159: INFO: Waiting for pod downwardapi-volume-852b4963-9416-41f2-bd6d-91091b40d39a to disappear
Mar 27 15:24:23.162: INFO: Pod downwardapi-volume-852b4963-9416-41f2-bd6d-91091b40d39a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 27 15:24:23.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9955" for this suite. 03/27/23 15:24:23.168
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":264,"skipped":5031,"failed":0}
------------------------------
• [SLOW TEST] [6.105 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:24:17.071
    Mar 27 15:24:17.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename downward-api 03/27/23 15:24:17.072
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:24:17.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:24:17.09
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 03/27/23 15:24:17.093
    Mar 27 15:24:17.107: INFO: Waiting up to 5m0s for pod "downwardapi-volume-852b4963-9416-41f2-bd6d-91091b40d39a" in namespace "downward-api-9955" to be "Succeeded or Failed"
    Mar 27 15:24:17.112: INFO: Pod "downwardapi-volume-852b4963-9416-41f2-bd6d-91091b40d39a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.096859ms
    Mar 27 15:24:19.118: INFO: Pod "downwardapi-volume-852b4963-9416-41f2-bd6d-91091b40d39a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011283988s
    Mar 27 15:24:21.117: INFO: Pod "downwardapi-volume-852b4963-9416-41f2-bd6d-91091b40d39a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009782932s
    Mar 27 15:24:23.123: INFO: Pod "downwardapi-volume-852b4963-9416-41f2-bd6d-91091b40d39a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016262149s
    STEP: Saw pod success 03/27/23 15:24:23.123
    Mar 27 15:24:23.123: INFO: Pod "downwardapi-volume-852b4963-9416-41f2-bd6d-91091b40d39a" satisfied condition "Succeeded or Failed"
    Mar 27 15:24:23.132: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-852b4963-9416-41f2-bd6d-91091b40d39a container client-container: <nil>
    STEP: delete the pod 03/27/23 15:24:23.142
    Mar 27 15:24:23.159: INFO: Waiting for pod downwardapi-volume-852b4963-9416-41f2-bd6d-91091b40d39a to disappear
    Mar 27 15:24:23.162: INFO: Pod downwardapi-volume-852b4963-9416-41f2-bd6d-91091b40d39a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 27 15:24:23.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9955" for this suite. 03/27/23 15:24:23.168
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:24:23.184
Mar 27 15:24:23.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 15:24:23.185
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:24:23.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:24:23.204
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 03/27/23 15:24:23.207
Mar 27 15:24:23.217: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf2c6932-817b-4f0e-ab5b-d45e87503e14" in namespace "projected-3847" to be "Succeeded or Failed"
Mar 27 15:24:23.223: INFO: Pod "downwardapi-volume-cf2c6932-817b-4f0e-ab5b-d45e87503e14": Phase="Pending", Reason="", readiness=false. Elapsed: 6.537713ms
Mar 27 15:24:25.227: INFO: Pod "downwardapi-volume-cf2c6932-817b-4f0e-ab5b-d45e87503e14": Phase="Running", Reason="", readiness=true. Elapsed: 2.01022682s
Mar 27 15:24:27.228: INFO: Pod "downwardapi-volume-cf2c6932-817b-4f0e-ab5b-d45e87503e14": Phase="Running", Reason="", readiness=false. Elapsed: 4.011609567s
Mar 27 15:24:29.229: INFO: Pod "downwardapi-volume-cf2c6932-817b-4f0e-ab5b-d45e87503e14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012313946s
STEP: Saw pod success 03/27/23 15:24:29.229
Mar 27 15:24:29.229: INFO: Pod "downwardapi-volume-cf2c6932-817b-4f0e-ab5b-d45e87503e14" satisfied condition "Succeeded or Failed"
Mar 27 15:24:29.233: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-cf2c6932-817b-4f0e-ab5b-d45e87503e14 container client-container: <nil>
STEP: delete the pod 03/27/23 15:24:29.24
Mar 27 15:24:29.251: INFO: Waiting for pod downwardapi-volume-cf2c6932-817b-4f0e-ab5b-d45e87503e14 to disappear
Mar 27 15:24:29.253: INFO: Pod downwardapi-volume-cf2c6932-817b-4f0e-ab5b-d45e87503e14 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 27 15:24:29.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3847" for this suite. 03/27/23 15:24:29.265
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":265,"skipped":5094,"failed":0}
------------------------------
• [SLOW TEST] [6.088 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:24:23.184
    Mar 27 15:24:23.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 15:24:23.185
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:24:23.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:24:23.204
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 03/27/23 15:24:23.207
    Mar 27 15:24:23.217: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf2c6932-817b-4f0e-ab5b-d45e87503e14" in namespace "projected-3847" to be "Succeeded or Failed"
    Mar 27 15:24:23.223: INFO: Pod "downwardapi-volume-cf2c6932-817b-4f0e-ab5b-d45e87503e14": Phase="Pending", Reason="", readiness=false. Elapsed: 6.537713ms
    Mar 27 15:24:25.227: INFO: Pod "downwardapi-volume-cf2c6932-817b-4f0e-ab5b-d45e87503e14": Phase="Running", Reason="", readiness=true. Elapsed: 2.01022682s
    Mar 27 15:24:27.228: INFO: Pod "downwardapi-volume-cf2c6932-817b-4f0e-ab5b-d45e87503e14": Phase="Running", Reason="", readiness=false. Elapsed: 4.011609567s
    Mar 27 15:24:29.229: INFO: Pod "downwardapi-volume-cf2c6932-817b-4f0e-ab5b-d45e87503e14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012313946s
    STEP: Saw pod success 03/27/23 15:24:29.229
    Mar 27 15:24:29.229: INFO: Pod "downwardapi-volume-cf2c6932-817b-4f0e-ab5b-d45e87503e14" satisfied condition "Succeeded or Failed"
    Mar 27 15:24:29.233: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-cf2c6932-817b-4f0e-ab5b-d45e87503e14 container client-container: <nil>
    STEP: delete the pod 03/27/23 15:24:29.24
    Mar 27 15:24:29.251: INFO: Waiting for pod downwardapi-volume-cf2c6932-817b-4f0e-ab5b-d45e87503e14 to disappear
    Mar 27 15:24:29.253: INFO: Pod downwardapi-volume-cf2c6932-817b-4f0e-ab5b-d45e87503e14 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 27 15:24:29.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3847" for this suite. 03/27/23 15:24:29.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:24:29.279
Mar 27 15:24:29.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename secrets 03/27/23 15:24:29.28
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:24:29.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:24:29.3
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-dc5462c2-ac96-4acd-baa8-c0ddbaa664e2 03/27/23 15:24:29.304
STEP: Creating a pod to test consume secrets 03/27/23 15:24:29.309
Mar 27 15:24:29.318: INFO: Waiting up to 5m0s for pod "pod-secrets-99c4fea6-cb76-4c2c-8478-d3d4271d3b84" in namespace "secrets-8540" to be "Succeeded or Failed"
Mar 27 15:24:29.321: INFO: Pod "pod-secrets-99c4fea6-cb76-4c2c-8478-d3d4271d3b84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.524802ms
Mar 27 15:24:31.324: INFO: Pod "pod-secrets-99c4fea6-cb76-4c2c-8478-d3d4271d3b84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005700423s
Mar 27 15:24:33.325: INFO: Pod "pod-secrets-99c4fea6-cb76-4c2c-8478-d3d4271d3b84": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006076864s
Mar 27 15:24:35.326: INFO: Pod "pod-secrets-99c4fea6-cb76-4c2c-8478-d3d4271d3b84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007329648s
STEP: Saw pod success 03/27/23 15:24:35.326
Mar 27 15:24:35.326: INFO: Pod "pod-secrets-99c4fea6-cb76-4c2c-8478-d3d4271d3b84" satisfied condition "Succeeded or Failed"
Mar 27 15:24:35.330: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-secrets-99c4fea6-cb76-4c2c-8478-d3d4271d3b84 container secret-volume-test: <nil>
STEP: delete the pod 03/27/23 15:24:35.341
Mar 27 15:24:35.358: INFO: Waiting for pod pod-secrets-99c4fea6-cb76-4c2c-8478-d3d4271d3b84 to disappear
Mar 27 15:24:35.361: INFO: Pod pod-secrets-99c4fea6-cb76-4c2c-8478-d3d4271d3b84 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 27 15:24:35.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8540" for this suite. 03/27/23 15:24:35.365
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":266,"skipped":5171,"failed":0}
------------------------------
• [SLOW TEST] [6.092 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:24:29.279
    Mar 27 15:24:29.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename secrets 03/27/23 15:24:29.28
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:24:29.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:24:29.3
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-dc5462c2-ac96-4acd-baa8-c0ddbaa664e2 03/27/23 15:24:29.304
    STEP: Creating a pod to test consume secrets 03/27/23 15:24:29.309
    Mar 27 15:24:29.318: INFO: Waiting up to 5m0s for pod "pod-secrets-99c4fea6-cb76-4c2c-8478-d3d4271d3b84" in namespace "secrets-8540" to be "Succeeded or Failed"
    Mar 27 15:24:29.321: INFO: Pod "pod-secrets-99c4fea6-cb76-4c2c-8478-d3d4271d3b84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.524802ms
    Mar 27 15:24:31.324: INFO: Pod "pod-secrets-99c4fea6-cb76-4c2c-8478-d3d4271d3b84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005700423s
    Mar 27 15:24:33.325: INFO: Pod "pod-secrets-99c4fea6-cb76-4c2c-8478-d3d4271d3b84": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006076864s
    Mar 27 15:24:35.326: INFO: Pod "pod-secrets-99c4fea6-cb76-4c2c-8478-d3d4271d3b84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007329648s
    STEP: Saw pod success 03/27/23 15:24:35.326
    Mar 27 15:24:35.326: INFO: Pod "pod-secrets-99c4fea6-cb76-4c2c-8478-d3d4271d3b84" satisfied condition "Succeeded or Failed"
    Mar 27 15:24:35.330: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-secrets-99c4fea6-cb76-4c2c-8478-d3d4271d3b84 container secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 15:24:35.341
    Mar 27 15:24:35.358: INFO: Waiting for pod pod-secrets-99c4fea6-cb76-4c2c-8478-d3d4271d3b84 to disappear
    Mar 27 15:24:35.361: INFO: Pod pod-secrets-99c4fea6-cb76-4c2c-8478-d3d4271d3b84 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 27 15:24:35.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8540" for this suite. 03/27/23 15:24:35.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:24:35.373
Mar 27 15:24:35.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename configmap 03/27/23 15:24:35.374
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:24:35.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:24:35.395
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-f1920639-0793-407b-9413-bca68c1538bd 03/27/23 15:24:35.399
STEP: Creating a pod to test consume configMaps 03/27/23 15:24:35.413
Mar 27 15:24:35.425: INFO: Waiting up to 5m0s for pod "pod-configmaps-90053e6e-ce5a-416d-9054-6ff25febd5a1" in namespace "configmap-7909" to be "Succeeded or Failed"
Mar 27 15:24:35.432: INFO: Pod "pod-configmaps-90053e6e-ce5a-416d-9054-6ff25febd5a1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.525438ms
Mar 27 15:24:37.436: INFO: Pod "pod-configmaps-90053e6e-ce5a-416d-9054-6ff25febd5a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01045373s
Mar 27 15:24:39.598: INFO: Pod "pod-configmaps-90053e6e-ce5a-416d-9054-6ff25febd5a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.172545416s
STEP: Saw pod success 03/27/23 15:24:39.598
Mar 27 15:24:39.598: INFO: Pod "pod-configmaps-90053e6e-ce5a-416d-9054-6ff25febd5a1" satisfied condition "Succeeded or Failed"
Mar 27 15:24:39.601: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-configmaps-90053e6e-ce5a-416d-9054-6ff25febd5a1 container configmap-volume-test: <nil>
STEP: delete the pod 03/27/23 15:24:39.69
Mar 27 15:24:39.708: INFO: Waiting for pod pod-configmaps-90053e6e-ce5a-416d-9054-6ff25febd5a1 to disappear
Mar 27 15:24:39.787: INFO: Pod pod-configmaps-90053e6e-ce5a-416d-9054-6ff25febd5a1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 27 15:24:39.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7909" for this suite. 03/27/23 15:24:39.792
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":267,"skipped":5195,"failed":0}
------------------------------
• [4.431 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:24:35.373
    Mar 27 15:24:35.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename configmap 03/27/23 15:24:35.374
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:24:35.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:24:35.395
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-f1920639-0793-407b-9413-bca68c1538bd 03/27/23 15:24:35.399
    STEP: Creating a pod to test consume configMaps 03/27/23 15:24:35.413
    Mar 27 15:24:35.425: INFO: Waiting up to 5m0s for pod "pod-configmaps-90053e6e-ce5a-416d-9054-6ff25febd5a1" in namespace "configmap-7909" to be "Succeeded or Failed"
    Mar 27 15:24:35.432: INFO: Pod "pod-configmaps-90053e6e-ce5a-416d-9054-6ff25febd5a1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.525438ms
    Mar 27 15:24:37.436: INFO: Pod "pod-configmaps-90053e6e-ce5a-416d-9054-6ff25febd5a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01045373s
    Mar 27 15:24:39.598: INFO: Pod "pod-configmaps-90053e6e-ce5a-416d-9054-6ff25febd5a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.172545416s
    STEP: Saw pod success 03/27/23 15:24:39.598
    Mar 27 15:24:39.598: INFO: Pod "pod-configmaps-90053e6e-ce5a-416d-9054-6ff25febd5a1" satisfied condition "Succeeded or Failed"
    Mar 27 15:24:39.601: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-configmaps-90053e6e-ce5a-416d-9054-6ff25febd5a1 container configmap-volume-test: <nil>
    STEP: delete the pod 03/27/23 15:24:39.69
    Mar 27 15:24:39.708: INFO: Waiting for pod pod-configmaps-90053e6e-ce5a-416d-9054-6ff25febd5a1 to disappear
    Mar 27 15:24:39.787: INFO: Pod pod-configmaps-90053e6e-ce5a-416d-9054-6ff25febd5a1 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 27 15:24:39.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7909" for this suite. 03/27/23 15:24:39.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:24:39.807
Mar 27 15:24:39.807: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename replicaset 03/27/23 15:24:39.808
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:24:39.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:24:39.828
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Mar 27 15:24:39.830: INFO: Creating ReplicaSet my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652
Mar 27 15:24:39.841: INFO: Pod name my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652: Found 0 pods out of 1
Mar 27 15:24:44.844: INFO: Pod name my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652: Found 1 pods out of 1
Mar 27 15:24:44.845: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652" is running
Mar 27 15:24:44.845: INFO: Waiting up to 5m0s for pod "my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652-q2kt9" in namespace "replicaset-4169" to be "running"
Mar 27 15:24:44.849: INFO: Pod "my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652-q2kt9": Phase="Running", Reason="", readiness=true. Elapsed: 4.280439ms
Mar 27 15:24:44.849: INFO: Pod "my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652-q2kt9" satisfied condition "running"
Mar 27 15:24:44.849: INFO: Pod "my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652-q2kt9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 15:24:39 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 15:24:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 15:24:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 15:24:39 +0000 UTC Reason: Message:}])
Mar 27 15:24:44.849: INFO: Trying to dial the pod
Mar 27 15:24:49.863: INFO: Controller my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652: Got expected result from replica 1 [my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652-q2kt9]: "my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652-q2kt9", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar 27 15:24:49.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4169" for this suite. 03/27/23 15:24:49.87
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":268,"skipped":5247,"failed":0}
------------------------------
• [SLOW TEST] [10.069 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:24:39.807
    Mar 27 15:24:39.807: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename replicaset 03/27/23 15:24:39.808
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:24:39.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:24:39.828
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Mar 27 15:24:39.830: INFO: Creating ReplicaSet my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652
    Mar 27 15:24:39.841: INFO: Pod name my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652: Found 0 pods out of 1
    Mar 27 15:24:44.844: INFO: Pod name my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652: Found 1 pods out of 1
    Mar 27 15:24:44.845: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652" is running
    Mar 27 15:24:44.845: INFO: Waiting up to 5m0s for pod "my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652-q2kt9" in namespace "replicaset-4169" to be "running"
    Mar 27 15:24:44.849: INFO: Pod "my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652-q2kt9": Phase="Running", Reason="", readiness=true. Elapsed: 4.280439ms
    Mar 27 15:24:44.849: INFO: Pod "my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652-q2kt9" satisfied condition "running"
    Mar 27 15:24:44.849: INFO: Pod "my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652-q2kt9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 15:24:39 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 15:24:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 15:24:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 15:24:39 +0000 UTC Reason: Message:}])
    Mar 27 15:24:44.849: INFO: Trying to dial the pod
    Mar 27 15:24:49.863: INFO: Controller my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652: Got expected result from replica 1 [my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652-q2kt9]: "my-hostname-basic-de6bcd12-5f8a-4128-a1be-7e7714ee0652-q2kt9", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar 27 15:24:49.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4169" for this suite. 03/27/23 15:24:49.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:24:49.876
Mar 27 15:24:49.877: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename resourcequota 03/27/23 15:24:49.878
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:24:49.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:24:49.897
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 03/27/23 15:24:49.899
STEP: Creating a ResourceQuota 03/27/23 15:24:54.907
STEP: Ensuring resource quota status is calculated 03/27/23 15:24:54.913
STEP: Creating a ReplicaSet 03/27/23 15:24:56.917
STEP: Ensuring resource quota status captures replicaset creation 03/27/23 15:24:56.93
STEP: Deleting a ReplicaSet 03/27/23 15:24:58.943
STEP: Ensuring resource quota status released usage 03/27/23 15:24:58.949
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 27 15:25:00.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-474" for this suite. 03/27/23 15:25:00.96
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":269,"skipped":5255,"failed":0}
------------------------------
• [SLOW TEST] [11.090 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:24:49.876
    Mar 27 15:24:49.877: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename resourcequota 03/27/23 15:24:49.878
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:24:49.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:24:49.897
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 03/27/23 15:24:49.899
    STEP: Creating a ResourceQuota 03/27/23 15:24:54.907
    STEP: Ensuring resource quota status is calculated 03/27/23 15:24:54.913
    STEP: Creating a ReplicaSet 03/27/23 15:24:56.917
    STEP: Ensuring resource quota status captures replicaset creation 03/27/23 15:24:56.93
    STEP: Deleting a ReplicaSet 03/27/23 15:24:58.943
    STEP: Ensuring resource quota status released usage 03/27/23 15:24:58.949
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 27 15:25:00.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-474" for this suite. 03/27/23 15:25:00.96
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:25:00.969
Mar 27 15:25:00.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 15:25:00.97
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:25:00.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:25:00.988
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-2cc5543a-6a44-4251-af50-4dd1bfc86285 03/27/23 15:25:00.995
STEP: Creating a pod to test consume secrets 03/27/23 15:25:01.002
Mar 27 15:25:01.012: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5fe334a3-0858-4a0b-9bfc-de41ddc380a9" in namespace "projected-7429" to be "Succeeded or Failed"
Mar 27 15:25:01.015: INFO: Pod "pod-projected-secrets-5fe334a3-0858-4a0b-9bfc-de41ddc380a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.990106ms
Mar 27 15:25:03.024: INFO: Pod "pod-projected-secrets-5fe334a3-0858-4a0b-9bfc-de41ddc380a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.011242507s
Mar 27 15:25:05.019: INFO: Pod "pod-projected-secrets-5fe334a3-0858-4a0b-9bfc-de41ddc380a9": Phase="Running", Reason="", readiness=false. Elapsed: 4.006475127s
Mar 27 15:25:07.020: INFO: Pod "pod-projected-secrets-5fe334a3-0858-4a0b-9bfc-de41ddc380a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007215369s
STEP: Saw pod success 03/27/23 15:25:07.02
Mar 27 15:25:07.020: INFO: Pod "pod-projected-secrets-5fe334a3-0858-4a0b-9bfc-de41ddc380a9" satisfied condition "Succeeded or Failed"
Mar 27 15:25:07.143: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-secrets-5fe334a3-0858-4a0b-9bfc-de41ddc380a9 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/27/23 15:25:07.156
Mar 27 15:25:07.412: INFO: Waiting for pod pod-projected-secrets-5fe334a3-0858-4a0b-9bfc-de41ddc380a9 to disappear
Mar 27 15:25:07.418: INFO: Pod pod-projected-secrets-5fe334a3-0858-4a0b-9bfc-de41ddc380a9 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 27 15:25:07.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7429" for this suite. 03/27/23 15:25:07.423
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":270,"skipped":5257,"failed":0}
------------------------------
• [SLOW TEST] [6.699 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:25:00.969
    Mar 27 15:25:00.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 15:25:00.97
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:25:00.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:25:00.988
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-2cc5543a-6a44-4251-af50-4dd1bfc86285 03/27/23 15:25:00.995
    STEP: Creating a pod to test consume secrets 03/27/23 15:25:01.002
    Mar 27 15:25:01.012: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5fe334a3-0858-4a0b-9bfc-de41ddc380a9" in namespace "projected-7429" to be "Succeeded or Failed"
    Mar 27 15:25:01.015: INFO: Pod "pod-projected-secrets-5fe334a3-0858-4a0b-9bfc-de41ddc380a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.990106ms
    Mar 27 15:25:03.024: INFO: Pod "pod-projected-secrets-5fe334a3-0858-4a0b-9bfc-de41ddc380a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.011242507s
    Mar 27 15:25:05.019: INFO: Pod "pod-projected-secrets-5fe334a3-0858-4a0b-9bfc-de41ddc380a9": Phase="Running", Reason="", readiness=false. Elapsed: 4.006475127s
    Mar 27 15:25:07.020: INFO: Pod "pod-projected-secrets-5fe334a3-0858-4a0b-9bfc-de41ddc380a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007215369s
    STEP: Saw pod success 03/27/23 15:25:07.02
    Mar 27 15:25:07.020: INFO: Pod "pod-projected-secrets-5fe334a3-0858-4a0b-9bfc-de41ddc380a9" satisfied condition "Succeeded or Failed"
    Mar 27 15:25:07.143: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-secrets-5fe334a3-0858-4a0b-9bfc-de41ddc380a9 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 15:25:07.156
    Mar 27 15:25:07.412: INFO: Waiting for pod pod-projected-secrets-5fe334a3-0858-4a0b-9bfc-de41ddc380a9 to disappear
    Mar 27 15:25:07.418: INFO: Pod pod-projected-secrets-5fe334a3-0858-4a0b-9bfc-de41ddc380a9 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 27 15:25:07.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7429" for this suite. 03/27/23 15:25:07.423
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:25:07.669
Mar 27 15:25:07.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename discovery 03/27/23 15:25:07.67
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:25:07.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:25:07.703
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 03/27/23 15:25:07.948
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Mar 27 15:25:08.463: INFO: Checking APIGroup: apiregistration.k8s.io
Mar 27 15:25:08.464: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar 27 15:25:08.464: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Mar 27 15:25:08.464: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar 27 15:25:08.464: INFO: Checking APIGroup: apps
Mar 27 15:25:08.466: INFO: PreferredVersion.GroupVersion: apps/v1
Mar 27 15:25:08.466: INFO: Versions found [{apps/v1 v1}]
Mar 27 15:25:08.466: INFO: apps/v1 matches apps/v1
Mar 27 15:25:08.466: INFO: Checking APIGroup: events.k8s.io
Mar 27 15:25:08.467: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar 27 15:25:08.467: INFO: Versions found [{events.k8s.io/v1 v1}]
Mar 27 15:25:08.467: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar 27 15:25:08.467: INFO: Checking APIGroup: authentication.k8s.io
Mar 27 15:25:08.469: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar 27 15:25:08.469: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Mar 27 15:25:08.469: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar 27 15:25:08.469: INFO: Checking APIGroup: authorization.k8s.io
Mar 27 15:25:08.470: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar 27 15:25:08.470: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Mar 27 15:25:08.470: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar 27 15:25:08.470: INFO: Checking APIGroup: autoscaling
Mar 27 15:25:08.472: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Mar 27 15:25:08.472: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Mar 27 15:25:08.472: INFO: autoscaling/v2 matches autoscaling/v2
Mar 27 15:25:08.472: INFO: Checking APIGroup: batch
Mar 27 15:25:08.473: INFO: PreferredVersion.GroupVersion: batch/v1
Mar 27 15:25:08.473: INFO: Versions found [{batch/v1 v1}]
Mar 27 15:25:08.473: INFO: batch/v1 matches batch/v1
Mar 27 15:25:08.473: INFO: Checking APIGroup: certificates.k8s.io
Mar 27 15:25:08.474: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar 27 15:25:08.474: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Mar 27 15:25:08.474: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar 27 15:25:08.474: INFO: Checking APIGroup: networking.k8s.io
Mar 27 15:25:08.475: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar 27 15:25:08.475: INFO: Versions found [{networking.k8s.io/v1 v1}]
Mar 27 15:25:08.475: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar 27 15:25:08.475: INFO: Checking APIGroup: policy
Mar 27 15:25:08.476: INFO: PreferredVersion.GroupVersion: policy/v1
Mar 27 15:25:08.476: INFO: Versions found [{policy/v1 v1}]
Mar 27 15:25:08.476: INFO: policy/v1 matches policy/v1
Mar 27 15:25:08.476: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar 27 15:25:08.477: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar 27 15:25:08.477: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Mar 27 15:25:08.477: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar 27 15:25:08.477: INFO: Checking APIGroup: storage.k8s.io
Mar 27 15:25:08.478: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar 27 15:25:08.478: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar 27 15:25:08.478: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar 27 15:25:08.478: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar 27 15:25:08.479: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar 27 15:25:08.479: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Mar 27 15:25:08.479: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar 27 15:25:08.479: INFO: Checking APIGroup: apiextensions.k8s.io
Mar 27 15:25:08.480: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar 27 15:25:08.480: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Mar 27 15:25:08.480: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar 27 15:25:08.480: INFO: Checking APIGroup: scheduling.k8s.io
Mar 27 15:25:08.482: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar 27 15:25:08.482: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Mar 27 15:25:08.482: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar 27 15:25:08.482: INFO: Checking APIGroup: coordination.k8s.io
Mar 27 15:25:08.484: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar 27 15:25:08.484: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Mar 27 15:25:08.484: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar 27 15:25:08.484: INFO: Checking APIGroup: node.k8s.io
Mar 27 15:25:08.485: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar 27 15:25:08.485: INFO: Versions found [{node.k8s.io/v1 v1}]
Mar 27 15:25:08.485: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar 27 15:25:08.485: INFO: Checking APIGroup: discovery.k8s.io
Mar 27 15:25:08.487: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Mar 27 15:25:08.487: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Mar 27 15:25:08.487: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Mar 27 15:25:08.487: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar 27 15:25:08.488: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Mar 27 15:25:08.488: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Mar 27 15:25:08.488: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Mar 27 15:25:08.488: INFO: Checking APIGroup: crd.projectcalico.org
Mar 27 15:25:08.490: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Mar 27 15:25:08.490: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Mar 27 15:25:08.490: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Mar 27 15:25:08.490: INFO: Checking APIGroup: cluster.k8s.io
Mar 27 15:25:08.491: INFO: PreferredVersion.GroupVersion: cluster.k8s.io/v1alpha1
Mar 27 15:25:08.491: INFO: Versions found [{cluster.k8s.io/v1alpha1 v1alpha1}]
Mar 27 15:25:08.491: INFO: cluster.k8s.io/v1alpha1 matches cluster.k8s.io/v1alpha1
Mar 27 15:25:08.491: INFO: Checking APIGroup: metrics.k8s.io
Mar 27 15:25:08.492: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Mar 27 15:25:08.492: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Mar 27 15:25:08.492: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Mar 27 15:25:08.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-4597" for this suite. 03/27/23 15:25:08.497
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":271,"skipped":5273,"failed":0}
------------------------------
• [0.932 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:25:07.669
    Mar 27 15:25:07.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename discovery 03/27/23 15:25:07.67
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:25:07.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:25:07.703
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 03/27/23 15:25:07.948
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Mar 27 15:25:08.463: INFO: Checking APIGroup: apiregistration.k8s.io
    Mar 27 15:25:08.464: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Mar 27 15:25:08.464: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Mar 27 15:25:08.464: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Mar 27 15:25:08.464: INFO: Checking APIGroup: apps
    Mar 27 15:25:08.466: INFO: PreferredVersion.GroupVersion: apps/v1
    Mar 27 15:25:08.466: INFO: Versions found [{apps/v1 v1}]
    Mar 27 15:25:08.466: INFO: apps/v1 matches apps/v1
    Mar 27 15:25:08.466: INFO: Checking APIGroup: events.k8s.io
    Mar 27 15:25:08.467: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Mar 27 15:25:08.467: INFO: Versions found [{events.k8s.io/v1 v1}]
    Mar 27 15:25:08.467: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Mar 27 15:25:08.467: INFO: Checking APIGroup: authentication.k8s.io
    Mar 27 15:25:08.469: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Mar 27 15:25:08.469: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Mar 27 15:25:08.469: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Mar 27 15:25:08.469: INFO: Checking APIGroup: authorization.k8s.io
    Mar 27 15:25:08.470: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Mar 27 15:25:08.470: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Mar 27 15:25:08.470: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Mar 27 15:25:08.470: INFO: Checking APIGroup: autoscaling
    Mar 27 15:25:08.472: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Mar 27 15:25:08.472: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Mar 27 15:25:08.472: INFO: autoscaling/v2 matches autoscaling/v2
    Mar 27 15:25:08.472: INFO: Checking APIGroup: batch
    Mar 27 15:25:08.473: INFO: PreferredVersion.GroupVersion: batch/v1
    Mar 27 15:25:08.473: INFO: Versions found [{batch/v1 v1}]
    Mar 27 15:25:08.473: INFO: batch/v1 matches batch/v1
    Mar 27 15:25:08.473: INFO: Checking APIGroup: certificates.k8s.io
    Mar 27 15:25:08.474: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Mar 27 15:25:08.474: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Mar 27 15:25:08.474: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Mar 27 15:25:08.474: INFO: Checking APIGroup: networking.k8s.io
    Mar 27 15:25:08.475: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Mar 27 15:25:08.475: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Mar 27 15:25:08.475: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Mar 27 15:25:08.475: INFO: Checking APIGroup: policy
    Mar 27 15:25:08.476: INFO: PreferredVersion.GroupVersion: policy/v1
    Mar 27 15:25:08.476: INFO: Versions found [{policy/v1 v1}]
    Mar 27 15:25:08.476: INFO: policy/v1 matches policy/v1
    Mar 27 15:25:08.476: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Mar 27 15:25:08.477: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Mar 27 15:25:08.477: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Mar 27 15:25:08.477: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Mar 27 15:25:08.477: INFO: Checking APIGroup: storage.k8s.io
    Mar 27 15:25:08.478: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Mar 27 15:25:08.478: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Mar 27 15:25:08.478: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Mar 27 15:25:08.478: INFO: Checking APIGroup: admissionregistration.k8s.io
    Mar 27 15:25:08.479: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Mar 27 15:25:08.479: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Mar 27 15:25:08.479: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Mar 27 15:25:08.479: INFO: Checking APIGroup: apiextensions.k8s.io
    Mar 27 15:25:08.480: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Mar 27 15:25:08.480: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Mar 27 15:25:08.480: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Mar 27 15:25:08.480: INFO: Checking APIGroup: scheduling.k8s.io
    Mar 27 15:25:08.482: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Mar 27 15:25:08.482: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Mar 27 15:25:08.482: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Mar 27 15:25:08.482: INFO: Checking APIGroup: coordination.k8s.io
    Mar 27 15:25:08.484: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Mar 27 15:25:08.484: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Mar 27 15:25:08.484: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Mar 27 15:25:08.484: INFO: Checking APIGroup: node.k8s.io
    Mar 27 15:25:08.485: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Mar 27 15:25:08.485: INFO: Versions found [{node.k8s.io/v1 v1}]
    Mar 27 15:25:08.485: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Mar 27 15:25:08.485: INFO: Checking APIGroup: discovery.k8s.io
    Mar 27 15:25:08.487: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Mar 27 15:25:08.487: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Mar 27 15:25:08.487: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Mar 27 15:25:08.487: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Mar 27 15:25:08.488: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Mar 27 15:25:08.488: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Mar 27 15:25:08.488: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Mar 27 15:25:08.488: INFO: Checking APIGroup: crd.projectcalico.org
    Mar 27 15:25:08.490: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Mar 27 15:25:08.490: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Mar 27 15:25:08.490: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Mar 27 15:25:08.490: INFO: Checking APIGroup: cluster.k8s.io
    Mar 27 15:25:08.491: INFO: PreferredVersion.GroupVersion: cluster.k8s.io/v1alpha1
    Mar 27 15:25:08.491: INFO: Versions found [{cluster.k8s.io/v1alpha1 v1alpha1}]
    Mar 27 15:25:08.491: INFO: cluster.k8s.io/v1alpha1 matches cluster.k8s.io/v1alpha1
    Mar 27 15:25:08.491: INFO: Checking APIGroup: metrics.k8s.io
    Mar 27 15:25:08.492: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Mar 27 15:25:08.492: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Mar 27 15:25:08.492: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Mar 27 15:25:08.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-4597" for this suite. 03/27/23 15:25:08.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:25:08.602
Mar 27 15:25:08.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename dns 03/27/23 15:25:08.603
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:25:08.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:25:08.835
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 03/27/23 15:25:08.839
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4106.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4106.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4106.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4106.svc.cluster.local;sleep 1; done
 03/27/23 15:25:09.248
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4106.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4106.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4106.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4106.svc.cluster.local;sleep 1; done
 03/27/23 15:25:09.248
STEP: creating a pod to probe DNS 03/27/23 15:25:09.248
STEP: submitting the pod to kubernetes 03/27/23 15:25:09.248
Mar 27 15:25:09.272: INFO: Waiting up to 15m0s for pod "dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29" in namespace "dns-4106" to be "running"
Mar 27 15:25:09.276: INFO: Pod "dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29": Phase="Pending", Reason="", readiness=false. Elapsed: 4.568669ms
Mar 27 15:25:11.292: INFO: Pod "dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020280195s
Mar 27 15:25:13.281: INFO: Pod "dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29": Phase="Running", Reason="", readiness=true. Elapsed: 4.009148962s
Mar 27 15:25:13.281: INFO: Pod "dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29" satisfied condition "running"
STEP: retrieving the pod 03/27/23 15:25:13.281
STEP: looking for the results for each expected name from probers 03/27/23 15:25:13.285
Mar 27 15:25:13.293: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local from pod dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29: the server could not find the requested resource (get pods dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29)
Mar 27 15:25:13.298: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local from pod dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29: the server could not find the requested resource (get pods dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29)
Mar 27 15:25:13.304: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4106.svc.cluster.local from pod dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29: the server could not find the requested resource (get pods dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29)
Mar 27 15:25:13.310: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4106.svc.cluster.local from pod dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29: the server could not find the requested resource (get pods dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29)
Mar 27 15:25:13.318: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local from pod dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29: the server could not find the requested resource (get pods dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29)
Mar 27 15:25:13.324: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local from pod dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29: the server could not find the requested resource (get pods dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29)
Mar 27 15:25:13.329: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4106.svc.cluster.local from pod dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29: the server could not find the requested resource (get pods dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29)
Mar 27 15:25:13.333: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4106.svc.cluster.local from pod dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29: the server could not find the requested resource (get pods dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29)
Mar 27 15:25:13.333: INFO: Lookups using dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4106.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4106.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local jessie_udp@dns-test-service-2.dns-4106.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4106.svc.cluster.local]

Mar 27 15:25:18.384: INFO: DNS probes using dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29 succeeded

STEP: deleting the pod 03/27/23 15:25:18.384
STEP: deleting the test headless service 03/27/23 15:25:18.399
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 27 15:25:18.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4106" for this suite. 03/27/23 15:25:18.417
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":272,"skipped":5282,"failed":0}
------------------------------
• [SLOW TEST] [9.824 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:25:08.602
    Mar 27 15:25:08.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename dns 03/27/23 15:25:08.603
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:25:08.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:25:08.835
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 03/27/23 15:25:08.839
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4106.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4106.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4106.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4106.svc.cluster.local;sleep 1; done
     03/27/23 15:25:09.248
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4106.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4106.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4106.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4106.svc.cluster.local;sleep 1; done
     03/27/23 15:25:09.248
    STEP: creating a pod to probe DNS 03/27/23 15:25:09.248
    STEP: submitting the pod to kubernetes 03/27/23 15:25:09.248
    Mar 27 15:25:09.272: INFO: Waiting up to 15m0s for pod "dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29" in namespace "dns-4106" to be "running"
    Mar 27 15:25:09.276: INFO: Pod "dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29": Phase="Pending", Reason="", readiness=false. Elapsed: 4.568669ms
    Mar 27 15:25:11.292: INFO: Pod "dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020280195s
    Mar 27 15:25:13.281: INFO: Pod "dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29": Phase="Running", Reason="", readiness=true. Elapsed: 4.009148962s
    Mar 27 15:25:13.281: INFO: Pod "dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29" satisfied condition "running"
    STEP: retrieving the pod 03/27/23 15:25:13.281
    STEP: looking for the results for each expected name from probers 03/27/23 15:25:13.285
    Mar 27 15:25:13.293: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local from pod dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29: the server could not find the requested resource (get pods dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29)
    Mar 27 15:25:13.298: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local from pod dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29: the server could not find the requested resource (get pods dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29)
    Mar 27 15:25:13.304: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4106.svc.cluster.local from pod dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29: the server could not find the requested resource (get pods dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29)
    Mar 27 15:25:13.310: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4106.svc.cluster.local from pod dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29: the server could not find the requested resource (get pods dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29)
    Mar 27 15:25:13.318: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local from pod dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29: the server could not find the requested resource (get pods dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29)
    Mar 27 15:25:13.324: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local from pod dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29: the server could not find the requested resource (get pods dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29)
    Mar 27 15:25:13.329: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4106.svc.cluster.local from pod dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29: the server could not find the requested resource (get pods dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29)
    Mar 27 15:25:13.333: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4106.svc.cluster.local from pod dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29: the server could not find the requested resource (get pods dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29)
    Mar 27 15:25:13.333: INFO: Lookups using dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4106.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4106.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4106.svc.cluster.local jessie_udp@dns-test-service-2.dns-4106.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4106.svc.cluster.local]

    Mar 27 15:25:18.384: INFO: DNS probes using dns-4106/dns-test-68743b10-8ef4-46ff-ae31-616b05f0dc29 succeeded

    STEP: deleting the pod 03/27/23 15:25:18.384
    STEP: deleting the test headless service 03/27/23 15:25:18.399
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 27 15:25:18.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4106" for this suite. 03/27/23 15:25:18.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:25:18.429
Mar 27 15:25:18.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 15:25:18.43
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:25:18.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:25:18.45
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-c2b88dd1-c8be-412c-858f-c538d0c1f1f8 03/27/23 15:25:18.458
STEP: Creating the pod 03/27/23 15:25:18.462
Mar 27 15:25:18.475: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c8fc67de-fe70-4486-a2b2-02fe31b39e0b" in namespace "projected-2808" to be "running and ready"
Mar 27 15:25:18.478: INFO: Pod "pod-projected-configmaps-c8fc67de-fe70-4486-a2b2-02fe31b39e0b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.589945ms
Mar 27 15:25:18.478: INFO: The phase of Pod pod-projected-configmaps-c8fc67de-fe70-4486-a2b2-02fe31b39e0b is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:25:20.483: INFO: Pod "pod-projected-configmaps-c8fc67de-fe70-4486-a2b2-02fe31b39e0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007930695s
Mar 27 15:25:20.483: INFO: The phase of Pod pod-projected-configmaps-c8fc67de-fe70-4486-a2b2-02fe31b39e0b is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:25:22.482: INFO: Pod "pod-projected-configmaps-c8fc67de-fe70-4486-a2b2-02fe31b39e0b": Phase="Running", Reason="", readiness=true. Elapsed: 4.007199489s
Mar 27 15:25:22.482: INFO: The phase of Pod pod-projected-configmaps-c8fc67de-fe70-4486-a2b2-02fe31b39e0b is Running (Ready = true)
Mar 27 15:25:22.482: INFO: Pod "pod-projected-configmaps-c8fc67de-fe70-4486-a2b2-02fe31b39e0b" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-c2b88dd1-c8be-412c-858f-c538d0c1f1f8 03/27/23 15:25:22.494
STEP: waiting to observe update in volume 03/27/23 15:25:22.499
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 27 15:26:25.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2808" for this suite. 03/27/23 15:26:25.55
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":273,"skipped":5314,"failed":0}
------------------------------
• [SLOW TEST] [67.130 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:25:18.429
    Mar 27 15:25:18.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 15:25:18.43
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:25:18.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:25:18.45
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-c2b88dd1-c8be-412c-858f-c538d0c1f1f8 03/27/23 15:25:18.458
    STEP: Creating the pod 03/27/23 15:25:18.462
    Mar 27 15:25:18.475: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c8fc67de-fe70-4486-a2b2-02fe31b39e0b" in namespace "projected-2808" to be "running and ready"
    Mar 27 15:25:18.478: INFO: Pod "pod-projected-configmaps-c8fc67de-fe70-4486-a2b2-02fe31b39e0b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.589945ms
    Mar 27 15:25:18.478: INFO: The phase of Pod pod-projected-configmaps-c8fc67de-fe70-4486-a2b2-02fe31b39e0b is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:25:20.483: INFO: Pod "pod-projected-configmaps-c8fc67de-fe70-4486-a2b2-02fe31b39e0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007930695s
    Mar 27 15:25:20.483: INFO: The phase of Pod pod-projected-configmaps-c8fc67de-fe70-4486-a2b2-02fe31b39e0b is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:25:22.482: INFO: Pod "pod-projected-configmaps-c8fc67de-fe70-4486-a2b2-02fe31b39e0b": Phase="Running", Reason="", readiness=true. Elapsed: 4.007199489s
    Mar 27 15:25:22.482: INFO: The phase of Pod pod-projected-configmaps-c8fc67de-fe70-4486-a2b2-02fe31b39e0b is Running (Ready = true)
    Mar 27 15:25:22.482: INFO: Pod "pod-projected-configmaps-c8fc67de-fe70-4486-a2b2-02fe31b39e0b" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-c2b88dd1-c8be-412c-858f-c538d0c1f1f8 03/27/23 15:25:22.494
    STEP: waiting to observe update in volume 03/27/23 15:25:22.499
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 27 15:26:25.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2808" for this suite. 03/27/23 15:26:25.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:26:25.566
Mar 27 15:26:25.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 15:26:25.567
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:26:25.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:26:25.585
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 03/27/23 15:26:25.588
Mar 27 15:26:25.599: INFO: Waiting up to 5m0s for pod "downwardapi-volume-346a7770-889e-459a-9b7f-c1a15c02aa12" in namespace "projected-3104" to be "Succeeded or Failed"
Mar 27 15:26:25.606: INFO: Pod "downwardapi-volume-346a7770-889e-459a-9b7f-c1a15c02aa12": Phase="Pending", Reason="", readiness=false. Elapsed: 7.551078ms
Mar 27 15:26:27.611: INFO: Pod "downwardapi-volume-346a7770-889e-459a-9b7f-c1a15c02aa12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0124731s
Mar 27 15:26:29.611: INFO: Pod "downwardapi-volume-346a7770-889e-459a-9b7f-c1a15c02aa12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011920179s
Mar 27 15:26:31.612: INFO: Pod "downwardapi-volume-346a7770-889e-459a-9b7f-c1a15c02aa12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012759144s
STEP: Saw pod success 03/27/23 15:26:31.612
Mar 27 15:26:31.612: INFO: Pod "downwardapi-volume-346a7770-889e-459a-9b7f-c1a15c02aa12" satisfied condition "Succeeded or Failed"
Mar 27 15:26:31.614: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-346a7770-889e-459a-9b7f-c1a15c02aa12 container client-container: <nil>
STEP: delete the pod 03/27/23 15:26:31.622
Mar 27 15:26:31.636: INFO: Waiting for pod downwardapi-volume-346a7770-889e-459a-9b7f-c1a15c02aa12 to disappear
Mar 27 15:26:31.638: INFO: Pod downwardapi-volume-346a7770-889e-459a-9b7f-c1a15c02aa12 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 27 15:26:31.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3104" for this suite. 03/27/23 15:26:31.643
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":274,"skipped":5329,"failed":0}
------------------------------
• [SLOW TEST] [6.084 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:26:25.566
    Mar 27 15:26:25.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 15:26:25.567
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:26:25.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:26:25.585
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 03/27/23 15:26:25.588
    Mar 27 15:26:25.599: INFO: Waiting up to 5m0s for pod "downwardapi-volume-346a7770-889e-459a-9b7f-c1a15c02aa12" in namespace "projected-3104" to be "Succeeded or Failed"
    Mar 27 15:26:25.606: INFO: Pod "downwardapi-volume-346a7770-889e-459a-9b7f-c1a15c02aa12": Phase="Pending", Reason="", readiness=false. Elapsed: 7.551078ms
    Mar 27 15:26:27.611: INFO: Pod "downwardapi-volume-346a7770-889e-459a-9b7f-c1a15c02aa12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0124731s
    Mar 27 15:26:29.611: INFO: Pod "downwardapi-volume-346a7770-889e-459a-9b7f-c1a15c02aa12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011920179s
    Mar 27 15:26:31.612: INFO: Pod "downwardapi-volume-346a7770-889e-459a-9b7f-c1a15c02aa12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012759144s
    STEP: Saw pod success 03/27/23 15:26:31.612
    Mar 27 15:26:31.612: INFO: Pod "downwardapi-volume-346a7770-889e-459a-9b7f-c1a15c02aa12" satisfied condition "Succeeded or Failed"
    Mar 27 15:26:31.614: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-346a7770-889e-459a-9b7f-c1a15c02aa12 container client-container: <nil>
    STEP: delete the pod 03/27/23 15:26:31.622
    Mar 27 15:26:31.636: INFO: Waiting for pod downwardapi-volume-346a7770-889e-459a-9b7f-c1a15c02aa12 to disappear
    Mar 27 15:26:31.638: INFO: Pod downwardapi-volume-346a7770-889e-459a-9b7f-c1a15c02aa12 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 27 15:26:31.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3104" for this suite. 03/27/23 15:26:31.643
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:26:31.65
Mar 27 15:26:31.650: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename pods 03/27/23 15:26:31.651
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:26:31.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:26:31.68
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Mar 27 15:26:31.693: INFO: Waiting up to 5m0s for pod "server-envvars-b613a045-7c02-4b01-9f73-fbdd8481350f" in namespace "pods-5682" to be "running and ready"
Mar 27 15:26:31.696: INFO: Pod "server-envvars-b613a045-7c02-4b01-9f73-fbdd8481350f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.671631ms
Mar 27 15:26:31.696: INFO: The phase of Pod server-envvars-b613a045-7c02-4b01-9f73-fbdd8481350f is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:26:33.700: INFO: Pod "server-envvars-b613a045-7c02-4b01-9f73-fbdd8481350f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007143204s
Mar 27 15:26:33.700: INFO: The phase of Pod server-envvars-b613a045-7c02-4b01-9f73-fbdd8481350f is Running (Ready = true)
Mar 27 15:26:33.700: INFO: Pod "server-envvars-b613a045-7c02-4b01-9f73-fbdd8481350f" satisfied condition "running and ready"
Mar 27 15:26:33.726: INFO: Waiting up to 5m0s for pod "client-envvars-f430bdfa-13d4-4101-96a3-ed271ed77a03" in namespace "pods-5682" to be "Succeeded or Failed"
Mar 27 15:26:33.731: INFO: Pod "client-envvars-f430bdfa-13d4-4101-96a3-ed271ed77a03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.731537ms
Mar 27 15:26:35.735: INFO: Pod "client-envvars-f430bdfa-13d4-4101-96a3-ed271ed77a03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008900343s
Mar 27 15:26:37.735: INFO: Pod "client-envvars-f430bdfa-13d4-4101-96a3-ed271ed77a03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009041852s
STEP: Saw pod success 03/27/23 15:26:37.736
Mar 27 15:26:37.736: INFO: Pod "client-envvars-f430bdfa-13d4-4101-96a3-ed271ed77a03" satisfied condition "Succeeded or Failed"
Mar 27 15:26:37.743: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod client-envvars-f430bdfa-13d4-4101-96a3-ed271ed77a03 container env3cont: <nil>
STEP: delete the pod 03/27/23 15:26:37.753
Mar 27 15:26:37.764: INFO: Waiting for pod client-envvars-f430bdfa-13d4-4101-96a3-ed271ed77a03 to disappear
Mar 27 15:26:37.767: INFO: Pod client-envvars-f430bdfa-13d4-4101-96a3-ed271ed77a03 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 27 15:26:37.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5682" for this suite. 03/27/23 15:26:37.773
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":275,"skipped":5330,"failed":0}
------------------------------
• [SLOW TEST] [6.129 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:26:31.65
    Mar 27 15:26:31.650: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename pods 03/27/23 15:26:31.651
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:26:31.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:26:31.68
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Mar 27 15:26:31.693: INFO: Waiting up to 5m0s for pod "server-envvars-b613a045-7c02-4b01-9f73-fbdd8481350f" in namespace "pods-5682" to be "running and ready"
    Mar 27 15:26:31.696: INFO: Pod "server-envvars-b613a045-7c02-4b01-9f73-fbdd8481350f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.671631ms
    Mar 27 15:26:31.696: INFO: The phase of Pod server-envvars-b613a045-7c02-4b01-9f73-fbdd8481350f is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:26:33.700: INFO: Pod "server-envvars-b613a045-7c02-4b01-9f73-fbdd8481350f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007143204s
    Mar 27 15:26:33.700: INFO: The phase of Pod server-envvars-b613a045-7c02-4b01-9f73-fbdd8481350f is Running (Ready = true)
    Mar 27 15:26:33.700: INFO: Pod "server-envvars-b613a045-7c02-4b01-9f73-fbdd8481350f" satisfied condition "running and ready"
    Mar 27 15:26:33.726: INFO: Waiting up to 5m0s for pod "client-envvars-f430bdfa-13d4-4101-96a3-ed271ed77a03" in namespace "pods-5682" to be "Succeeded or Failed"
    Mar 27 15:26:33.731: INFO: Pod "client-envvars-f430bdfa-13d4-4101-96a3-ed271ed77a03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.731537ms
    Mar 27 15:26:35.735: INFO: Pod "client-envvars-f430bdfa-13d4-4101-96a3-ed271ed77a03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008900343s
    Mar 27 15:26:37.735: INFO: Pod "client-envvars-f430bdfa-13d4-4101-96a3-ed271ed77a03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009041852s
    STEP: Saw pod success 03/27/23 15:26:37.736
    Mar 27 15:26:37.736: INFO: Pod "client-envvars-f430bdfa-13d4-4101-96a3-ed271ed77a03" satisfied condition "Succeeded or Failed"
    Mar 27 15:26:37.743: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod client-envvars-f430bdfa-13d4-4101-96a3-ed271ed77a03 container env3cont: <nil>
    STEP: delete the pod 03/27/23 15:26:37.753
    Mar 27 15:26:37.764: INFO: Waiting for pod client-envvars-f430bdfa-13d4-4101-96a3-ed271ed77a03 to disappear
    Mar 27 15:26:37.767: INFO: Pod client-envvars-f430bdfa-13d4-4101-96a3-ed271ed77a03 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 27 15:26:37.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5682" for this suite. 03/27/23 15:26:37.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:26:37.782
Mar 27 15:26:37.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename resourcequota 03/27/23 15:26:37.783
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:26:37.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:26:37.801
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 03/27/23 15:26:37.803
STEP: Creating a ResourceQuota 03/27/23 15:26:42.807
STEP: Ensuring resource quota status is calculated 03/27/23 15:26:42.817
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 27 15:26:44.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6779" for this suite. 03/27/23 15:26:44.827
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":276,"skipped":5344,"failed":0}
------------------------------
• [SLOW TEST] [7.055 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:26:37.782
    Mar 27 15:26:37.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename resourcequota 03/27/23 15:26:37.783
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:26:37.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:26:37.801
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 03/27/23 15:26:37.803
    STEP: Creating a ResourceQuota 03/27/23 15:26:42.807
    STEP: Ensuring resource quota status is calculated 03/27/23 15:26:42.817
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 27 15:26:44.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6779" for this suite. 03/27/23 15:26:44.827
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:26:44.839
Mar 27 15:26:44.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename deployment 03/27/23 15:26:44.84
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:26:44.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:26:44.859
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 03/27/23 15:26:44.874
Mar 27 15:26:44.874: INFO: Creating simple deployment test-deployment-tbfnm
Mar 27 15:26:44.889: INFO: deployment "test-deployment-tbfnm" doesn't have the required revision set
STEP: Getting /status 03/27/23 15:26:46.911
Mar 27 15:26:46.914: INFO: Deployment test-deployment-tbfnm has Conditions: [{Available True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tbfnm-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 03/27/23 15:26:46.914
Mar 27 15:26:46.927: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 15, 26, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 15, 26, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 15, 26, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 15, 26, 44, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-tbfnm-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 03/27/23 15:26:46.927
Mar 27 15:26:46.929: INFO: Observed &Deployment event: ADDED
Mar 27 15:26:46.929: INFO: Observed Deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tbfnm-777898ffcc"}
Mar 27 15:26:46.929: INFO: Observed &Deployment event: MODIFIED
Mar 27 15:26:46.929: INFO: Observed Deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tbfnm-777898ffcc"}
Mar 27 15:26:46.929: INFO: Observed Deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 27 15:26:46.929: INFO: Observed &Deployment event: MODIFIED
Mar 27 15:26:46.929: INFO: Observed Deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 27 15:26:46.929: INFO: Observed Deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-tbfnm-777898ffcc" is progressing.}
Mar 27 15:26:46.929: INFO: Observed &Deployment event: MODIFIED
Mar 27 15:26:46.929: INFO: Observed Deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 27 15:26:46.929: INFO: Observed Deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tbfnm-777898ffcc" has successfully progressed.}
Mar 27 15:26:46.929: INFO: Observed &Deployment event: MODIFIED
Mar 27 15:26:46.929: INFO: Observed Deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 27 15:26:46.929: INFO: Observed Deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tbfnm-777898ffcc" has successfully progressed.}
Mar 27 15:26:46.929: INFO: Found Deployment test-deployment-tbfnm in namespace deployment-7244 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 27 15:26:46.929: INFO: Deployment test-deployment-tbfnm has an updated status
STEP: patching the Statefulset Status 03/27/23 15:26:46.929
Mar 27 15:26:46.929: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 27 15:26:46.937: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 03/27/23 15:26:46.937
Mar 27 15:26:46.939: INFO: Observed &Deployment event: ADDED
Mar 27 15:26:46.939: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tbfnm-777898ffcc"}
Mar 27 15:26:46.939: INFO: Observed &Deployment event: MODIFIED
Mar 27 15:26:46.939: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tbfnm-777898ffcc"}
Mar 27 15:26:46.939: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 27 15:26:46.940: INFO: Observed &Deployment event: MODIFIED
Mar 27 15:26:46.940: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 27 15:26:46.940: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-tbfnm-777898ffcc" is progressing.}
Mar 27 15:26:46.940: INFO: Observed &Deployment event: MODIFIED
Mar 27 15:26:46.940: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 27 15:26:46.940: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tbfnm-777898ffcc" has successfully progressed.}
Mar 27 15:26:46.940: INFO: Observed &Deployment event: MODIFIED
Mar 27 15:26:46.940: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 27 15:26:46.940: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tbfnm-777898ffcc" has successfully progressed.}
Mar 27 15:26:46.940: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 27 15:26:46.941: INFO: Observed &Deployment event: MODIFIED
Mar 27 15:26:46.941: INFO: Found deployment test-deployment-tbfnm in namespace deployment-7244 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Mar 27 15:26:46.941: INFO: Deployment test-deployment-tbfnm has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 27 15:26:46.944: INFO: Deployment "test-deployment-tbfnm":
&Deployment{ObjectMeta:{test-deployment-tbfnm  deployment-7244  ac222d7d-cc6b-4d8c-95fb-956463a5f4e5 48860 1 2023-03-27 15:26:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-27 15:26:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-27 15:26:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-27 15:26:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006112c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-tbfnm-777898ffcc",LastUpdateTime:2023-03-27 15:26:46 +0000 UTC,LastTransitionTime:2023-03-27 15:26:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 27 15:26:46.947: INFO: New ReplicaSet "test-deployment-tbfnm-777898ffcc" of Deployment "test-deployment-tbfnm":
&ReplicaSet{ObjectMeta:{test-deployment-tbfnm-777898ffcc  deployment-7244  863bfd03-e2d4-45bc-b267-c7b5f7e44872 48852 1 2023-03-27 15:26:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-tbfnm ac222d7d-cc6b-4d8c-95fb-956463a5f4e5 0xc006112fe7 0xc006112fe8}] [] [{kube-controller-manager Update apps/v1 2023-03-27 15:26:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac222d7d-cc6b-4d8c-95fb-956463a5f4e5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 15:26:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006113098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 27 15:26:46.950: INFO: Pod "test-deployment-tbfnm-777898ffcc-jf9fg" is available:
&Pod{ObjectMeta:{test-deployment-tbfnm-777898ffcc-jf9fg test-deployment-tbfnm-777898ffcc- deployment-7244  2c32fdf3-3235-46a1-ac8d-a7fcd07da074 48851 0 2023-03-27 15:26:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:84a20fdec09a3197a899cb5c8312a6cc804141e0f64087c0bc5916435184b1d2 cni.projectcalico.org/podIP:172.25.2.56/32 cni.projectcalico.org/podIPs:172.25.2.56/32] [{apps/v1 ReplicaSet test-deployment-tbfnm-777898ffcc 863bfd03-e2d4-45bc-b267-c7b5f7e44872 0xc006113460 0xc006113461}] [] [{kube-controller-manager Update v1 2023-03-27 15:26:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"863bfd03-e2d4-45bc-b267-c7b5f7e44872\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 15:26:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 15:26:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jwr9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jwr9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 15:26:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 15:26:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 15:26:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 15:26:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:172.25.2.56,StartTime:2023-03-27 15:26:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 15:26:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://bf7f56a8245a2dc851d1df7bd61e7ed1962b99272acb688e61ba005952401dab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 27 15:26:46.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7244" for this suite. 03/27/23 15:26:46.955
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":277,"skipped":5385,"failed":0}
------------------------------
• [2.123 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:26:44.839
    Mar 27 15:26:44.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename deployment 03/27/23 15:26:44.84
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:26:44.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:26:44.859
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 03/27/23 15:26:44.874
    Mar 27 15:26:44.874: INFO: Creating simple deployment test-deployment-tbfnm
    Mar 27 15:26:44.889: INFO: deployment "test-deployment-tbfnm" doesn't have the required revision set
    STEP: Getting /status 03/27/23 15:26:46.911
    Mar 27 15:26:46.914: INFO: Deployment test-deployment-tbfnm has Conditions: [{Available True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tbfnm-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 03/27/23 15:26:46.914
    Mar 27 15:26:46.927: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 15, 26, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 15, 26, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 15, 26, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 15, 26, 44, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-tbfnm-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 03/27/23 15:26:46.927
    Mar 27 15:26:46.929: INFO: Observed &Deployment event: ADDED
    Mar 27 15:26:46.929: INFO: Observed Deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tbfnm-777898ffcc"}
    Mar 27 15:26:46.929: INFO: Observed &Deployment event: MODIFIED
    Mar 27 15:26:46.929: INFO: Observed Deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tbfnm-777898ffcc"}
    Mar 27 15:26:46.929: INFO: Observed Deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 27 15:26:46.929: INFO: Observed &Deployment event: MODIFIED
    Mar 27 15:26:46.929: INFO: Observed Deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 27 15:26:46.929: INFO: Observed Deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-tbfnm-777898ffcc" is progressing.}
    Mar 27 15:26:46.929: INFO: Observed &Deployment event: MODIFIED
    Mar 27 15:26:46.929: INFO: Observed Deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 27 15:26:46.929: INFO: Observed Deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tbfnm-777898ffcc" has successfully progressed.}
    Mar 27 15:26:46.929: INFO: Observed &Deployment event: MODIFIED
    Mar 27 15:26:46.929: INFO: Observed Deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 27 15:26:46.929: INFO: Observed Deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tbfnm-777898ffcc" has successfully progressed.}
    Mar 27 15:26:46.929: INFO: Found Deployment test-deployment-tbfnm in namespace deployment-7244 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 27 15:26:46.929: INFO: Deployment test-deployment-tbfnm has an updated status
    STEP: patching the Statefulset Status 03/27/23 15:26:46.929
    Mar 27 15:26:46.929: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 27 15:26:46.937: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 03/27/23 15:26:46.937
    Mar 27 15:26:46.939: INFO: Observed &Deployment event: ADDED
    Mar 27 15:26:46.939: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tbfnm-777898ffcc"}
    Mar 27 15:26:46.939: INFO: Observed &Deployment event: MODIFIED
    Mar 27 15:26:46.939: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tbfnm-777898ffcc"}
    Mar 27 15:26:46.939: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 27 15:26:46.940: INFO: Observed &Deployment event: MODIFIED
    Mar 27 15:26:46.940: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 27 15:26:46.940: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:44 +0000 UTC 2023-03-27 15:26:44 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-tbfnm-777898ffcc" is progressing.}
    Mar 27 15:26:46.940: INFO: Observed &Deployment event: MODIFIED
    Mar 27 15:26:46.940: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 27 15:26:46.940: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tbfnm-777898ffcc" has successfully progressed.}
    Mar 27 15:26:46.940: INFO: Observed &Deployment event: MODIFIED
    Mar 27 15:26:46.940: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 27 15:26:46.940: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 15:26:46 +0000 UTC 2023-03-27 15:26:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tbfnm-777898ffcc" has successfully progressed.}
    Mar 27 15:26:46.940: INFO: Observed deployment test-deployment-tbfnm in namespace deployment-7244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 27 15:26:46.941: INFO: Observed &Deployment event: MODIFIED
    Mar 27 15:26:46.941: INFO: Found deployment test-deployment-tbfnm in namespace deployment-7244 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Mar 27 15:26:46.941: INFO: Deployment test-deployment-tbfnm has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 27 15:26:46.944: INFO: Deployment "test-deployment-tbfnm":
    &Deployment{ObjectMeta:{test-deployment-tbfnm  deployment-7244  ac222d7d-cc6b-4d8c-95fb-956463a5f4e5 48860 1 2023-03-27 15:26:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-27 15:26:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-27 15:26:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-27 15:26:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006112c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-tbfnm-777898ffcc",LastUpdateTime:2023-03-27 15:26:46 +0000 UTC,LastTransitionTime:2023-03-27 15:26:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 27 15:26:46.947: INFO: New ReplicaSet "test-deployment-tbfnm-777898ffcc" of Deployment "test-deployment-tbfnm":
    &ReplicaSet{ObjectMeta:{test-deployment-tbfnm-777898ffcc  deployment-7244  863bfd03-e2d4-45bc-b267-c7b5f7e44872 48852 1 2023-03-27 15:26:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-tbfnm ac222d7d-cc6b-4d8c-95fb-956463a5f4e5 0xc006112fe7 0xc006112fe8}] [] [{kube-controller-manager Update apps/v1 2023-03-27 15:26:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac222d7d-cc6b-4d8c-95fb-956463a5f4e5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 15:26:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006113098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 15:26:46.950: INFO: Pod "test-deployment-tbfnm-777898ffcc-jf9fg" is available:
    &Pod{ObjectMeta:{test-deployment-tbfnm-777898ffcc-jf9fg test-deployment-tbfnm-777898ffcc- deployment-7244  2c32fdf3-3235-46a1-ac8d-a7fcd07da074 48851 0 2023-03-27 15:26:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:84a20fdec09a3197a899cb5c8312a6cc804141e0f64087c0bc5916435184b1d2 cni.projectcalico.org/podIP:172.25.2.56/32 cni.projectcalico.org/podIPs:172.25.2.56/32] [{apps/v1 ReplicaSet test-deployment-tbfnm-777898ffcc 863bfd03-e2d4-45bc-b267-c7b5f7e44872 0xc006113460 0xc006113461}] [] [{kube-controller-manager Update v1 2023-03-27 15:26:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"863bfd03-e2d4-45bc-b267-c7b5f7e44872\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 15:26:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 15:26:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jwr9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jwr9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 15:26:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 15:26:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 15:26:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 15:26:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:172.25.2.56,StartTime:2023-03-27 15:26:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 15:26:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://bf7f56a8245a2dc851d1df7bd61e7ed1962b99272acb688e61ba005952401dab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 27 15:26:46.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7244" for this suite. 03/27/23 15:26:46.955
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:26:46.963
Mar 27 15:26:46.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubelet-test 03/27/23 15:26:46.964
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:26:46.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:26:46.977
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar 27 15:26:50.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2795" for this suite. 03/27/23 15:26:51.001
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":278,"skipped":5387,"failed":0}
------------------------------
• [4.049 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:26:46.963
    Mar 27 15:26:46.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubelet-test 03/27/23 15:26:46.964
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:26:46.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:26:46.977
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar 27 15:26:50.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-2795" for this suite. 03/27/23 15:26:51.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:26:51.014
Mar 27 15:26:51.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename resourcequota 03/27/23 15:26:51.016
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:26:51.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:26:51.04
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 03/27/23 15:26:51.045
STEP: Creating a ResourceQuota 03/27/23 15:26:56.049
STEP: Ensuring resource quota status is calculated 03/27/23 15:26:56.056
STEP: Creating a Pod that fits quota 03/27/23 15:26:58.062
STEP: Ensuring ResourceQuota status captures the pod usage 03/27/23 15:26:58.081
STEP: Not allowing a pod to be created that exceeds remaining quota 03/27/23 15:27:00.086
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/27/23 15:27:00.089
STEP: Ensuring a pod cannot update its resource requirements 03/27/23 15:27:00.091
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/27/23 15:27:00.095
STEP: Deleting the pod 03/27/23 15:27:02.102
STEP: Ensuring resource quota status released the pod usage 03/27/23 15:27:02.113
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 27 15:27:04.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7435" for this suite. 03/27/23 15:27:04.124
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":279,"skipped":5408,"failed":0}
------------------------------
• [SLOW TEST] [13.115 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:26:51.014
    Mar 27 15:26:51.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename resourcequota 03/27/23 15:26:51.016
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:26:51.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:26:51.04
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 03/27/23 15:26:51.045
    STEP: Creating a ResourceQuota 03/27/23 15:26:56.049
    STEP: Ensuring resource quota status is calculated 03/27/23 15:26:56.056
    STEP: Creating a Pod that fits quota 03/27/23 15:26:58.062
    STEP: Ensuring ResourceQuota status captures the pod usage 03/27/23 15:26:58.081
    STEP: Not allowing a pod to be created that exceeds remaining quota 03/27/23 15:27:00.086
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/27/23 15:27:00.089
    STEP: Ensuring a pod cannot update its resource requirements 03/27/23 15:27:00.091
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/27/23 15:27:00.095
    STEP: Deleting the pod 03/27/23 15:27:02.102
    STEP: Ensuring resource quota status released the pod usage 03/27/23 15:27:02.113
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 27 15:27:04.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7435" for this suite. 03/27/23 15:27:04.124
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:27:04.13
Mar 27 15:27:04.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename controllerrevisions 03/27/23 15:27:04.13
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:27:04.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:27:04.152
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-f8qxc-daemon-set" 03/27/23 15:27:04.179
STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 15:27:04.187
Mar 27 15:27:04.195: INFO: Number of nodes with available pods controlled by daemonset e2e-f8qxc-daemon-set: 0
Mar 27 15:27:04.195: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 15:27:05.204: INFO: Number of nodes with available pods controlled by daemonset e2e-f8qxc-daemon-set: 0
Mar 27 15:27:05.204: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 15:27:06.206: INFO: Number of nodes with available pods controlled by daemonset e2e-f8qxc-daemon-set: 2
Mar 27 15:27:06.206: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv is running 0 daemon pod, expected 1
Mar 27 15:27:07.205: INFO: Number of nodes with available pods controlled by daemonset e2e-f8qxc-daemon-set: 3
Mar 27 15:27:07.205: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-f8qxc-daemon-set
STEP: Confirm DaemonSet "e2e-f8qxc-daemon-set" successfully created with "daemonset-name=e2e-f8qxc-daemon-set" label 03/27/23 15:27:07.207
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-f8qxc-daemon-set" 03/27/23 15:27:07.215
Mar 27 15:27:07.219: INFO: Located ControllerRevision: "e2e-f8qxc-daemon-set-7f944b5959"
STEP: Patching ControllerRevision "e2e-f8qxc-daemon-set-7f944b5959" 03/27/23 15:27:07.222
Mar 27 15:27:07.228: INFO: e2e-f8qxc-daemon-set-7f944b5959 has been patched
STEP: Create a new ControllerRevision 03/27/23 15:27:07.228
Mar 27 15:27:07.236: INFO: Created ControllerRevision: e2e-f8qxc-daemon-set-d544c4df5
STEP: Confirm that there are two ControllerRevisions 03/27/23 15:27:07.236
Mar 27 15:27:07.237: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 27 15:27:07.240: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-f8qxc-daemon-set-7f944b5959" 03/27/23 15:27:07.241
STEP: Confirm that there is only one ControllerRevision 03/27/23 15:27:07.246
Mar 27 15:27:07.246: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 27 15:27:07.249: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-f8qxc-daemon-set-d544c4df5" 03/27/23 15:27:07.252
Mar 27 15:27:07.261: INFO: e2e-f8qxc-daemon-set-d544c4df5 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 03/27/23 15:27:07.261
W0327 15:27:07.270516      24 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 03/27/23 15:27:07.27
Mar 27 15:27:07.270: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 27 15:27:08.274: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 27 15:27:08.282: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-f8qxc-daemon-set-d544c4df5=updated" 03/27/23 15:27:08.282
STEP: Confirm that there is only one ControllerRevision 03/27/23 15:27:08.29
Mar 27 15:27:08.290: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 27 15:27:08.293: INFO: Found 1 ControllerRevisions
Mar 27 15:27:08.296: INFO: ControllerRevision "e2e-f8qxc-daemon-set-6fc58fd598" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-f8qxc-daemon-set" 03/27/23 15:27:08.299
STEP: deleting DaemonSet.extensions e2e-f8qxc-daemon-set in namespace controllerrevisions-3369, will wait for the garbage collector to delete the pods 03/27/23 15:27:08.299
Mar 27 15:27:08.359: INFO: Deleting DaemonSet.extensions e2e-f8qxc-daemon-set took: 6.512361ms
Mar 27 15:27:08.460: INFO: Terminating DaemonSet.extensions e2e-f8qxc-daemon-set pods took: 100.833231ms
Mar 27 15:27:10.271: INFO: Number of nodes with available pods controlled by daemonset e2e-f8qxc-daemon-set: 0
Mar 27 15:27:10.271: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-f8qxc-daemon-set
Mar 27 15:27:10.274: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49135"},"items":null}

Mar 27 15:27:10.278: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49135"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Mar 27 15:27:10.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-3369" for this suite. 03/27/23 15:27:10.297
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":280,"skipped":5411,"failed":0}
------------------------------
• [SLOW TEST] [6.174 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:27:04.13
    Mar 27 15:27:04.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename controllerrevisions 03/27/23 15:27:04.13
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:27:04.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:27:04.152
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-f8qxc-daemon-set" 03/27/23 15:27:04.179
    STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 15:27:04.187
    Mar 27 15:27:04.195: INFO: Number of nodes with available pods controlled by daemonset e2e-f8qxc-daemon-set: 0
    Mar 27 15:27:04.195: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 15:27:05.204: INFO: Number of nodes with available pods controlled by daemonset e2e-f8qxc-daemon-set: 0
    Mar 27 15:27:05.204: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 15:27:06.206: INFO: Number of nodes with available pods controlled by daemonset e2e-f8qxc-daemon-set: 2
    Mar 27 15:27:06.206: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv is running 0 daemon pod, expected 1
    Mar 27 15:27:07.205: INFO: Number of nodes with available pods controlled by daemonset e2e-f8qxc-daemon-set: 3
    Mar 27 15:27:07.205: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-f8qxc-daemon-set
    STEP: Confirm DaemonSet "e2e-f8qxc-daemon-set" successfully created with "daemonset-name=e2e-f8qxc-daemon-set" label 03/27/23 15:27:07.207
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-f8qxc-daemon-set" 03/27/23 15:27:07.215
    Mar 27 15:27:07.219: INFO: Located ControllerRevision: "e2e-f8qxc-daemon-set-7f944b5959"
    STEP: Patching ControllerRevision "e2e-f8qxc-daemon-set-7f944b5959" 03/27/23 15:27:07.222
    Mar 27 15:27:07.228: INFO: e2e-f8qxc-daemon-set-7f944b5959 has been patched
    STEP: Create a new ControllerRevision 03/27/23 15:27:07.228
    Mar 27 15:27:07.236: INFO: Created ControllerRevision: e2e-f8qxc-daemon-set-d544c4df5
    STEP: Confirm that there are two ControllerRevisions 03/27/23 15:27:07.236
    Mar 27 15:27:07.237: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 27 15:27:07.240: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-f8qxc-daemon-set-7f944b5959" 03/27/23 15:27:07.241
    STEP: Confirm that there is only one ControllerRevision 03/27/23 15:27:07.246
    Mar 27 15:27:07.246: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 27 15:27:07.249: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-f8qxc-daemon-set-d544c4df5" 03/27/23 15:27:07.252
    Mar 27 15:27:07.261: INFO: e2e-f8qxc-daemon-set-d544c4df5 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 03/27/23 15:27:07.261
    W0327 15:27:07.270516      24 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 03/27/23 15:27:07.27
    Mar 27 15:27:07.270: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 27 15:27:08.274: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 27 15:27:08.282: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-f8qxc-daemon-set-d544c4df5=updated" 03/27/23 15:27:08.282
    STEP: Confirm that there is only one ControllerRevision 03/27/23 15:27:08.29
    Mar 27 15:27:08.290: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 27 15:27:08.293: INFO: Found 1 ControllerRevisions
    Mar 27 15:27:08.296: INFO: ControllerRevision "e2e-f8qxc-daemon-set-6fc58fd598" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-f8qxc-daemon-set" 03/27/23 15:27:08.299
    STEP: deleting DaemonSet.extensions e2e-f8qxc-daemon-set in namespace controllerrevisions-3369, will wait for the garbage collector to delete the pods 03/27/23 15:27:08.299
    Mar 27 15:27:08.359: INFO: Deleting DaemonSet.extensions e2e-f8qxc-daemon-set took: 6.512361ms
    Mar 27 15:27:08.460: INFO: Terminating DaemonSet.extensions e2e-f8qxc-daemon-set pods took: 100.833231ms
    Mar 27 15:27:10.271: INFO: Number of nodes with available pods controlled by daemonset e2e-f8qxc-daemon-set: 0
    Mar 27 15:27:10.271: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-f8qxc-daemon-set
    Mar 27 15:27:10.274: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49135"},"items":null}

    Mar 27 15:27:10.278: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49135"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 15:27:10.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-3369" for this suite. 03/27/23 15:27:10.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:27:10.305
Mar 27 15:27:10.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename emptydir 03/27/23 15:27:10.306
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:27:10.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:27:10.318
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/27/23 15:27:10.322
Mar 27 15:27:10.330: INFO: Waiting up to 5m0s for pod "pod-f9eb2ca9-4680-4ec6-8c88-55d6c05e838c" in namespace "emptydir-7929" to be "Succeeded or Failed"
Mar 27 15:27:10.334: INFO: Pod "pod-f9eb2ca9-4680-4ec6-8c88-55d6c05e838c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.699961ms
Mar 27 15:27:12.339: INFO: Pod "pod-f9eb2ca9-4680-4ec6-8c88-55d6c05e838c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008794581s
Mar 27 15:27:14.338: INFO: Pod "pod-f9eb2ca9-4680-4ec6-8c88-55d6c05e838c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007533376s
STEP: Saw pod success 03/27/23 15:27:14.338
Mar 27 15:27:14.338: INFO: Pod "pod-f9eb2ca9-4680-4ec6-8c88-55d6c05e838c" satisfied condition "Succeeded or Failed"
Mar 27 15:27:14.341: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-f9eb2ca9-4680-4ec6-8c88-55d6c05e838c container test-container: <nil>
STEP: delete the pod 03/27/23 15:27:14.35
Mar 27 15:27:14.366: INFO: Waiting for pod pod-f9eb2ca9-4680-4ec6-8c88-55d6c05e838c to disappear
Mar 27 15:27:14.373: INFO: Pod pod-f9eb2ca9-4680-4ec6-8c88-55d6c05e838c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 27 15:27:14.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7929" for this suite. 03/27/23 15:27:14.377
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":281,"skipped":5444,"failed":0}
------------------------------
• [4.078 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:27:10.305
    Mar 27 15:27:10.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename emptydir 03/27/23 15:27:10.306
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:27:10.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:27:10.318
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/27/23 15:27:10.322
    Mar 27 15:27:10.330: INFO: Waiting up to 5m0s for pod "pod-f9eb2ca9-4680-4ec6-8c88-55d6c05e838c" in namespace "emptydir-7929" to be "Succeeded or Failed"
    Mar 27 15:27:10.334: INFO: Pod "pod-f9eb2ca9-4680-4ec6-8c88-55d6c05e838c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.699961ms
    Mar 27 15:27:12.339: INFO: Pod "pod-f9eb2ca9-4680-4ec6-8c88-55d6c05e838c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008794581s
    Mar 27 15:27:14.338: INFO: Pod "pod-f9eb2ca9-4680-4ec6-8c88-55d6c05e838c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007533376s
    STEP: Saw pod success 03/27/23 15:27:14.338
    Mar 27 15:27:14.338: INFO: Pod "pod-f9eb2ca9-4680-4ec6-8c88-55d6c05e838c" satisfied condition "Succeeded or Failed"
    Mar 27 15:27:14.341: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-f9eb2ca9-4680-4ec6-8c88-55d6c05e838c container test-container: <nil>
    STEP: delete the pod 03/27/23 15:27:14.35
    Mar 27 15:27:14.366: INFO: Waiting for pod pod-f9eb2ca9-4680-4ec6-8c88-55d6c05e838c to disappear
    Mar 27 15:27:14.373: INFO: Pod pod-f9eb2ca9-4680-4ec6-8c88-55d6c05e838c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 27 15:27:14.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7929" for this suite. 03/27/23 15:27:14.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:27:14.384
Mar 27 15:27:14.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename services 03/27/23 15:27:14.385
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:27:14.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:27:14.402
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9916 03/27/23 15:27:14.405
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/27/23 15:27:14.422
STEP: creating service externalsvc in namespace services-9916 03/27/23 15:27:14.423
STEP: creating replication controller externalsvc in namespace services-9916 03/27/23 15:27:14.437
I0327 15:27:14.448595      24 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9916, replica count: 2
I0327 15:27:17.500886      24 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 03/27/23 15:27:17.505
Mar 27 15:27:17.522: INFO: Creating new exec pod
Mar 27 15:27:17.533: INFO: Waiting up to 5m0s for pod "execpodjmtq9" in namespace "services-9916" to be "running"
Mar 27 15:27:17.538: INFO: Pod "execpodjmtq9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.675698ms
Mar 27 15:27:19.548: INFO: Pod "execpodjmtq9": Phase="Running", Reason="", readiness=true. Elapsed: 2.014684774s
Mar 27 15:27:19.548: INFO: Pod "execpodjmtq9" satisfied condition "running"
Mar 27 15:27:19.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-9916 exec execpodjmtq9 -- /bin/sh -x -c nslookup nodeport-service.services-9916.svc.cluster.local'
Mar 27 15:27:19.762: INFO: stderr: "+ nslookup nodeport-service.services-9916.svc.cluster.local\n"
Mar 27 15:27:19.762: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-9916.svc.cluster.local\tcanonical name = externalsvc.services-9916.svc.cluster.local.\nName:\texternalsvc.services-9916.svc.cluster.local\nAddress: 10.240.17.215\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9916, will wait for the garbage collector to delete the pods 03/27/23 15:27:19.762
Mar 27 15:27:19.839: INFO: Deleting ReplicationController externalsvc took: 10.414897ms
Mar 27 15:27:19.940: INFO: Terminating ReplicationController externalsvc pods took: 101.207989ms
Mar 27 15:27:22.262: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 27 15:27:22.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9916" for this suite. 03/27/23 15:27:22.28
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":282,"skipped":5457,"failed":0}
------------------------------
• [SLOW TEST] [7.905 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:27:14.384
    Mar 27 15:27:14.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename services 03/27/23 15:27:14.385
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:27:14.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:27:14.402
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-9916 03/27/23 15:27:14.405
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/27/23 15:27:14.422
    STEP: creating service externalsvc in namespace services-9916 03/27/23 15:27:14.423
    STEP: creating replication controller externalsvc in namespace services-9916 03/27/23 15:27:14.437
    I0327 15:27:14.448595      24 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9916, replica count: 2
    I0327 15:27:17.500886      24 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 03/27/23 15:27:17.505
    Mar 27 15:27:17.522: INFO: Creating new exec pod
    Mar 27 15:27:17.533: INFO: Waiting up to 5m0s for pod "execpodjmtq9" in namespace "services-9916" to be "running"
    Mar 27 15:27:17.538: INFO: Pod "execpodjmtq9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.675698ms
    Mar 27 15:27:19.548: INFO: Pod "execpodjmtq9": Phase="Running", Reason="", readiness=true. Elapsed: 2.014684774s
    Mar 27 15:27:19.548: INFO: Pod "execpodjmtq9" satisfied condition "running"
    Mar 27 15:27:19.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-9916 exec execpodjmtq9 -- /bin/sh -x -c nslookup nodeport-service.services-9916.svc.cluster.local'
    Mar 27 15:27:19.762: INFO: stderr: "+ nslookup nodeport-service.services-9916.svc.cluster.local\n"
    Mar 27 15:27:19.762: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-9916.svc.cluster.local\tcanonical name = externalsvc.services-9916.svc.cluster.local.\nName:\texternalsvc.services-9916.svc.cluster.local\nAddress: 10.240.17.215\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9916, will wait for the garbage collector to delete the pods 03/27/23 15:27:19.762
    Mar 27 15:27:19.839: INFO: Deleting ReplicationController externalsvc took: 10.414897ms
    Mar 27 15:27:19.940: INFO: Terminating ReplicationController externalsvc pods took: 101.207989ms
    Mar 27 15:27:22.262: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 27 15:27:22.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9916" for this suite. 03/27/23 15:27:22.28
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:27:22.292
Mar 27 15:27:22.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename pods 03/27/23 15:27:22.293
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:27:22.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:27:22.309
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 03/27/23 15:27:22.312
STEP: submitting the pod to kubernetes 03/27/23 15:27:22.312
Mar 27 15:27:22.320: INFO: Waiting up to 5m0s for pod "pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3" in namespace "pods-2949" to be "running and ready"
Mar 27 15:27:22.330: INFO: Pod "pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.597747ms
Mar 27 15:27:22.330: INFO: The phase of Pod pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:27:24.336: INFO: Pod "pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3": Phase="Running", Reason="", readiness=true. Elapsed: 2.015536258s
Mar 27 15:27:24.336: INFO: The phase of Pod pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3 is Running (Ready = true)
Mar 27 15:27:24.336: INFO: Pod "pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/27/23 15:27:24.34
STEP: updating the pod 03/27/23 15:27:24.345
Mar 27 15:27:24.860: INFO: Successfully updated pod "pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3"
Mar 27 15:27:24.860: INFO: Waiting up to 5m0s for pod "pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3" in namespace "pods-2949" to be "running"
Mar 27 15:27:24.868: INFO: Pod "pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3": Phase="Running", Reason="", readiness=true. Elapsed: 7.57007ms
Mar 27 15:27:24.868: INFO: Pod "pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 03/27/23 15:27:24.868
Mar 27 15:27:24.870: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 27 15:27:24.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2949" for this suite. 03/27/23 15:27:24.874
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":283,"skipped":5464,"failed":0}
------------------------------
• [2.588 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:27:22.292
    Mar 27 15:27:22.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename pods 03/27/23 15:27:22.293
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:27:22.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:27:22.309
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 03/27/23 15:27:22.312
    STEP: submitting the pod to kubernetes 03/27/23 15:27:22.312
    Mar 27 15:27:22.320: INFO: Waiting up to 5m0s for pod "pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3" in namespace "pods-2949" to be "running and ready"
    Mar 27 15:27:22.330: INFO: Pod "pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.597747ms
    Mar 27 15:27:22.330: INFO: The phase of Pod pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:27:24.336: INFO: Pod "pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3": Phase="Running", Reason="", readiness=true. Elapsed: 2.015536258s
    Mar 27 15:27:24.336: INFO: The phase of Pod pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3 is Running (Ready = true)
    Mar 27 15:27:24.336: INFO: Pod "pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/27/23 15:27:24.34
    STEP: updating the pod 03/27/23 15:27:24.345
    Mar 27 15:27:24.860: INFO: Successfully updated pod "pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3"
    Mar 27 15:27:24.860: INFO: Waiting up to 5m0s for pod "pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3" in namespace "pods-2949" to be "running"
    Mar 27 15:27:24.868: INFO: Pod "pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3": Phase="Running", Reason="", readiness=true. Elapsed: 7.57007ms
    Mar 27 15:27:24.868: INFO: Pod "pod-update-d100e6e8-3350-401b-bb29-02ab1418bed3" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 03/27/23 15:27:24.868
    Mar 27 15:27:24.870: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 27 15:27:24.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2949" for this suite. 03/27/23 15:27:24.874
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:27:24.882
Mar 27 15:27:24.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename events 03/27/23 15:27:24.883
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:27:24.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:27:24.902
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 03/27/23 15:27:24.905
STEP: get a list of Events with a label in the current namespace 03/27/23 15:27:24.926
STEP: delete a list of events 03/27/23 15:27:24.932
Mar 27 15:27:24.932: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/27/23 15:27:24.952
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Mar 27 15:27:24.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6997" for this suite. 03/27/23 15:27:24.959
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":284,"skipped":5465,"failed":0}
------------------------------
• [0.084 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:27:24.882
    Mar 27 15:27:24.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename events 03/27/23 15:27:24.883
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:27:24.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:27:24.902
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 03/27/23 15:27:24.905
    STEP: get a list of Events with a label in the current namespace 03/27/23 15:27:24.926
    STEP: delete a list of events 03/27/23 15:27:24.932
    Mar 27 15:27:24.932: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/27/23 15:27:24.952
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Mar 27 15:27:24.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6997" for this suite. 03/27/23 15:27:24.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:27:24.968
Mar 27 15:27:24.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename pods 03/27/23 15:27:24.97
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:27:24.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:27:24.988
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Mar 27 15:27:24.991: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: creating the pod 03/27/23 15:27:24.992
STEP: submitting the pod to kubernetes 03/27/23 15:27:24.992
Mar 27 15:27:25.002: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-13d65f5b-f6bd-49bb-8744-fcb36e0188b1" in namespace "pods-5311" to be "running and ready"
Mar 27 15:27:25.008: INFO: Pod "pod-exec-websocket-13d65f5b-f6bd-49bb-8744-fcb36e0188b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.100635ms
Mar 27 15:27:25.008: INFO: The phase of Pod pod-exec-websocket-13d65f5b-f6bd-49bb-8744-fcb36e0188b1 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:27:27.012: INFO: Pod "pod-exec-websocket-13d65f5b-f6bd-49bb-8744-fcb36e0188b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010127939s
Mar 27 15:27:27.012: INFO: The phase of Pod pod-exec-websocket-13d65f5b-f6bd-49bb-8744-fcb36e0188b1 is Running (Ready = true)
Mar 27 15:27:27.012: INFO: Pod "pod-exec-websocket-13d65f5b-f6bd-49bb-8744-fcb36e0188b1" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 27 15:27:27.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5311" for this suite. 03/27/23 15:27:27.184
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":285,"skipped":5477,"failed":0}
------------------------------
• [2.223 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:27:24.968
    Mar 27 15:27:24.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename pods 03/27/23 15:27:24.97
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:27:24.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:27:24.988
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Mar 27 15:27:24.991: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: creating the pod 03/27/23 15:27:24.992
    STEP: submitting the pod to kubernetes 03/27/23 15:27:24.992
    Mar 27 15:27:25.002: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-13d65f5b-f6bd-49bb-8744-fcb36e0188b1" in namespace "pods-5311" to be "running and ready"
    Mar 27 15:27:25.008: INFO: Pod "pod-exec-websocket-13d65f5b-f6bd-49bb-8744-fcb36e0188b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.100635ms
    Mar 27 15:27:25.008: INFO: The phase of Pod pod-exec-websocket-13d65f5b-f6bd-49bb-8744-fcb36e0188b1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:27:27.012: INFO: Pod "pod-exec-websocket-13d65f5b-f6bd-49bb-8744-fcb36e0188b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010127939s
    Mar 27 15:27:27.012: INFO: The phase of Pod pod-exec-websocket-13d65f5b-f6bd-49bb-8744-fcb36e0188b1 is Running (Ready = true)
    Mar 27 15:27:27.012: INFO: Pod "pod-exec-websocket-13d65f5b-f6bd-49bb-8744-fcb36e0188b1" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 27 15:27:27.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5311" for this suite. 03/27/23 15:27:27.184
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:27:27.192
Mar 27 15:27:27.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename container-probe 03/27/23 15:27:27.193
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:27:27.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:27:27.208
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-7042d527-40b7-406a-997f-9d2e17bf481f in namespace container-probe-6095 03/27/23 15:27:27.211
Mar 27 15:27:27.221: INFO: Waiting up to 5m0s for pod "liveness-7042d527-40b7-406a-997f-9d2e17bf481f" in namespace "container-probe-6095" to be "not pending"
Mar 27 15:27:27.225: INFO: Pod "liveness-7042d527-40b7-406a-997f-9d2e17bf481f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.721823ms
Mar 27 15:27:29.230: INFO: Pod "liveness-7042d527-40b7-406a-997f-9d2e17bf481f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008876582s
Mar 27 15:27:29.230: INFO: Pod "liveness-7042d527-40b7-406a-997f-9d2e17bf481f" satisfied condition "not pending"
Mar 27 15:27:29.230: INFO: Started pod liveness-7042d527-40b7-406a-997f-9d2e17bf481f in namespace container-probe-6095
STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 15:27:29.23
Mar 27 15:27:29.233: INFO: Initial restart count of pod liveness-7042d527-40b7-406a-997f-9d2e17bf481f is 0
STEP: deleting the pod 03/27/23 15:31:29.842
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 27 15:31:29.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6095" for this suite. 03/27/23 15:31:29.861
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":286,"skipped":5480,"failed":0}
------------------------------
• [SLOW TEST] [242.679 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:27:27.192
    Mar 27 15:27:27.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename container-probe 03/27/23 15:27:27.193
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:27:27.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:27:27.208
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-7042d527-40b7-406a-997f-9d2e17bf481f in namespace container-probe-6095 03/27/23 15:27:27.211
    Mar 27 15:27:27.221: INFO: Waiting up to 5m0s for pod "liveness-7042d527-40b7-406a-997f-9d2e17bf481f" in namespace "container-probe-6095" to be "not pending"
    Mar 27 15:27:27.225: INFO: Pod "liveness-7042d527-40b7-406a-997f-9d2e17bf481f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.721823ms
    Mar 27 15:27:29.230: INFO: Pod "liveness-7042d527-40b7-406a-997f-9d2e17bf481f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008876582s
    Mar 27 15:27:29.230: INFO: Pod "liveness-7042d527-40b7-406a-997f-9d2e17bf481f" satisfied condition "not pending"
    Mar 27 15:27:29.230: INFO: Started pod liveness-7042d527-40b7-406a-997f-9d2e17bf481f in namespace container-probe-6095
    STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 15:27:29.23
    Mar 27 15:27:29.233: INFO: Initial restart count of pod liveness-7042d527-40b7-406a-997f-9d2e17bf481f is 0
    STEP: deleting the pod 03/27/23 15:31:29.842
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 27 15:31:29.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6095" for this suite. 03/27/23 15:31:29.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:31:29.873
Mar 27 15:31:29.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename downward-api 03/27/23 15:31:29.88
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:31:29.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:31:29.907
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 03/27/23 15:31:29.91
Mar 27 15:31:29.918: INFO: Waiting up to 5m0s for pod "annotationupdate46061e0e-1b30-45dc-84f6-22c88eeddd77" in namespace "downward-api-9342" to be "running and ready"
Mar 27 15:31:29.922: INFO: Pod "annotationupdate46061e0e-1b30-45dc-84f6-22c88eeddd77": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009847ms
Mar 27 15:31:29.922: INFO: The phase of Pod annotationupdate46061e0e-1b30-45dc-84f6-22c88eeddd77 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:31:31.926: INFO: Pod "annotationupdate46061e0e-1b30-45dc-84f6-22c88eeddd77": Phase="Running", Reason="", readiness=true. Elapsed: 2.007794661s
Mar 27 15:31:31.926: INFO: The phase of Pod annotationupdate46061e0e-1b30-45dc-84f6-22c88eeddd77 is Running (Ready = true)
Mar 27 15:31:31.926: INFO: Pod "annotationupdate46061e0e-1b30-45dc-84f6-22c88eeddd77" satisfied condition "running and ready"
Mar 27 15:31:32.456: INFO: Successfully updated pod "annotationupdate46061e0e-1b30-45dc-84f6-22c88eeddd77"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 27 15:31:34.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9342" for this suite. 03/27/23 15:31:34.479
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":287,"skipped":5493,"failed":0}
------------------------------
• [4.613 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:31:29.873
    Mar 27 15:31:29.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename downward-api 03/27/23 15:31:29.88
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:31:29.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:31:29.907
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 03/27/23 15:31:29.91
    Mar 27 15:31:29.918: INFO: Waiting up to 5m0s for pod "annotationupdate46061e0e-1b30-45dc-84f6-22c88eeddd77" in namespace "downward-api-9342" to be "running and ready"
    Mar 27 15:31:29.922: INFO: Pod "annotationupdate46061e0e-1b30-45dc-84f6-22c88eeddd77": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009847ms
    Mar 27 15:31:29.922: INFO: The phase of Pod annotationupdate46061e0e-1b30-45dc-84f6-22c88eeddd77 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:31:31.926: INFO: Pod "annotationupdate46061e0e-1b30-45dc-84f6-22c88eeddd77": Phase="Running", Reason="", readiness=true. Elapsed: 2.007794661s
    Mar 27 15:31:31.926: INFO: The phase of Pod annotationupdate46061e0e-1b30-45dc-84f6-22c88eeddd77 is Running (Ready = true)
    Mar 27 15:31:31.926: INFO: Pod "annotationupdate46061e0e-1b30-45dc-84f6-22c88eeddd77" satisfied condition "running and ready"
    Mar 27 15:31:32.456: INFO: Successfully updated pod "annotationupdate46061e0e-1b30-45dc-84f6-22c88eeddd77"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 27 15:31:34.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9342" for this suite. 03/27/23 15:31:34.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:31:34.486
Mar 27 15:31:34.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename runtimeclass 03/27/23 15:31:34.487
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:31:34.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:31:34.504
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-7764-delete-me 03/27/23 15:31:34.512
STEP: Waiting for the RuntimeClass to disappear 03/27/23 15:31:34.521
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar 27 15:31:34.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7764" for this suite. 03/27/23 15:31:34.535
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":288,"skipped":5498,"failed":0}
------------------------------
• [0.055 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:31:34.486
    Mar 27 15:31:34.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename runtimeclass 03/27/23 15:31:34.487
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:31:34.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:31:34.504
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-7764-delete-me 03/27/23 15:31:34.512
    STEP: Waiting for the RuntimeClass to disappear 03/27/23 15:31:34.521
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar 27 15:31:34.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-7764" for this suite. 03/27/23 15:31:34.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:31:34.542
Mar 27 15:31:34.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename resourcequota 03/27/23 15:31:34.543
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:31:34.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:31:34.561
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 03/27/23 15:31:34.564
STEP: Creating a ResourceQuota 03/27/23 15:31:39.577
STEP: Ensuring resource quota status is calculated 03/27/23 15:31:39.581
STEP: Creating a Service 03/27/23 15:31:41.588
STEP: Creating a NodePort Service 03/27/23 15:31:41.605
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/27/23 15:31:41.626
STEP: Ensuring resource quota status captures service creation 03/27/23 15:31:41.644
STEP: Deleting Services 03/27/23 15:31:43.648
STEP: Ensuring resource quota status released usage 03/27/23 15:31:43.687
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 27 15:31:45.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6885" for this suite. 03/27/23 15:31:45.696
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":289,"skipped":5503,"failed":0}
------------------------------
• [SLOW TEST] [11.160 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:31:34.542
    Mar 27 15:31:34.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename resourcequota 03/27/23 15:31:34.543
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:31:34.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:31:34.561
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 03/27/23 15:31:34.564
    STEP: Creating a ResourceQuota 03/27/23 15:31:39.577
    STEP: Ensuring resource quota status is calculated 03/27/23 15:31:39.581
    STEP: Creating a Service 03/27/23 15:31:41.588
    STEP: Creating a NodePort Service 03/27/23 15:31:41.605
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/27/23 15:31:41.626
    STEP: Ensuring resource quota status captures service creation 03/27/23 15:31:41.644
    STEP: Deleting Services 03/27/23 15:31:43.648
    STEP: Ensuring resource quota status released usage 03/27/23 15:31:43.687
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 27 15:31:45.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6885" for this suite. 03/27/23 15:31:45.696
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:31:45.702
Mar 27 15:31:45.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename services 03/27/23 15:31:45.703
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:31:45.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:31:45.725
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2811 03/27/23 15:31:45.728
STEP: changing the ExternalName service to type=ClusterIP 03/27/23 15:31:45.735
STEP: creating replication controller externalname-service in namespace services-2811 03/27/23 15:31:45.757
I0327 15:31:45.764194      24 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2811, replica count: 2
I0327 15:31:48.816335      24 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 15:31:48.816: INFO: Creating new exec pod
Mar 27 15:31:48.825: INFO: Waiting up to 5m0s for pod "execpodjwzpp" in namespace "services-2811" to be "running"
Mar 27 15:31:48.828: INFO: Pod "execpodjwzpp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.78775ms
Mar 27 15:31:50.832: INFO: Pod "execpodjwzpp": Phase="Running", Reason="", readiness=true. Elapsed: 2.007177124s
Mar 27 15:31:50.833: INFO: Pod "execpodjwzpp" satisfied condition "running"
Mar 27 15:31:51.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2811 exec execpodjwzpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 27 15:31:51.999: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 27 15:31:51.999: INFO: stdout: "externalname-service-csmvs"
Mar 27 15:31:51.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2811 exec execpodjwzpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.31.240 80'
Mar 27 15:31:52.177: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.31.240 80\nConnection to 10.240.31.240 80 port [tcp/http] succeeded!\n"
Mar 27 15:31:52.177: INFO: stdout: "externalname-service-cg5lp"
Mar 27 15:31:52.177: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 27 15:31:52.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2811" for this suite. 03/27/23 15:31:52.199
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":290,"skipped":5505,"failed":0}
------------------------------
• [SLOW TEST] [6.504 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:31:45.702
    Mar 27 15:31:45.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename services 03/27/23 15:31:45.703
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:31:45.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:31:45.725
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-2811 03/27/23 15:31:45.728
    STEP: changing the ExternalName service to type=ClusterIP 03/27/23 15:31:45.735
    STEP: creating replication controller externalname-service in namespace services-2811 03/27/23 15:31:45.757
    I0327 15:31:45.764194      24 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2811, replica count: 2
    I0327 15:31:48.816335      24 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 15:31:48.816: INFO: Creating new exec pod
    Mar 27 15:31:48.825: INFO: Waiting up to 5m0s for pod "execpodjwzpp" in namespace "services-2811" to be "running"
    Mar 27 15:31:48.828: INFO: Pod "execpodjwzpp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.78775ms
    Mar 27 15:31:50.832: INFO: Pod "execpodjwzpp": Phase="Running", Reason="", readiness=true. Elapsed: 2.007177124s
    Mar 27 15:31:50.833: INFO: Pod "execpodjwzpp" satisfied condition "running"
    Mar 27 15:31:51.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2811 exec execpodjwzpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar 27 15:31:51.999: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 27 15:31:51.999: INFO: stdout: "externalname-service-csmvs"
    Mar 27 15:31:51.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-2811 exec execpodjwzpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.31.240 80'
    Mar 27 15:31:52.177: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.31.240 80\nConnection to 10.240.31.240 80 port [tcp/http] succeeded!\n"
    Mar 27 15:31:52.177: INFO: stdout: "externalname-service-cg5lp"
    Mar 27 15:31:52.177: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 27 15:31:52.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2811" for this suite. 03/27/23 15:31:52.199
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:31:52.207
Mar 27 15:31:52.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename emptydir 03/27/23 15:31:52.207
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:31:52.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:31:52.227
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 03/27/23 15:31:52.229
Mar 27 15:31:52.238: INFO: Waiting up to 5m0s for pod "pod-6b3a464e-b035-442b-8646-2ccce321920b" in namespace "emptydir-8849" to be "Succeeded or Failed"
Mar 27 15:31:52.242: INFO: Pod "pod-6b3a464e-b035-442b-8646-2ccce321920b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.601465ms
Mar 27 15:31:54.246: INFO: Pod "pod-6b3a464e-b035-442b-8646-2ccce321920b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008102627s
Mar 27 15:31:56.246: INFO: Pod "pod-6b3a464e-b035-442b-8646-2ccce321920b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008374625s
STEP: Saw pod success 03/27/23 15:31:56.246
Mar 27 15:31:56.246: INFO: Pod "pod-6b3a464e-b035-442b-8646-2ccce321920b" satisfied condition "Succeeded or Failed"
Mar 27 15:31:56.249: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-6b3a464e-b035-442b-8646-2ccce321920b container test-container: <nil>
STEP: delete the pod 03/27/23 15:31:56.26
Mar 27 15:31:56.274: INFO: Waiting for pod pod-6b3a464e-b035-442b-8646-2ccce321920b to disappear
Mar 27 15:31:56.276: INFO: Pod pod-6b3a464e-b035-442b-8646-2ccce321920b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 27 15:31:56.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8849" for this suite. 03/27/23 15:31:56.28
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":291,"skipped":5508,"failed":0}
------------------------------
• [4.083 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:31:52.207
    Mar 27 15:31:52.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename emptydir 03/27/23 15:31:52.207
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:31:52.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:31:52.227
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/27/23 15:31:52.229
    Mar 27 15:31:52.238: INFO: Waiting up to 5m0s for pod "pod-6b3a464e-b035-442b-8646-2ccce321920b" in namespace "emptydir-8849" to be "Succeeded or Failed"
    Mar 27 15:31:52.242: INFO: Pod "pod-6b3a464e-b035-442b-8646-2ccce321920b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.601465ms
    Mar 27 15:31:54.246: INFO: Pod "pod-6b3a464e-b035-442b-8646-2ccce321920b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008102627s
    Mar 27 15:31:56.246: INFO: Pod "pod-6b3a464e-b035-442b-8646-2ccce321920b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008374625s
    STEP: Saw pod success 03/27/23 15:31:56.246
    Mar 27 15:31:56.246: INFO: Pod "pod-6b3a464e-b035-442b-8646-2ccce321920b" satisfied condition "Succeeded or Failed"
    Mar 27 15:31:56.249: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-6b3a464e-b035-442b-8646-2ccce321920b container test-container: <nil>
    STEP: delete the pod 03/27/23 15:31:56.26
    Mar 27 15:31:56.274: INFO: Waiting for pod pod-6b3a464e-b035-442b-8646-2ccce321920b to disappear
    Mar 27 15:31:56.276: INFO: Pod pod-6b3a464e-b035-442b-8646-2ccce321920b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 27 15:31:56.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8849" for this suite. 03/27/23 15:31:56.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:31:56.29
Mar 27 15:31:56.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename downward-api 03/27/23 15:31:56.291
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:31:56.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:31:56.307
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 03/27/23 15:31:56.31
Mar 27 15:31:56.323: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de6e0a1a-876f-41bc-818b-f8d06ab4df72" in namespace "downward-api-8536" to be "Succeeded or Failed"
Mar 27 15:31:56.329: INFO: Pod "downwardapi-volume-de6e0a1a-876f-41bc-818b-f8d06ab4df72": Phase="Pending", Reason="", readiness=false. Elapsed: 6.232539ms
Mar 27 15:31:58.335: INFO: Pod "downwardapi-volume-de6e0a1a-876f-41bc-818b-f8d06ab4df72": Phase="Running", Reason="", readiness=true. Elapsed: 2.011564355s
Mar 27 15:32:00.333: INFO: Pod "downwardapi-volume-de6e0a1a-876f-41bc-818b-f8d06ab4df72": Phase="Running", Reason="", readiness=false. Elapsed: 4.009450733s
Mar 27 15:32:02.335: INFO: Pod "downwardapi-volume-de6e0a1a-876f-41bc-818b-f8d06ab4df72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011600182s
STEP: Saw pod success 03/27/23 15:32:02.335
Mar 27 15:32:02.335: INFO: Pod "downwardapi-volume-de6e0a1a-876f-41bc-818b-f8d06ab4df72" satisfied condition "Succeeded or Failed"
Mar 27 15:32:02.338: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-de6e0a1a-876f-41bc-818b-f8d06ab4df72 container client-container: <nil>
STEP: delete the pod 03/27/23 15:32:02.345
Mar 27 15:32:02.363: INFO: Waiting for pod downwardapi-volume-de6e0a1a-876f-41bc-818b-f8d06ab4df72 to disappear
Mar 27 15:32:02.367: INFO: Pod downwardapi-volume-de6e0a1a-876f-41bc-818b-f8d06ab4df72 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 27 15:32:02.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8536" for this suite. 03/27/23 15:32:02.371
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":292,"skipped":5514,"failed":0}
------------------------------
• [SLOW TEST] [6.089 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:31:56.29
    Mar 27 15:31:56.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename downward-api 03/27/23 15:31:56.291
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:31:56.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:31:56.307
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 03/27/23 15:31:56.31
    Mar 27 15:31:56.323: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de6e0a1a-876f-41bc-818b-f8d06ab4df72" in namespace "downward-api-8536" to be "Succeeded or Failed"
    Mar 27 15:31:56.329: INFO: Pod "downwardapi-volume-de6e0a1a-876f-41bc-818b-f8d06ab4df72": Phase="Pending", Reason="", readiness=false. Elapsed: 6.232539ms
    Mar 27 15:31:58.335: INFO: Pod "downwardapi-volume-de6e0a1a-876f-41bc-818b-f8d06ab4df72": Phase="Running", Reason="", readiness=true. Elapsed: 2.011564355s
    Mar 27 15:32:00.333: INFO: Pod "downwardapi-volume-de6e0a1a-876f-41bc-818b-f8d06ab4df72": Phase="Running", Reason="", readiness=false. Elapsed: 4.009450733s
    Mar 27 15:32:02.335: INFO: Pod "downwardapi-volume-de6e0a1a-876f-41bc-818b-f8d06ab4df72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011600182s
    STEP: Saw pod success 03/27/23 15:32:02.335
    Mar 27 15:32:02.335: INFO: Pod "downwardapi-volume-de6e0a1a-876f-41bc-818b-f8d06ab4df72" satisfied condition "Succeeded or Failed"
    Mar 27 15:32:02.338: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-de6e0a1a-876f-41bc-818b-f8d06ab4df72 container client-container: <nil>
    STEP: delete the pod 03/27/23 15:32:02.345
    Mar 27 15:32:02.363: INFO: Waiting for pod downwardapi-volume-de6e0a1a-876f-41bc-818b-f8d06ab4df72 to disappear
    Mar 27 15:32:02.367: INFO: Pod downwardapi-volume-de6e0a1a-876f-41bc-818b-f8d06ab4df72 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 27 15:32:02.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8536" for this suite. 03/27/23 15:32:02.371
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:32:02.38
Mar 27 15:32:02.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename services 03/27/23 15:32:02.381
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:32:02.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:32:02.394
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-6355 03/27/23 15:32:02.397
STEP: creating service affinity-nodeport-transition in namespace services-6355 03/27/23 15:32:02.397
STEP: creating replication controller affinity-nodeport-transition in namespace services-6355 03/27/23 15:32:02.417
I0327 15:32:02.425867      24 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6355, replica count: 3
I0327 15:32:05.476711      24 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 15:32:05.488: INFO: Creating new exec pod
Mar 27 15:32:05.495: INFO: Waiting up to 5m0s for pod "execpod-affinityjwz9p" in namespace "services-6355" to be "running"
Mar 27 15:32:05.499: INFO: Pod "execpod-affinityjwz9p": Phase="Pending", Reason="", readiness=false. Elapsed: 3.803542ms
Mar 27 15:32:07.502: INFO: Pod "execpod-affinityjwz9p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006891848s
Mar 27 15:32:09.503: INFO: Pod "execpod-affinityjwz9p": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007619164s
Mar 27 15:32:11.505: INFO: Pod "execpod-affinityjwz9p": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009430396s
Mar 27 15:32:13.503: INFO: Pod "execpod-affinityjwz9p": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007713463s
Mar 27 15:32:15.503: INFO: Pod "execpod-affinityjwz9p": Phase="Running", Reason="", readiness=true. Elapsed: 10.008229093s
Mar 27 15:32:15.503: INFO: Pod "execpod-affinityjwz9p" satisfied condition "running"
Mar 27 15:32:16.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-6355 exec execpod-affinityjwz9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Mar 27 15:32:16.690: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar 27 15:32:16.690: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 15:32:16.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-6355 exec execpod-affinityjwz9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.25.61 80'
Mar 27 15:32:16.843: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.25.61 80\nConnection to 10.240.25.61 80 port [tcp/http] succeeded!\n"
Mar 27 15:32:16.843: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 15:32:16.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-6355 exec execpod-affinityjwz9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.3 32164'
Mar 27 15:32:17.014: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.3 32164\nConnection to 192.168.1.3 32164 port [tcp/*] succeeded!\n"
Mar 27 15:32:17.014: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 15:32:17.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-6355 exec execpod-affinityjwz9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.7 32164'
Mar 27 15:32:17.167: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.7 32164\nConnection to 192.168.1.7 32164 port [tcp/*] succeeded!\n"
Mar 27 15:32:17.167: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 15:32:17.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-6355 exec execpod-affinityjwz9p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.3:32164/ ; done'
Mar 27 15:32:17.493: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n"
Mar 27 15:32:17.493: INFO: stdout: "\naffinity-nodeport-transition-hwjrx\naffinity-nodeport-transition-fbbwl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-hwjrx\naffinity-nodeport-transition-fbbwl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-hwjrx\naffinity-nodeport-transition-fbbwl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-hwjrx\naffinity-nodeport-transition-fbbwl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-hwjrx\naffinity-nodeport-transition-fbbwl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-hwjrx"
Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-hwjrx
Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-fbbwl
Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-hwjrx
Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-fbbwl
Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-hwjrx
Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-fbbwl
Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-hwjrx
Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-fbbwl
Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-hwjrx
Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-fbbwl
Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-hwjrx
Mar 27 15:32:17.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-6355 exec execpod-affinityjwz9p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.3:32164/ ; done'
Mar 27 15:32:17.815: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n"
Mar 27 15:32:17.815: INFO: stdout: "\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl"
Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
Mar 27 15:32:17.815: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6355, will wait for the garbage collector to delete the pods 03/27/23 15:32:17.831
Mar 27 15:32:17.890: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.798942ms
Mar 27 15:32:17.991: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.733304ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 27 15:32:20.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6355" for this suite. 03/27/23 15:32:20.827
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":293,"skipped":5528,"failed":0}
------------------------------
• [SLOW TEST] [18.454 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:32:02.38
    Mar 27 15:32:02.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename services 03/27/23 15:32:02.381
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:32:02.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:32:02.394
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-6355 03/27/23 15:32:02.397
    STEP: creating service affinity-nodeport-transition in namespace services-6355 03/27/23 15:32:02.397
    STEP: creating replication controller affinity-nodeport-transition in namespace services-6355 03/27/23 15:32:02.417
    I0327 15:32:02.425867      24 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6355, replica count: 3
    I0327 15:32:05.476711      24 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 15:32:05.488: INFO: Creating new exec pod
    Mar 27 15:32:05.495: INFO: Waiting up to 5m0s for pod "execpod-affinityjwz9p" in namespace "services-6355" to be "running"
    Mar 27 15:32:05.499: INFO: Pod "execpod-affinityjwz9p": Phase="Pending", Reason="", readiness=false. Elapsed: 3.803542ms
    Mar 27 15:32:07.502: INFO: Pod "execpod-affinityjwz9p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006891848s
    Mar 27 15:32:09.503: INFO: Pod "execpod-affinityjwz9p": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007619164s
    Mar 27 15:32:11.505: INFO: Pod "execpod-affinityjwz9p": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009430396s
    Mar 27 15:32:13.503: INFO: Pod "execpod-affinityjwz9p": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007713463s
    Mar 27 15:32:15.503: INFO: Pod "execpod-affinityjwz9p": Phase="Running", Reason="", readiness=true. Elapsed: 10.008229093s
    Mar 27 15:32:15.503: INFO: Pod "execpod-affinityjwz9p" satisfied condition "running"
    Mar 27 15:32:16.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-6355 exec execpod-affinityjwz9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Mar 27 15:32:16.690: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Mar 27 15:32:16.690: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 15:32:16.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-6355 exec execpod-affinityjwz9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.25.61 80'
    Mar 27 15:32:16.843: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.25.61 80\nConnection to 10.240.25.61 80 port [tcp/http] succeeded!\n"
    Mar 27 15:32:16.843: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 15:32:16.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-6355 exec execpod-affinityjwz9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.3 32164'
    Mar 27 15:32:17.014: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.3 32164\nConnection to 192.168.1.3 32164 port [tcp/*] succeeded!\n"
    Mar 27 15:32:17.014: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 15:32:17.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-6355 exec execpod-affinityjwz9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.7 32164'
    Mar 27 15:32:17.167: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.7 32164\nConnection to 192.168.1.7 32164 port [tcp/*] succeeded!\n"
    Mar 27 15:32:17.167: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 15:32:17.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-6355 exec execpod-affinityjwz9p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.3:32164/ ; done'
    Mar 27 15:32:17.493: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n"
    Mar 27 15:32:17.493: INFO: stdout: "\naffinity-nodeport-transition-hwjrx\naffinity-nodeport-transition-fbbwl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-hwjrx\naffinity-nodeport-transition-fbbwl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-hwjrx\naffinity-nodeport-transition-fbbwl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-hwjrx\naffinity-nodeport-transition-fbbwl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-hwjrx\naffinity-nodeport-transition-fbbwl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-hwjrx"
    Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-hwjrx
    Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-fbbwl
    Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-hwjrx
    Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-fbbwl
    Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-hwjrx
    Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-fbbwl
    Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-hwjrx
    Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-fbbwl
    Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-hwjrx
    Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-fbbwl
    Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.493: INFO: Received response from host: affinity-nodeport-transition-hwjrx
    Mar 27 15:32:17.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-6355 exec execpod-affinityjwz9p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.3:32164/ ; done'
    Mar 27 15:32:17.815: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.3:32164/\n"
    Mar 27 15:32:17.815: INFO: stdout: "\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl\naffinity-nodeport-transition-2sbxl"
    Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.815: INFO: Received response from host: affinity-nodeport-transition-2sbxl
    Mar 27 15:32:17.815: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6355, will wait for the garbage collector to delete the pods 03/27/23 15:32:17.831
    Mar 27 15:32:17.890: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.798942ms
    Mar 27 15:32:17.991: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.733304ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 27 15:32:20.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6355" for this suite. 03/27/23 15:32:20.827
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:32:20.835
Mar 27 15:32:20.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename disruption 03/27/23 15:32:20.836
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:32:20.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:32:20.858
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 03/27/23 15:32:20.867
STEP: Waiting for all pods to be running 03/27/23 15:32:22.912
Mar 27 15:32:22.923: INFO: running pods: 0 < 3
Mar 27 15:32:24.936: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar 27 15:32:26.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5954" for this suite. 03/27/23 15:32:26.935
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":294,"skipped":5547,"failed":0}
------------------------------
• [SLOW TEST] [6.107 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:32:20.835
    Mar 27 15:32:20.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename disruption 03/27/23 15:32:20.836
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:32:20.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:32:20.858
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 03/27/23 15:32:20.867
    STEP: Waiting for all pods to be running 03/27/23 15:32:22.912
    Mar 27 15:32:22.923: INFO: running pods: 0 < 3
    Mar 27 15:32:24.936: INFO: running pods: 2 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar 27 15:32:26.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5954" for this suite. 03/27/23 15:32:26.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:32:26.942
Mar 27 15:32:26.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename dns 03/27/23 15:32:26.944
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:32:26.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:32:26.959
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 03/27/23 15:32:26.962
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4402.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4402.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 03/27/23 15:32:26.968
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4402.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4402.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 03/27/23 15:32:26.968
STEP: creating a pod to probe DNS 03/27/23 15:32:26.968
STEP: submitting the pod to kubernetes 03/27/23 15:32:26.968
Mar 27 15:32:26.978: INFO: Waiting up to 15m0s for pod "dns-test-d0576b97-fe0d-4b2a-a305-024530e1b3a7" in namespace "dns-4402" to be "running"
Mar 27 15:32:26.981: INFO: Pod "dns-test-d0576b97-fe0d-4b2a-a305-024530e1b3a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.838419ms
Mar 27 15:32:28.986: INFO: Pod "dns-test-d0576b97-fe0d-4b2a-a305-024530e1b3a7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008514293s
Mar 27 15:32:28.986: INFO: Pod "dns-test-d0576b97-fe0d-4b2a-a305-024530e1b3a7" satisfied condition "running"
STEP: retrieving the pod 03/27/23 15:32:28.986
STEP: looking for the results for each expected name from probers 03/27/23 15:32:28.991
Mar 27 15:32:29.015: INFO: DNS probes using dns-4402/dns-test-d0576b97-fe0d-4b2a-a305-024530e1b3a7 succeeded

STEP: deleting the pod 03/27/23 15:32:29.015
STEP: deleting the test headless service 03/27/23 15:32:29.029
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 27 15:32:29.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4402" for this suite. 03/27/23 15:32:29.044
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":295,"skipped":5552,"failed":0}
------------------------------
• [2.111 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:32:26.942
    Mar 27 15:32:26.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename dns 03/27/23 15:32:26.944
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:32:26.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:32:26.959
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 03/27/23 15:32:26.962
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4402.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4402.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     03/27/23 15:32:26.968
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4402.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4402.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     03/27/23 15:32:26.968
    STEP: creating a pod to probe DNS 03/27/23 15:32:26.968
    STEP: submitting the pod to kubernetes 03/27/23 15:32:26.968
    Mar 27 15:32:26.978: INFO: Waiting up to 15m0s for pod "dns-test-d0576b97-fe0d-4b2a-a305-024530e1b3a7" in namespace "dns-4402" to be "running"
    Mar 27 15:32:26.981: INFO: Pod "dns-test-d0576b97-fe0d-4b2a-a305-024530e1b3a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.838419ms
    Mar 27 15:32:28.986: INFO: Pod "dns-test-d0576b97-fe0d-4b2a-a305-024530e1b3a7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008514293s
    Mar 27 15:32:28.986: INFO: Pod "dns-test-d0576b97-fe0d-4b2a-a305-024530e1b3a7" satisfied condition "running"
    STEP: retrieving the pod 03/27/23 15:32:28.986
    STEP: looking for the results for each expected name from probers 03/27/23 15:32:28.991
    Mar 27 15:32:29.015: INFO: DNS probes using dns-4402/dns-test-d0576b97-fe0d-4b2a-a305-024530e1b3a7 succeeded

    STEP: deleting the pod 03/27/23 15:32:29.015
    STEP: deleting the test headless service 03/27/23 15:32:29.029
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 27 15:32:29.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4402" for this suite. 03/27/23 15:32:29.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:32:29.054
Mar 27 15:32:29.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename var-expansion 03/27/23 15:32:29.055
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:32:29.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:32:29.073
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Mar 27 15:32:29.091: INFO: Waiting up to 2m0s for pod "var-expansion-2afa9f6c-68d1-4e8e-8de7-dc5b2f2e4931" in namespace "var-expansion-7966" to be "container 0 failed with reason CreateContainerConfigError"
Mar 27 15:32:29.095: INFO: Pod "var-expansion-2afa9f6c-68d1-4e8e-8de7-dc5b2f2e4931": Phase="Pending", Reason="", readiness=false. Elapsed: 4.369863ms
Mar 27 15:32:31.102: INFO: Pod "var-expansion-2afa9f6c-68d1-4e8e-8de7-dc5b2f2e4931": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010654077s
Mar 27 15:32:31.102: INFO: Pod "var-expansion-2afa9f6c-68d1-4e8e-8de7-dc5b2f2e4931" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar 27 15:32:31.102: INFO: Deleting pod "var-expansion-2afa9f6c-68d1-4e8e-8de7-dc5b2f2e4931" in namespace "var-expansion-7966"
Mar 27 15:32:31.111: INFO: Wait up to 5m0s for pod "var-expansion-2afa9f6c-68d1-4e8e-8de7-dc5b2f2e4931" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 27 15:32:35.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7966" for this suite. 03/27/23 15:32:35.124
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":296,"skipped":5588,"failed":0}
------------------------------
• [SLOW TEST] [6.080 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:32:29.054
    Mar 27 15:32:29.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename var-expansion 03/27/23 15:32:29.055
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:32:29.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:32:29.073
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Mar 27 15:32:29.091: INFO: Waiting up to 2m0s for pod "var-expansion-2afa9f6c-68d1-4e8e-8de7-dc5b2f2e4931" in namespace "var-expansion-7966" to be "container 0 failed with reason CreateContainerConfigError"
    Mar 27 15:32:29.095: INFO: Pod "var-expansion-2afa9f6c-68d1-4e8e-8de7-dc5b2f2e4931": Phase="Pending", Reason="", readiness=false. Elapsed: 4.369863ms
    Mar 27 15:32:31.102: INFO: Pod "var-expansion-2afa9f6c-68d1-4e8e-8de7-dc5b2f2e4931": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010654077s
    Mar 27 15:32:31.102: INFO: Pod "var-expansion-2afa9f6c-68d1-4e8e-8de7-dc5b2f2e4931" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar 27 15:32:31.102: INFO: Deleting pod "var-expansion-2afa9f6c-68d1-4e8e-8de7-dc5b2f2e4931" in namespace "var-expansion-7966"
    Mar 27 15:32:31.111: INFO: Wait up to 5m0s for pod "var-expansion-2afa9f6c-68d1-4e8e-8de7-dc5b2f2e4931" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 27 15:32:35.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7966" for this suite. 03/27/23 15:32:35.124
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:32:35.134
Mar 27 15:32:35.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename watch 03/27/23 15:32:35.135
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:32:35.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:32:35.149
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 03/27/23 15:32:35.152
STEP: modifying the configmap once 03/27/23 15:32:35.162
STEP: modifying the configmap a second time 03/27/23 15:32:35.17
STEP: deleting the configmap 03/27/23 15:32:35.179
STEP: creating a watch on configmaps from the resource version returned by the first update 03/27/23 15:32:35.184
STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/27/23 15:32:35.185
Mar 27 15:32:35.185: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9293  5eb9e417-7fb8-4c18-ab48-5b4483c09ee4 51535 0 2023-03-27 15:32:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-27 15:32:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 15:32:35.189: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9293  5eb9e417-7fb8-4c18-ab48-5b4483c09ee4 51536 0 2023-03-27 15:32:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-27 15:32:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar 27 15:32:35.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9293" for this suite. 03/27/23 15:32:35.196
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":297,"skipped":5592,"failed":0}
------------------------------
• [0.072 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:32:35.134
    Mar 27 15:32:35.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename watch 03/27/23 15:32:35.135
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:32:35.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:32:35.149
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 03/27/23 15:32:35.152
    STEP: modifying the configmap once 03/27/23 15:32:35.162
    STEP: modifying the configmap a second time 03/27/23 15:32:35.17
    STEP: deleting the configmap 03/27/23 15:32:35.179
    STEP: creating a watch on configmaps from the resource version returned by the first update 03/27/23 15:32:35.184
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/27/23 15:32:35.185
    Mar 27 15:32:35.185: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9293  5eb9e417-7fb8-4c18-ab48-5b4483c09ee4 51535 0 2023-03-27 15:32:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-27 15:32:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 15:32:35.189: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9293  5eb9e417-7fb8-4c18-ab48-5b4483c09ee4 51536 0 2023-03-27 15:32:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-27 15:32:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar 27 15:32:35.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9293" for this suite. 03/27/23 15:32:35.196
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:32:35.207
Mar 27 15:32:35.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename endpointslice 03/27/23 15:32:35.208
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:32:35.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:32:35.236
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar 27 15:32:37.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1340" for this suite. 03/27/23 15:32:37.307
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":298,"skipped":5595,"failed":0}
------------------------------
• [2.108 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:32:35.207
    Mar 27 15:32:35.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename endpointslice 03/27/23 15:32:35.208
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:32:35.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:32:35.236
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar 27 15:32:37.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-1340" for this suite. 03/27/23 15:32:37.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:32:37.317
Mar 27 15:32:37.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename podtemplate 03/27/23 15:32:37.318
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:32:37.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:32:37.339
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar 27 15:32:37.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4899" for this suite. 03/27/23 15:32:37.397
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":299,"skipped":5621,"failed":0}
------------------------------
• [0.088 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:32:37.317
    Mar 27 15:32:37.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename podtemplate 03/27/23 15:32:37.318
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:32:37.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:32:37.339
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar 27 15:32:37.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-4899" for this suite. 03/27/23 15:32:37.397
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:32:37.406
Mar 27 15:32:37.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 15:32:37.407
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:32:37.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:32:37.419
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Mar 27 15:32:37.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 15:32:40.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2204" for this suite. 03/27/23 15:32:40.658
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":300,"skipped":5636,"failed":0}
------------------------------
• [3.260 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:32:37.406
    Mar 27 15:32:37.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 15:32:37.407
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:32:37.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:32:37.419
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Mar 27 15:32:37.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 15:32:40.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2204" for this suite. 03/27/23 15:32:40.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:32:40.667
Mar 27 15:32:40.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename subpath 03/27/23 15:32:40.668
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:32:40.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:32:40.686
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/27/23 15:32:40.689
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-sntp 03/27/23 15:32:40.699
STEP: Creating a pod to test atomic-volume-subpath 03/27/23 15:32:40.699
Mar 27 15:32:40.710: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-sntp" in namespace "subpath-7960" to be "Succeeded or Failed"
Mar 27 15:32:40.714: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.563142ms
Mar 27 15:32:42.718: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 2.008632543s
Mar 27 15:32:44.720: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 4.009882299s
Mar 27 15:32:46.718: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 6.008480984s
Mar 27 15:32:48.718: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 8.008724442s
Mar 27 15:32:50.721: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 10.011771053s
Mar 27 15:32:52.718: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 12.008751489s
Mar 27 15:32:54.719: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 14.009577499s
Mar 27 15:32:56.718: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 16.008325531s
Mar 27 15:32:58.720: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 18.00985551s
Mar 27 15:33:00.719: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 20.00938151s
Mar 27 15:33:02.721: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 22.010909856s
Mar 27 15:33:04.721: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=false. Elapsed: 24.011020016s
Mar 27 15:33:06.719: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.009188819s
STEP: Saw pod success 03/27/23 15:33:06.719
Mar 27 15:33:06.719: INFO: Pod "pod-subpath-test-secret-sntp" satisfied condition "Succeeded or Failed"
Mar 27 15:33:06.723: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-subpath-test-secret-sntp container test-container-subpath-secret-sntp: <nil>
STEP: delete the pod 03/27/23 15:33:06.735
Mar 27 15:33:06.749: INFO: Waiting for pod pod-subpath-test-secret-sntp to disappear
Mar 27 15:33:06.752: INFO: Pod pod-subpath-test-secret-sntp no longer exists
STEP: Deleting pod pod-subpath-test-secret-sntp 03/27/23 15:33:06.752
Mar 27 15:33:06.753: INFO: Deleting pod "pod-subpath-test-secret-sntp" in namespace "subpath-7960"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar 27 15:33:06.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7960" for this suite. 03/27/23 15:33:06.763
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":301,"skipped":5702,"failed":0}
------------------------------
• [SLOW TEST] [26.105 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:32:40.667
    Mar 27 15:32:40.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename subpath 03/27/23 15:32:40.668
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:32:40.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:32:40.686
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/27/23 15:32:40.689
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-sntp 03/27/23 15:32:40.699
    STEP: Creating a pod to test atomic-volume-subpath 03/27/23 15:32:40.699
    Mar 27 15:32:40.710: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-sntp" in namespace "subpath-7960" to be "Succeeded or Failed"
    Mar 27 15:32:40.714: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.563142ms
    Mar 27 15:32:42.718: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 2.008632543s
    Mar 27 15:32:44.720: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 4.009882299s
    Mar 27 15:32:46.718: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 6.008480984s
    Mar 27 15:32:48.718: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 8.008724442s
    Mar 27 15:32:50.721: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 10.011771053s
    Mar 27 15:32:52.718: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 12.008751489s
    Mar 27 15:32:54.719: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 14.009577499s
    Mar 27 15:32:56.718: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 16.008325531s
    Mar 27 15:32:58.720: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 18.00985551s
    Mar 27 15:33:00.719: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 20.00938151s
    Mar 27 15:33:02.721: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=true. Elapsed: 22.010909856s
    Mar 27 15:33:04.721: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Running", Reason="", readiness=false. Elapsed: 24.011020016s
    Mar 27 15:33:06.719: INFO: Pod "pod-subpath-test-secret-sntp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.009188819s
    STEP: Saw pod success 03/27/23 15:33:06.719
    Mar 27 15:33:06.719: INFO: Pod "pod-subpath-test-secret-sntp" satisfied condition "Succeeded or Failed"
    Mar 27 15:33:06.723: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-subpath-test-secret-sntp container test-container-subpath-secret-sntp: <nil>
    STEP: delete the pod 03/27/23 15:33:06.735
    Mar 27 15:33:06.749: INFO: Waiting for pod pod-subpath-test-secret-sntp to disappear
    Mar 27 15:33:06.752: INFO: Pod pod-subpath-test-secret-sntp no longer exists
    STEP: Deleting pod pod-subpath-test-secret-sntp 03/27/23 15:33:06.752
    Mar 27 15:33:06.753: INFO: Deleting pod "pod-subpath-test-secret-sntp" in namespace "subpath-7960"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar 27 15:33:06.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7960" for this suite. 03/27/23 15:33:06.763
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:33:06.774
Mar 27 15:33:06.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename emptydir 03/27/23 15:33:06.775
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:33:06.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:33:06.795
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 03/27/23 15:33:06.798
Mar 27 15:33:06.807: INFO: Waiting up to 5m0s for pod "pod-229b9ecb-b81d-40d3-81de-bda5d99d70bb" in namespace "emptydir-5363" to be "Succeeded or Failed"
Mar 27 15:33:06.810: INFO: Pod "pod-229b9ecb-b81d-40d3-81de-bda5d99d70bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.719824ms
Mar 27 15:33:08.814: INFO: Pod "pod-229b9ecb-b81d-40d3-81de-bda5d99d70bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006545529s
Mar 27 15:33:10.814: INFO: Pod "pod-229b9ecb-b81d-40d3-81de-bda5d99d70bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006980051s
Mar 27 15:33:12.815: INFO: Pod "pod-229b9ecb-b81d-40d3-81de-bda5d99d70bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007609544s
STEP: Saw pod success 03/27/23 15:33:12.815
Mar 27 15:33:12.815: INFO: Pod "pod-229b9ecb-b81d-40d3-81de-bda5d99d70bb" satisfied condition "Succeeded or Failed"
Mar 27 15:33:12.818: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-229b9ecb-b81d-40d3-81de-bda5d99d70bb container test-container: <nil>
STEP: delete the pod 03/27/23 15:33:12.826
Mar 27 15:33:12.838: INFO: Waiting for pod pod-229b9ecb-b81d-40d3-81de-bda5d99d70bb to disappear
Mar 27 15:33:12.841: INFO: Pod pod-229b9ecb-b81d-40d3-81de-bda5d99d70bb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 27 15:33:12.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5363" for this suite. 03/27/23 15:33:12.849
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":302,"skipped":5703,"failed":0}
------------------------------
• [SLOW TEST] [6.082 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:33:06.774
    Mar 27 15:33:06.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename emptydir 03/27/23 15:33:06.775
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:33:06.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:33:06.795
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/27/23 15:33:06.798
    Mar 27 15:33:06.807: INFO: Waiting up to 5m0s for pod "pod-229b9ecb-b81d-40d3-81de-bda5d99d70bb" in namespace "emptydir-5363" to be "Succeeded or Failed"
    Mar 27 15:33:06.810: INFO: Pod "pod-229b9ecb-b81d-40d3-81de-bda5d99d70bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.719824ms
    Mar 27 15:33:08.814: INFO: Pod "pod-229b9ecb-b81d-40d3-81de-bda5d99d70bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006545529s
    Mar 27 15:33:10.814: INFO: Pod "pod-229b9ecb-b81d-40d3-81de-bda5d99d70bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006980051s
    Mar 27 15:33:12.815: INFO: Pod "pod-229b9ecb-b81d-40d3-81de-bda5d99d70bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007609544s
    STEP: Saw pod success 03/27/23 15:33:12.815
    Mar 27 15:33:12.815: INFO: Pod "pod-229b9ecb-b81d-40d3-81de-bda5d99d70bb" satisfied condition "Succeeded or Failed"
    Mar 27 15:33:12.818: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-229b9ecb-b81d-40d3-81de-bda5d99d70bb container test-container: <nil>
    STEP: delete the pod 03/27/23 15:33:12.826
    Mar 27 15:33:12.838: INFO: Waiting for pod pod-229b9ecb-b81d-40d3-81de-bda5d99d70bb to disappear
    Mar 27 15:33:12.841: INFO: Pod pod-229b9ecb-b81d-40d3-81de-bda5d99d70bb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 27 15:33:12.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5363" for this suite. 03/27/23 15:33:12.849
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:33:12.857
Mar 27 15:33:12.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 15:33:12.858
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:33:12.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:33:12.873
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 03/27/23 15:33:12.876
Mar 27 15:33:12.886: INFO: Waiting up to 5m0s for pod "labelsupdate5c7f72d3-72e4-4b3b-97aa-5c17d55d7104" in namespace "projected-1659" to be "running and ready"
Mar 27 15:33:12.892: INFO: Pod "labelsupdate5c7f72d3-72e4-4b3b-97aa-5c17d55d7104": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084803ms
Mar 27 15:33:12.892: INFO: The phase of Pod labelsupdate5c7f72d3-72e4-4b3b-97aa-5c17d55d7104 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:33:14.897: INFO: Pod "labelsupdate5c7f72d3-72e4-4b3b-97aa-5c17d55d7104": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010782125s
Mar 27 15:33:14.897: INFO: The phase of Pod labelsupdate5c7f72d3-72e4-4b3b-97aa-5c17d55d7104 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:33:16.897: INFO: Pod "labelsupdate5c7f72d3-72e4-4b3b-97aa-5c17d55d7104": Phase="Running", Reason="", readiness=true. Elapsed: 4.010980531s
Mar 27 15:33:16.897: INFO: The phase of Pod labelsupdate5c7f72d3-72e4-4b3b-97aa-5c17d55d7104 is Running (Ready = true)
Mar 27 15:33:16.897: INFO: Pod "labelsupdate5c7f72d3-72e4-4b3b-97aa-5c17d55d7104" satisfied condition "running and ready"
Mar 27 15:33:17.424: INFO: Successfully updated pod "labelsupdate5c7f72d3-72e4-4b3b-97aa-5c17d55d7104"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 27 15:33:19.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1659" for this suite. 03/27/23 15:33:19.448
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":303,"skipped":5705,"failed":0}
------------------------------
• [SLOW TEST] [6.597 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:33:12.857
    Mar 27 15:33:12.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 15:33:12.858
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:33:12.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:33:12.873
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 03/27/23 15:33:12.876
    Mar 27 15:33:12.886: INFO: Waiting up to 5m0s for pod "labelsupdate5c7f72d3-72e4-4b3b-97aa-5c17d55d7104" in namespace "projected-1659" to be "running and ready"
    Mar 27 15:33:12.892: INFO: Pod "labelsupdate5c7f72d3-72e4-4b3b-97aa-5c17d55d7104": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084803ms
    Mar 27 15:33:12.892: INFO: The phase of Pod labelsupdate5c7f72d3-72e4-4b3b-97aa-5c17d55d7104 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:33:14.897: INFO: Pod "labelsupdate5c7f72d3-72e4-4b3b-97aa-5c17d55d7104": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010782125s
    Mar 27 15:33:14.897: INFO: The phase of Pod labelsupdate5c7f72d3-72e4-4b3b-97aa-5c17d55d7104 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:33:16.897: INFO: Pod "labelsupdate5c7f72d3-72e4-4b3b-97aa-5c17d55d7104": Phase="Running", Reason="", readiness=true. Elapsed: 4.010980531s
    Mar 27 15:33:16.897: INFO: The phase of Pod labelsupdate5c7f72d3-72e4-4b3b-97aa-5c17d55d7104 is Running (Ready = true)
    Mar 27 15:33:16.897: INFO: Pod "labelsupdate5c7f72d3-72e4-4b3b-97aa-5c17d55d7104" satisfied condition "running and ready"
    Mar 27 15:33:17.424: INFO: Successfully updated pod "labelsupdate5c7f72d3-72e4-4b3b-97aa-5c17d55d7104"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 27 15:33:19.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1659" for this suite. 03/27/23 15:33:19.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:33:19.463
Mar 27 15:33:19.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename gc 03/27/23 15:33:19.464
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:33:19.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:33:19.483
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 03/27/23 15:33:19.493
STEP: delete the rc 03/27/23 15:33:24.511
STEP: wait for the rc to be deleted 03/27/23 15:33:24.518
Mar 27 15:33:25.536: INFO: 80 pods remaining
Mar 27 15:33:25.536: INFO: 80 pods has nil DeletionTimestamp
Mar 27 15:33:25.536: INFO: 
Mar 27 15:33:26.537: INFO: 71 pods remaining
Mar 27 15:33:26.537: INFO: 71 pods has nil DeletionTimestamp
Mar 27 15:33:26.537: INFO: 
Mar 27 15:33:27.530: INFO: 60 pods remaining
Mar 27 15:33:27.530: INFO: 60 pods has nil DeletionTimestamp
Mar 27 15:33:27.530: INFO: 
Mar 27 15:33:28.528: INFO: 40 pods remaining
Mar 27 15:33:28.528: INFO: 40 pods has nil DeletionTimestamp
Mar 27 15:33:28.528: INFO: 
Mar 27 15:33:29.532: INFO: 31 pods remaining
Mar 27 15:33:29.532: INFO: 31 pods has nil DeletionTimestamp
Mar 27 15:33:29.532: INFO: 
Mar 27 15:33:30.527: INFO: 20 pods remaining
Mar 27 15:33:30.527: INFO: 20 pods has nil DeletionTimestamp
Mar 27 15:33:30.527: INFO: 
STEP: Gathering metrics 03/27/23 15:33:31.526
W0327 15:33:31.537181      24 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 27 15:33:31.537: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 27 15:33:31.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2699" for this suite. 03/27/23 15:33:31.542
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":304,"skipped":5782,"failed":0}
------------------------------
• [SLOW TEST] [12.089 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:33:19.463
    Mar 27 15:33:19.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename gc 03/27/23 15:33:19.464
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:33:19.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:33:19.483
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 03/27/23 15:33:19.493
    STEP: delete the rc 03/27/23 15:33:24.511
    STEP: wait for the rc to be deleted 03/27/23 15:33:24.518
    Mar 27 15:33:25.536: INFO: 80 pods remaining
    Mar 27 15:33:25.536: INFO: 80 pods has nil DeletionTimestamp
    Mar 27 15:33:25.536: INFO: 
    Mar 27 15:33:26.537: INFO: 71 pods remaining
    Mar 27 15:33:26.537: INFO: 71 pods has nil DeletionTimestamp
    Mar 27 15:33:26.537: INFO: 
    Mar 27 15:33:27.530: INFO: 60 pods remaining
    Mar 27 15:33:27.530: INFO: 60 pods has nil DeletionTimestamp
    Mar 27 15:33:27.530: INFO: 
    Mar 27 15:33:28.528: INFO: 40 pods remaining
    Mar 27 15:33:28.528: INFO: 40 pods has nil DeletionTimestamp
    Mar 27 15:33:28.528: INFO: 
    Mar 27 15:33:29.532: INFO: 31 pods remaining
    Mar 27 15:33:29.532: INFO: 31 pods has nil DeletionTimestamp
    Mar 27 15:33:29.532: INFO: 
    Mar 27 15:33:30.527: INFO: 20 pods remaining
    Mar 27 15:33:30.527: INFO: 20 pods has nil DeletionTimestamp
    Mar 27 15:33:30.527: INFO: 
    STEP: Gathering metrics 03/27/23 15:33:31.526
    W0327 15:33:31.537181      24 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 27 15:33:31.537: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 27 15:33:31.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2699" for this suite. 03/27/23 15:33:31.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:33:31.552
Mar 27 15:33:31.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename secrets 03/27/23 15:33:31.554
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:33:31.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:33:31.571
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-bf59eb17-8900-452b-a4a2-331b95b7659d 03/27/23 15:33:31.605
STEP: Creating a pod to test consume secrets 03/27/23 15:33:31.609
Mar 27 15:33:31.616: INFO: Waiting up to 5m0s for pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10" in namespace "secrets-3272" to be "Succeeded or Failed"
Mar 27 15:33:31.621: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.603128ms
Mar 27 15:33:33.626: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010307514s
Mar 27 15:33:35.625: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009508453s
Mar 27 15:33:37.625: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009341205s
Mar 27 15:33:39.628: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011850214s
Mar 27 15:33:41.626: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009629259s
Mar 27 15:33:43.626: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00997427s
Mar 27 15:33:45.625: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009030333s
Mar 27 15:33:47.626: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Running", Reason="", readiness=true. Elapsed: 16.009976836s
Mar 27 15:33:49.626: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 18.009723486s
STEP: Saw pod success 03/27/23 15:33:49.626
Mar 27 15:33:49.626: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10" satisfied condition "Succeeded or Failed"
Mar 27 15:33:49.629: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10 container secret-volume-test: <nil>
STEP: delete the pod 03/27/23 15:33:49.636
Mar 27 15:33:49.652: INFO: Waiting for pod pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10 to disappear
Mar 27 15:33:49.655: INFO: Pod pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 27 15:33:49.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3272" for this suite. 03/27/23 15:33:49.66
STEP: Destroying namespace "secret-namespace-5709" for this suite. 03/27/23 15:33:49.668
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":305,"skipped":5790,"failed":0}
------------------------------
• [SLOW TEST] [18.123 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:33:31.552
    Mar 27 15:33:31.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename secrets 03/27/23 15:33:31.554
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:33:31.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:33:31.571
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-bf59eb17-8900-452b-a4a2-331b95b7659d 03/27/23 15:33:31.605
    STEP: Creating a pod to test consume secrets 03/27/23 15:33:31.609
    Mar 27 15:33:31.616: INFO: Waiting up to 5m0s for pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10" in namespace "secrets-3272" to be "Succeeded or Failed"
    Mar 27 15:33:31.621: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.603128ms
    Mar 27 15:33:33.626: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010307514s
    Mar 27 15:33:35.625: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009508453s
    Mar 27 15:33:37.625: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009341205s
    Mar 27 15:33:39.628: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011850214s
    Mar 27 15:33:41.626: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009629259s
    Mar 27 15:33:43.626: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00997427s
    Mar 27 15:33:45.625: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009030333s
    Mar 27 15:33:47.626: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Running", Reason="", readiness=true. Elapsed: 16.009976836s
    Mar 27 15:33:49.626: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 18.009723486s
    STEP: Saw pod success 03/27/23 15:33:49.626
    Mar 27 15:33:49.626: INFO: Pod "pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10" satisfied condition "Succeeded or Failed"
    Mar 27 15:33:49.629: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10 container secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 15:33:49.636
    Mar 27 15:33:49.652: INFO: Waiting for pod pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10 to disappear
    Mar 27 15:33:49.655: INFO: Pod pod-secrets-cad487c6-a01c-4cd0-bf0b-aa372b521b10 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 27 15:33:49.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3272" for this suite. 03/27/23 15:33:49.66
    STEP: Destroying namespace "secret-namespace-5709" for this suite. 03/27/23 15:33:49.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:33:49.68
Mar 27 15:33:49.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename downward-api 03/27/23 15:33:49.681
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:33:49.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:33:49.698
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 03/27/23 15:33:49.701
Mar 27 15:33:49.713: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12afcc1e-0ec1-4d7f-a093-1ebab5b8b5fb" in namespace "downward-api-6827" to be "Succeeded or Failed"
Mar 27 15:33:49.721: INFO: Pod "downwardapi-volume-12afcc1e-0ec1-4d7f-a093-1ebab5b8b5fb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.218989ms
Mar 27 15:33:51.726: INFO: Pod "downwardapi-volume-12afcc1e-0ec1-4d7f-a093-1ebab5b8b5fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.013443101s
Mar 27 15:33:53.726: INFO: Pod "downwardapi-volume-12afcc1e-0ec1-4d7f-a093-1ebab5b8b5fb": Phase="Running", Reason="", readiness=false. Elapsed: 4.013722256s
Mar 27 15:33:55.726: INFO: Pod "downwardapi-volume-12afcc1e-0ec1-4d7f-a093-1ebab5b8b5fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012991571s
STEP: Saw pod success 03/27/23 15:33:55.726
Mar 27 15:33:55.726: INFO: Pod "downwardapi-volume-12afcc1e-0ec1-4d7f-a093-1ebab5b8b5fb" satisfied condition "Succeeded or Failed"
Mar 27 15:33:55.728: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-12afcc1e-0ec1-4d7f-a093-1ebab5b8b5fb container client-container: <nil>
STEP: delete the pod 03/27/23 15:33:55.736
Mar 27 15:33:55.751: INFO: Waiting for pod downwardapi-volume-12afcc1e-0ec1-4d7f-a093-1ebab5b8b5fb to disappear
Mar 27 15:33:55.753: INFO: Pod downwardapi-volume-12afcc1e-0ec1-4d7f-a093-1ebab5b8b5fb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 27 15:33:55.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6827" for this suite. 03/27/23 15:33:55.759
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":306,"skipped":5803,"failed":0}
------------------------------
• [SLOW TEST] [6.085 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:33:49.68
    Mar 27 15:33:49.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename downward-api 03/27/23 15:33:49.681
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:33:49.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:33:49.698
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 03/27/23 15:33:49.701
    Mar 27 15:33:49.713: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12afcc1e-0ec1-4d7f-a093-1ebab5b8b5fb" in namespace "downward-api-6827" to be "Succeeded or Failed"
    Mar 27 15:33:49.721: INFO: Pod "downwardapi-volume-12afcc1e-0ec1-4d7f-a093-1ebab5b8b5fb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.218989ms
    Mar 27 15:33:51.726: INFO: Pod "downwardapi-volume-12afcc1e-0ec1-4d7f-a093-1ebab5b8b5fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.013443101s
    Mar 27 15:33:53.726: INFO: Pod "downwardapi-volume-12afcc1e-0ec1-4d7f-a093-1ebab5b8b5fb": Phase="Running", Reason="", readiness=false. Elapsed: 4.013722256s
    Mar 27 15:33:55.726: INFO: Pod "downwardapi-volume-12afcc1e-0ec1-4d7f-a093-1ebab5b8b5fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012991571s
    STEP: Saw pod success 03/27/23 15:33:55.726
    Mar 27 15:33:55.726: INFO: Pod "downwardapi-volume-12afcc1e-0ec1-4d7f-a093-1ebab5b8b5fb" satisfied condition "Succeeded or Failed"
    Mar 27 15:33:55.728: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-12afcc1e-0ec1-4d7f-a093-1ebab5b8b5fb container client-container: <nil>
    STEP: delete the pod 03/27/23 15:33:55.736
    Mar 27 15:33:55.751: INFO: Waiting for pod downwardapi-volume-12afcc1e-0ec1-4d7f-a093-1ebab5b8b5fb to disappear
    Mar 27 15:33:55.753: INFO: Pod downwardapi-volume-12afcc1e-0ec1-4d7f-a093-1ebab5b8b5fb no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 27 15:33:55.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6827" for this suite. 03/27/23 15:33:55.759
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:33:55.765
Mar 27 15:33:55.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename downward-api 03/27/23 15:33:55.767
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:33:55.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:33:55.785
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 03/27/23 15:33:55.788
Mar 27 15:33:55.806: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6adef86-a7df-44e4-bfb4-ea176c955872" in namespace "downward-api-519" to be "Succeeded or Failed"
Mar 27 15:33:55.811: INFO: Pod "downwardapi-volume-b6adef86-a7df-44e4-bfb4-ea176c955872": Phase="Pending", Reason="", readiness=false. Elapsed: 4.484085ms
Mar 27 15:33:57.814: INFO: Pod "downwardapi-volume-b6adef86-a7df-44e4-bfb4-ea176c955872": Phase="Running", Reason="", readiness=true. Elapsed: 2.008197645s
Mar 27 15:33:59.815: INFO: Pod "downwardapi-volume-b6adef86-a7df-44e4-bfb4-ea176c955872": Phase="Running", Reason="", readiness=false. Elapsed: 4.008713822s
Mar 27 15:34:01.815: INFO: Pod "downwardapi-volume-b6adef86-a7df-44e4-bfb4-ea176c955872": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008974738s
STEP: Saw pod success 03/27/23 15:34:01.815
Mar 27 15:34:01.815: INFO: Pod "downwardapi-volume-b6adef86-a7df-44e4-bfb4-ea176c955872" satisfied condition "Succeeded or Failed"
Mar 27 15:34:01.820: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-b6adef86-a7df-44e4-bfb4-ea176c955872 container client-container: <nil>
STEP: delete the pod 03/27/23 15:34:01.826
Mar 27 15:34:01.841: INFO: Waiting for pod downwardapi-volume-b6adef86-a7df-44e4-bfb4-ea176c955872 to disappear
Mar 27 15:34:01.843: INFO: Pod downwardapi-volume-b6adef86-a7df-44e4-bfb4-ea176c955872 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 27 15:34:01.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-519" for this suite. 03/27/23 15:34:01.849
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":307,"skipped":5805,"failed":0}
------------------------------
• [SLOW TEST] [6.090 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:33:55.765
    Mar 27 15:33:55.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename downward-api 03/27/23 15:33:55.767
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:33:55.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:33:55.785
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 03/27/23 15:33:55.788
    Mar 27 15:33:55.806: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6adef86-a7df-44e4-bfb4-ea176c955872" in namespace "downward-api-519" to be "Succeeded or Failed"
    Mar 27 15:33:55.811: INFO: Pod "downwardapi-volume-b6adef86-a7df-44e4-bfb4-ea176c955872": Phase="Pending", Reason="", readiness=false. Elapsed: 4.484085ms
    Mar 27 15:33:57.814: INFO: Pod "downwardapi-volume-b6adef86-a7df-44e4-bfb4-ea176c955872": Phase="Running", Reason="", readiness=true. Elapsed: 2.008197645s
    Mar 27 15:33:59.815: INFO: Pod "downwardapi-volume-b6adef86-a7df-44e4-bfb4-ea176c955872": Phase="Running", Reason="", readiness=false. Elapsed: 4.008713822s
    Mar 27 15:34:01.815: INFO: Pod "downwardapi-volume-b6adef86-a7df-44e4-bfb4-ea176c955872": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008974738s
    STEP: Saw pod success 03/27/23 15:34:01.815
    Mar 27 15:34:01.815: INFO: Pod "downwardapi-volume-b6adef86-a7df-44e4-bfb4-ea176c955872" satisfied condition "Succeeded or Failed"
    Mar 27 15:34:01.820: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-b6adef86-a7df-44e4-bfb4-ea176c955872 container client-container: <nil>
    STEP: delete the pod 03/27/23 15:34:01.826
    Mar 27 15:34:01.841: INFO: Waiting for pod downwardapi-volume-b6adef86-a7df-44e4-bfb4-ea176c955872 to disappear
    Mar 27 15:34:01.843: INFO: Pod downwardapi-volume-b6adef86-a7df-44e4-bfb4-ea176c955872 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 27 15:34:01.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-519" for this suite. 03/27/23 15:34:01.849
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:34:01.855
Mar 27 15:34:01.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename pods 03/27/23 15:34:01.856
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:34:01.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:34:01.877
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Mar 27 15:34:01.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: creating the pod 03/27/23 15:34:01.881
STEP: submitting the pod to kubernetes 03/27/23 15:34:01.881
Mar 27 15:34:01.894: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-0ecf5eff-09b1-414a-91d9-cb0c1fa074cd" in namespace "pods-8847" to be "running and ready"
Mar 27 15:34:01.902: INFO: Pod "pod-logs-websocket-0ecf5eff-09b1-414a-91d9-cb0c1fa074cd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.67318ms
Mar 27 15:34:01.902: INFO: The phase of Pod pod-logs-websocket-0ecf5eff-09b1-414a-91d9-cb0c1fa074cd is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:34:03.907: INFO: Pod "pod-logs-websocket-0ecf5eff-09b1-414a-91d9-cb0c1fa074cd": Phase="Running", Reason="", readiness=true. Elapsed: 2.012402941s
Mar 27 15:34:03.907: INFO: The phase of Pod pod-logs-websocket-0ecf5eff-09b1-414a-91d9-cb0c1fa074cd is Running (Ready = true)
Mar 27 15:34:03.907: INFO: Pod "pod-logs-websocket-0ecf5eff-09b1-414a-91d9-cb0c1fa074cd" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 27 15:34:03.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8847" for this suite. 03/27/23 15:34:03.929
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":308,"skipped":5810,"failed":0}
------------------------------
• [2.080 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:34:01.855
    Mar 27 15:34:01.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename pods 03/27/23 15:34:01.856
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:34:01.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:34:01.877
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Mar 27 15:34:01.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: creating the pod 03/27/23 15:34:01.881
    STEP: submitting the pod to kubernetes 03/27/23 15:34:01.881
    Mar 27 15:34:01.894: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-0ecf5eff-09b1-414a-91d9-cb0c1fa074cd" in namespace "pods-8847" to be "running and ready"
    Mar 27 15:34:01.902: INFO: Pod "pod-logs-websocket-0ecf5eff-09b1-414a-91d9-cb0c1fa074cd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.67318ms
    Mar 27 15:34:01.902: INFO: The phase of Pod pod-logs-websocket-0ecf5eff-09b1-414a-91d9-cb0c1fa074cd is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:34:03.907: INFO: Pod "pod-logs-websocket-0ecf5eff-09b1-414a-91d9-cb0c1fa074cd": Phase="Running", Reason="", readiness=true. Elapsed: 2.012402941s
    Mar 27 15:34:03.907: INFO: The phase of Pod pod-logs-websocket-0ecf5eff-09b1-414a-91d9-cb0c1fa074cd is Running (Ready = true)
    Mar 27 15:34:03.907: INFO: Pod "pod-logs-websocket-0ecf5eff-09b1-414a-91d9-cb0c1fa074cd" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 27 15:34:03.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8847" for this suite. 03/27/23 15:34:03.929
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:34:03.937
Mar 27 15:34:03.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 15:34:03.938
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:34:03.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:34:03.957
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-ede70155-e564-49fa-83c8-e7b60f63c9d1 03/27/23 15:34:03.962
STEP: Creating a pod to test consume secrets 03/27/23 15:34:03.968
Mar 27 15:34:03.977: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5328e74b-c843-4f94-86ec-a7fe107bf8c5" in namespace "projected-968" to be "Succeeded or Failed"
Mar 27 15:34:03.979: INFO: Pod "pod-projected-secrets-5328e74b-c843-4f94-86ec-a7fe107bf8c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.44107ms
Mar 27 15:34:05.985: INFO: Pod "pod-projected-secrets-5328e74b-c843-4f94-86ec-a7fe107bf8c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.008355543s
Mar 27 15:34:07.984: INFO: Pod "pod-projected-secrets-5328e74b-c843-4f94-86ec-a7fe107bf8c5": Phase="Running", Reason="", readiness=false. Elapsed: 4.007103987s
Mar 27 15:34:09.983: INFO: Pod "pod-projected-secrets-5328e74b-c843-4f94-86ec-a7fe107bf8c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005580741s
STEP: Saw pod success 03/27/23 15:34:09.983
Mar 27 15:34:09.983: INFO: Pod "pod-projected-secrets-5328e74b-c843-4f94-86ec-a7fe107bf8c5" satisfied condition "Succeeded or Failed"
Mar 27 15:34:09.989: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-secrets-5328e74b-c843-4f94-86ec-a7fe107bf8c5 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/27/23 15:34:09.996
Mar 27 15:34:10.010: INFO: Waiting for pod pod-projected-secrets-5328e74b-c843-4f94-86ec-a7fe107bf8c5 to disappear
Mar 27 15:34:10.013: INFO: Pod pod-projected-secrets-5328e74b-c843-4f94-86ec-a7fe107bf8c5 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 27 15:34:10.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-968" for this suite. 03/27/23 15:34:10.019
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":309,"skipped":5812,"failed":0}
------------------------------
• [SLOW TEST] [6.090 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:34:03.937
    Mar 27 15:34:03.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 15:34:03.938
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:34:03.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:34:03.957
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-ede70155-e564-49fa-83c8-e7b60f63c9d1 03/27/23 15:34:03.962
    STEP: Creating a pod to test consume secrets 03/27/23 15:34:03.968
    Mar 27 15:34:03.977: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5328e74b-c843-4f94-86ec-a7fe107bf8c5" in namespace "projected-968" to be "Succeeded or Failed"
    Mar 27 15:34:03.979: INFO: Pod "pod-projected-secrets-5328e74b-c843-4f94-86ec-a7fe107bf8c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.44107ms
    Mar 27 15:34:05.985: INFO: Pod "pod-projected-secrets-5328e74b-c843-4f94-86ec-a7fe107bf8c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.008355543s
    Mar 27 15:34:07.984: INFO: Pod "pod-projected-secrets-5328e74b-c843-4f94-86ec-a7fe107bf8c5": Phase="Running", Reason="", readiness=false. Elapsed: 4.007103987s
    Mar 27 15:34:09.983: INFO: Pod "pod-projected-secrets-5328e74b-c843-4f94-86ec-a7fe107bf8c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005580741s
    STEP: Saw pod success 03/27/23 15:34:09.983
    Mar 27 15:34:09.983: INFO: Pod "pod-projected-secrets-5328e74b-c843-4f94-86ec-a7fe107bf8c5" satisfied condition "Succeeded or Failed"
    Mar 27 15:34:09.989: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-projected-secrets-5328e74b-c843-4f94-86ec-a7fe107bf8c5 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 15:34:09.996
    Mar 27 15:34:10.010: INFO: Waiting for pod pod-projected-secrets-5328e74b-c843-4f94-86ec-a7fe107bf8c5 to disappear
    Mar 27 15:34:10.013: INFO: Pod pod-projected-secrets-5328e74b-c843-4f94-86ec-a7fe107bf8c5 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 27 15:34:10.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-968" for this suite. 03/27/23 15:34:10.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:34:10.029
Mar 27 15:34:10.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 15:34:10.03
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:34:10.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:34:10.047
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 03/27/23 15:34:10.05
Mar 27 15:34:10.061: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb9d3f12-f665-47be-ad0b-f0fa2e7a77d8" in namespace "projected-9941" to be "Succeeded or Failed"
Mar 27 15:34:10.072: INFO: Pod "downwardapi-volume-eb9d3f12-f665-47be-ad0b-f0fa2e7a77d8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.818964ms
Mar 27 15:34:12.078: INFO: Pod "downwardapi-volume-eb9d3f12-f665-47be-ad0b-f0fa2e7a77d8": Phase="Running", Reason="", readiness=true. Elapsed: 2.016500791s
Mar 27 15:34:14.077: INFO: Pod "downwardapi-volume-eb9d3f12-f665-47be-ad0b-f0fa2e7a77d8": Phase="Running", Reason="", readiness=false. Elapsed: 4.01591462s
Mar 27 15:34:16.079: INFO: Pod "downwardapi-volume-eb9d3f12-f665-47be-ad0b-f0fa2e7a77d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018001115s
STEP: Saw pod success 03/27/23 15:34:16.079
Mar 27 15:34:16.079: INFO: Pod "downwardapi-volume-eb9d3f12-f665-47be-ad0b-f0fa2e7a77d8" satisfied condition "Succeeded or Failed"
Mar 27 15:34:16.082: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod downwardapi-volume-eb9d3f12-f665-47be-ad0b-f0fa2e7a77d8 container client-container: <nil>
STEP: delete the pod 03/27/23 15:34:16.088
Mar 27 15:34:16.106: INFO: Waiting for pod downwardapi-volume-eb9d3f12-f665-47be-ad0b-f0fa2e7a77d8 to disappear
Mar 27 15:34:16.109: INFO: Pod downwardapi-volume-eb9d3f12-f665-47be-ad0b-f0fa2e7a77d8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 27 15:34:16.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9941" for this suite. 03/27/23 15:34:16.115
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":310,"skipped":5829,"failed":0}
------------------------------
• [SLOW TEST] [6.091 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:34:10.029
    Mar 27 15:34:10.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 15:34:10.03
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:34:10.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:34:10.047
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 03/27/23 15:34:10.05
    Mar 27 15:34:10.061: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb9d3f12-f665-47be-ad0b-f0fa2e7a77d8" in namespace "projected-9941" to be "Succeeded or Failed"
    Mar 27 15:34:10.072: INFO: Pod "downwardapi-volume-eb9d3f12-f665-47be-ad0b-f0fa2e7a77d8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.818964ms
    Mar 27 15:34:12.078: INFO: Pod "downwardapi-volume-eb9d3f12-f665-47be-ad0b-f0fa2e7a77d8": Phase="Running", Reason="", readiness=true. Elapsed: 2.016500791s
    Mar 27 15:34:14.077: INFO: Pod "downwardapi-volume-eb9d3f12-f665-47be-ad0b-f0fa2e7a77d8": Phase="Running", Reason="", readiness=false. Elapsed: 4.01591462s
    Mar 27 15:34:16.079: INFO: Pod "downwardapi-volume-eb9d3f12-f665-47be-ad0b-f0fa2e7a77d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018001115s
    STEP: Saw pod success 03/27/23 15:34:16.079
    Mar 27 15:34:16.079: INFO: Pod "downwardapi-volume-eb9d3f12-f665-47be-ad0b-f0fa2e7a77d8" satisfied condition "Succeeded or Failed"
    Mar 27 15:34:16.082: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod downwardapi-volume-eb9d3f12-f665-47be-ad0b-f0fa2e7a77d8 container client-container: <nil>
    STEP: delete the pod 03/27/23 15:34:16.088
    Mar 27 15:34:16.106: INFO: Waiting for pod downwardapi-volume-eb9d3f12-f665-47be-ad0b-f0fa2e7a77d8 to disappear
    Mar 27 15:34:16.109: INFO: Pod downwardapi-volume-eb9d3f12-f665-47be-ad0b-f0fa2e7a77d8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 27 15:34:16.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9941" for this suite. 03/27/23 15:34:16.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:34:16.121
Mar 27 15:34:16.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename sched-pred 03/27/23 15:34:16.123
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:34:16.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:34:16.139
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar 27 15:34:16.145: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 27 15:34:16.153: INFO: Waiting for terminating namespaces to be deleted...
Mar 27 15:34:16.158: INFO: 
Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq before test
Mar 27 15:34:16.167: INFO: canal-vhmnn from kube-system started at 2023-03-27 14:03:29 +0000 UTC (2 container statuses recorded)
Mar 27 15:34:16.167: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 15:34:16.167: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 27 15:34:16.167: INFO: csi-cinder-nodeplugin-lhmqp from kube-system started at 2023-03-27 14:03:29 +0000 UTC (3 container statuses recorded)
Mar 27 15:34:16.167: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 27 15:34:16.167: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 27 15:34:16.167: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 27 15:34:16.167: INFO: kube-proxy-4njm5 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.167: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 27 15:34:16.168: INFO: node-exporter-x8df2 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.168: INFO: 	Container node-exporter ready: true, restart count 0
Mar 27 15:34:16.168: INFO: node-local-dns-zvl8z from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.168: INFO: 	Container node-cache ready: true, restart count 0
Mar 27 15:34:16.168: INFO: syseleven-node-problem-detector-2x2wt from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.168: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 27 15:34:16.168: INFO: user-ssh-keys-agent-65hx9 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.168: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Mar 27 15:34:16.168: INFO: sonobuoy-e2e-job-267544c30cdc4a58 from sonobuoy started at 2023-03-27 14:05:45 +0000 UTC (2 container statuses recorded)
Mar 27 15:34:16.168: INFO: 	Container e2e ready: true, restart count 0
Mar 27 15:34:16.168: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 15:34:16.168: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-bg2pk from sonobuoy started at 2023-03-27 14:05:46 +0000 UTC (2 container statuses recorded)
Mar 27 15:34:16.169: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 15:34:16.169: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 27 15:34:16.169: INFO: 
Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc before test
Mar 27 15:34:16.177: INFO: canal-8sfms from kube-system started at 2023-03-27 14:03:34 +0000 UTC (2 container statuses recorded)
Mar 27 15:34:16.177: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 15:34:16.177: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 27 15:34:16.178: INFO: csi-cinder-nodeplugin-lmbcb from kube-system started at 2023-03-27 14:03:34 +0000 UTC (3 container statuses recorded)
Mar 27 15:34:16.178: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 27 15:34:16.178: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 27 15:34:16.178: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 27 15:34:16.178: INFO: kube-proxy-gj9h9 from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.178: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 27 15:34:16.178: INFO: node-exporter-fgptx from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.178: INFO: 	Container node-exporter ready: true, restart count 0
Mar 27 15:34:16.178: INFO: node-local-dns-tmg5z from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.178: INFO: 	Container node-cache ready: true, restart count 0
Mar 27 15:34:16.178: INFO: syseleven-node-problem-detector-xh9cq from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.178: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 27 15:34:16.178: INFO: user-ssh-keys-agent-x7kpb from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.179: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Mar 27 15:34:16.179: INFO: pod-logs-websocket-0ecf5eff-09b1-414a-91d9-cb0c1fa074cd from pods-8847 started at 2023-03-27 15:34:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.179: INFO: 	Container main ready: true, restart count 0
Mar 27 15:34:16.179: INFO: sonobuoy from sonobuoy started at 2023-03-27 14:05:39 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.179: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 27 15:34:16.179: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-4ztlm from sonobuoy started at 2023-03-27 14:05:46 +0000 UTC (2 container statuses recorded)
Mar 27 15:34:16.179: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 15:34:16.179: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 27 15:34:16.179: INFO: 
Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv before test
Mar 27 15:34:16.187: INFO: calico-kube-controllers-659979cbcb-rxz7c from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.187: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 27 15:34:16.187: INFO: canal-llxzv from kube-system started at 2023-03-27 14:02:28 +0000 UTC (2 container statuses recorded)
Mar 27 15:34:16.187: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 15:34:16.187: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 27 15:34:16.187: INFO: cluster-autoscaler-855c4bc847-b7mwh from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.187: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Mar 27 15:34:16.187: INFO: coredns-5954c46f46-k6bxj from kube-system started at 2023-03-27 14:03:06 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.187: INFO: 	Container coredns ready: true, restart count 0
Mar 27 15:34:16.187: INFO: coredns-5954c46f46-qg6wb from kube-system started at 2023-03-27 14:03:06 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.187: INFO: 	Container coredns ready: true, restart count 0
Mar 27 15:34:16.187: INFO: csi-cinder-nodeplugin-4gffp from kube-system started at 2023-03-27 14:02:28 +0000 UTC (3 container statuses recorded)
Mar 27 15:34:16.187: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 27 15:34:16.187: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 27 15:34:16.187: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 27 15:34:16.187: INFO: dns-autoscaler-69d8768b6-qg5fp from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.187: INFO: 	Container autoscaler ready: true, restart count 0
Mar 27 15:34:16.187: INFO: konnectivity-agent-7c486cb7b8-7wxj6 from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.187: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 15:34:16.187: INFO: konnectivity-agent-7c486cb7b8-ns5pt from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.187: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 15:34:16.187: INFO: kube-proxy-4w8t9 from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.187: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 27 15:34:16.187: INFO: metrics-server-67b4667644-d4vmh from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.187: INFO: 	Container metrics-server ready: true, restart count 0
Mar 27 15:34:16.187: INFO: metrics-server-67b4667644-t5ftq from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.187: INFO: 	Container metrics-server ready: true, restart count 0
Mar 27 15:34:16.187: INFO: node-exporter-8hspl from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.187: INFO: 	Container node-exporter ready: true, restart count 0
Mar 27 15:34:16.187: INFO: node-local-dns-zgn95 from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.187: INFO: 	Container node-cache ready: true, restart count 0
Mar 27 15:34:16.187: INFO: syseleven-node-problem-detector-nvvwp from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.187: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 27 15:34:16.187: INFO: user-ssh-keys-agent-svptx from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 15:34:16.187: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Mar 27 15:34:16.187: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-dgx79 from sonobuoy started at 2023-03-27 14:05:45 +0000 UTC (2 container statuses recorded)
Mar 27 15:34:16.187: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 15:34:16.187: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/27/23 15:34:16.187
Mar 27 15:34:16.197: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3765" to be "running"
Mar 27 15:34:16.206: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.883348ms
Mar 27 15:34:18.211: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.013816085s
Mar 27 15:34:18.211: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/27/23 15:34:18.216
STEP: Trying to apply a random label on the found node. 03/27/23 15:34:18.231
STEP: verifying the node has the label kubernetes.io/e2e-ac5a650b-a3cc-4a75-9fe8-7c60bd1017b7 42 03/27/23 15:34:18.251
STEP: Trying to relaunch the pod, now with labels. 03/27/23 15:34:18.254
Mar 27 15:34:18.261: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-3765" to be "not pending"
Mar 27 15:34:18.270: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 8.925242ms
Mar 27 15:34:20.276: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.015149105s
Mar 27 15:34:20.277: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-ac5a650b-a3cc-4a75-9fe8-7c60bd1017b7 off the node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq 03/27/23 15:34:20.281
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ac5a650b-a3cc-4a75-9fe8-7c60bd1017b7 03/27/23 15:34:20.304
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar 27 15:34:20.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3765" for this suite. 03/27/23 15:34:20.315
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":311,"skipped":5847,"failed":0}
------------------------------
• [4.201 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:34:16.121
    Mar 27 15:34:16.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename sched-pred 03/27/23 15:34:16.123
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:34:16.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:34:16.139
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar 27 15:34:16.145: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 27 15:34:16.153: INFO: Waiting for terminating namespaces to be deleted...
    Mar 27 15:34:16.158: INFO: 
    Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq before test
    Mar 27 15:34:16.167: INFO: canal-vhmnn from kube-system started at 2023-03-27 14:03:29 +0000 UTC (2 container statuses recorded)
    Mar 27 15:34:16.167: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 15:34:16.167: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 27 15:34:16.167: INFO: csi-cinder-nodeplugin-lhmqp from kube-system started at 2023-03-27 14:03:29 +0000 UTC (3 container statuses recorded)
    Mar 27 15:34:16.167: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar 27 15:34:16.167: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 27 15:34:16.167: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 27 15:34:16.167: INFO: kube-proxy-4njm5 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.167: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 27 15:34:16.168: INFO: node-exporter-x8df2 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.168: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 27 15:34:16.168: INFO: node-local-dns-zvl8z from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.168: INFO: 	Container node-cache ready: true, restart count 0
    Mar 27 15:34:16.168: INFO: syseleven-node-problem-detector-2x2wt from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.168: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 27 15:34:16.168: INFO: user-ssh-keys-agent-65hx9 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.168: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Mar 27 15:34:16.168: INFO: sonobuoy-e2e-job-267544c30cdc4a58 from sonobuoy started at 2023-03-27 14:05:45 +0000 UTC (2 container statuses recorded)
    Mar 27 15:34:16.168: INFO: 	Container e2e ready: true, restart count 0
    Mar 27 15:34:16.168: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 15:34:16.168: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-bg2pk from sonobuoy started at 2023-03-27 14:05:46 +0000 UTC (2 container statuses recorded)
    Mar 27 15:34:16.169: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 15:34:16.169: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 27 15:34:16.169: INFO: 
    Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc before test
    Mar 27 15:34:16.177: INFO: canal-8sfms from kube-system started at 2023-03-27 14:03:34 +0000 UTC (2 container statuses recorded)
    Mar 27 15:34:16.177: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 15:34:16.177: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 27 15:34:16.178: INFO: csi-cinder-nodeplugin-lmbcb from kube-system started at 2023-03-27 14:03:34 +0000 UTC (3 container statuses recorded)
    Mar 27 15:34:16.178: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar 27 15:34:16.178: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 27 15:34:16.178: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 27 15:34:16.178: INFO: kube-proxy-gj9h9 from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.178: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 27 15:34:16.178: INFO: node-exporter-fgptx from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.178: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 27 15:34:16.178: INFO: node-local-dns-tmg5z from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.178: INFO: 	Container node-cache ready: true, restart count 0
    Mar 27 15:34:16.178: INFO: syseleven-node-problem-detector-xh9cq from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.178: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 27 15:34:16.178: INFO: user-ssh-keys-agent-x7kpb from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.179: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Mar 27 15:34:16.179: INFO: pod-logs-websocket-0ecf5eff-09b1-414a-91d9-cb0c1fa074cd from pods-8847 started at 2023-03-27 15:34:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.179: INFO: 	Container main ready: true, restart count 0
    Mar 27 15:34:16.179: INFO: sonobuoy from sonobuoy started at 2023-03-27 14:05:39 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.179: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 27 15:34:16.179: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-4ztlm from sonobuoy started at 2023-03-27 14:05:46 +0000 UTC (2 container statuses recorded)
    Mar 27 15:34:16.179: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 15:34:16.179: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 27 15:34:16.179: INFO: 
    Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv before test
    Mar 27 15:34:16.187: INFO: calico-kube-controllers-659979cbcb-rxz7c from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.187: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: canal-llxzv from kube-system started at 2023-03-27 14:02:28 +0000 UTC (2 container statuses recorded)
    Mar 27 15:34:16.187: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: cluster-autoscaler-855c4bc847-b7mwh from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.187: INFO: 	Container cluster-autoscaler ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: coredns-5954c46f46-k6bxj from kube-system started at 2023-03-27 14:03:06 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.187: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: coredns-5954c46f46-qg6wb from kube-system started at 2023-03-27 14:03:06 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.187: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: csi-cinder-nodeplugin-4gffp from kube-system started at 2023-03-27 14:02:28 +0000 UTC (3 container statuses recorded)
    Mar 27 15:34:16.187: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: dns-autoscaler-69d8768b6-qg5fp from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.187: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: konnectivity-agent-7c486cb7b8-7wxj6 from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.187: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: konnectivity-agent-7c486cb7b8-ns5pt from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.187: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: kube-proxy-4w8t9 from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.187: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: metrics-server-67b4667644-d4vmh from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.187: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: metrics-server-67b4667644-t5ftq from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.187: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: node-exporter-8hspl from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.187: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: node-local-dns-zgn95 from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.187: INFO: 	Container node-cache ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: syseleven-node-problem-detector-nvvwp from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.187: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: user-ssh-keys-agent-svptx from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 15:34:16.187: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-dgx79 from sonobuoy started at 2023-03-27 14:05:45 +0000 UTC (2 container statuses recorded)
    Mar 27 15:34:16.187: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 15:34:16.187: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/27/23 15:34:16.187
    Mar 27 15:34:16.197: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3765" to be "running"
    Mar 27 15:34:16.206: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.883348ms
    Mar 27 15:34:18.211: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.013816085s
    Mar 27 15:34:18.211: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/27/23 15:34:18.216
    STEP: Trying to apply a random label on the found node. 03/27/23 15:34:18.231
    STEP: verifying the node has the label kubernetes.io/e2e-ac5a650b-a3cc-4a75-9fe8-7c60bd1017b7 42 03/27/23 15:34:18.251
    STEP: Trying to relaunch the pod, now with labels. 03/27/23 15:34:18.254
    Mar 27 15:34:18.261: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-3765" to be "not pending"
    Mar 27 15:34:18.270: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 8.925242ms
    Mar 27 15:34:20.276: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.015149105s
    Mar 27 15:34:20.277: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-ac5a650b-a3cc-4a75-9fe8-7c60bd1017b7 off the node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq 03/27/23 15:34:20.281
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-ac5a650b-a3cc-4a75-9fe8-7c60bd1017b7 03/27/23 15:34:20.304
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 15:34:20.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-3765" for this suite. 03/27/23 15:34:20.315
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:34:20.324
Mar 27 15:34:20.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubectl 03/27/23 15:34:20.327
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:34:20.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:34:20.345
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 03/27/23 15:34:20.347
Mar 27 15:34:20.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-1476 create -f -'
Mar 27 15:34:21.090: INFO: stderr: ""
Mar 27 15:34:21.090: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 03/27/23 15:34:21.09
Mar 27 15:34:21.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-1476 diff -f -'
Mar 27 15:34:21.304: INFO: rc: 1
Mar 27 15:34:21.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-1476 delete -f -'
Mar 27 15:34:21.389: INFO: stderr: ""
Mar 27 15:34:21.389: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 27 15:34:21.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1476" for this suite. 03/27/23 15:34:21.396
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":312,"skipped":5850,"failed":0}
------------------------------
• [1.077 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:34:20.324
    Mar 27 15:34:20.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubectl 03/27/23 15:34:20.327
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:34:20.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:34:20.345
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 03/27/23 15:34:20.347
    Mar 27 15:34:20.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-1476 create -f -'
    Mar 27 15:34:21.090: INFO: stderr: ""
    Mar 27 15:34:21.090: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 03/27/23 15:34:21.09
    Mar 27 15:34:21.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-1476 diff -f -'
    Mar 27 15:34:21.304: INFO: rc: 1
    Mar 27 15:34:21.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-1476 delete -f -'
    Mar 27 15:34:21.389: INFO: stderr: ""
    Mar 27 15:34:21.389: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 27 15:34:21.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1476" for this suite. 03/27/23 15:34:21.396
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:34:21.402
Mar 27 15:34:21.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename services 03/27/23 15:34:21.403
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:34:21.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:34:21.422
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-1539 03/27/23 15:34:21.425
Mar 27 15:34:21.436: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1539" to be "running and ready"
Mar 27 15:34:21.440: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 4.732844ms
Mar 27 15:34:21.440: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:34:23.444: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.00801685s
Mar 27 15:34:23.444: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar 27 15:34:23.444: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Mar 27 15:34:23.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1539 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar 27 15:34:23.615: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar 27 15:34:23.615: INFO: stdout: "ipvs"
Mar 27 15:34:23.615: INFO: proxyMode: ipvs
Mar 27 15:34:23.631: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 27 15:34:23.636: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-1539 03/27/23 15:34:23.636
STEP: creating replication controller affinity-clusterip-timeout in namespace services-1539 03/27/23 15:34:23.644
I0327 15:34:23.656602      24 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1539, replica count: 3
I0327 15:34:26.707450      24 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 15:34:26.715: INFO: Creating new exec pod
Mar 27 15:34:26.724: INFO: Waiting up to 5m0s for pod "execpod-affinitylt5cr" in namespace "services-1539" to be "running"
Mar 27 15:34:26.727: INFO: Pod "execpod-affinitylt5cr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.348906ms
Mar 27 15:34:28.730: INFO: Pod "execpod-affinitylt5cr": Phase="Running", Reason="", readiness=true. Elapsed: 2.005836163s
Mar 27 15:34:28.730: INFO: Pod "execpod-affinitylt5cr" satisfied condition "running"
Mar 27 15:34:29.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1539 exec execpod-affinitylt5cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Mar 27 15:34:29.891: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Mar 27 15:34:29.891: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 15:34:29.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1539 exec execpod-affinitylt5cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.20.107 80'
Mar 27 15:34:30.071: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.20.107 80\nConnection to 10.240.20.107 80 port [tcp/http] succeeded!\n"
Mar 27 15:34:30.071: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 15:34:30.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1539 exec execpod-affinitylt5cr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.20.107:80/ ; done'
Mar 27 15:34:30.339: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n"
Mar 27 15:34:30.339: INFO: stdout: "\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj"
Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
Mar 27 15:34:30.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1539 exec execpod-affinitylt5cr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.20.107:80/'
Mar 27 15:34:30.528: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n"
Mar 27 15:34:30.528: INFO: stdout: "affinity-clusterip-timeout-2j9mj"
Mar 27 15:36:40.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1539 exec execpod-affinitylt5cr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.20.107:80/'
Mar 27 15:36:40.692: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n"
Mar 27 15:36:40.692: INFO: stdout: "affinity-clusterip-timeout-mwbsr"
Mar 27 15:36:40.692: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1539, will wait for the garbage collector to delete the pods 03/27/23 15:36:40.705
Mar 27 15:36:40.767: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.312628ms
Mar 27 15:36:40.867: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.479179ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 27 15:36:43.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1539" for this suite. 03/27/23 15:36:43.599
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":313,"skipped":5855,"failed":0}
------------------------------
• [SLOW TEST] [142.210 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:34:21.402
    Mar 27 15:34:21.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename services 03/27/23 15:34:21.403
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:34:21.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:34:21.422
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-1539 03/27/23 15:34:21.425
    Mar 27 15:34:21.436: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1539" to be "running and ready"
    Mar 27 15:34:21.440: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 4.732844ms
    Mar 27 15:34:21.440: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:34:23.444: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.00801685s
    Mar 27 15:34:23.444: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Mar 27 15:34:23.444: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Mar 27 15:34:23.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1539 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Mar 27 15:34:23.615: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Mar 27 15:34:23.615: INFO: stdout: "ipvs"
    Mar 27 15:34:23.615: INFO: proxyMode: ipvs
    Mar 27 15:34:23.631: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Mar 27 15:34:23.636: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-1539 03/27/23 15:34:23.636
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-1539 03/27/23 15:34:23.644
    I0327 15:34:23.656602      24 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1539, replica count: 3
    I0327 15:34:26.707450      24 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 15:34:26.715: INFO: Creating new exec pod
    Mar 27 15:34:26.724: INFO: Waiting up to 5m0s for pod "execpod-affinitylt5cr" in namespace "services-1539" to be "running"
    Mar 27 15:34:26.727: INFO: Pod "execpod-affinitylt5cr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.348906ms
    Mar 27 15:34:28.730: INFO: Pod "execpod-affinitylt5cr": Phase="Running", Reason="", readiness=true. Elapsed: 2.005836163s
    Mar 27 15:34:28.730: INFO: Pod "execpod-affinitylt5cr" satisfied condition "running"
    Mar 27 15:34:29.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1539 exec execpod-affinitylt5cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Mar 27 15:34:29.891: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Mar 27 15:34:29.891: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 15:34:29.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1539 exec execpod-affinitylt5cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.20.107 80'
    Mar 27 15:34:30.071: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.20.107 80\nConnection to 10.240.20.107 80 port [tcp/http] succeeded!\n"
    Mar 27 15:34:30.071: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 15:34:30.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1539 exec execpod-affinitylt5cr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.20.107:80/ ; done'
    Mar 27 15:34:30.339: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n"
    Mar 27 15:34:30.339: INFO: stdout: "\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj\naffinity-clusterip-timeout-2j9mj"
    Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
    Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
    Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
    Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
    Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
    Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
    Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
    Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
    Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
    Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
    Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
    Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
    Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
    Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
    Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
    Mar 27 15:34:30.339: INFO: Received response from host: affinity-clusterip-timeout-2j9mj
    Mar 27 15:34:30.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1539 exec execpod-affinitylt5cr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.20.107:80/'
    Mar 27 15:34:30.528: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n"
    Mar 27 15:34:30.528: INFO: stdout: "affinity-clusterip-timeout-2j9mj"
    Mar 27 15:36:40.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-1539 exec execpod-affinitylt5cr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.20.107:80/'
    Mar 27 15:36:40.692: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.20.107:80/\n"
    Mar 27 15:36:40.692: INFO: stdout: "affinity-clusterip-timeout-mwbsr"
    Mar 27 15:36:40.692: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1539, will wait for the garbage collector to delete the pods 03/27/23 15:36:40.705
    Mar 27 15:36:40.767: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.312628ms
    Mar 27 15:36:40.867: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.479179ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 27 15:36:43.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1539" for this suite. 03/27/23 15:36:43.599
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:36:43.613
Mar 27 15:36:43.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 15:36:43.614
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:36:43.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:36:43.632
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 03/27/23 15:36:43.635
Mar 27 15:36:43.646: INFO: Waiting up to 5m0s for pod "downwardapi-volume-609ad70e-a952-445c-8076-5d2a08b94c21" in namespace "projected-6298" to be "Succeeded or Failed"
Mar 27 15:36:43.652: INFO: Pod "downwardapi-volume-609ad70e-a952-445c-8076-5d2a08b94c21": Phase="Pending", Reason="", readiness=false. Elapsed: 6.555251ms
Mar 27 15:36:45.657: INFO: Pod "downwardapi-volume-609ad70e-a952-445c-8076-5d2a08b94c21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011702274s
Mar 27 15:36:47.658: INFO: Pod "downwardapi-volume-609ad70e-a952-445c-8076-5d2a08b94c21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012383663s
STEP: Saw pod success 03/27/23 15:36:47.658
Mar 27 15:36:47.658: INFO: Pod "downwardapi-volume-609ad70e-a952-445c-8076-5d2a08b94c21" satisfied condition "Succeeded or Failed"
Mar 27 15:36:47.661: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-609ad70e-a952-445c-8076-5d2a08b94c21 container client-container: <nil>
STEP: delete the pod 03/27/23 15:36:47.669
Mar 27 15:36:47.683: INFO: Waiting for pod downwardapi-volume-609ad70e-a952-445c-8076-5d2a08b94c21 to disappear
Mar 27 15:36:47.685: INFO: Pod downwardapi-volume-609ad70e-a952-445c-8076-5d2a08b94c21 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 27 15:36:47.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6298" for this suite. 03/27/23 15:36:47.691
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":314,"skipped":5875,"failed":0}
------------------------------
• [4.085 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:36:43.613
    Mar 27 15:36:43.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 15:36:43.614
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:36:43.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:36:43.632
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 03/27/23 15:36:43.635
    Mar 27 15:36:43.646: INFO: Waiting up to 5m0s for pod "downwardapi-volume-609ad70e-a952-445c-8076-5d2a08b94c21" in namespace "projected-6298" to be "Succeeded or Failed"
    Mar 27 15:36:43.652: INFO: Pod "downwardapi-volume-609ad70e-a952-445c-8076-5d2a08b94c21": Phase="Pending", Reason="", readiness=false. Elapsed: 6.555251ms
    Mar 27 15:36:45.657: INFO: Pod "downwardapi-volume-609ad70e-a952-445c-8076-5d2a08b94c21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011702274s
    Mar 27 15:36:47.658: INFO: Pod "downwardapi-volume-609ad70e-a952-445c-8076-5d2a08b94c21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012383663s
    STEP: Saw pod success 03/27/23 15:36:47.658
    Mar 27 15:36:47.658: INFO: Pod "downwardapi-volume-609ad70e-a952-445c-8076-5d2a08b94c21" satisfied condition "Succeeded or Failed"
    Mar 27 15:36:47.661: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod downwardapi-volume-609ad70e-a952-445c-8076-5d2a08b94c21 container client-container: <nil>
    STEP: delete the pod 03/27/23 15:36:47.669
    Mar 27 15:36:47.683: INFO: Waiting for pod downwardapi-volume-609ad70e-a952-445c-8076-5d2a08b94c21 to disappear
    Mar 27 15:36:47.685: INFO: Pod downwardapi-volume-609ad70e-a952-445c-8076-5d2a08b94c21 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 27 15:36:47.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6298" for this suite. 03/27/23 15:36:47.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:36:47.703
Mar 27 15:36:47.703: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename container-runtime 03/27/23 15:36:47.704
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:36:47.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:36:47.718
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 03/27/23 15:36:47.721
STEP: wait for the container to reach Succeeded 03/27/23 15:36:47.73
STEP: get the container status 03/27/23 15:36:51.755
STEP: the container should be terminated 03/27/23 15:36:51.758
STEP: the termination message should be set 03/27/23 15:36:51.758
Mar 27 15:36:51.758: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 03/27/23 15:36:51.758
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar 27 15:36:51.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6796" for this suite. 03/27/23 15:36:51.779
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":315,"skipped":5916,"failed":0}
------------------------------
• [4.084 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:36:47.703
    Mar 27 15:36:47.703: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename container-runtime 03/27/23 15:36:47.704
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:36:47.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:36:47.718
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 03/27/23 15:36:47.721
    STEP: wait for the container to reach Succeeded 03/27/23 15:36:47.73
    STEP: get the container status 03/27/23 15:36:51.755
    STEP: the container should be terminated 03/27/23 15:36:51.758
    STEP: the termination message should be set 03/27/23 15:36:51.758
    Mar 27 15:36:51.758: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 03/27/23 15:36:51.758
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar 27 15:36:51.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6796" for this suite. 03/27/23 15:36:51.779
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:36:51.787
Mar 27 15:36:51.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename cronjob 03/27/23 15:36:51.788
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:36:51.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:36:51.803
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 03/27/23 15:36:51.806
STEP: creating 03/27/23 15:36:51.806
STEP: getting 03/27/23 15:36:51.816
STEP: listing 03/27/23 15:36:51.819
STEP: watching 03/27/23 15:36:51.822
Mar 27 15:36:51.822: INFO: starting watch
STEP: cluster-wide listing 03/27/23 15:36:51.823
STEP: cluster-wide watching 03/27/23 15:36:51.826
Mar 27 15:36:51.827: INFO: starting watch
STEP: patching 03/27/23 15:36:51.828
STEP: updating 03/27/23 15:36:51.835
Mar 27 15:36:51.844: INFO: waiting for watch events with expected annotations
Mar 27 15:36:51.844: INFO: saw patched and updated annotations
STEP: patching /status 03/27/23 15:36:51.844
STEP: updating /status 03/27/23 15:36:51.851
STEP: get /status 03/27/23 15:36:51.858
STEP: deleting 03/27/23 15:36:51.861
STEP: deleting a collection 03/27/23 15:36:51.877
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar 27 15:36:51.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4294" for this suite. 03/27/23 15:36:51.891
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":316,"skipped":5919,"failed":0}
------------------------------
• [0.109 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:36:51.787
    Mar 27 15:36:51.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename cronjob 03/27/23 15:36:51.788
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:36:51.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:36:51.803
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 03/27/23 15:36:51.806
    STEP: creating 03/27/23 15:36:51.806
    STEP: getting 03/27/23 15:36:51.816
    STEP: listing 03/27/23 15:36:51.819
    STEP: watching 03/27/23 15:36:51.822
    Mar 27 15:36:51.822: INFO: starting watch
    STEP: cluster-wide listing 03/27/23 15:36:51.823
    STEP: cluster-wide watching 03/27/23 15:36:51.826
    Mar 27 15:36:51.827: INFO: starting watch
    STEP: patching 03/27/23 15:36:51.828
    STEP: updating 03/27/23 15:36:51.835
    Mar 27 15:36:51.844: INFO: waiting for watch events with expected annotations
    Mar 27 15:36:51.844: INFO: saw patched and updated annotations
    STEP: patching /status 03/27/23 15:36:51.844
    STEP: updating /status 03/27/23 15:36:51.851
    STEP: get /status 03/27/23 15:36:51.858
    STEP: deleting 03/27/23 15:36:51.861
    STEP: deleting a collection 03/27/23 15:36:51.877
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar 27 15:36:51.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-4294" for this suite. 03/27/23 15:36:51.891
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:36:51.897
Mar 27 15:36:51.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename replicaset 03/27/23 15:36:51.898
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:36:51.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:36:51.917
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/27/23 15:36:51.92
Mar 27 15:36:51.930: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-359" to be "running and ready"
Mar 27 15:36:51.936: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 5.754416ms
Mar 27 15:36:51.936: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:36:53.940: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.009667782s
Mar 27 15:36:53.940: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Mar 27 15:36:53.940: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 03/27/23 15:36:53.945
STEP: Then the orphan pod is adopted 03/27/23 15:36:53.951
STEP: When the matched label of one of its pods change 03/27/23 15:36:54.964
Mar 27 15:36:54.968: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 03/27/23 15:36:54.981
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar 27 15:36:55.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-359" for this suite. 03/27/23 15:36:55.996
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":317,"skipped":5919,"failed":0}
------------------------------
• [4.109 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:36:51.897
    Mar 27 15:36:51.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename replicaset 03/27/23 15:36:51.898
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:36:51.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:36:51.917
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/27/23 15:36:51.92
    Mar 27 15:36:51.930: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-359" to be "running and ready"
    Mar 27 15:36:51.936: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 5.754416ms
    Mar 27 15:36:51.936: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:36:53.940: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.009667782s
    Mar 27 15:36:53.940: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Mar 27 15:36:53.940: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 03/27/23 15:36:53.945
    STEP: Then the orphan pod is adopted 03/27/23 15:36:53.951
    STEP: When the matched label of one of its pods change 03/27/23 15:36:54.964
    Mar 27 15:36:54.968: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/27/23 15:36:54.981
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar 27 15:36:55.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-359" for this suite. 03/27/23 15:36:55.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:36:56.006
Mar 27 15:36:56.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename daemonsets 03/27/23 15:36:56.007
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:36:56.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:36:56.022
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 03/27/23 15:36:56.059
STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 15:36:56.064
Mar 27 15:36:56.071: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 15:36:56.071: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 15:36:57.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 27 15:36:57.079: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
Mar 27 15:36:58.081: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 27 15:36:58.082: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 03/27/23 15:36:58.085
STEP: DeleteCollection of the DaemonSets 03/27/23 15:36:58.089
STEP: Verify that ReplicaSets have been deleted 03/27/23 15:36:58.103
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Mar 27 15:36:58.125: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"54801"},"items":null}

Mar 27 15:36:58.134: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"54801"},"items":[{"metadata":{"name":"daemon-set-47lpw","generateName":"daemon-set-","namespace":"daemonsets-4929","uid":"268d72c5-bbdf-4ba3-b5a7-e8a5db3ee56c","resourceVersion":"54799","creationTimestamp":"2023-03-27T15:36:56Z","deletionTimestamp":"2023-03-27T15:37:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7c358593224244b81eaadca518f5e33420f6995063350ef596efafb29cd694a0","cni.projectcalico.org/podIP":"172.25.1.230/32","cni.projectcalico.org/podIPs":"172.25.1.230/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9f5e00f0-a866-4e42-ac66-6894ef66c8de","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-27T15:36:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-27T15:36:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f5e00f0-a866-4e42-ac66-6894ef66c8de\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-27T15:36:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-q5qbl","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-q5qbl","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:57Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:57Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:56Z"}],"hostIP":"192.168.1.3","podIP":"172.25.1.230","podIPs":[{"ip":"172.25.1.230"}],"startTime":"2023-03-27T15:36:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-27T15:36:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://e296b77d999411bde18d9541c7797b1da2a59cda6efd181623a0a38a46e3beac","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-5hvlt","generateName":"daemon-set-","namespace":"daemonsets-4929","uid":"38666821-5418-41c2-8612-b54b372d0ee7","resourceVersion":"54798","creationTimestamp":"2023-03-27T15:36:56Z","deletionTimestamp":"2023-03-27T15:37:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"45afedb7c7c1d8658def33f4229f18acb60ee60b9814859c44d559ca560e313c","cni.projectcalico.org/podIP":"172.25.2.116/32","cni.projectcalico.org/podIPs":"172.25.2.116/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9f5e00f0-a866-4e42-ac66-6894ef66c8de","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-27T15:36:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-27T15:36:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f5e00f0-a866-4e42-ac66-6894ef66c8de\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-27T15:36:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-t5xzn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-t5xzn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:57Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:57Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:56Z"}],"hostIP":"192.168.1.4","podIP":"172.25.2.116","podIPs":[{"ip":"172.25.2.116"}],"startTime":"2023-03-27T15:36:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-27T15:36:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://a99a214027d4912872e299afa5def8deabefd00a2e32a82f464334dcf87b5547","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vs8t4","generateName":"daemon-set-","namespace":"daemonsets-4929","uid":"64061163-20ba-4a1d-874a-b9711f993b64","resourceVersion":"54801","creationTimestamp":"2023-03-27T15:36:56Z","deletionTimestamp":"2023-03-27T15:37:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"078ac8b9dd300231cbf18df60afcb5e6c4005e19160ee197cf3383ac9f097df8","cni.projectcalico.org/podIP":"172.25.0.168/32","cni.projectcalico.org/podIPs":"172.25.0.168/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9f5e00f0-a866-4e42-ac66-6894ef66c8de","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-27T15:36:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-27T15:36:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f5e00f0-a866-4e42-ac66-6894ef66c8de\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-27T15:36:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-flsrq","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-flsrq","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:56Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:56Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:56Z"}],"hostIP":"192.168.1.7","podIP":"172.25.0.168","podIPs":[{"ip":"172.25.0.168"}],"startTime":"2023-03-27T15:36:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-27T15:36:56Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://d7049bfc5d8f8e7465597eb025d762beac844f82b3d729e5146b45df7fe94149","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 27 15:36:58.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4929" for this suite. 03/27/23 15:36:58.155
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":318,"skipped":5931,"failed":0}
------------------------------
• [2.156 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:36:56.006
    Mar 27 15:36:56.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename daemonsets 03/27/23 15:36:56.007
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:36:56.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:36:56.022
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 03/27/23 15:36:56.059
    STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 15:36:56.064
    Mar 27 15:36:56.071: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 15:36:56.071: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 15:36:57.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 27 15:36:57.079: INFO: Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq is running 0 daemon pod, expected 1
    Mar 27 15:36:58.081: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 27 15:36:58.082: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 03/27/23 15:36:58.085
    STEP: DeleteCollection of the DaemonSets 03/27/23 15:36:58.089
    STEP: Verify that ReplicaSets have been deleted 03/27/23 15:36:58.103
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Mar 27 15:36:58.125: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"54801"},"items":null}

    Mar 27 15:36:58.134: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"54801"},"items":[{"metadata":{"name":"daemon-set-47lpw","generateName":"daemon-set-","namespace":"daemonsets-4929","uid":"268d72c5-bbdf-4ba3-b5a7-e8a5db3ee56c","resourceVersion":"54799","creationTimestamp":"2023-03-27T15:36:56Z","deletionTimestamp":"2023-03-27T15:37:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7c358593224244b81eaadca518f5e33420f6995063350ef596efafb29cd694a0","cni.projectcalico.org/podIP":"172.25.1.230/32","cni.projectcalico.org/podIPs":"172.25.1.230/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9f5e00f0-a866-4e42-ac66-6894ef66c8de","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-27T15:36:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-27T15:36:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f5e00f0-a866-4e42-ac66-6894ef66c8de\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-27T15:36:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-q5qbl","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-q5qbl","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:57Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:57Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:56Z"}],"hostIP":"192.168.1.3","podIP":"172.25.1.230","podIPs":[{"ip":"172.25.1.230"}],"startTime":"2023-03-27T15:36:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-27T15:36:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://e296b77d999411bde18d9541c7797b1da2a59cda6efd181623a0a38a46e3beac","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-5hvlt","generateName":"daemon-set-","namespace":"daemonsets-4929","uid":"38666821-5418-41c2-8612-b54b372d0ee7","resourceVersion":"54798","creationTimestamp":"2023-03-27T15:36:56Z","deletionTimestamp":"2023-03-27T15:37:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"45afedb7c7c1d8658def33f4229f18acb60ee60b9814859c44d559ca560e313c","cni.projectcalico.org/podIP":"172.25.2.116/32","cni.projectcalico.org/podIPs":"172.25.2.116/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9f5e00f0-a866-4e42-ac66-6894ef66c8de","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-27T15:36:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-27T15:36:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f5e00f0-a866-4e42-ac66-6894ef66c8de\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-27T15:36:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-t5xzn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-t5xzn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:57Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:57Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:56Z"}],"hostIP":"192.168.1.4","podIP":"172.25.2.116","podIPs":[{"ip":"172.25.2.116"}],"startTime":"2023-03-27T15:36:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-27T15:36:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://a99a214027d4912872e299afa5def8deabefd00a2e32a82f464334dcf87b5547","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vs8t4","generateName":"daemon-set-","namespace":"daemonsets-4929","uid":"64061163-20ba-4a1d-874a-b9711f993b64","resourceVersion":"54801","creationTimestamp":"2023-03-27T15:36:56Z","deletionTimestamp":"2023-03-27T15:37:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"078ac8b9dd300231cbf18df60afcb5e6c4005e19160ee197cf3383ac9f097df8","cni.projectcalico.org/podIP":"172.25.0.168/32","cni.projectcalico.org/podIPs":"172.25.0.168/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9f5e00f0-a866-4e42-ac66-6894ef66c8de","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-27T15:36:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-27T15:36:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f5e00f0-a866-4e42-ac66-6894ef66c8de\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-27T15:36:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-flsrq","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-flsrq","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:56Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:56Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T15:36:56Z"}],"hostIP":"192.168.1.7","podIP":"172.25.0.168","podIPs":[{"ip":"172.25.0.168"}],"startTime":"2023-03-27T15:36:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-27T15:36:56Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://d7049bfc5d8f8e7465597eb025d762beac844f82b3d729e5146b45df7fe94149","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 15:36:58.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4929" for this suite. 03/27/23 15:36:58.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:36:58.171
Mar 27 15:36:58.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename configmap 03/27/23 15:36:58.172
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:36:58.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:36:58.191
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-0d82e873-264b-427c-9b89-d7f24bcfd259 03/27/23 15:36:58.199
STEP: Creating the pod 03/27/23 15:36:58.205
Mar 27 15:36:58.214: INFO: Waiting up to 5m0s for pod "pod-configmaps-7fe69d08-2071-4182-9f4b-fee1747c3409" in namespace "configmap-2504" to be "running"
Mar 27 15:36:58.221: INFO: Pod "pod-configmaps-7fe69d08-2071-4182-9f4b-fee1747c3409": Phase="Pending", Reason="", readiness=false. Elapsed: 6.375479ms
Mar 27 15:37:00.240: INFO: Pod "pod-configmaps-7fe69d08-2071-4182-9f4b-fee1747c3409": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025949978s
Mar 27 15:37:02.226: INFO: Pod "pod-configmaps-7fe69d08-2071-4182-9f4b-fee1747c3409": Phase="Running", Reason="", readiness=false. Elapsed: 4.011839175s
Mar 27 15:37:02.226: INFO: Pod "pod-configmaps-7fe69d08-2071-4182-9f4b-fee1747c3409" satisfied condition "running"
STEP: Waiting for pod with text data 03/27/23 15:37:02.226
STEP: Waiting for pod with binary data 03/27/23 15:37:02.234
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 27 15:37:02.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2504" for this suite. 03/27/23 15:37:02.248
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":319,"skipped":5961,"failed":0}
------------------------------
• [4.086 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:36:58.171
    Mar 27 15:36:58.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename configmap 03/27/23 15:36:58.172
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:36:58.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:36:58.191
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-0d82e873-264b-427c-9b89-d7f24bcfd259 03/27/23 15:36:58.199
    STEP: Creating the pod 03/27/23 15:36:58.205
    Mar 27 15:36:58.214: INFO: Waiting up to 5m0s for pod "pod-configmaps-7fe69d08-2071-4182-9f4b-fee1747c3409" in namespace "configmap-2504" to be "running"
    Mar 27 15:36:58.221: INFO: Pod "pod-configmaps-7fe69d08-2071-4182-9f4b-fee1747c3409": Phase="Pending", Reason="", readiness=false. Elapsed: 6.375479ms
    Mar 27 15:37:00.240: INFO: Pod "pod-configmaps-7fe69d08-2071-4182-9f4b-fee1747c3409": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025949978s
    Mar 27 15:37:02.226: INFO: Pod "pod-configmaps-7fe69d08-2071-4182-9f4b-fee1747c3409": Phase="Running", Reason="", readiness=false. Elapsed: 4.011839175s
    Mar 27 15:37:02.226: INFO: Pod "pod-configmaps-7fe69d08-2071-4182-9f4b-fee1747c3409" satisfied condition "running"
    STEP: Waiting for pod with text data 03/27/23 15:37:02.226
    STEP: Waiting for pod with binary data 03/27/23 15:37:02.234
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 27 15:37:02.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2504" for this suite. 03/27/23 15:37:02.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:37:02.261
Mar 27 15:37:02.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename services 03/27/23 15:37:02.262
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:37:02.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:37:02.279
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-7575 03/27/23 15:37:02.282
STEP: creating service affinity-clusterip in namespace services-7575 03/27/23 15:37:02.282
STEP: creating replication controller affinity-clusterip in namespace services-7575 03/27/23 15:37:02.297
I0327 15:37:02.303871      24 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7575, replica count: 3
I0327 15:37:05.354716      24 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 15:37:05.360: INFO: Creating new exec pod
Mar 27 15:37:05.364: INFO: Waiting up to 5m0s for pod "execpod-affinityffljf" in namespace "services-7575" to be "running"
Mar 27 15:37:05.368: INFO: Pod "execpod-affinityffljf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.06816ms
Mar 27 15:37:07.371: INFO: Pod "execpod-affinityffljf": Phase="Running", Reason="", readiness=true. Elapsed: 2.006821175s
Mar 27 15:37:07.371: INFO: Pod "execpod-affinityffljf" satisfied condition "running"
Mar 27 15:37:08.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-7575 exec execpod-affinityffljf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Mar 27 15:37:08.616: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar 27 15:37:08.616: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 15:37:08.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-7575 exec execpod-affinityffljf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.17.38 80'
Mar 27 15:37:08.761: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.17.38 80\nConnection to 10.240.17.38 80 port [tcp/http] succeeded!\n"
Mar 27 15:37:08.761: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 15:37:08.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-7575 exec execpod-affinityffljf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.17.38:80/ ; done'
Mar 27 15:37:08.999: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n"
Mar 27 15:37:08.999: INFO: stdout: "\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g"
Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
Mar 27 15:37:08.999: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7575, will wait for the garbage collector to delete the pods 03/27/23 15:37:09.016
Mar 27 15:37:09.081: INFO: Deleting ReplicationController affinity-clusterip took: 7.269324ms
Mar 27 15:37:09.182: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.927484ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 27 15:37:11.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7575" for this suite. 03/27/23 15:37:11.704
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":320,"skipped":6000,"failed":0}
------------------------------
• [SLOW TEST] [9.451 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:37:02.261
    Mar 27 15:37:02.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename services 03/27/23 15:37:02.262
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:37:02.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:37:02.279
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-7575 03/27/23 15:37:02.282
    STEP: creating service affinity-clusterip in namespace services-7575 03/27/23 15:37:02.282
    STEP: creating replication controller affinity-clusterip in namespace services-7575 03/27/23 15:37:02.297
    I0327 15:37:02.303871      24 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7575, replica count: 3
    I0327 15:37:05.354716      24 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 15:37:05.360: INFO: Creating new exec pod
    Mar 27 15:37:05.364: INFO: Waiting up to 5m0s for pod "execpod-affinityffljf" in namespace "services-7575" to be "running"
    Mar 27 15:37:05.368: INFO: Pod "execpod-affinityffljf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.06816ms
    Mar 27 15:37:07.371: INFO: Pod "execpod-affinityffljf": Phase="Running", Reason="", readiness=true. Elapsed: 2.006821175s
    Mar 27 15:37:07.371: INFO: Pod "execpod-affinityffljf" satisfied condition "running"
    Mar 27 15:37:08.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-7575 exec execpod-affinityffljf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Mar 27 15:37:08.616: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Mar 27 15:37:08.616: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 15:37:08.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-7575 exec execpod-affinityffljf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.17.38 80'
    Mar 27 15:37:08.761: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.17.38 80\nConnection to 10.240.17.38 80 port [tcp/http] succeeded!\n"
    Mar 27 15:37:08.761: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 15:37:08.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-7575 exec execpod-affinityffljf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.17.38:80/ ; done'
    Mar 27 15:37:08.999: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.38:80/\n"
    Mar 27 15:37:08.999: INFO: stdout: "\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g\naffinity-clusterip-fdk4g"
    Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
    Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
    Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
    Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
    Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
    Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
    Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
    Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
    Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
    Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
    Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
    Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
    Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
    Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
    Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
    Mar 27 15:37:08.999: INFO: Received response from host: affinity-clusterip-fdk4g
    Mar 27 15:37:08.999: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-7575, will wait for the garbage collector to delete the pods 03/27/23 15:37:09.016
    Mar 27 15:37:09.081: INFO: Deleting ReplicationController affinity-clusterip took: 7.269324ms
    Mar 27 15:37:09.182: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.927484ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 27 15:37:11.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7575" for this suite. 03/27/23 15:37:11.704
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:37:11.716
Mar 27 15:37:11.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename deployment 03/27/23 15:37:11.717
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:37:11.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:37:11.742
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Mar 27 15:37:11.755: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 27 15:37:16.760: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/27/23 15:37:16.76
Mar 27 15:37:16.760: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/27/23 15:37:16.768
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 27 15:37:16.777: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5904  c81e243c-7014-44cb-bcc8-13808314d415 55112 1 2023-03-27 15:37:16 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-03-27 15:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f4be88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar 27 15:37:16.782: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Mar 27 15:37:16.782: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar 27 15:37:16.782: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5904  d3cd4821-aeaf-4a13-af95-9da118cf0895 55113 1 2023-03-27 15:37:11 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment c81e243c-7014-44cb-bcc8-13808314d415 0xc0003ebb27 0xc0003ebb28}] [] [{e2e.test Update apps/v1 2023-03-27 15:37:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 15:37:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-27 15:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"c81e243c-7014-44cb-bcc8-13808314d415\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0003ebbe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 27 15:37:16.787: INFO: Pod "test-cleanup-controller-nxpjd" is available:
&Pod{ObjectMeta:{test-cleanup-controller-nxpjd test-cleanup-controller- deployment-5904  27202190-a128-4a05-8ef5-5f4273d4e858 55089 0 2023-03-27 15:37:11 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:ea99b01c6b782213d88c223928ac04261f8afb707675c9b6a3bbede29f621129 cni.projectcalico.org/podIP:172.25.2.119/32 cni.projectcalico.org/podIPs:172.25.2.119/32] [{apps/v1 ReplicaSet test-cleanup-controller d3cd4821-aeaf-4a13-af95-9da118cf0895 0xc0003ebfd7 0xc0003ebfd8}] [] [{kube-controller-manager Update v1 2023-03-27 15:37:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3cd4821-aeaf-4a13-af95-9da118cf0895\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 15:37:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 15:37:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rf4xz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rf4xz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 15:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 15:37:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 15:37:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 15:37:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:172.25.2.119,StartTime:2023-03-27 15:37:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 15:37:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://826c851dc70ecd9e83817dc56f863c4ff2a698a26812283ece99fbc8637813e5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 27 15:37:16.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5904" for this suite. 03/27/23 15:37:16.799
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":321,"skipped":6032,"failed":0}
------------------------------
• [SLOW TEST] [5.095 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:37:11.716
    Mar 27 15:37:11.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename deployment 03/27/23 15:37:11.717
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:37:11.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:37:11.742
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Mar 27 15:37:11.755: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Mar 27 15:37:16.760: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/27/23 15:37:16.76
    Mar 27 15:37:16.760: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/27/23 15:37:16.768
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 27 15:37:16.777: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5904  c81e243c-7014-44cb-bcc8-13808314d415 55112 1 2023-03-27 15:37:16 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-03-27 15:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f4be88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Mar 27 15:37:16.782: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Mar 27 15:37:16.782: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Mar 27 15:37:16.782: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5904  d3cd4821-aeaf-4a13-af95-9da118cf0895 55113 1 2023-03-27 15:37:11 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment c81e243c-7014-44cb-bcc8-13808314d415 0xc0003ebb27 0xc0003ebb28}] [] [{e2e.test Update apps/v1 2023-03-27 15:37:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 15:37:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-27 15:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"c81e243c-7014-44cb-bcc8-13808314d415\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0003ebbe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 15:37:16.787: INFO: Pod "test-cleanup-controller-nxpjd" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-nxpjd test-cleanup-controller- deployment-5904  27202190-a128-4a05-8ef5-5f4273d4e858 55089 0 2023-03-27 15:37:11 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:ea99b01c6b782213d88c223928ac04261f8afb707675c9b6a3bbede29f621129 cni.projectcalico.org/podIP:172.25.2.119/32 cni.projectcalico.org/podIPs:172.25.2.119/32] [{apps/v1 ReplicaSet test-cleanup-controller d3cd4821-aeaf-4a13-af95-9da118cf0895 0xc0003ebfd7 0xc0003ebfd8}] [] [{kube-controller-manager Update v1 2023-03-27 15:37:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3cd4821-aeaf-4a13-af95-9da118cf0895\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 15:37:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 15:37:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rf4xz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rf4xz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 15:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 15:37:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 15:37:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 15:37:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.4,PodIP:172.25.2.119,StartTime:2023-03-27 15:37:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 15:37:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://826c851dc70ecd9e83817dc56f863c4ff2a698a26812283ece99fbc8637813e5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 27 15:37:16.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5904" for this suite. 03/27/23 15:37:16.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:37:16.815
Mar 27 15:37:16.816: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename container-probe 03/27/23 15:37:16.816
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:37:16.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:37:16.833
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6 in namespace container-probe-9262 03/27/23 15:37:16.836
Mar 27 15:37:16.845: INFO: Waiting up to 5m0s for pod "liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6" in namespace "container-probe-9262" to be "not pending"
Mar 27 15:37:16.852: INFO: Pod "liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.049719ms
Mar 27 15:37:18.857: INFO: Pod "liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6": Phase="Running", Reason="", readiness=true. Elapsed: 2.012426809s
Mar 27 15:37:18.857: INFO: Pod "liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6" satisfied condition "not pending"
Mar 27 15:37:18.857: INFO: Started pod liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6 in namespace container-probe-9262
STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 15:37:18.857
Mar 27 15:37:18.861: INFO: Initial restart count of pod liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6 is 0
Mar 27 15:37:38.917: INFO: Restart count of pod container-probe-9262/liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6 is now 1 (20.056583739s elapsed)
Mar 27 15:37:58.966: INFO: Restart count of pod container-probe-9262/liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6 is now 2 (40.105700321s elapsed)
Mar 27 15:38:19.011: INFO: Restart count of pod container-probe-9262/liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6 is now 3 (1m0.150356753s elapsed)
Mar 27 15:38:39.059: INFO: Restart count of pod container-probe-9262/liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6 is now 4 (1m20.19866207s elapsed)
Mar 27 15:39:43.262: INFO: Restart count of pod container-probe-9262/liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6 is now 5 (2m24.401368088s elapsed)
STEP: deleting the pod 03/27/23 15:39:43.262
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 27 15:39:43.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9262" for this suite. 03/27/23 15:39:43.283
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":322,"skipped":6060,"failed":0}
------------------------------
• [SLOW TEST] [146.473 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:37:16.815
    Mar 27 15:37:16.816: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename container-probe 03/27/23 15:37:16.816
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:37:16.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:37:16.833
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6 in namespace container-probe-9262 03/27/23 15:37:16.836
    Mar 27 15:37:16.845: INFO: Waiting up to 5m0s for pod "liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6" in namespace "container-probe-9262" to be "not pending"
    Mar 27 15:37:16.852: INFO: Pod "liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.049719ms
    Mar 27 15:37:18.857: INFO: Pod "liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6": Phase="Running", Reason="", readiness=true. Elapsed: 2.012426809s
    Mar 27 15:37:18.857: INFO: Pod "liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6" satisfied condition "not pending"
    Mar 27 15:37:18.857: INFO: Started pod liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6 in namespace container-probe-9262
    STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 15:37:18.857
    Mar 27 15:37:18.861: INFO: Initial restart count of pod liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6 is 0
    Mar 27 15:37:38.917: INFO: Restart count of pod container-probe-9262/liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6 is now 1 (20.056583739s elapsed)
    Mar 27 15:37:58.966: INFO: Restart count of pod container-probe-9262/liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6 is now 2 (40.105700321s elapsed)
    Mar 27 15:38:19.011: INFO: Restart count of pod container-probe-9262/liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6 is now 3 (1m0.150356753s elapsed)
    Mar 27 15:38:39.059: INFO: Restart count of pod container-probe-9262/liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6 is now 4 (1m20.19866207s elapsed)
    Mar 27 15:39:43.262: INFO: Restart count of pod container-probe-9262/liveness-15d57312-fac2-4c3b-aab5-9948397e4aa6 is now 5 (2m24.401368088s elapsed)
    STEP: deleting the pod 03/27/23 15:39:43.262
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 27 15:39:43.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9262" for this suite. 03/27/23 15:39:43.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:39:43.289
Mar 27 15:39:43.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename proxy 03/27/23 15:39:43.291
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:39:43.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:39:43.312
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Mar 27 15:39:43.315: INFO: Creating pod...
Mar 27 15:39:43.326: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4969" to be "running"
Mar 27 15:39:43.329: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.640216ms
Mar 27 15:39:45.333: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.00674941s
Mar 27 15:39:45.333: INFO: Pod "agnhost" satisfied condition "running"
Mar 27 15:39:45.333: INFO: Creating service...
Mar 27 15:39:45.343: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/pods/agnhost/proxy/some/path/with/DELETE
Mar 27 15:39:45.350: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 27 15:39:45.350: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/pods/agnhost/proxy/some/path/with/GET
Mar 27 15:39:45.358: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 27 15:39:45.358: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/pods/agnhost/proxy/some/path/with/HEAD
Mar 27 15:39:45.372: INFO: http.Client request:HEAD | StatusCode:200
Mar 27 15:39:45.372: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/pods/agnhost/proxy/some/path/with/OPTIONS
Mar 27 15:39:45.376: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 27 15:39:45.376: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/pods/agnhost/proxy/some/path/with/PATCH
Mar 27 15:39:45.380: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 27 15:39:45.380: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/pods/agnhost/proxy/some/path/with/POST
Mar 27 15:39:45.384: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 27 15:39:45.384: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/pods/agnhost/proxy/some/path/with/PUT
Mar 27 15:39:45.389: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 27 15:39:45.389: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/services/test-service/proxy/some/path/with/DELETE
Mar 27 15:39:45.396: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 27 15:39:45.396: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/services/test-service/proxy/some/path/with/GET
Mar 27 15:39:45.402: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 27 15:39:45.402: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/services/test-service/proxy/some/path/with/HEAD
Mar 27 15:39:45.408: INFO: http.Client request:HEAD | StatusCode:200
Mar 27 15:39:45.408: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/services/test-service/proxy/some/path/with/OPTIONS
Mar 27 15:39:45.414: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 27 15:39:45.414: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/services/test-service/proxy/some/path/with/PATCH
Mar 27 15:39:45.421: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 27 15:39:45.421: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/services/test-service/proxy/some/path/with/POST
Mar 27 15:39:45.430: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 27 15:39:45.430: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/services/test-service/proxy/some/path/with/PUT
Mar 27 15:39:45.436: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar 27 15:39:45.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4969" for this suite. 03/27/23 15:39:45.44
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":323,"skipped":6084,"failed":0}
------------------------------
• [2.156 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:39:43.289
    Mar 27 15:39:43.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename proxy 03/27/23 15:39:43.291
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:39:43.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:39:43.312
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Mar 27 15:39:43.315: INFO: Creating pod...
    Mar 27 15:39:43.326: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4969" to be "running"
    Mar 27 15:39:43.329: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.640216ms
    Mar 27 15:39:45.333: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.00674941s
    Mar 27 15:39:45.333: INFO: Pod "agnhost" satisfied condition "running"
    Mar 27 15:39:45.333: INFO: Creating service...
    Mar 27 15:39:45.343: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/pods/agnhost/proxy/some/path/with/DELETE
    Mar 27 15:39:45.350: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 27 15:39:45.350: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/pods/agnhost/proxy/some/path/with/GET
    Mar 27 15:39:45.358: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar 27 15:39:45.358: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/pods/agnhost/proxy/some/path/with/HEAD
    Mar 27 15:39:45.372: INFO: http.Client request:HEAD | StatusCode:200
    Mar 27 15:39:45.372: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/pods/agnhost/proxy/some/path/with/OPTIONS
    Mar 27 15:39:45.376: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 27 15:39:45.376: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/pods/agnhost/proxy/some/path/with/PATCH
    Mar 27 15:39:45.380: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 27 15:39:45.380: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/pods/agnhost/proxy/some/path/with/POST
    Mar 27 15:39:45.384: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 27 15:39:45.384: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/pods/agnhost/proxy/some/path/with/PUT
    Mar 27 15:39:45.389: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 27 15:39:45.389: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/services/test-service/proxy/some/path/with/DELETE
    Mar 27 15:39:45.396: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 27 15:39:45.396: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/services/test-service/proxy/some/path/with/GET
    Mar 27 15:39:45.402: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar 27 15:39:45.402: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/services/test-service/proxy/some/path/with/HEAD
    Mar 27 15:39:45.408: INFO: http.Client request:HEAD | StatusCode:200
    Mar 27 15:39:45.408: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/services/test-service/proxy/some/path/with/OPTIONS
    Mar 27 15:39:45.414: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 27 15:39:45.414: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/services/test-service/proxy/some/path/with/PATCH
    Mar 27 15:39:45.421: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 27 15:39:45.421: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/services/test-service/proxy/some/path/with/POST
    Mar 27 15:39:45.430: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 27 15:39:45.430: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-4969/services/test-service/proxy/some/path/with/PUT
    Mar 27 15:39:45.436: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar 27 15:39:45.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-4969" for this suite. 03/27/23 15:39:45.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:39:45.447
Mar 27 15:39:45.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename pods 03/27/23 15:39:45.448
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:39:45.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:39:45.469
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 03/27/23 15:39:45.472
STEP: submitting the pod to kubernetes 03/27/23 15:39:45.473
Mar 27 15:39:45.482: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f" in namespace "pods-2124" to be "running and ready"
Mar 27 15:39:45.490: INFO: Pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.894667ms
Mar 27 15:39:45.490: INFO: The phase of Pod pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:39:47.496: INFO: Pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f": Phase="Running", Reason="", readiness=true. Elapsed: 2.013374836s
Mar 27 15:39:47.496: INFO: The phase of Pod pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f is Running (Ready = true)
Mar 27 15:39:47.496: INFO: Pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/27/23 15:39:47.5
STEP: updating the pod 03/27/23 15:39:47.503
Mar 27 15:39:48.019: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f"
Mar 27 15:39:48.019: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f" in namespace "pods-2124" to be "terminated with reason DeadlineExceeded"
Mar 27 15:39:48.025: INFO: Pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f": Phase="Running", Reason="", readiness=true. Elapsed: 5.953227ms
Mar 27 15:39:50.029: INFO: Pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f": Phase="Running", Reason="", readiness=true. Elapsed: 2.01080294s
Mar 27 15:39:52.029: INFO: Pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.01011203s
Mar 27 15:39:52.029: INFO: Pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 27 15:39:52.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2124" for this suite. 03/27/23 15:39:52.034
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":324,"skipped":6112,"failed":0}
------------------------------
• [SLOW TEST] [6.611 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:39:45.447
    Mar 27 15:39:45.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename pods 03/27/23 15:39:45.448
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:39:45.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:39:45.469
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 03/27/23 15:39:45.472
    STEP: submitting the pod to kubernetes 03/27/23 15:39:45.473
    Mar 27 15:39:45.482: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f" in namespace "pods-2124" to be "running and ready"
    Mar 27 15:39:45.490: INFO: Pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.894667ms
    Mar 27 15:39:45.490: INFO: The phase of Pod pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:39:47.496: INFO: Pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f": Phase="Running", Reason="", readiness=true. Elapsed: 2.013374836s
    Mar 27 15:39:47.496: INFO: The phase of Pod pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f is Running (Ready = true)
    Mar 27 15:39:47.496: INFO: Pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/27/23 15:39:47.5
    STEP: updating the pod 03/27/23 15:39:47.503
    Mar 27 15:39:48.019: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f"
    Mar 27 15:39:48.019: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f" in namespace "pods-2124" to be "terminated with reason DeadlineExceeded"
    Mar 27 15:39:48.025: INFO: Pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f": Phase="Running", Reason="", readiness=true. Elapsed: 5.953227ms
    Mar 27 15:39:50.029: INFO: Pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f": Phase="Running", Reason="", readiness=true. Elapsed: 2.01080294s
    Mar 27 15:39:52.029: INFO: Pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.01011203s
    Mar 27 15:39:52.029: INFO: Pod "pod-update-activedeadlineseconds-2c19f635-caef-4a05-a3e6-209cd13dc21f" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 27 15:39:52.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2124" for this suite. 03/27/23 15:39:52.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:39:52.06
Mar 27 15:39:52.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename containers 03/27/23 15:39:52.061
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:39:52.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:39:52.081
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 03/27/23 15:39:52.084
Mar 27 15:39:52.093: INFO: Waiting up to 5m0s for pod "client-containers-755ca36a-7b48-41fe-9732-23abf880a23f" in namespace "containers-4698" to be "Succeeded or Failed"
Mar 27 15:39:52.096: INFO: Pod "client-containers-755ca36a-7b48-41fe-9732-23abf880a23f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.50546ms
Mar 27 15:39:54.100: INFO: Pod "client-containers-755ca36a-7b48-41fe-9732-23abf880a23f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007476682s
Mar 27 15:39:56.100: INFO: Pod "client-containers-755ca36a-7b48-41fe-9732-23abf880a23f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007636599s
STEP: Saw pod success 03/27/23 15:39:56.1
Mar 27 15:39:56.101: INFO: Pod "client-containers-755ca36a-7b48-41fe-9732-23abf880a23f" satisfied condition "Succeeded or Failed"
Mar 27 15:39:56.104: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod client-containers-755ca36a-7b48-41fe-9732-23abf880a23f container agnhost-container: <nil>
STEP: delete the pod 03/27/23 15:39:56.117
Mar 27 15:39:56.132: INFO: Waiting for pod client-containers-755ca36a-7b48-41fe-9732-23abf880a23f to disappear
Mar 27 15:39:56.134: INFO: Pod client-containers-755ca36a-7b48-41fe-9732-23abf880a23f no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar 27 15:39:56.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4698" for this suite. 03/27/23 15:39:56.139
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":325,"skipped":6136,"failed":0}
------------------------------
• [4.085 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:39:52.06
    Mar 27 15:39:52.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename containers 03/27/23 15:39:52.061
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:39:52.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:39:52.081
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 03/27/23 15:39:52.084
    Mar 27 15:39:52.093: INFO: Waiting up to 5m0s for pod "client-containers-755ca36a-7b48-41fe-9732-23abf880a23f" in namespace "containers-4698" to be "Succeeded or Failed"
    Mar 27 15:39:52.096: INFO: Pod "client-containers-755ca36a-7b48-41fe-9732-23abf880a23f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.50546ms
    Mar 27 15:39:54.100: INFO: Pod "client-containers-755ca36a-7b48-41fe-9732-23abf880a23f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007476682s
    Mar 27 15:39:56.100: INFO: Pod "client-containers-755ca36a-7b48-41fe-9732-23abf880a23f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007636599s
    STEP: Saw pod success 03/27/23 15:39:56.1
    Mar 27 15:39:56.101: INFO: Pod "client-containers-755ca36a-7b48-41fe-9732-23abf880a23f" satisfied condition "Succeeded or Failed"
    Mar 27 15:39:56.104: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod client-containers-755ca36a-7b48-41fe-9732-23abf880a23f container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 15:39:56.117
    Mar 27 15:39:56.132: INFO: Waiting for pod client-containers-755ca36a-7b48-41fe-9732-23abf880a23f to disappear
    Mar 27 15:39:56.134: INFO: Pod client-containers-755ca36a-7b48-41fe-9732-23abf880a23f no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar 27 15:39:56.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-4698" for this suite. 03/27/23 15:39:56.139
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:39:56.145
Mar 27 15:39:56.145: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename security-context-test 03/27/23 15:39:56.146
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:39:56.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:39:56.164
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Mar 27 15:39:56.179: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-ac2723d2-e210-4670-a26b-bac210d16bd2" in namespace "security-context-test-1993" to be "Succeeded or Failed"
Mar 27 15:39:56.182: INFO: Pod "alpine-nnp-false-ac2723d2-e210-4670-a26b-bac210d16bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.449661ms
Mar 27 15:39:58.187: INFO: Pod "alpine-nnp-false-ac2723d2-e210-4670-a26b-bac210d16bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007588638s
Mar 27 15:40:00.199: INFO: Pod "alpine-nnp-false-ac2723d2-e210-4670-a26b-bac210d16bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019636102s
Mar 27 15:40:02.186: INFO: Pod "alpine-nnp-false-ac2723d2-e210-4670-a26b-bac210d16bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006838247s
Mar 27 15:40:04.187: INFO: Pod "alpine-nnp-false-ac2723d2-e210-4670-a26b-bac210d16bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.00741135s
Mar 27 15:40:04.187: INFO: Pod "alpine-nnp-false-ac2723d2-e210-4670-a26b-bac210d16bd2" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar 27 15:40:04.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1993" for this suite. 03/27/23 15:40:04.2
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":326,"skipped":6136,"failed":0}
------------------------------
• [SLOW TEST] [8.063 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:39:56.145
    Mar 27 15:39:56.145: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename security-context-test 03/27/23 15:39:56.146
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:39:56.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:39:56.164
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Mar 27 15:39:56.179: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-ac2723d2-e210-4670-a26b-bac210d16bd2" in namespace "security-context-test-1993" to be "Succeeded or Failed"
    Mar 27 15:39:56.182: INFO: Pod "alpine-nnp-false-ac2723d2-e210-4670-a26b-bac210d16bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.449661ms
    Mar 27 15:39:58.187: INFO: Pod "alpine-nnp-false-ac2723d2-e210-4670-a26b-bac210d16bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007588638s
    Mar 27 15:40:00.199: INFO: Pod "alpine-nnp-false-ac2723d2-e210-4670-a26b-bac210d16bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019636102s
    Mar 27 15:40:02.186: INFO: Pod "alpine-nnp-false-ac2723d2-e210-4670-a26b-bac210d16bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006838247s
    Mar 27 15:40:04.187: INFO: Pod "alpine-nnp-false-ac2723d2-e210-4670-a26b-bac210d16bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.00741135s
    Mar 27 15:40:04.187: INFO: Pod "alpine-nnp-false-ac2723d2-e210-4670-a26b-bac210d16bd2" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar 27 15:40:04.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-1993" for this suite. 03/27/23 15:40:04.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:40:04.208
Mar 27 15:40:04.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubelet-test 03/27/23 15:40:04.209
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:40:04.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:40:04.225
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Mar 27 15:40:04.237: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs81a6a0d2-6283-452c-9732-5b84ca8337b2" in namespace "kubelet-test-3039" to be "running and ready"
Mar 27 15:40:04.243: INFO: Pod "busybox-readonly-fs81a6a0d2-6283-452c-9732-5b84ca8337b2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048647ms
Mar 27 15:40:04.243: INFO: The phase of Pod busybox-readonly-fs81a6a0d2-6283-452c-9732-5b84ca8337b2 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:40:06.248: INFO: Pod "busybox-readonly-fs81a6a0d2-6283-452c-9732-5b84ca8337b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010878411s
Mar 27 15:40:06.248: INFO: The phase of Pod busybox-readonly-fs81a6a0d2-6283-452c-9732-5b84ca8337b2 is Running (Ready = true)
Mar 27 15:40:06.248: INFO: Pod "busybox-readonly-fs81a6a0d2-6283-452c-9732-5b84ca8337b2" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar 27 15:40:06.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3039" for this suite. 03/27/23 15:40:06.265
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":327,"skipped":6146,"failed":0}
------------------------------
• [2.062 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:40:04.208
    Mar 27 15:40:04.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubelet-test 03/27/23 15:40:04.209
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:40:04.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:40:04.225
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Mar 27 15:40:04.237: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs81a6a0d2-6283-452c-9732-5b84ca8337b2" in namespace "kubelet-test-3039" to be "running and ready"
    Mar 27 15:40:04.243: INFO: Pod "busybox-readonly-fs81a6a0d2-6283-452c-9732-5b84ca8337b2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048647ms
    Mar 27 15:40:04.243: INFO: The phase of Pod busybox-readonly-fs81a6a0d2-6283-452c-9732-5b84ca8337b2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:40:06.248: INFO: Pod "busybox-readonly-fs81a6a0d2-6283-452c-9732-5b84ca8337b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010878411s
    Mar 27 15:40:06.248: INFO: The phase of Pod busybox-readonly-fs81a6a0d2-6283-452c-9732-5b84ca8337b2 is Running (Ready = true)
    Mar 27 15:40:06.248: INFO: Pod "busybox-readonly-fs81a6a0d2-6283-452c-9732-5b84ca8337b2" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar 27 15:40:06.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-3039" for this suite. 03/27/23 15:40:06.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:40:06.272
Mar 27 15:40:06.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubectl 03/27/23 15:40:06.273
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:40:06.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:40:06.291
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Mar 27 15:40:06.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-540 version'
Mar 27 15:40:06.376: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Mar 27 15:40:06.376: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.7\", GitCommit:\"723bcdb232300aaf5e147ff19b4df7ec8a20278d\", GitTreeState:\"clean\", BuildDate:\"2023-02-22T14:05:25Z\", GoVersion:\"go1.19.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.7\", GitCommit:\"723bcdb232300aaf5e147ff19b4df7ec8a20278d\", GitTreeState:\"clean\", BuildDate:\"2023-02-22T13:58:23Z\", GoVersion:\"go1.19.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 27 15:40:06.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-540" for this suite. 03/27/23 15:40:06.383
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":328,"skipped":6152,"failed":0}
------------------------------
• [0.118 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:40:06.272
    Mar 27 15:40:06.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubectl 03/27/23 15:40:06.273
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:40:06.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:40:06.291
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Mar 27 15:40:06.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-540 version'
    Mar 27 15:40:06.376: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Mar 27 15:40:06.376: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.7\", GitCommit:\"723bcdb232300aaf5e147ff19b4df7ec8a20278d\", GitTreeState:\"clean\", BuildDate:\"2023-02-22T14:05:25Z\", GoVersion:\"go1.19.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.7\", GitCommit:\"723bcdb232300aaf5e147ff19b4df7ec8a20278d\", GitTreeState:\"clean\", BuildDate:\"2023-02-22T13:58:23Z\", GoVersion:\"go1.19.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 27 15:40:06.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-540" for this suite. 03/27/23 15:40:06.383
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:40:06.39
Mar 27 15:40:06.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename job 03/27/23 15:40:06.391
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:40:06.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:40:06.414
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 03/27/23 15:40:06.417
STEP: Ensuring active pods == parallelism 03/27/23 15:40:06.423
STEP: Orphaning one of the Job's Pods 03/27/23 15:40:08.429
Mar 27 15:40:08.952: INFO: Successfully updated pod "adopt-release-gsz76"
STEP: Checking that the Job readopts the Pod 03/27/23 15:40:08.952
Mar 27 15:40:08.952: INFO: Waiting up to 15m0s for pod "adopt-release-gsz76" in namespace "job-8404" to be "adopted"
Mar 27 15:40:08.955: INFO: Pod "adopt-release-gsz76": Phase="Running", Reason="", readiness=true. Elapsed: 2.395669ms
Mar 27 15:40:10.960: INFO: Pod "adopt-release-gsz76": Phase="Running", Reason="", readiness=true. Elapsed: 2.007698986s
Mar 27 15:40:10.960: INFO: Pod "adopt-release-gsz76" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 03/27/23 15:40:10.96
Mar 27 15:40:11.475: INFO: Successfully updated pod "adopt-release-gsz76"
STEP: Checking that the Job releases the Pod 03/27/23 15:40:11.475
Mar 27 15:40:11.475: INFO: Waiting up to 15m0s for pod "adopt-release-gsz76" in namespace "job-8404" to be "released"
Mar 27 15:40:11.480: INFO: Pod "adopt-release-gsz76": Phase="Running", Reason="", readiness=true. Elapsed: 4.766265ms
Mar 27 15:40:13.484: INFO: Pod "adopt-release-gsz76": Phase="Running", Reason="", readiness=true. Elapsed: 2.008335028s
Mar 27 15:40:13.484: INFO: Pod "adopt-release-gsz76" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar 27 15:40:13.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8404" for this suite. 03/27/23 15:40:13.488
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":329,"skipped":6154,"failed":0}
------------------------------
• [SLOW TEST] [7.104 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:40:06.39
    Mar 27 15:40:06.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename job 03/27/23 15:40:06.391
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:40:06.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:40:06.414
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 03/27/23 15:40:06.417
    STEP: Ensuring active pods == parallelism 03/27/23 15:40:06.423
    STEP: Orphaning one of the Job's Pods 03/27/23 15:40:08.429
    Mar 27 15:40:08.952: INFO: Successfully updated pod "adopt-release-gsz76"
    STEP: Checking that the Job readopts the Pod 03/27/23 15:40:08.952
    Mar 27 15:40:08.952: INFO: Waiting up to 15m0s for pod "adopt-release-gsz76" in namespace "job-8404" to be "adopted"
    Mar 27 15:40:08.955: INFO: Pod "adopt-release-gsz76": Phase="Running", Reason="", readiness=true. Elapsed: 2.395669ms
    Mar 27 15:40:10.960: INFO: Pod "adopt-release-gsz76": Phase="Running", Reason="", readiness=true. Elapsed: 2.007698986s
    Mar 27 15:40:10.960: INFO: Pod "adopt-release-gsz76" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 03/27/23 15:40:10.96
    Mar 27 15:40:11.475: INFO: Successfully updated pod "adopt-release-gsz76"
    STEP: Checking that the Job releases the Pod 03/27/23 15:40:11.475
    Mar 27 15:40:11.475: INFO: Waiting up to 15m0s for pod "adopt-release-gsz76" in namespace "job-8404" to be "released"
    Mar 27 15:40:11.480: INFO: Pod "adopt-release-gsz76": Phase="Running", Reason="", readiness=true. Elapsed: 4.766265ms
    Mar 27 15:40:13.484: INFO: Pod "adopt-release-gsz76": Phase="Running", Reason="", readiness=true. Elapsed: 2.008335028s
    Mar 27 15:40:13.484: INFO: Pod "adopt-release-gsz76" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar 27 15:40:13.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8404" for this suite. 03/27/23 15:40:13.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:40:13.495
Mar 27 15:40:13.495: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename configmap 03/27/23 15:40:13.496
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:40:13.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:40:13.508
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-15176a02-93c3-4be6-8f8f-d7cf97d965a0 03/27/23 15:40:13.511
STEP: Creating a pod to test consume configMaps 03/27/23 15:40:13.516
Mar 27 15:40:13.528: INFO: Waiting up to 5m0s for pod "pod-configmaps-3433e0b2-7e0a-43a0-bf34-16dfcda17a66" in namespace "configmap-462" to be "Succeeded or Failed"
Mar 27 15:40:13.531: INFO: Pod "pod-configmaps-3433e0b2-7e0a-43a0-bf34-16dfcda17a66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.790609ms
Mar 27 15:40:15.537: INFO: Pod "pod-configmaps-3433e0b2-7e0a-43a0-bf34-16dfcda17a66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008290866s
Mar 27 15:40:17.536: INFO: Pod "pod-configmaps-3433e0b2-7e0a-43a0-bf34-16dfcda17a66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007630262s
STEP: Saw pod success 03/27/23 15:40:17.536
Mar 27 15:40:17.536: INFO: Pod "pod-configmaps-3433e0b2-7e0a-43a0-bf34-16dfcda17a66" satisfied condition "Succeeded or Failed"
Mar 27 15:40:17.539: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-configmaps-3433e0b2-7e0a-43a0-bf34-16dfcda17a66 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 15:40:17.549
Mar 27 15:40:17.564: INFO: Waiting for pod pod-configmaps-3433e0b2-7e0a-43a0-bf34-16dfcda17a66 to disappear
Mar 27 15:40:17.567: INFO: Pod pod-configmaps-3433e0b2-7e0a-43a0-bf34-16dfcda17a66 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 27 15:40:17.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-462" for this suite. 03/27/23 15:40:17.573
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":330,"skipped":6167,"failed":0}
------------------------------
• [4.088 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:40:13.495
    Mar 27 15:40:13.495: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename configmap 03/27/23 15:40:13.496
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:40:13.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:40:13.508
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-15176a02-93c3-4be6-8f8f-d7cf97d965a0 03/27/23 15:40:13.511
    STEP: Creating a pod to test consume configMaps 03/27/23 15:40:13.516
    Mar 27 15:40:13.528: INFO: Waiting up to 5m0s for pod "pod-configmaps-3433e0b2-7e0a-43a0-bf34-16dfcda17a66" in namespace "configmap-462" to be "Succeeded or Failed"
    Mar 27 15:40:13.531: INFO: Pod "pod-configmaps-3433e0b2-7e0a-43a0-bf34-16dfcda17a66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.790609ms
    Mar 27 15:40:15.537: INFO: Pod "pod-configmaps-3433e0b2-7e0a-43a0-bf34-16dfcda17a66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008290866s
    Mar 27 15:40:17.536: INFO: Pod "pod-configmaps-3433e0b2-7e0a-43a0-bf34-16dfcda17a66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007630262s
    STEP: Saw pod success 03/27/23 15:40:17.536
    Mar 27 15:40:17.536: INFO: Pod "pod-configmaps-3433e0b2-7e0a-43a0-bf34-16dfcda17a66" satisfied condition "Succeeded or Failed"
    Mar 27 15:40:17.539: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq pod pod-configmaps-3433e0b2-7e0a-43a0-bf34-16dfcda17a66 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 15:40:17.549
    Mar 27 15:40:17.564: INFO: Waiting for pod pod-configmaps-3433e0b2-7e0a-43a0-bf34-16dfcda17a66 to disappear
    Mar 27 15:40:17.567: INFO: Pod pod-configmaps-3433e0b2-7e0a-43a0-bf34-16dfcda17a66 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 27 15:40:17.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-462" for this suite. 03/27/23 15:40:17.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:40:17.584
Mar 27 15:40:17.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubectl 03/27/23 15:40:17.584
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:40:17.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:40:17.602
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 03/27/23 15:40:17.605
Mar 27 15:40:17.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4931 create -f -'
Mar 27 15:40:17.801: INFO: stderr: ""
Mar 27 15:40:17.801: INFO: stdout: "pod/pause created\n"
Mar 27 15:40:17.801: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 27 15:40:17.801: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4931" to be "running and ready"
Mar 27 15:40:17.805: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.259437ms
Mar 27 15:40:17.805: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq' to be 'Running' but was 'Pending'
Mar 27 15:40:19.811: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009505998s
Mar 27 15:40:19.811: INFO: Pod "pause" satisfied condition "running and ready"
Mar 27 15:40:19.811: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 03/27/23 15:40:19.811
Mar 27 15:40:19.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4931 label pods pause testing-label=testing-label-value'
Mar 27 15:40:19.903: INFO: stderr: ""
Mar 27 15:40:19.903: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 03/27/23 15:40:19.903
Mar 27 15:40:19.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4931 get pod pause -L testing-label'
Mar 27 15:40:19.976: INFO: stderr: ""
Mar 27 15:40:19.976: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 03/27/23 15:40:19.976
Mar 27 15:40:19.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4931 label pods pause testing-label-'
Mar 27 15:40:20.076: INFO: stderr: ""
Mar 27 15:40:20.076: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 03/27/23 15:40:20.076
Mar 27 15:40:20.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4931 get pod pause -L testing-label'
Mar 27 15:40:20.156: INFO: stderr: ""
Mar 27 15:40:20.157: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 03/27/23 15:40:20.157
Mar 27 15:40:20.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4931 delete --grace-period=0 --force -f -'
Mar 27 15:40:20.250: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 27 15:40:20.250: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 27 15:40:20.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4931 get rc,svc -l name=pause --no-headers'
Mar 27 15:40:20.332: INFO: stderr: "No resources found in kubectl-4931 namespace.\n"
Mar 27 15:40:20.332: INFO: stdout: ""
Mar 27 15:40:20.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4931 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 27 15:40:20.403: INFO: stderr: ""
Mar 27 15:40:20.403: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 27 15:40:20.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4931" for this suite. 03/27/23 15:40:20.412
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":331,"skipped":6180,"failed":0}
------------------------------
• [2.840 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:40:17.584
    Mar 27 15:40:17.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubectl 03/27/23 15:40:17.584
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:40:17.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:40:17.602
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 03/27/23 15:40:17.605
    Mar 27 15:40:17.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4931 create -f -'
    Mar 27 15:40:17.801: INFO: stderr: ""
    Mar 27 15:40:17.801: INFO: stdout: "pod/pause created\n"
    Mar 27 15:40:17.801: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Mar 27 15:40:17.801: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4931" to be "running and ready"
    Mar 27 15:40:17.805: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.259437ms
    Mar 27 15:40:17.805: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq' to be 'Running' but was 'Pending'
    Mar 27 15:40:19.811: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009505998s
    Mar 27 15:40:19.811: INFO: Pod "pause" satisfied condition "running and ready"
    Mar 27 15:40:19.811: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 03/27/23 15:40:19.811
    Mar 27 15:40:19.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4931 label pods pause testing-label=testing-label-value'
    Mar 27 15:40:19.903: INFO: stderr: ""
    Mar 27 15:40:19.903: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 03/27/23 15:40:19.903
    Mar 27 15:40:19.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4931 get pod pause -L testing-label'
    Mar 27 15:40:19.976: INFO: stderr: ""
    Mar 27 15:40:19.976: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 03/27/23 15:40:19.976
    Mar 27 15:40:19.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4931 label pods pause testing-label-'
    Mar 27 15:40:20.076: INFO: stderr: ""
    Mar 27 15:40:20.076: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 03/27/23 15:40:20.076
    Mar 27 15:40:20.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4931 get pod pause -L testing-label'
    Mar 27 15:40:20.156: INFO: stderr: ""
    Mar 27 15:40:20.157: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 03/27/23 15:40:20.157
    Mar 27 15:40:20.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4931 delete --grace-period=0 --force -f -'
    Mar 27 15:40:20.250: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 27 15:40:20.250: INFO: stdout: "pod \"pause\" force deleted\n"
    Mar 27 15:40:20.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4931 get rc,svc -l name=pause --no-headers'
    Mar 27 15:40:20.332: INFO: stderr: "No resources found in kubectl-4931 namespace.\n"
    Mar 27 15:40:20.332: INFO: stdout: ""
    Mar 27 15:40:20.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4931 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 27 15:40:20.403: INFO: stderr: ""
    Mar 27 15:40:20.403: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 27 15:40:20.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4931" for this suite. 03/27/23 15:40:20.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:40:20.424
Mar 27 15:40:20.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename sched-pred 03/27/23 15:40:20.426
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:40:20.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:40:20.447
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar 27 15:40:20.451: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 27 15:40:20.465: INFO: Waiting for terminating namespaces to be deleted...
Mar 27 15:40:20.468: INFO: 
Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq before test
Mar 27 15:40:20.478: INFO: adopt-release-pq86t from job-8404 started at 2023-03-27 15:40:06 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.478: INFO: 	Container c ready: true, restart count 0
Mar 27 15:40:20.478: INFO: canal-vhmnn from kube-system started at 2023-03-27 14:03:29 +0000 UTC (2 container statuses recorded)
Mar 27 15:40:20.478: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 15:40:20.478: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 27 15:40:20.478: INFO: csi-cinder-nodeplugin-lhmqp from kube-system started at 2023-03-27 14:03:29 +0000 UTC (3 container statuses recorded)
Mar 27 15:40:20.478: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 27 15:40:20.478: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 27 15:40:20.478: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 27 15:40:20.478: INFO: kube-proxy-4njm5 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.478: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 27 15:40:20.478: INFO: node-exporter-x8df2 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.478: INFO: 	Container node-exporter ready: true, restart count 0
Mar 27 15:40:20.478: INFO: node-local-dns-zvl8z from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.478: INFO: 	Container node-cache ready: true, restart count 0
Mar 27 15:40:20.478: INFO: syseleven-node-problem-detector-2x2wt from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.478: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 27 15:40:20.478: INFO: user-ssh-keys-agent-65hx9 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.478: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Mar 27 15:40:20.478: INFO: sonobuoy-e2e-job-267544c30cdc4a58 from sonobuoy started at 2023-03-27 14:05:45 +0000 UTC (2 container statuses recorded)
Mar 27 15:40:20.478: INFO: 	Container e2e ready: true, restart count 0
Mar 27 15:40:20.478: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 15:40:20.478: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-bg2pk from sonobuoy started at 2023-03-27 14:05:46 +0000 UTC (2 container statuses recorded)
Mar 27 15:40:20.478: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 15:40:20.478: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 27 15:40:20.478: INFO: 
Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc before test
Mar 27 15:40:20.490: INFO: adopt-release-fpfpx from job-8404 started at 2023-03-27 15:40:12 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.490: INFO: 	Container c ready: true, restart count 0
Mar 27 15:40:20.490: INFO: adopt-release-gsz76 from job-8404 started at 2023-03-27 15:40:06 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.490: INFO: 	Container c ready: true, restart count 0
Mar 27 15:40:20.490: INFO: canal-8sfms from kube-system started at 2023-03-27 14:03:34 +0000 UTC (2 container statuses recorded)
Mar 27 15:40:20.490: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 15:40:20.490: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 27 15:40:20.490: INFO: csi-cinder-nodeplugin-lmbcb from kube-system started at 2023-03-27 14:03:34 +0000 UTC (3 container statuses recorded)
Mar 27 15:40:20.490: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 27 15:40:20.490: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 27 15:40:20.490: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 27 15:40:20.490: INFO: kube-proxy-gj9h9 from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.490: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 27 15:40:20.490: INFO: node-exporter-fgptx from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.490: INFO: 	Container node-exporter ready: true, restart count 0
Mar 27 15:40:20.490: INFO: node-local-dns-tmg5z from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.490: INFO: 	Container node-cache ready: true, restart count 0
Mar 27 15:40:20.490: INFO: syseleven-node-problem-detector-xh9cq from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.490: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 27 15:40:20.490: INFO: user-ssh-keys-agent-x7kpb from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.490: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Mar 27 15:40:20.490: INFO: busybox-readonly-fs81a6a0d2-6283-452c-9732-5b84ca8337b2 from kubelet-test-3039 started at 2023-03-27 15:40:04 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.490: INFO: 	Container busybox-readonly-fs81a6a0d2-6283-452c-9732-5b84ca8337b2 ready: true, restart count 0
Mar 27 15:40:20.490: INFO: sonobuoy from sonobuoy started at 2023-03-27 14:05:39 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.490: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 27 15:40:20.490: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-4ztlm from sonobuoy started at 2023-03-27 14:05:46 +0000 UTC (2 container statuses recorded)
Mar 27 15:40:20.490: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 15:40:20.490: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 27 15:40:20.490: INFO: 
Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv before test
Mar 27 15:40:20.507: INFO: calico-kube-controllers-659979cbcb-rxz7c from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.507: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 27 15:40:20.507: INFO: canal-llxzv from kube-system started at 2023-03-27 14:02:28 +0000 UTC (2 container statuses recorded)
Mar 27 15:40:20.507: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 15:40:20.507: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 27 15:40:20.507: INFO: cluster-autoscaler-855c4bc847-b7mwh from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.507: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Mar 27 15:40:20.507: INFO: coredns-5954c46f46-k6bxj from kube-system started at 2023-03-27 14:03:06 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.507: INFO: 	Container coredns ready: true, restart count 0
Mar 27 15:40:20.507: INFO: coredns-5954c46f46-qg6wb from kube-system started at 2023-03-27 14:03:06 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.507: INFO: 	Container coredns ready: true, restart count 0
Mar 27 15:40:20.507: INFO: csi-cinder-nodeplugin-4gffp from kube-system started at 2023-03-27 14:02:28 +0000 UTC (3 container statuses recorded)
Mar 27 15:40:20.507: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 27 15:40:20.507: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 27 15:40:20.507: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 27 15:40:20.507: INFO: dns-autoscaler-69d8768b6-qg5fp from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.507: INFO: 	Container autoscaler ready: true, restart count 0
Mar 27 15:40:20.507: INFO: konnectivity-agent-7c486cb7b8-7wxj6 from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.507: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 15:40:20.507: INFO: konnectivity-agent-7c486cb7b8-ns5pt from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.507: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 15:40:20.507: INFO: kube-proxy-4w8t9 from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.507: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 27 15:40:20.507: INFO: metrics-server-67b4667644-d4vmh from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.507: INFO: 	Container metrics-server ready: true, restart count 0
Mar 27 15:40:20.507: INFO: metrics-server-67b4667644-t5ftq from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.507: INFO: 	Container metrics-server ready: true, restart count 0
Mar 27 15:40:20.507: INFO: node-exporter-8hspl from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.507: INFO: 	Container node-exporter ready: true, restart count 0
Mar 27 15:40:20.507: INFO: node-local-dns-zgn95 from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.507: INFO: 	Container node-cache ready: true, restart count 0
Mar 27 15:40:20.507: INFO: syseleven-node-problem-detector-nvvwp from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.507: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 27 15:40:20.507: INFO: user-ssh-keys-agent-svptx from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
Mar 27 15:40:20.507: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Mar 27 15:40:20.507: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-dgx79 from sonobuoy started at 2023-03-27 14:05:45 +0000 UTC (2 container statuses recorded)
Mar 27 15:40:20.507: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 15:40:20.507: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq 03/27/23 15:40:20.536
STEP: verifying the node has the label node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc 03/27/23 15:40:20.557
STEP: verifying the node has the label node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv 03/27/23 15:40:20.576
Mar 27 15:40:20.593: INFO: Pod adopt-release-fpfpx requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
Mar 27 15:40:20.593: INFO: Pod adopt-release-gsz76 requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
Mar 27 15:40:20.593: INFO: Pod adopt-release-pq86t requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
Mar 27 15:40:20.593: INFO: Pod calico-kube-controllers-659979cbcb-rxz7c requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
Mar 27 15:40:20.593: INFO: Pod canal-8sfms requesting resource cpu=250m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
Mar 27 15:40:20.593: INFO: Pod canal-llxzv requesting resource cpu=250m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
Mar 27 15:40:20.593: INFO: Pod canal-vhmnn requesting resource cpu=250m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
Mar 27 15:40:20.593: INFO: Pod cluster-autoscaler-855c4bc847-b7mwh requesting resource cpu=100m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
Mar 27 15:40:20.593: INFO: Pod coredns-5954c46f46-k6bxj requesting resource cpu=50m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
Mar 27 15:40:20.593: INFO: Pod coredns-5954c46f46-qg6wb requesting resource cpu=50m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
Mar 27 15:40:20.593: INFO: Pod csi-cinder-nodeplugin-4gffp requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
Mar 27 15:40:20.593: INFO: Pod csi-cinder-nodeplugin-lhmqp requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
Mar 27 15:40:20.593: INFO: Pod csi-cinder-nodeplugin-lmbcb requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
Mar 27 15:40:20.593: INFO: Pod dns-autoscaler-69d8768b6-qg5fp requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
Mar 27 15:40:20.593: INFO: Pod konnectivity-agent-7c486cb7b8-7wxj6 requesting resource cpu=10m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
Mar 27 15:40:20.593: INFO: Pod konnectivity-agent-7c486cb7b8-ns5pt requesting resource cpu=10m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
Mar 27 15:40:20.593: INFO: Pod kube-proxy-4njm5 requesting resource cpu=75m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
Mar 27 15:40:20.593: INFO: Pod kube-proxy-4w8t9 requesting resource cpu=75m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
Mar 27 15:40:20.593: INFO: Pod kube-proxy-gj9h9 requesting resource cpu=75m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
Mar 27 15:40:20.593: INFO: Pod metrics-server-67b4667644-d4vmh requesting resource cpu=100m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
Mar 27 15:40:20.593: INFO: Pod metrics-server-67b4667644-t5ftq requesting resource cpu=100m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
Mar 27 15:40:20.593: INFO: Pod node-exporter-8hspl requesting resource cpu=3m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
Mar 27 15:40:20.593: INFO: Pod node-exporter-fgptx requesting resource cpu=3m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
Mar 27 15:40:20.593: INFO: Pod node-exporter-x8df2 requesting resource cpu=3m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
Mar 27 15:40:20.593: INFO: Pod node-local-dns-tmg5z requesting resource cpu=50m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
Mar 27 15:40:20.593: INFO: Pod node-local-dns-zgn95 requesting resource cpu=50m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
Mar 27 15:40:20.593: INFO: Pod node-local-dns-zvl8z requesting resource cpu=50m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
Mar 27 15:40:20.593: INFO: Pod syseleven-node-problem-detector-2x2wt requesting resource cpu=10m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
Mar 27 15:40:20.593: INFO: Pod syseleven-node-problem-detector-nvvwp requesting resource cpu=10m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
Mar 27 15:40:20.593: INFO: Pod syseleven-node-problem-detector-xh9cq requesting resource cpu=10m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
Mar 27 15:40:20.593: INFO: Pod user-ssh-keys-agent-65hx9 requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
Mar 27 15:40:20.593: INFO: Pod user-ssh-keys-agent-svptx requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
Mar 27 15:40:20.593: INFO: Pod user-ssh-keys-agent-x7kpb requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
Mar 27 15:40:20.593: INFO: Pod busybox-readonly-fs81a6a0d2-6283-452c-9732-5b84ca8337b2 requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
Mar 27 15:40:20.593: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
Mar 27 15:40:20.593: INFO: Pod sonobuoy-e2e-job-267544c30cdc4a58 requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
Mar 27 15:40:20.593: INFO: Pod sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-4ztlm requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
Mar 27 15:40:20.593: INFO: Pod sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-bg2pk requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
Mar 27 15:40:20.593: INFO: Pod sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-dgx79 requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
STEP: Starting Pods to consume most of the cluster CPU. 03/27/23 15:40:20.593
Mar 27 15:40:20.593: INFO: Creating a pod which consumes cpu=2248m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
Mar 27 15:40:20.601: INFO: Creating a pod which consumes cpu=2248m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
Mar 27 15:40:20.609: INFO: Creating a pod which consumes cpu=1954m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
Mar 27 15:40:20.626: INFO: Waiting up to 5m0s for pod "filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe" in namespace "sched-pred-6838" to be "running"
Mar 27 15:40:20.637: INFO: Pod "filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.697357ms
Mar 27 15:40:22.641: INFO: Pod "filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.014639224s
Mar 27 15:40:22.641: INFO: Pod "filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe" satisfied condition "running"
Mar 27 15:40:22.641: INFO: Waiting up to 5m0s for pod "filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99" in namespace "sched-pred-6838" to be "running"
Mar 27 15:40:22.644: INFO: Pod "filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99": Phase="Running", Reason="", readiness=true. Elapsed: 2.751996ms
Mar 27 15:40:22.644: INFO: Pod "filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99" satisfied condition "running"
Mar 27 15:40:22.644: INFO: Waiting up to 5m0s for pod "filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4" in namespace "sched-pred-6838" to be "running"
Mar 27 15:40:22.648: INFO: Pod "filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4": Phase="Running", Reason="", readiness=true. Elapsed: 4.000761ms
Mar 27 15:40:22.648: INFO: Pod "filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 03/27/23 15:40:22.648
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe.175050e8850b9cef], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6838/filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe to k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq] 03/27/23 15:40:22.651
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe.175050e8af53fc5f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/27/23 15:40:22.651
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe.175050e8dc9997b4], Reason = [Created], Message = [Created container filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe] 03/27/23 15:40:22.651
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe.175050e8f16bc107], Reason = [Started], Message = [Started container filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe] 03/27/23 15:40:22.651
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4.175050e8864e50db], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6838/filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4 to k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv] 03/27/23 15:40:22.652
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4.175050e8afcf867e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/27/23 15:40:22.652
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4.175050e8b54ab47f], Reason = [Created], Message = [Created container filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4] 03/27/23 15:40:22.652
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4.175050e8bc0d040d], Reason = [Started], Message = [Started container filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4] 03/27/23 15:40:22.652
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99.175050e8857313db], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6838/filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99 to k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc] 03/27/23 15:40:22.652
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99.175050e8b3866562], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/27/23 15:40:22.652
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99.175050e8b9eab7f3], Reason = [Created], Message = [Created container filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99] 03/27/23 15:40:22.652
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99.175050e8c352ff54], Reason = [Started], Message = [Started container filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99] 03/27/23 15:40:22.652
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.175050e8ff35ba2e], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 03/27/23 15:40:22.67
STEP: removing the label node off the node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq 03/27/23 15:40:23.666
STEP: verifying the node doesn't have the label node 03/27/23 15:40:23.681
STEP: removing the label node off the node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc 03/27/23 15:40:23.687
STEP: verifying the node doesn't have the label node 03/27/23 15:40:23.701
STEP: removing the label node off the node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv 03/27/23 15:40:23.705
STEP: verifying the node doesn't have the label node 03/27/23 15:40:23.72
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar 27 15:40:23.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6838" for this suite. 03/27/23 15:40:23.728
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":332,"skipped":6198,"failed":0}
------------------------------
• [3.311 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:40:20.424
    Mar 27 15:40:20.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename sched-pred 03/27/23 15:40:20.426
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:40:20.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:40:20.447
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar 27 15:40:20.451: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 27 15:40:20.465: INFO: Waiting for terminating namespaces to be deleted...
    Mar 27 15:40:20.468: INFO: 
    Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq before test
    Mar 27 15:40:20.478: INFO: adopt-release-pq86t from job-8404 started at 2023-03-27 15:40:06 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.478: INFO: 	Container c ready: true, restart count 0
    Mar 27 15:40:20.478: INFO: canal-vhmnn from kube-system started at 2023-03-27 14:03:29 +0000 UTC (2 container statuses recorded)
    Mar 27 15:40:20.478: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 15:40:20.478: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 27 15:40:20.478: INFO: csi-cinder-nodeplugin-lhmqp from kube-system started at 2023-03-27 14:03:29 +0000 UTC (3 container statuses recorded)
    Mar 27 15:40:20.478: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar 27 15:40:20.478: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 27 15:40:20.478: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 27 15:40:20.478: INFO: kube-proxy-4njm5 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.478: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 27 15:40:20.478: INFO: node-exporter-x8df2 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.478: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 27 15:40:20.478: INFO: node-local-dns-zvl8z from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.478: INFO: 	Container node-cache ready: true, restart count 0
    Mar 27 15:40:20.478: INFO: syseleven-node-problem-detector-2x2wt from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.478: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 27 15:40:20.478: INFO: user-ssh-keys-agent-65hx9 from kube-system started at 2023-03-27 14:03:29 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.478: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Mar 27 15:40:20.478: INFO: sonobuoy-e2e-job-267544c30cdc4a58 from sonobuoy started at 2023-03-27 14:05:45 +0000 UTC (2 container statuses recorded)
    Mar 27 15:40:20.478: INFO: 	Container e2e ready: true, restart count 0
    Mar 27 15:40:20.478: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 15:40:20.478: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-bg2pk from sonobuoy started at 2023-03-27 14:05:46 +0000 UTC (2 container statuses recorded)
    Mar 27 15:40:20.478: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 15:40:20.478: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 27 15:40:20.478: INFO: 
    Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc before test
    Mar 27 15:40:20.490: INFO: adopt-release-fpfpx from job-8404 started at 2023-03-27 15:40:12 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.490: INFO: 	Container c ready: true, restart count 0
    Mar 27 15:40:20.490: INFO: adopt-release-gsz76 from job-8404 started at 2023-03-27 15:40:06 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.490: INFO: 	Container c ready: true, restart count 0
    Mar 27 15:40:20.490: INFO: canal-8sfms from kube-system started at 2023-03-27 14:03:34 +0000 UTC (2 container statuses recorded)
    Mar 27 15:40:20.490: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 15:40:20.490: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 27 15:40:20.490: INFO: csi-cinder-nodeplugin-lmbcb from kube-system started at 2023-03-27 14:03:34 +0000 UTC (3 container statuses recorded)
    Mar 27 15:40:20.490: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar 27 15:40:20.490: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 27 15:40:20.490: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 27 15:40:20.490: INFO: kube-proxy-gj9h9 from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.490: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 27 15:40:20.490: INFO: node-exporter-fgptx from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.490: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 27 15:40:20.490: INFO: node-local-dns-tmg5z from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.490: INFO: 	Container node-cache ready: true, restart count 0
    Mar 27 15:40:20.490: INFO: syseleven-node-problem-detector-xh9cq from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.490: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 27 15:40:20.490: INFO: user-ssh-keys-agent-x7kpb from kube-system started at 2023-03-27 14:03:34 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.490: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Mar 27 15:40:20.490: INFO: busybox-readonly-fs81a6a0d2-6283-452c-9732-5b84ca8337b2 from kubelet-test-3039 started at 2023-03-27 15:40:04 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.490: INFO: 	Container busybox-readonly-fs81a6a0d2-6283-452c-9732-5b84ca8337b2 ready: true, restart count 0
    Mar 27 15:40:20.490: INFO: sonobuoy from sonobuoy started at 2023-03-27 14:05:39 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.490: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 27 15:40:20.490: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-4ztlm from sonobuoy started at 2023-03-27 14:05:46 +0000 UTC (2 container statuses recorded)
    Mar 27 15:40:20.490: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 15:40:20.490: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 27 15:40:20.490: INFO: 
    Logging pods the apiserver thinks is on node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv before test
    Mar 27 15:40:20.507: INFO: calico-kube-controllers-659979cbcb-rxz7c from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.507: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: canal-llxzv from kube-system started at 2023-03-27 14:02:28 +0000 UTC (2 container statuses recorded)
    Mar 27 15:40:20.507: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: cluster-autoscaler-855c4bc847-b7mwh from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.507: INFO: 	Container cluster-autoscaler ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: coredns-5954c46f46-k6bxj from kube-system started at 2023-03-27 14:03:06 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.507: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: coredns-5954c46f46-qg6wb from kube-system started at 2023-03-27 14:03:06 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.507: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: csi-cinder-nodeplugin-4gffp from kube-system started at 2023-03-27 14:02:28 +0000 UTC (3 container statuses recorded)
    Mar 27 15:40:20.507: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: dns-autoscaler-69d8768b6-qg5fp from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.507: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: konnectivity-agent-7c486cb7b8-7wxj6 from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.507: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: konnectivity-agent-7c486cb7b8-ns5pt from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.507: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: kube-proxy-4w8t9 from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.507: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: metrics-server-67b4667644-d4vmh from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.507: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: metrics-server-67b4667644-t5ftq from kube-system started at 2023-03-27 14:05:01 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.507: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: node-exporter-8hspl from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.507: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: node-local-dns-zgn95 from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.507: INFO: 	Container node-cache ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: syseleven-node-problem-detector-nvvwp from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.507: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: user-ssh-keys-agent-svptx from kube-system started at 2023-03-27 14:02:28 +0000 UTC (1 container statuses recorded)
    Mar 27 15:40:20.507: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-dgx79 from sonobuoy started at 2023-03-27 14:05:45 +0000 UTC (2 container statuses recorded)
    Mar 27 15:40:20.507: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 15:40:20.507: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq 03/27/23 15:40:20.536
    STEP: verifying the node has the label node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc 03/27/23 15:40:20.557
    STEP: verifying the node has the label node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv 03/27/23 15:40:20.576
    Mar 27 15:40:20.593: INFO: Pod adopt-release-fpfpx requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
    Mar 27 15:40:20.593: INFO: Pod adopt-release-gsz76 requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
    Mar 27 15:40:20.593: INFO: Pod adopt-release-pq86t requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
    Mar 27 15:40:20.593: INFO: Pod calico-kube-controllers-659979cbcb-rxz7c requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
    Mar 27 15:40:20.593: INFO: Pod canal-8sfms requesting resource cpu=250m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
    Mar 27 15:40:20.593: INFO: Pod canal-llxzv requesting resource cpu=250m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
    Mar 27 15:40:20.593: INFO: Pod canal-vhmnn requesting resource cpu=250m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
    Mar 27 15:40:20.593: INFO: Pod cluster-autoscaler-855c4bc847-b7mwh requesting resource cpu=100m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
    Mar 27 15:40:20.593: INFO: Pod coredns-5954c46f46-k6bxj requesting resource cpu=50m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
    Mar 27 15:40:20.593: INFO: Pod coredns-5954c46f46-qg6wb requesting resource cpu=50m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
    Mar 27 15:40:20.593: INFO: Pod csi-cinder-nodeplugin-4gffp requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
    Mar 27 15:40:20.593: INFO: Pod csi-cinder-nodeplugin-lhmqp requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
    Mar 27 15:40:20.593: INFO: Pod csi-cinder-nodeplugin-lmbcb requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
    Mar 27 15:40:20.593: INFO: Pod dns-autoscaler-69d8768b6-qg5fp requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
    Mar 27 15:40:20.593: INFO: Pod konnectivity-agent-7c486cb7b8-7wxj6 requesting resource cpu=10m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
    Mar 27 15:40:20.593: INFO: Pod konnectivity-agent-7c486cb7b8-ns5pt requesting resource cpu=10m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
    Mar 27 15:40:20.593: INFO: Pod kube-proxy-4njm5 requesting resource cpu=75m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
    Mar 27 15:40:20.593: INFO: Pod kube-proxy-4w8t9 requesting resource cpu=75m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
    Mar 27 15:40:20.593: INFO: Pod kube-proxy-gj9h9 requesting resource cpu=75m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
    Mar 27 15:40:20.593: INFO: Pod metrics-server-67b4667644-d4vmh requesting resource cpu=100m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
    Mar 27 15:40:20.593: INFO: Pod metrics-server-67b4667644-t5ftq requesting resource cpu=100m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
    Mar 27 15:40:20.593: INFO: Pod node-exporter-8hspl requesting resource cpu=3m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
    Mar 27 15:40:20.593: INFO: Pod node-exporter-fgptx requesting resource cpu=3m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
    Mar 27 15:40:20.593: INFO: Pod node-exporter-x8df2 requesting resource cpu=3m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
    Mar 27 15:40:20.593: INFO: Pod node-local-dns-tmg5z requesting resource cpu=50m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
    Mar 27 15:40:20.593: INFO: Pod node-local-dns-zgn95 requesting resource cpu=50m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
    Mar 27 15:40:20.593: INFO: Pod node-local-dns-zvl8z requesting resource cpu=50m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
    Mar 27 15:40:20.593: INFO: Pod syseleven-node-problem-detector-2x2wt requesting resource cpu=10m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
    Mar 27 15:40:20.593: INFO: Pod syseleven-node-problem-detector-nvvwp requesting resource cpu=10m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
    Mar 27 15:40:20.593: INFO: Pod syseleven-node-problem-detector-xh9cq requesting resource cpu=10m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
    Mar 27 15:40:20.593: INFO: Pod user-ssh-keys-agent-65hx9 requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
    Mar 27 15:40:20.593: INFO: Pod user-ssh-keys-agent-svptx requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
    Mar 27 15:40:20.593: INFO: Pod user-ssh-keys-agent-x7kpb requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
    Mar 27 15:40:20.593: INFO: Pod busybox-readonly-fs81a6a0d2-6283-452c-9732-5b84ca8337b2 requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
    Mar 27 15:40:20.593: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
    Mar 27 15:40:20.593: INFO: Pod sonobuoy-e2e-job-267544c30cdc4a58 requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
    Mar 27 15:40:20.593: INFO: Pod sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-4ztlm requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
    Mar 27 15:40:20.593: INFO: Pod sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-bg2pk requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
    Mar 27 15:40:20.593: INFO: Pod sonobuoy-systemd-logs-daemon-set-8ec233163a5c410e-dgx79 requesting resource cpu=0m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
    STEP: Starting Pods to consume most of the cluster CPU. 03/27/23 15:40:20.593
    Mar 27 15:40:20.593: INFO: Creating a pod which consumes cpu=2248m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq
    Mar 27 15:40:20.601: INFO: Creating a pod which consumes cpu=2248m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc
    Mar 27 15:40:20.609: INFO: Creating a pod which consumes cpu=1954m on Node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv
    Mar 27 15:40:20.626: INFO: Waiting up to 5m0s for pod "filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe" in namespace "sched-pred-6838" to be "running"
    Mar 27 15:40:20.637: INFO: Pod "filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.697357ms
    Mar 27 15:40:22.641: INFO: Pod "filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.014639224s
    Mar 27 15:40:22.641: INFO: Pod "filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe" satisfied condition "running"
    Mar 27 15:40:22.641: INFO: Waiting up to 5m0s for pod "filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99" in namespace "sched-pred-6838" to be "running"
    Mar 27 15:40:22.644: INFO: Pod "filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99": Phase="Running", Reason="", readiness=true. Elapsed: 2.751996ms
    Mar 27 15:40:22.644: INFO: Pod "filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99" satisfied condition "running"
    Mar 27 15:40:22.644: INFO: Waiting up to 5m0s for pod "filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4" in namespace "sched-pred-6838" to be "running"
    Mar 27 15:40:22.648: INFO: Pod "filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4": Phase="Running", Reason="", readiness=true. Elapsed: 4.000761ms
    Mar 27 15:40:22.648: INFO: Pod "filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 03/27/23 15:40:22.648
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe.175050e8850b9cef], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6838/filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe to k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq] 03/27/23 15:40:22.651
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe.175050e8af53fc5f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/27/23 15:40:22.651
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe.175050e8dc9997b4], Reason = [Created], Message = [Created container filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe] 03/27/23 15:40:22.651
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe.175050e8f16bc107], Reason = [Started], Message = [Started container filler-pod-3f11a2a9-941d-47dc-a084-57e229efa9fe] 03/27/23 15:40:22.651
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4.175050e8864e50db], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6838/filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4 to k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv] 03/27/23 15:40:22.652
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4.175050e8afcf867e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/27/23 15:40:22.652
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4.175050e8b54ab47f], Reason = [Created], Message = [Created container filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4] 03/27/23 15:40:22.652
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4.175050e8bc0d040d], Reason = [Started], Message = [Started container filler-pod-c3ef3a26-707f-474a-9cef-fe501b6839c4] 03/27/23 15:40:22.652
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99.175050e8857313db], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6838/filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99 to k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc] 03/27/23 15:40:22.652
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99.175050e8b3866562], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/27/23 15:40:22.652
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99.175050e8b9eab7f3], Reason = [Created], Message = [Created container filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99] 03/27/23 15:40:22.652
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99.175050e8c352ff54], Reason = [Started], Message = [Started container filler-pod-e50eb736-77f1-452c-9514-f58ac400ea99] 03/27/23 15:40:22.652
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.175050e8ff35ba2e], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 03/27/23 15:40:22.67
    STEP: removing the label node off the node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-4lfzq 03/27/23 15:40:23.666
    STEP: verifying the node doesn't have the label node 03/27/23 15:40:23.681
    STEP: removing the label node off the node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc 03/27/23 15:40:23.687
    STEP: verifying the node doesn't have the label node 03/27/23 15:40:23.701
    STEP: removing the label node off the node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-xfwrv 03/27/23 15:40:23.705
    STEP: verifying the node doesn't have the label node 03/27/23 15:40:23.72
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar 27 15:40:23.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6838" for this suite. 03/27/23 15:40:23.728
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:40:23.739
Mar 27 15:40:23.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename cronjob 03/27/23 15:40:23.74
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:40:23.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:40:23.759
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 03/27/23 15:40:23.762
STEP: Ensuring no jobs are scheduled 03/27/23 15:40:23.768
STEP: Ensuring no job exists by listing jobs explicitly 03/27/23 15:45:23.777
STEP: Removing cronjob 03/27/23 15:45:23.78
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar 27 15:45:23.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-944" for this suite. 03/27/23 15:45:23.794
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":333,"skipped":6219,"failed":0}
------------------------------
• [SLOW TEST] [300.062 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:40:23.739
    Mar 27 15:40:23.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename cronjob 03/27/23 15:40:23.74
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:40:23.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:40:23.759
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 03/27/23 15:40:23.762
    STEP: Ensuring no jobs are scheduled 03/27/23 15:40:23.768
    STEP: Ensuring no job exists by listing jobs explicitly 03/27/23 15:45:23.777
    STEP: Removing cronjob 03/27/23 15:45:23.78
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar 27 15:45:23.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-944" for this suite. 03/27/23 15:45:23.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:45:23.803
Mar 27 15:45:23.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubectl 03/27/23 15:45:23.804
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:45:23.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:45:23.824
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 03/27/23 15:45:23.828
Mar 27 15:45:23.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4913 create -f -'
Mar 27 15:45:24.037: INFO: stderr: ""
Mar 27 15:45:24.037: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/27/23 15:45:24.037
Mar 27 15:45:25.042: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 15:45:25.042: INFO: Found 0 / 1
Mar 27 15:45:26.042: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 15:45:26.042: INFO: Found 1 / 1
Mar 27 15:45:26.042: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 03/27/23 15:45:26.042
Mar 27 15:45:26.046: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 15:45:26.046: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 27 15:45:26.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4913 patch pod agnhost-primary-hslr8 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 27 15:45:26.123: INFO: stderr: ""
Mar 27 15:45:26.123: INFO: stdout: "pod/agnhost-primary-hslr8 patched\n"
STEP: checking annotations 03/27/23 15:45:26.123
Mar 27 15:45:26.127: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 15:45:26.127: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 27 15:45:26.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4913" for this suite. 03/27/23 15:45:26.132
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":334,"skipped":6224,"failed":0}
------------------------------
• [2.333 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:45:23.803
    Mar 27 15:45:23.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubectl 03/27/23 15:45:23.804
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:45:23.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:45:23.824
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 03/27/23 15:45:23.828
    Mar 27 15:45:23.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4913 create -f -'
    Mar 27 15:45:24.037: INFO: stderr: ""
    Mar 27 15:45:24.037: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/27/23 15:45:24.037
    Mar 27 15:45:25.042: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 15:45:25.042: INFO: Found 0 / 1
    Mar 27 15:45:26.042: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 15:45:26.042: INFO: Found 1 / 1
    Mar 27 15:45:26.042: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 03/27/23 15:45:26.042
    Mar 27 15:45:26.046: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 15:45:26.046: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 27 15:45:26.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=kubectl-4913 patch pod agnhost-primary-hslr8 -p {"metadata":{"annotations":{"x":"y"}}}'
    Mar 27 15:45:26.123: INFO: stderr: ""
    Mar 27 15:45:26.123: INFO: stdout: "pod/agnhost-primary-hslr8 patched\n"
    STEP: checking annotations 03/27/23 15:45:26.123
    Mar 27 15:45:26.127: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 15:45:26.127: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 27 15:45:26.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4913" for this suite. 03/27/23 15:45:26.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:45:26.138
Mar 27 15:45:26.138: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename projected 03/27/23 15:45:26.139
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:45:26.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:45:26.155
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 03/27/23 15:45:26.159
Mar 27 15:45:26.169: INFO: Waiting up to 5m0s for pod "annotationupdate6df07b67-b57a-4fd7-8a1b-5fa5f070dc6d" in namespace "projected-7454" to be "running and ready"
Mar 27 15:45:26.174: INFO: Pod "annotationupdate6df07b67-b57a-4fd7-8a1b-5fa5f070dc6d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.907772ms
Mar 27 15:45:26.174: INFO: The phase of Pod annotationupdate6df07b67-b57a-4fd7-8a1b-5fa5f070dc6d is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:45:28.179: INFO: Pod "annotationupdate6df07b67-b57a-4fd7-8a1b-5fa5f070dc6d": Phase="Running", Reason="", readiness=true. Elapsed: 2.01006631s
Mar 27 15:45:28.179: INFO: The phase of Pod annotationupdate6df07b67-b57a-4fd7-8a1b-5fa5f070dc6d is Running (Ready = true)
Mar 27 15:45:28.179: INFO: Pod "annotationupdate6df07b67-b57a-4fd7-8a1b-5fa5f070dc6d" satisfied condition "running and ready"
Mar 27 15:45:28.716: INFO: Successfully updated pod "annotationupdate6df07b67-b57a-4fd7-8a1b-5fa5f070dc6d"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 27 15:45:30.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7454" for this suite. 03/27/23 15:45:30.743
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":335,"skipped":6240,"failed":0}
------------------------------
• [4.611 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:45:26.138
    Mar 27 15:45:26.138: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename projected 03/27/23 15:45:26.139
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:45:26.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:45:26.155
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 03/27/23 15:45:26.159
    Mar 27 15:45:26.169: INFO: Waiting up to 5m0s for pod "annotationupdate6df07b67-b57a-4fd7-8a1b-5fa5f070dc6d" in namespace "projected-7454" to be "running and ready"
    Mar 27 15:45:26.174: INFO: Pod "annotationupdate6df07b67-b57a-4fd7-8a1b-5fa5f070dc6d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.907772ms
    Mar 27 15:45:26.174: INFO: The phase of Pod annotationupdate6df07b67-b57a-4fd7-8a1b-5fa5f070dc6d is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:45:28.179: INFO: Pod "annotationupdate6df07b67-b57a-4fd7-8a1b-5fa5f070dc6d": Phase="Running", Reason="", readiness=true. Elapsed: 2.01006631s
    Mar 27 15:45:28.179: INFO: The phase of Pod annotationupdate6df07b67-b57a-4fd7-8a1b-5fa5f070dc6d is Running (Ready = true)
    Mar 27 15:45:28.179: INFO: Pod "annotationupdate6df07b67-b57a-4fd7-8a1b-5fa5f070dc6d" satisfied condition "running and ready"
    Mar 27 15:45:28.716: INFO: Successfully updated pod "annotationupdate6df07b67-b57a-4fd7-8a1b-5fa5f070dc6d"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 27 15:45:30.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7454" for this suite. 03/27/23 15:45:30.743
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:45:30.75
Mar 27 15:45:30.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename kubelet-test 03/27/23 15:45:30.751
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:45:30.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:45:30.818
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 03/27/23 15:45:30.83
Mar 27 15:45:30.830: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases54839434-2943-47f3-a11d-c0d6404bf419" in namespace "kubelet-test-3023" to be "completed"
Mar 27 15:45:30.838: INFO: Pod "agnhost-host-aliases54839434-2943-47f3-a11d-c0d6404bf419": Phase="Pending", Reason="", readiness=false. Elapsed: 7.512512ms
Mar 27 15:45:32.843: INFO: Pod "agnhost-host-aliases54839434-2943-47f3-a11d-c0d6404bf419": Phase="Running", Reason="", readiness=true. Elapsed: 2.012617038s
Mar 27 15:45:34.843: INFO: Pod "agnhost-host-aliases54839434-2943-47f3-a11d-c0d6404bf419": Phase="Running", Reason="", readiness=false. Elapsed: 4.012606264s
Mar 27 15:45:36.843: INFO: Pod "agnhost-host-aliases54839434-2943-47f3-a11d-c0d6404bf419": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012238073s
Mar 27 15:45:36.843: INFO: Pod "agnhost-host-aliases54839434-2943-47f3-a11d-c0d6404bf419" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar 27 15:45:36.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3023" for this suite. 03/27/23 15:45:36.859
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":336,"skipped":6244,"failed":0}
------------------------------
• [SLOW TEST] [6.118 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:45:30.75
    Mar 27 15:45:30.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename kubelet-test 03/27/23 15:45:30.751
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:45:30.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:45:30.818
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 03/27/23 15:45:30.83
    Mar 27 15:45:30.830: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases54839434-2943-47f3-a11d-c0d6404bf419" in namespace "kubelet-test-3023" to be "completed"
    Mar 27 15:45:30.838: INFO: Pod "agnhost-host-aliases54839434-2943-47f3-a11d-c0d6404bf419": Phase="Pending", Reason="", readiness=false. Elapsed: 7.512512ms
    Mar 27 15:45:32.843: INFO: Pod "agnhost-host-aliases54839434-2943-47f3-a11d-c0d6404bf419": Phase="Running", Reason="", readiness=true. Elapsed: 2.012617038s
    Mar 27 15:45:34.843: INFO: Pod "agnhost-host-aliases54839434-2943-47f3-a11d-c0d6404bf419": Phase="Running", Reason="", readiness=false. Elapsed: 4.012606264s
    Mar 27 15:45:36.843: INFO: Pod "agnhost-host-aliases54839434-2943-47f3-a11d-c0d6404bf419": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012238073s
    Mar 27 15:45:36.843: INFO: Pod "agnhost-host-aliases54839434-2943-47f3-a11d-c0d6404bf419" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar 27 15:45:36.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-3023" for this suite. 03/27/23 15:45:36.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:45:36.869
Mar 27 15:45:36.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename webhook 03/27/23 15:45:36.871
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:45:36.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:45:36.885
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/27/23 15:45:36.902
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:45:37.102
STEP: Deploying the webhook pod 03/27/23 15:45:37.11
STEP: Wait for the deployment to be ready 03/27/23 15:45:37.124
Mar 27 15:45:37.130: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 03/27/23 15:45:39.145
STEP: Verifying the service has paired with the endpoint 03/27/23 15:45:39.162
Mar 27 15:45:40.163: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 03/27/23 15:45:40.168
STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 15:45:40.191
STEP: Updating a validating webhook configuration's rules to not include the create operation 03/27/23 15:45:40.204
STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 15:45:40.214
STEP: Patching a validating webhook configuration's rules to include the create operation 03/27/23 15:45:40.227
STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 15:45:40.234
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 15:45:40.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1308" for this suite. 03/27/23 15:45:40.248
STEP: Destroying namespace "webhook-1308-markers" for this suite. 03/27/23 15:45:40.26
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":337,"skipped":6258,"failed":0}
------------------------------
• [3.440 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:45:36.869
    Mar 27 15:45:36.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename webhook 03/27/23 15:45:36.871
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:45:36.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:45:36.885
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/27/23 15:45:36.902
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:45:37.102
    STEP: Deploying the webhook pod 03/27/23 15:45:37.11
    STEP: Wait for the deployment to be ready 03/27/23 15:45:37.124
    Mar 27 15:45:37.130: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 03/27/23 15:45:39.145
    STEP: Verifying the service has paired with the endpoint 03/27/23 15:45:39.162
    Mar 27 15:45:40.163: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 03/27/23 15:45:40.168
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 15:45:40.191
    STEP: Updating a validating webhook configuration's rules to not include the create operation 03/27/23 15:45:40.204
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 15:45:40.214
    STEP: Patching a validating webhook configuration's rules to include the create operation 03/27/23 15:45:40.227
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 15:45:40.234
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 15:45:40.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1308" for this suite. 03/27/23 15:45:40.248
    STEP: Destroying namespace "webhook-1308-markers" for this suite. 03/27/23 15:45:40.26
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:45:40.309
Mar 27 15:45:40.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename containers 03/27/23 15:45:40.31
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:45:40.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:45:40.33
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 03/27/23 15:45:40.334
Mar 27 15:45:40.344: INFO: Waiting up to 5m0s for pod "client-containers-0f629f9f-fe84-4d02-bd51-0c8e5b1e10f9" in namespace "containers-2519" to be "Succeeded or Failed"
Mar 27 15:45:40.349: INFO: Pod "client-containers-0f629f9f-fe84-4d02-bd51-0c8e5b1e10f9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.437535ms
Mar 27 15:45:42.355: INFO: Pod "client-containers-0f629f9f-fe84-4d02-bd51-0c8e5b1e10f9": Phase="Running", Reason="", readiness=true. Elapsed: 2.011216502s
Mar 27 15:45:44.354: INFO: Pod "client-containers-0f629f9f-fe84-4d02-bd51-0c8e5b1e10f9": Phase="Running", Reason="", readiness=false. Elapsed: 4.010482115s
Mar 27 15:45:46.380: INFO: Pod "client-containers-0f629f9f-fe84-4d02-bd51-0c8e5b1e10f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036599033s
STEP: Saw pod success 03/27/23 15:45:46.38
Mar 27 15:45:46.380: INFO: Pod "client-containers-0f629f9f-fe84-4d02-bd51-0c8e5b1e10f9" satisfied condition "Succeeded or Failed"
Mar 27 15:45:46.428: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod client-containers-0f629f9f-fe84-4d02-bd51-0c8e5b1e10f9 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 15:45:46.476
Mar 27 15:45:46.655: INFO: Waiting for pod client-containers-0f629f9f-fe84-4d02-bd51-0c8e5b1e10f9 to disappear
Mar 27 15:45:46.658: INFO: Pod client-containers-0f629f9f-fe84-4d02-bd51-0c8e5b1e10f9 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar 27 15:45:46.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2519" for this suite. 03/27/23 15:45:46.666
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":338,"skipped":6268,"failed":0}
------------------------------
• [SLOW TEST] [6.446 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:45:40.309
    Mar 27 15:45:40.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename containers 03/27/23 15:45:40.31
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:45:40.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:45:40.33
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 03/27/23 15:45:40.334
    Mar 27 15:45:40.344: INFO: Waiting up to 5m0s for pod "client-containers-0f629f9f-fe84-4d02-bd51-0c8e5b1e10f9" in namespace "containers-2519" to be "Succeeded or Failed"
    Mar 27 15:45:40.349: INFO: Pod "client-containers-0f629f9f-fe84-4d02-bd51-0c8e5b1e10f9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.437535ms
    Mar 27 15:45:42.355: INFO: Pod "client-containers-0f629f9f-fe84-4d02-bd51-0c8e5b1e10f9": Phase="Running", Reason="", readiness=true. Elapsed: 2.011216502s
    Mar 27 15:45:44.354: INFO: Pod "client-containers-0f629f9f-fe84-4d02-bd51-0c8e5b1e10f9": Phase="Running", Reason="", readiness=false. Elapsed: 4.010482115s
    Mar 27 15:45:46.380: INFO: Pod "client-containers-0f629f9f-fe84-4d02-bd51-0c8e5b1e10f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036599033s
    STEP: Saw pod success 03/27/23 15:45:46.38
    Mar 27 15:45:46.380: INFO: Pod "client-containers-0f629f9f-fe84-4d02-bd51-0c8e5b1e10f9" satisfied condition "Succeeded or Failed"
    Mar 27 15:45:46.428: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod client-containers-0f629f9f-fe84-4d02-bd51-0c8e5b1e10f9 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 15:45:46.476
    Mar 27 15:45:46.655: INFO: Waiting for pod client-containers-0f629f9f-fe84-4d02-bd51-0c8e5b1e10f9 to disappear
    Mar 27 15:45:46.658: INFO: Pod client-containers-0f629f9f-fe84-4d02-bd51-0c8e5b1e10f9 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar 27 15:45:46.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-2519" for this suite. 03/27/23 15:45:46.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:45:46.757
Mar 27 15:45:46.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename var-expansion 03/27/23 15:45:46.758
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:45:47.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:45:47.179
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 03/27/23 15:45:47.182
Mar 27 15:45:47.383: INFO: Waiting up to 5m0s for pod "var-expansion-d2fcf0d5-ba10-4646-9bd5-9a7f24327f8a" in namespace "var-expansion-1299" to be "Succeeded or Failed"
Mar 27 15:45:47.456: INFO: Pod "var-expansion-d2fcf0d5-ba10-4646-9bd5-9a7f24327f8a": Phase="Pending", Reason="", readiness=false. Elapsed: 73.058173ms
Mar 27 15:45:49.460: INFO: Pod "var-expansion-d2fcf0d5-ba10-4646-9bd5-9a7f24327f8a": Phase="Running", Reason="", readiness=true. Elapsed: 2.077397435s
Mar 27 15:45:51.461: INFO: Pod "var-expansion-d2fcf0d5-ba10-4646-9bd5-9a7f24327f8a": Phase="Running", Reason="", readiness=false. Elapsed: 4.07769751s
Mar 27 15:45:53.462: INFO: Pod "var-expansion-d2fcf0d5-ba10-4646-9bd5-9a7f24327f8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.078800865s
STEP: Saw pod success 03/27/23 15:45:53.462
Mar 27 15:45:53.462: INFO: Pod "var-expansion-d2fcf0d5-ba10-4646-9bd5-9a7f24327f8a" satisfied condition "Succeeded or Failed"
Mar 27 15:45:53.465: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod var-expansion-d2fcf0d5-ba10-4646-9bd5-9a7f24327f8a container dapi-container: <nil>
STEP: delete the pod 03/27/23 15:45:53.474
Mar 27 15:45:53.490: INFO: Waiting for pod var-expansion-d2fcf0d5-ba10-4646-9bd5-9a7f24327f8a to disappear
Mar 27 15:45:53.493: INFO: Pod var-expansion-d2fcf0d5-ba10-4646-9bd5-9a7f24327f8a no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 27 15:45:53.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1299" for this suite. 03/27/23 15:45:53.498
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":339,"skipped":6273,"failed":0}
------------------------------
• [SLOW TEST] [6.748 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:45:46.757
    Mar 27 15:45:46.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename var-expansion 03/27/23 15:45:46.758
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:45:47.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:45:47.179
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 03/27/23 15:45:47.182
    Mar 27 15:45:47.383: INFO: Waiting up to 5m0s for pod "var-expansion-d2fcf0d5-ba10-4646-9bd5-9a7f24327f8a" in namespace "var-expansion-1299" to be "Succeeded or Failed"
    Mar 27 15:45:47.456: INFO: Pod "var-expansion-d2fcf0d5-ba10-4646-9bd5-9a7f24327f8a": Phase="Pending", Reason="", readiness=false. Elapsed: 73.058173ms
    Mar 27 15:45:49.460: INFO: Pod "var-expansion-d2fcf0d5-ba10-4646-9bd5-9a7f24327f8a": Phase="Running", Reason="", readiness=true. Elapsed: 2.077397435s
    Mar 27 15:45:51.461: INFO: Pod "var-expansion-d2fcf0d5-ba10-4646-9bd5-9a7f24327f8a": Phase="Running", Reason="", readiness=false. Elapsed: 4.07769751s
    Mar 27 15:45:53.462: INFO: Pod "var-expansion-d2fcf0d5-ba10-4646-9bd5-9a7f24327f8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.078800865s
    STEP: Saw pod success 03/27/23 15:45:53.462
    Mar 27 15:45:53.462: INFO: Pod "var-expansion-d2fcf0d5-ba10-4646-9bd5-9a7f24327f8a" satisfied condition "Succeeded or Failed"
    Mar 27 15:45:53.465: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod var-expansion-d2fcf0d5-ba10-4646-9bd5-9a7f24327f8a container dapi-container: <nil>
    STEP: delete the pod 03/27/23 15:45:53.474
    Mar 27 15:45:53.490: INFO: Waiting for pod var-expansion-d2fcf0d5-ba10-4646-9bd5-9a7f24327f8a to disappear
    Mar 27 15:45:53.493: INFO: Pod var-expansion-d2fcf0d5-ba10-4646-9bd5-9a7f24327f8a no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 27 15:45:53.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1299" for this suite. 03/27/23 15:45:53.498
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:45:53.505
Mar 27 15:45:53.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename events 03/27/23 15:45:53.506
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:45:53.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:45:53.524
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 03/27/23 15:45:53.527
Mar 27 15:45:53.533: INFO: created test-event-1
Mar 27 15:45:53.542: INFO: created test-event-2
Mar 27 15:45:53.549: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 03/27/23 15:45:53.549
STEP: delete collection of events 03/27/23 15:45:53.553
Mar 27 15:45:53.553: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/27/23 15:45:53.575
Mar 27 15:45:53.575: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Mar 27 15:45:53.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1384" for this suite. 03/27/23 15:45:53.585
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":340,"skipped":6275,"failed":0}
------------------------------
• [0.088 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:45:53.505
    Mar 27 15:45:53.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename events 03/27/23 15:45:53.506
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:45:53.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:45:53.524
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 03/27/23 15:45:53.527
    Mar 27 15:45:53.533: INFO: created test-event-1
    Mar 27 15:45:53.542: INFO: created test-event-2
    Mar 27 15:45:53.549: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 03/27/23 15:45:53.549
    STEP: delete collection of events 03/27/23 15:45:53.553
    Mar 27 15:45:53.553: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/27/23 15:45:53.575
    Mar 27 15:45:53.575: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Mar 27 15:45:53.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-1384" for this suite. 03/27/23 15:45:53.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:45:53.594
Mar 27 15:45:53.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename limitrange 03/27/23 15:45:53.595
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:45:53.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:45:53.66
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 03/27/23 15:45:53.663
STEP: Setting up watch 03/27/23 15:45:53.663
STEP: Submitting a LimitRange 03/27/23 15:45:53.767
STEP: Verifying LimitRange creation was observed 03/27/23 15:45:53.777
STEP: Fetching the LimitRange to ensure it has proper values 03/27/23 15:45:53.777
Mar 27 15:45:53.781: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 27 15:45:53.781: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 03/27/23 15:45:53.781
STEP: Ensuring Pod has resource requirements applied from LimitRange 03/27/23 15:45:53.787
Mar 27 15:45:53.790: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 27 15:45:53.790: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 03/27/23 15:45:53.79
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/27/23 15:45:53.799
Mar 27 15:45:53.803: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar 27 15:45:53.803: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 03/27/23 15:45:53.803
STEP: Failing to create a Pod with more than max resources 03/27/23 15:45:53.805
STEP: Updating a LimitRange 03/27/23 15:45:53.807
STEP: Verifying LimitRange updating is effective 03/27/23 15:45:53.811
STEP: Creating a Pod with less than former min resources 03/27/23 15:45:55.816
STEP: Failing to create a Pod with more than max resources 03/27/23 15:45:55.824
STEP: Deleting a LimitRange 03/27/23 15:45:55.83
STEP: Verifying the LimitRange was deleted 03/27/23 15:45:55.838
Mar 27 15:46:00.847: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 03/27/23 15:46:00.847
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Mar 27 15:46:00.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4303" for this suite. 03/27/23 15:46:00.86
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":341,"skipped":6291,"failed":0}
------------------------------
• [SLOW TEST] [7.274 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:45:53.594
    Mar 27 15:45:53.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename limitrange 03/27/23 15:45:53.595
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:45:53.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:45:53.66
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 03/27/23 15:45:53.663
    STEP: Setting up watch 03/27/23 15:45:53.663
    STEP: Submitting a LimitRange 03/27/23 15:45:53.767
    STEP: Verifying LimitRange creation was observed 03/27/23 15:45:53.777
    STEP: Fetching the LimitRange to ensure it has proper values 03/27/23 15:45:53.777
    Mar 27 15:45:53.781: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar 27 15:45:53.781: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 03/27/23 15:45:53.781
    STEP: Ensuring Pod has resource requirements applied from LimitRange 03/27/23 15:45:53.787
    Mar 27 15:45:53.790: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar 27 15:45:53.790: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 03/27/23 15:45:53.79
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/27/23 15:45:53.799
    Mar 27 15:45:53.803: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Mar 27 15:45:53.803: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 03/27/23 15:45:53.803
    STEP: Failing to create a Pod with more than max resources 03/27/23 15:45:53.805
    STEP: Updating a LimitRange 03/27/23 15:45:53.807
    STEP: Verifying LimitRange updating is effective 03/27/23 15:45:53.811
    STEP: Creating a Pod with less than former min resources 03/27/23 15:45:55.816
    STEP: Failing to create a Pod with more than max resources 03/27/23 15:45:55.824
    STEP: Deleting a LimitRange 03/27/23 15:45:55.83
    STEP: Verifying the LimitRange was deleted 03/27/23 15:45:55.838
    Mar 27 15:46:00.847: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 03/27/23 15:46:00.847
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Mar 27 15:46:00.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-4303" for this suite. 03/27/23 15:46:00.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:46:00.869
Mar 27 15:46:00.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename secrets 03/27/23 15:46:00.87
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:46:00.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:46:00.886
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-ff14c9c4-f389-477b-8bbf-9d6dfb9641de 03/27/23 15:46:00.89
STEP: Creating a pod to test consume secrets 03/27/23 15:46:00.895
Mar 27 15:46:00.903: INFO: Waiting up to 5m0s for pod "pod-secrets-3e9fa48c-f1c3-4174-9121-580e4e6f6860" in namespace "secrets-4037" to be "Succeeded or Failed"
Mar 27 15:46:00.909: INFO: Pod "pod-secrets-3e9fa48c-f1c3-4174-9121-580e4e6f6860": Phase="Pending", Reason="", readiness=false. Elapsed: 6.312611ms
Mar 27 15:46:02.914: INFO: Pod "pod-secrets-3e9fa48c-f1c3-4174-9121-580e4e6f6860": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011705648s
Mar 27 15:46:04.913: INFO: Pod "pod-secrets-3e9fa48c-f1c3-4174-9121-580e4e6f6860": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010664079s
Mar 27 15:46:06.915: INFO: Pod "pod-secrets-3e9fa48c-f1c3-4174-9121-580e4e6f6860": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011851983s
STEP: Saw pod success 03/27/23 15:46:06.915
Mar 27 15:46:06.915: INFO: Pod "pod-secrets-3e9fa48c-f1c3-4174-9121-580e4e6f6860" satisfied condition "Succeeded or Failed"
Mar 27 15:46:06.918: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-secrets-3e9fa48c-f1c3-4174-9121-580e4e6f6860 container secret-volume-test: <nil>
STEP: delete the pod 03/27/23 15:46:06.925
Mar 27 15:46:06.939: INFO: Waiting for pod pod-secrets-3e9fa48c-f1c3-4174-9121-580e4e6f6860 to disappear
Mar 27 15:46:06.942: INFO: Pod pod-secrets-3e9fa48c-f1c3-4174-9121-580e4e6f6860 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 27 15:46:06.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4037" for this suite. 03/27/23 15:46:06.948
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":342,"skipped":6300,"failed":0}
------------------------------
• [SLOW TEST] [6.093 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:46:00.869
    Mar 27 15:46:00.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename secrets 03/27/23 15:46:00.87
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:46:00.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:46:00.886
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-ff14c9c4-f389-477b-8bbf-9d6dfb9641de 03/27/23 15:46:00.89
    STEP: Creating a pod to test consume secrets 03/27/23 15:46:00.895
    Mar 27 15:46:00.903: INFO: Waiting up to 5m0s for pod "pod-secrets-3e9fa48c-f1c3-4174-9121-580e4e6f6860" in namespace "secrets-4037" to be "Succeeded or Failed"
    Mar 27 15:46:00.909: INFO: Pod "pod-secrets-3e9fa48c-f1c3-4174-9121-580e4e6f6860": Phase="Pending", Reason="", readiness=false. Elapsed: 6.312611ms
    Mar 27 15:46:02.914: INFO: Pod "pod-secrets-3e9fa48c-f1c3-4174-9121-580e4e6f6860": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011705648s
    Mar 27 15:46:04.913: INFO: Pod "pod-secrets-3e9fa48c-f1c3-4174-9121-580e4e6f6860": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010664079s
    Mar 27 15:46:06.915: INFO: Pod "pod-secrets-3e9fa48c-f1c3-4174-9121-580e4e6f6860": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011851983s
    STEP: Saw pod success 03/27/23 15:46:06.915
    Mar 27 15:46:06.915: INFO: Pod "pod-secrets-3e9fa48c-f1c3-4174-9121-580e4e6f6860" satisfied condition "Succeeded or Failed"
    Mar 27 15:46:06.918: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-secrets-3e9fa48c-f1c3-4174-9121-580e4e6f6860 container secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 15:46:06.925
    Mar 27 15:46:06.939: INFO: Waiting for pod pod-secrets-3e9fa48c-f1c3-4174-9121-580e4e6f6860 to disappear
    Mar 27 15:46:06.942: INFO: Pod pod-secrets-3e9fa48c-f1c3-4174-9121-580e4e6f6860 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 27 15:46:06.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4037" for this suite. 03/27/23 15:46:06.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:46:06.963
Mar 27 15:46:06.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename gc 03/27/23 15:46:06.964
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:46:06.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:46:06.981
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Mar 27 15:46:07.011: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"59308092-8f3f-4514-9f5c-720c294e3e47", Controller:(*bool)(0xc0003ebbfe), BlockOwnerDeletion:(*bool)(0xc0003ebbff)}}
Mar 27 15:46:07.018: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"0215a6e7-46af-497b-8a0d-3b7582eaf6af", Controller:(*bool)(0xc0003ebeca), BlockOwnerDeletion:(*bool)(0xc0003ebecb)}}
Mar 27 15:46:07.042: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"1ee74013-40bd-410b-babb-9b2a4c3bb13e", Controller:(*bool)(0xc0040e6c66), BlockOwnerDeletion:(*bool)(0xc0040e6c67)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 27 15:46:12.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7861" for this suite. 03/27/23 15:46:12.06
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":343,"skipped":6326,"failed":0}
------------------------------
• [SLOW TEST] [5.108 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:46:06.963
    Mar 27 15:46:06.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename gc 03/27/23 15:46:06.964
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:46:06.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:46:06.981
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Mar 27 15:46:07.011: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"59308092-8f3f-4514-9f5c-720c294e3e47", Controller:(*bool)(0xc0003ebbfe), BlockOwnerDeletion:(*bool)(0xc0003ebbff)}}
    Mar 27 15:46:07.018: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"0215a6e7-46af-497b-8a0d-3b7582eaf6af", Controller:(*bool)(0xc0003ebeca), BlockOwnerDeletion:(*bool)(0xc0003ebecb)}}
    Mar 27 15:46:07.042: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"1ee74013-40bd-410b-babb-9b2a4c3bb13e", Controller:(*bool)(0xc0040e6c66), BlockOwnerDeletion:(*bool)(0xc0040e6c67)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 27 15:46:12.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7861" for this suite. 03/27/23 15:46:12.06
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:46:12.072
Mar 27 15:46:12.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename disruption 03/27/23 15:46:12.073
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:46:12.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:46:12.094
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:46:12.097
Mar 27 15:46:12.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename disruption-2 03/27/23 15:46:12.098
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:46:12.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:46:12.117
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 03/27/23 15:46:12.128
STEP: Waiting for the pdb to be processed 03/27/23 15:46:14.143
STEP: Waiting for the pdb to be processed 03/27/23 15:46:14.156
STEP: listing a collection of PDBs across all namespaces 03/27/23 15:46:16.166
STEP: listing a collection of PDBs in namespace disruption-6765 03/27/23 15:46:16.169
STEP: deleting a collection of PDBs 03/27/23 15:46:16.175
STEP: Waiting for the PDB collection to be deleted 03/27/23 15:46:16.187
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Mar 27 15:46:16.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-6563" for this suite. 03/27/23 15:46:16.198
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar 27 15:46:16.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6765" for this suite. 03/27/23 15:46:16.208
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":344,"skipped":6329,"failed":0}
------------------------------
• [4.145 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:46:12.072
    Mar 27 15:46:12.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename disruption 03/27/23 15:46:12.073
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:46:12.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:46:12.094
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:46:12.097
    Mar 27 15:46:12.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename disruption-2 03/27/23 15:46:12.098
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:46:12.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:46:12.117
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 03/27/23 15:46:12.128
    STEP: Waiting for the pdb to be processed 03/27/23 15:46:14.143
    STEP: Waiting for the pdb to be processed 03/27/23 15:46:14.156
    STEP: listing a collection of PDBs across all namespaces 03/27/23 15:46:16.166
    STEP: listing a collection of PDBs in namespace disruption-6765 03/27/23 15:46:16.169
    STEP: deleting a collection of PDBs 03/27/23 15:46:16.175
    STEP: Waiting for the PDB collection to be deleted 03/27/23 15:46:16.187
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Mar 27 15:46:16.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-6563" for this suite. 03/27/23 15:46:16.198
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar 27 15:46:16.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-6765" for this suite. 03/27/23 15:46:16.208
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:46:16.217
Mar 27 15:46:16.217: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename webhook 03/27/23 15:46:16.218
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:46:16.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:46:16.234
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/27/23 15:46:16.249
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:46:16.85
STEP: Deploying the webhook pod 03/27/23 15:46:16.858
STEP: Wait for the deployment to be ready 03/27/23 15:46:16.871
Mar 27 15:46:16.884: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 27 15:46:18.902: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 15, 46, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 15, 46, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 15, 46, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 15, 46, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/27/23 15:46:20.906
STEP: Verifying the service has paired with the endpoint 03/27/23 15:46:20.921
Mar 27 15:46:21.921: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Mar 27 15:46:21.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8414-crds.webhook.example.com via the AdmissionRegistration API 03/27/23 15:46:22.441
STEP: Creating a custom resource that should be mutated by the webhook 03/27/23 15:46:22.463
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 15:46:25.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1614" for this suite. 03/27/23 15:46:25.028
STEP: Destroying namespace "webhook-1614-markers" for this suite. 03/27/23 15:46:25.034
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":345,"skipped":6332,"failed":0}
------------------------------
• [SLOW TEST] [8.861 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:46:16.217
    Mar 27 15:46:16.217: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename webhook 03/27/23 15:46:16.218
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:46:16.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:46:16.234
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/27/23 15:46:16.249
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:46:16.85
    STEP: Deploying the webhook pod 03/27/23 15:46:16.858
    STEP: Wait for the deployment to be ready 03/27/23 15:46:16.871
    Mar 27 15:46:16.884: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar 27 15:46:18.902: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 15, 46, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 15, 46, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 15, 46, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 15, 46, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/27/23 15:46:20.906
    STEP: Verifying the service has paired with the endpoint 03/27/23 15:46:20.921
    Mar 27 15:46:21.921: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Mar 27 15:46:21.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8414-crds.webhook.example.com via the AdmissionRegistration API 03/27/23 15:46:22.441
    STEP: Creating a custom resource that should be mutated by the webhook 03/27/23 15:46:22.463
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 15:46:25.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1614" for this suite. 03/27/23 15:46:25.028
    STEP: Destroying namespace "webhook-1614-markers" for this suite. 03/27/23 15:46:25.034
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:46:25.084
Mar 27 15:46:25.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename statefulset 03/27/23 15:46:25.086
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:46:25.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:46:25.105
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3860 03/27/23 15:46:25.109
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 03/27/23 15:46:25.114
STEP: Creating stateful set ss in namespace statefulset-3860 03/27/23 15:46:25.118
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3860 03/27/23 15:46:25.126
Mar 27 15:46:25.134: INFO: Found 0 stateful pods, waiting for 1
Mar 27 15:46:35.140: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/27/23 15:46:35.14
Mar 27 15:46:35.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3860 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 15:46:35.361: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 15:46:35.361: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 15:46:35.361: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 27 15:46:35.366: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 27 15:46:45.370: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 27 15:46:45.370: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 15:46:45.396: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999712s
Mar 27 15:46:46.401: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997040225s
Mar 27 15:46:47.406: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992839879s
Mar 27 15:46:48.410: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987775293s
Mar 27 15:46:49.416: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982738326s
Mar 27 15:46:50.437: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.977188879s
Mar 27 15:46:51.443: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.956584447s
Mar 27 15:46:52.447: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.950457064s
Mar 27 15:46:53.452: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.945943376s
Mar 27 15:46:54.458: INFO: Verifying statefulset ss doesn't scale past 1 for another 941.188563ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3860 03/27/23 15:46:55.458
Mar 27 15:46:55.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3860 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 27 15:46:55.618: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 27 15:46:55.618: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 27 15:46:55.618: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 27 15:46:55.623: INFO: Found 1 stateful pods, waiting for 3
Mar 27 15:47:05.630: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 15:47:05.630: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 15:47:05.630: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 03/27/23 15:47:05.631
STEP: Scale down will halt with unhealthy stateful pod 03/27/23 15:47:05.631
Mar 27 15:47:05.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3860 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 15:47:05.802: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 15:47:05.802: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 15:47:05.802: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 27 15:47:05.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3860 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 15:47:06.058: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 15:47:06.058: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 15:47:06.058: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 27 15:47:06.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3860 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 15:47:06.235: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 15:47:06.235: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 15:47:06.235: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 27 15:47:06.235: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 15:47:06.240: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 27 15:47:16.249: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 27 15:47:16.249: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 27 15:47:16.249: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 27 15:47:16.262: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999718s
Mar 27 15:47:17.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99640076s
Mar 27 15:47:18.274: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991012958s
Mar 27 15:47:19.279: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98548106s
Mar 27 15:47:20.283: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980299549s
Mar 27 15:47:21.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975824229s
Mar 27 15:47:22.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972220856s
Mar 27 15:47:23.296: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.967372535s
Mar 27 15:47:24.308: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.960883866s
Mar 27 15:47:25.314: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.627821ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3860 03/27/23 15:47:26.314
Mar 27 15:47:26.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3860 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 27 15:47:26.507: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 27 15:47:26.507: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 27 15:47:26.507: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 27 15:47:26.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3860 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 27 15:47:26.665: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 27 15:47:26.665: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 27 15:47:26.665: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 27 15:47:26.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3860 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 27 15:47:26.829: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 27 15:47:26.829: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 27 15:47:26.829: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 27 15:47:26.829: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 03/27/23 15:47:36.845
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 27 15:47:36.845: INFO: Deleting all statefulset in ns statefulset-3860
Mar 27 15:47:36.848: INFO: Scaling statefulset ss to 0
Mar 27 15:47:36.859: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 15:47:36.862: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 27 15:47:36.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3860" for this suite. 03/27/23 15:47:36.876
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":346,"skipped":6353,"failed":0}
------------------------------
• [SLOW TEST] [71.798 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:46:25.084
    Mar 27 15:46:25.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename statefulset 03/27/23 15:46:25.086
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:46:25.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:46:25.105
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3860 03/27/23 15:46:25.109
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 03/27/23 15:46:25.114
    STEP: Creating stateful set ss in namespace statefulset-3860 03/27/23 15:46:25.118
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3860 03/27/23 15:46:25.126
    Mar 27 15:46:25.134: INFO: Found 0 stateful pods, waiting for 1
    Mar 27 15:46:35.140: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/27/23 15:46:35.14
    Mar 27 15:46:35.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3860 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 15:46:35.361: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 15:46:35.361: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 15:46:35.361: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 27 15:46:35.366: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar 27 15:46:45.370: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 27 15:46:45.370: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 15:46:45.396: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999712s
    Mar 27 15:46:46.401: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997040225s
    Mar 27 15:46:47.406: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992839879s
    Mar 27 15:46:48.410: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987775293s
    Mar 27 15:46:49.416: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982738326s
    Mar 27 15:46:50.437: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.977188879s
    Mar 27 15:46:51.443: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.956584447s
    Mar 27 15:46:52.447: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.950457064s
    Mar 27 15:46:53.452: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.945943376s
    Mar 27 15:46:54.458: INFO: Verifying statefulset ss doesn't scale past 1 for another 941.188563ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3860 03/27/23 15:46:55.458
    Mar 27 15:46:55.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3860 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 27 15:46:55.618: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 27 15:46:55.618: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 27 15:46:55.618: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 27 15:46:55.623: INFO: Found 1 stateful pods, waiting for 3
    Mar 27 15:47:05.630: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 15:47:05.630: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 15:47:05.630: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 03/27/23 15:47:05.631
    STEP: Scale down will halt with unhealthy stateful pod 03/27/23 15:47:05.631
    Mar 27 15:47:05.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3860 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 15:47:05.802: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 15:47:05.802: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 15:47:05.802: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 27 15:47:05.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3860 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 15:47:06.058: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 15:47:06.058: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 15:47:06.058: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 27 15:47:06.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3860 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 15:47:06.235: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 15:47:06.235: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 15:47:06.235: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 27 15:47:06.235: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 15:47:06.240: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Mar 27 15:47:16.249: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 27 15:47:16.249: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar 27 15:47:16.249: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar 27 15:47:16.262: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999718s
    Mar 27 15:47:17.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99640076s
    Mar 27 15:47:18.274: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991012958s
    Mar 27 15:47:19.279: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98548106s
    Mar 27 15:47:20.283: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980299549s
    Mar 27 15:47:21.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975824229s
    Mar 27 15:47:22.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972220856s
    Mar 27 15:47:23.296: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.967372535s
    Mar 27 15:47:24.308: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.960883866s
    Mar 27 15:47:25.314: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.627821ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3860 03/27/23 15:47:26.314
    Mar 27 15:47:26.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3860 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 27 15:47:26.507: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 27 15:47:26.507: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 27 15:47:26.507: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 27 15:47:26.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3860 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 27 15:47:26.665: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 27 15:47:26.665: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 27 15:47:26.665: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 27 15:47:26.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=statefulset-3860 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 27 15:47:26.829: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 27 15:47:26.829: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 27 15:47:26.829: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 27 15:47:26.829: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 03/27/23 15:47:36.845
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 27 15:47:36.845: INFO: Deleting all statefulset in ns statefulset-3860
    Mar 27 15:47:36.848: INFO: Scaling statefulset ss to 0
    Mar 27 15:47:36.859: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 15:47:36.862: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 27 15:47:36.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3860" for this suite. 03/27/23 15:47:36.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:47:36.884
Mar 27 15:47:36.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename events 03/27/23 15:47:36.885
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:47:36.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:47:36.901
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 03/27/23 15:47:36.903
STEP: listing all events in all namespaces 03/27/23 15:47:36.911
STEP: patching the test event 03/27/23 15:47:36.916
STEP: fetching the test event 03/27/23 15:47:36.922
STEP: updating the test event 03/27/23 15:47:36.925
STEP: getting the test event 03/27/23 15:47:36.937
STEP: deleting the test event 03/27/23 15:47:36.94
STEP: listing all events in all namespaces 03/27/23 15:47:36.947
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Mar 27 15:47:36.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9996" for this suite. 03/27/23 15:47:36.955
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":347,"skipped":6374,"failed":0}
------------------------------
• [0.077 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:47:36.884
    Mar 27 15:47:36.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename events 03/27/23 15:47:36.885
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:47:36.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:47:36.901
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 03/27/23 15:47:36.903
    STEP: listing all events in all namespaces 03/27/23 15:47:36.911
    STEP: patching the test event 03/27/23 15:47:36.916
    STEP: fetching the test event 03/27/23 15:47:36.922
    STEP: updating the test event 03/27/23 15:47:36.925
    STEP: getting the test event 03/27/23 15:47:36.937
    STEP: deleting the test event 03/27/23 15:47:36.94
    STEP: listing all events in all namespaces 03/27/23 15:47:36.947
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Mar 27 15:47:36.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-9996" for this suite. 03/27/23 15:47:36.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:47:36.964
Mar 27 15:47:36.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename ephemeral-containers-test 03/27/23 15:47:36.965
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:47:36.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:47:36.987
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 03/27/23 15:47:36.991
Mar 27 15:47:37.000: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6739" to be "running and ready"
Mar 27 15:47:37.002: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.772666ms
Mar 27 15:47:37.003: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:47:39.008: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007971758s
Mar 27 15:47:39.008: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Mar 27 15:47:39.008: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 03/27/23 15:47:39.011
Mar 27 15:47:39.022: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6739" to be "container debugger running"
Mar 27 15:47:39.025: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.282414ms
Mar 27 15:47:41.030: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007314653s
Mar 27 15:47:43.029: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007115461s
Mar 27 15:47:43.029: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 03/27/23 15:47:43.029
Mar 27 15:47:43.029: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-6739 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 15:47:43.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
Mar 27 15:47:43.030: INFO: ExecWithOptions: Clientset creation
Mar 27 15:47:43.030: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/ephemeral-containers-test-6739/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Mar 27 15:47:43.118: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 27 15:47:43.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-6739" for this suite. 03/27/23 15:47:43.132
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":348,"skipped":6392,"failed":0}
------------------------------
• [SLOW TEST] [6.175 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:47:36.964
    Mar 27 15:47:36.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename ephemeral-containers-test 03/27/23 15:47:36.965
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:47:36.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:47:36.987
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 03/27/23 15:47:36.991
    Mar 27 15:47:37.000: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6739" to be "running and ready"
    Mar 27 15:47:37.002: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.772666ms
    Mar 27 15:47:37.003: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:47:39.008: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007971758s
    Mar 27 15:47:39.008: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Mar 27 15:47:39.008: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 03/27/23 15:47:39.011
    Mar 27 15:47:39.022: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6739" to be "container debugger running"
    Mar 27 15:47:39.025: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.282414ms
    Mar 27 15:47:41.030: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007314653s
    Mar 27 15:47:43.029: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007115461s
    Mar 27 15:47:43.029: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 03/27/23 15:47:43.029
    Mar 27 15:47:43.029: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-6739 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 15:47:43.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    Mar 27 15:47:43.030: INFO: ExecWithOptions: Clientset creation
    Mar 27 15:47:43.030: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/ephemeral-containers-test-6739/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Mar 27 15:47:43.118: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 27 15:47:43.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-6739" for this suite. 03/27/23 15:47:43.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:47:43.146
Mar 27 15:47:43.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename container-probe 03/27/23 15:47:43.148
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:47:43.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:47:43.163
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Mar 27 15:47:43.175: INFO: Waiting up to 5m0s for pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772" in namespace "container-probe-2308" to be "running and ready"
Mar 27 15:47:43.184: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Pending", Reason="", readiness=false. Elapsed: 8.666709ms
Mar 27 15:47:43.184: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 15:47:45.188: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 2.01292502s
Mar 27 15:47:45.188: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
Mar 27 15:47:47.192: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 4.016704379s
Mar 27 15:47:47.192: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
Mar 27 15:47:49.189: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 6.013500078s
Mar 27 15:47:49.189: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
Mar 27 15:47:51.189: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 8.013772063s
Mar 27 15:47:51.189: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
Mar 27 15:47:53.189: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 10.013980815s
Mar 27 15:47:53.189: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
Mar 27 15:47:55.189: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 12.013485158s
Mar 27 15:47:55.189: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
Mar 27 15:47:57.190: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 14.01462209s
Mar 27 15:47:57.190: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
Mar 27 15:47:59.190: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 16.014913942s
Mar 27 15:47:59.190: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
Mar 27 15:48:01.190: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 18.014780761s
Mar 27 15:48:01.190: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
Mar 27 15:48:03.190: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 20.01420391s
Mar 27 15:48:03.190: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
Mar 27 15:48:05.192: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=true. Elapsed: 22.016886729s
Mar 27 15:48:05.192: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = true)
Mar 27 15:48:05.192: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772" satisfied condition "running and ready"
Mar 27 15:48:05.196: INFO: Container started at 2023-03-27 15:47:44 +0000 UTC, pod became ready at 2023-03-27 15:48:03 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 27 15:48:05.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2308" for this suite. 03/27/23 15:48:05.202
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":349,"skipped":6401,"failed":0}
------------------------------
• [SLOW TEST] [22.063 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:47:43.146
    Mar 27 15:47:43.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename container-probe 03/27/23 15:47:43.148
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:47:43.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:47:43.163
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Mar 27 15:47:43.175: INFO: Waiting up to 5m0s for pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772" in namespace "container-probe-2308" to be "running and ready"
    Mar 27 15:47:43.184: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Pending", Reason="", readiness=false. Elapsed: 8.666709ms
    Mar 27 15:47:43.184: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 15:47:45.188: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 2.01292502s
    Mar 27 15:47:45.188: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
    Mar 27 15:47:47.192: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 4.016704379s
    Mar 27 15:47:47.192: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
    Mar 27 15:47:49.189: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 6.013500078s
    Mar 27 15:47:49.189: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
    Mar 27 15:47:51.189: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 8.013772063s
    Mar 27 15:47:51.189: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
    Mar 27 15:47:53.189: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 10.013980815s
    Mar 27 15:47:53.189: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
    Mar 27 15:47:55.189: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 12.013485158s
    Mar 27 15:47:55.189: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
    Mar 27 15:47:57.190: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 14.01462209s
    Mar 27 15:47:57.190: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
    Mar 27 15:47:59.190: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 16.014913942s
    Mar 27 15:47:59.190: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
    Mar 27 15:48:01.190: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 18.014780761s
    Mar 27 15:48:01.190: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
    Mar 27 15:48:03.190: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=false. Elapsed: 20.01420391s
    Mar 27 15:48:03.190: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = false)
    Mar 27 15:48:05.192: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772": Phase="Running", Reason="", readiness=true. Elapsed: 22.016886729s
    Mar 27 15:48:05.192: INFO: The phase of Pod test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772 is Running (Ready = true)
    Mar 27 15:48:05.192: INFO: Pod "test-webserver-c7403e27-9a9c-45e5-8eba-eaf97acf4772" satisfied condition "running and ready"
    Mar 27 15:48:05.196: INFO: Container started at 2023-03-27 15:47:44 +0000 UTC, pod became ready at 2023-03-27 15:48:03 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 27 15:48:05.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2308" for this suite. 03/27/23 15:48:05.202
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:48:05.213
Mar 27 15:48:05.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename svc-latency 03/27/23 15:48:05.215
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:48:05.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:48:05.236
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Mar 27 15:48:05.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6898 03/27/23 15:48:05.24
I0327 15:48:05.246257      24 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6898, replica count: 1
I0327 15:48:06.297672      24 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0327 15:48:07.298775      24 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 15:48:07.411: INFO: Created: latency-svc-6pbl7
Mar 27 15:48:07.421: INFO: Got endpoints: latency-svc-6pbl7 [21.914315ms]
Mar 27 15:48:07.434: INFO: Created: latency-svc-7wprc
Mar 27 15:48:07.442: INFO: Created: latency-svc-w9w7t
Mar 27 15:48:07.442: INFO: Got endpoints: latency-svc-7wprc [20.603342ms]
Mar 27 15:48:07.449: INFO: Got endpoints: latency-svc-w9w7t [27.116545ms]
Mar 27 15:48:07.453: INFO: Created: latency-svc-g9zc9
Mar 27 15:48:07.460: INFO: Created: latency-svc-r5vz2
Mar 27 15:48:07.464: INFO: Got endpoints: latency-svc-g9zc9 [41.87065ms]
Mar 27 15:48:07.465: INFO: Created: latency-svc-xp6tm
Mar 27 15:48:07.475: INFO: Got endpoints: latency-svc-r5vz2 [53.490871ms]
Mar 27 15:48:07.480: INFO: Created: latency-svc-t8qc9
Mar 27 15:48:07.480: INFO: Got endpoints: latency-svc-xp6tm [58.635268ms]
Mar 27 15:48:07.482: INFO: Created: latency-svc-nnktn
Mar 27 15:48:07.485: INFO: Got endpoints: latency-svc-t8qc9 [63.621258ms]
Mar 27 15:48:07.489: INFO: Created: latency-svc-6jbt4
Mar 27 15:48:07.490: INFO: Got endpoints: latency-svc-nnktn [68.25924ms]
Mar 27 15:48:07.498: INFO: Got endpoints: latency-svc-6jbt4 [75.835121ms]
Mar 27 15:48:07.502: INFO: Created: latency-svc-ks9tr
Mar 27 15:48:07.507: INFO: Created: latency-svc-csjcf
Mar 27 15:48:07.508: INFO: Got endpoints: latency-svc-ks9tr [86.287625ms]
Mar 27 15:48:07.516: INFO: Created: latency-svc-kn6t2
Mar 27 15:48:07.516: INFO: Got endpoints: latency-svc-csjcf [94.607106ms]
Mar 27 15:48:07.524: INFO: Got endpoints: latency-svc-kn6t2 [102.441806ms]
Mar 27 15:48:07.526: INFO: Created: latency-svc-x6gt5
Mar 27 15:48:07.534: INFO: Got endpoints: latency-svc-x6gt5 [111.796915ms]
Mar 27 15:48:07.538: INFO: Created: latency-svc-ncw8b
Mar 27 15:48:07.540: INFO: Created: latency-svc-lc5gx
Mar 27 15:48:07.542: INFO: Got endpoints: latency-svc-ncw8b [120.533081ms]
Mar 27 15:48:07.549: INFO: Got endpoints: latency-svc-lc5gx [126.963378ms]
Mar 27 15:48:07.554: INFO: Created: latency-svc-nbhqd
Mar 27 15:48:07.560: INFO: Created: latency-svc-mbtvz
Mar 27 15:48:07.567: INFO: Got endpoints: latency-svc-nbhqd [145.694408ms]
Mar 27 15:48:07.570: INFO: Got endpoints: latency-svc-mbtvz [127.664198ms]
Mar 27 15:48:07.570: INFO: Created: latency-svc-lq4rz
Mar 27 15:48:07.578: INFO: Got endpoints: latency-svc-lq4rz [129.726324ms]
Mar 27 15:48:07.584: INFO: Created: latency-svc-lpwf4
Mar 27 15:48:07.589: INFO: Got endpoints: latency-svc-lpwf4 [125.727592ms]
Mar 27 15:48:07.590: INFO: Created: latency-svc-vrdfw
Mar 27 15:48:07.599: INFO: Created: latency-svc-7lgxd
Mar 27 15:48:07.600: INFO: Got endpoints: latency-svc-vrdfw [125.533461ms]
Mar 27 15:48:07.609: INFO: Got endpoints: latency-svc-7lgxd [129.03705ms]
Mar 27 15:48:07.610: INFO: Created: latency-svc-jdjxz
Mar 27 15:48:07.616: INFO: Got endpoints: latency-svc-jdjxz [130.599807ms]
Mar 27 15:48:07.617: INFO: Created: latency-svc-r8mp5
Mar 27 15:48:07.628: INFO: Got endpoints: latency-svc-r8mp5 [138.247675ms]
Mar 27 15:48:07.628: INFO: Created: latency-svc-wck9x
Mar 27 15:48:07.631: INFO: Got endpoints: latency-svc-wck9x [133.553096ms]
Mar 27 15:48:07.640: INFO: Created: latency-svc-6ktfn
Mar 27 15:48:07.644: INFO: Created: latency-svc-khkp9
Mar 27 15:48:07.651: INFO: Got endpoints: latency-svc-6ktfn [142.426349ms]
Mar 27 15:48:07.651: INFO: Got endpoints: latency-svc-khkp9 [134.912819ms]
Mar 27 15:48:07.655: INFO: Created: latency-svc-sfmlb
Mar 27 15:48:07.664: INFO: Got endpoints: latency-svc-sfmlb [139.357993ms]
Mar 27 15:48:07.667: INFO: Created: latency-svc-hws4f
Mar 27 15:48:07.673: INFO: Got endpoints: latency-svc-hws4f [139.017959ms]
Mar 27 15:48:07.675: INFO: Created: latency-svc-x46dm
Mar 27 15:48:07.679: INFO: Created: latency-svc-dv7wv
Mar 27 15:48:07.686: INFO: Got endpoints: latency-svc-x46dm [143.686399ms]
Mar 27 15:48:07.691: INFO: Got endpoints: latency-svc-dv7wv [142.030749ms]
Mar 27 15:48:07.691: INFO: Created: latency-svc-8q8gx
Mar 27 15:48:07.696: INFO: Got endpoints: latency-svc-8q8gx [128.785637ms]
Mar 27 15:48:07.699: INFO: Created: latency-svc-sl9f9
Mar 27 15:48:07.705: INFO: Created: latency-svc-wnz5t
Mar 27 15:48:07.706: INFO: Got endpoints: latency-svc-sl9f9 [135.475741ms]
Mar 27 15:48:07.713: INFO: Got endpoints: latency-svc-wnz5t [134.645711ms]
Mar 27 15:48:07.716: INFO: Created: latency-svc-2kw8v
Mar 27 15:48:07.723: INFO: Created: latency-svc-cwbrw
Mar 27 15:48:07.723: INFO: Got endpoints: latency-svc-2kw8v [134.140261ms]
Mar 27 15:48:07.731: INFO: Created: latency-svc-gkt7z
Mar 27 15:48:07.732: INFO: Got endpoints: latency-svc-cwbrw [131.056497ms]
Mar 27 15:48:07.737: INFO: Got endpoints: latency-svc-gkt7z [127.420349ms]
Mar 27 15:48:07.741: INFO: Created: latency-svc-pb74s
Mar 27 15:48:07.749: INFO: Created: latency-svc-r5mpg
Mar 27 15:48:07.754: INFO: Created: latency-svc-222qx
Mar 27 15:48:07.760: INFO: Created: latency-svc-8n8r2
Mar 27 15:48:07.767: INFO: Created: latency-svc-crzd4
Mar 27 15:48:07.770: INFO: Got endpoints: latency-svc-pb74s [153.599438ms]
Mar 27 15:48:07.774: INFO: Created: latency-svc-nn7b6
Mar 27 15:48:07.779: INFO: Created: latency-svc-lf9lj
Mar 27 15:48:07.789: INFO: Created: latency-svc-2wvbl
Mar 27 15:48:07.796: INFO: Created: latency-svc-657cd
Mar 27 15:48:07.804: INFO: Created: latency-svc-85gg7
Mar 27 15:48:07.813: INFO: Created: latency-svc-bh9cq
Mar 27 15:48:07.822: INFO: Got endpoints: latency-svc-r5mpg [194.327769ms]
Mar 27 15:48:07.825: INFO: Created: latency-svc-t579k
Mar 27 15:48:07.834: INFO: Created: latency-svc-tl58w
Mar 27 15:48:07.849: INFO: Created: latency-svc-v9l68
Mar 27 15:48:07.859: INFO: Created: latency-svc-dn4fc
Mar 27 15:48:07.869: INFO: Created: latency-svc-m6tqg
Mar 27 15:48:07.871: INFO: Got endpoints: latency-svc-222qx [239.331181ms]
Mar 27 15:48:07.877: INFO: Created: latency-svc-jwv6d
Mar 27 15:48:07.884: INFO: Created: latency-svc-wfcr5
Mar 27 15:48:07.922: INFO: Got endpoints: latency-svc-8n8r2 [271.346507ms]
Mar 27 15:48:07.934: INFO: Created: latency-svc-hwrmd
Mar 27 15:48:07.984: INFO: Got endpoints: latency-svc-crzd4 [332.830456ms]
Mar 27 15:48:08.013: INFO: Created: latency-svc-8gzzc
Mar 27 15:48:08.018: INFO: Got endpoints: latency-svc-nn7b6 [354.805168ms]
Mar 27 15:48:08.033: INFO: Created: latency-svc-wpkm9
Mar 27 15:48:08.075: INFO: Got endpoints: latency-svc-lf9lj [402.530417ms]
Mar 27 15:48:08.090: INFO: Created: latency-svc-696f6
Mar 27 15:48:08.121: INFO: Got endpoints: latency-svc-2wvbl [434.284645ms]
Mar 27 15:48:08.139: INFO: Created: latency-svc-h55zf
Mar 27 15:48:08.172: INFO: Got endpoints: latency-svc-657cd [481.112518ms]
Mar 27 15:48:08.183: INFO: Created: latency-svc-8ld8f
Mar 27 15:48:08.221: INFO: Got endpoints: latency-svc-85gg7 [525.153546ms]
Mar 27 15:48:08.233: INFO: Created: latency-svc-frdws
Mar 27 15:48:08.272: INFO: Got endpoints: latency-svc-bh9cq [566.063345ms]
Mar 27 15:48:08.281: INFO: Created: latency-svc-5hvjj
Mar 27 15:48:08.319: INFO: Got endpoints: latency-svc-t579k [605.758006ms]
Mar 27 15:48:08.331: INFO: Created: latency-svc-7c2ql
Mar 27 15:48:08.370: INFO: Got endpoints: latency-svc-tl58w [646.463001ms]
Mar 27 15:48:08.380: INFO: Created: latency-svc-m7scw
Mar 27 15:48:08.424: INFO: Got endpoints: latency-svc-v9l68 [692.025177ms]
Mar 27 15:48:08.436: INFO: Created: latency-svc-nmpdg
Mar 27 15:48:08.472: INFO: Got endpoints: latency-svc-dn4fc [735.694076ms]
Mar 27 15:48:08.483: INFO: Created: latency-svc-shxrg
Mar 27 15:48:08.519: INFO: Got endpoints: latency-svc-m6tqg [749.656587ms]
Mar 27 15:48:08.531: INFO: Created: latency-svc-t69qd
Mar 27 15:48:08.569: INFO: Got endpoints: latency-svc-jwv6d [746.844433ms]
Mar 27 15:48:08.585: INFO: Created: latency-svc-bvn4k
Mar 27 15:48:08.620: INFO: Got endpoints: latency-svc-wfcr5 [748.938668ms]
Mar 27 15:48:08.632: INFO: Created: latency-svc-pbg4m
Mar 27 15:48:08.674: INFO: Got endpoints: latency-svc-hwrmd [751.998247ms]
Mar 27 15:48:08.684: INFO: Created: latency-svc-qhw6d
Mar 27 15:48:08.722: INFO: Got endpoints: latency-svc-8gzzc [737.326031ms]
Mar 27 15:48:08.743: INFO: Created: latency-svc-plc2w
Mar 27 15:48:08.772: INFO: Got endpoints: latency-svc-wpkm9 [753.597854ms]
Mar 27 15:48:08.783: INFO: Created: latency-svc-kfjqd
Mar 27 15:48:08.820: INFO: Got endpoints: latency-svc-696f6 [744.040556ms]
Mar 27 15:48:08.832: INFO: Created: latency-svc-kl84k
Mar 27 15:48:08.869: INFO: Got endpoints: latency-svc-h55zf [748.891686ms]
Mar 27 15:48:08.882: INFO: Created: latency-svc-n5vpg
Mar 27 15:48:08.922: INFO: Got endpoints: latency-svc-8ld8f [749.830638ms]
Mar 27 15:48:08.936: INFO: Created: latency-svc-xh7q9
Mar 27 15:48:08.973: INFO: Got endpoints: latency-svc-frdws [752.063685ms]
Mar 27 15:48:08.986: INFO: Created: latency-svc-lvnzk
Mar 27 15:48:09.019: INFO: Got endpoints: latency-svc-5hvjj [747.497284ms]
Mar 27 15:48:09.030: INFO: Created: latency-svc-kr6pb
Mar 27 15:48:09.071: INFO: Got endpoints: latency-svc-7c2ql [751.736563ms]
Mar 27 15:48:09.081: INFO: Created: latency-svc-sgq68
Mar 27 15:48:09.120: INFO: Got endpoints: latency-svc-m7scw [750.384772ms]
Mar 27 15:48:09.133: INFO: Created: latency-svc-vw8db
Mar 27 15:48:09.173: INFO: Got endpoints: latency-svc-nmpdg [749.269405ms]
Mar 27 15:48:09.189: INFO: Created: latency-svc-wqnx9
Mar 27 15:48:09.219: INFO: Got endpoints: latency-svc-shxrg [746.03724ms]
Mar 27 15:48:09.229: INFO: Created: latency-svc-nhz5b
Mar 27 15:48:09.268: INFO: Got endpoints: latency-svc-t69qd [748.673591ms]
Mar 27 15:48:09.281: INFO: Created: latency-svc-7b88z
Mar 27 15:48:09.321: INFO: Got endpoints: latency-svc-bvn4k [746.249603ms]
Mar 27 15:48:09.338: INFO: Created: latency-svc-l4dfn
Mar 27 15:48:09.369: INFO: Got endpoints: latency-svc-pbg4m [747.188952ms]
Mar 27 15:48:09.384: INFO: Created: latency-svc-4hgc5
Mar 27 15:48:09.423: INFO: Got endpoints: latency-svc-qhw6d [749.549972ms]
Mar 27 15:48:09.435: INFO: Created: latency-svc-9g9hf
Mar 27 15:48:09.469: INFO: Got endpoints: latency-svc-plc2w [746.993343ms]
Mar 27 15:48:09.489: INFO: Created: latency-svc-85wsw
Mar 27 15:48:09.521: INFO: Got endpoints: latency-svc-kfjqd [748.732735ms]
Mar 27 15:48:09.534: INFO: Created: latency-svc-8hfrf
Mar 27 15:48:09.570: INFO: Got endpoints: latency-svc-kl84k [750.200412ms]
Mar 27 15:48:09.581: INFO: Created: latency-svc-2fh47
Mar 27 15:48:09.619: INFO: Got endpoints: latency-svc-n5vpg [749.736277ms]
Mar 27 15:48:09.630: INFO: Created: latency-svc-rbbjx
Mar 27 15:48:09.669: INFO: Got endpoints: latency-svc-xh7q9 [746.89948ms]
Mar 27 15:48:09.681: INFO: Created: latency-svc-2fs44
Mar 27 15:48:09.720: INFO: Got endpoints: latency-svc-lvnzk [746.464793ms]
Mar 27 15:48:09.735: INFO: Created: latency-svc-4wt2k
Mar 27 15:48:09.776: INFO: Got endpoints: latency-svc-kr6pb [756.91397ms]
Mar 27 15:48:09.790: INFO: Created: latency-svc-t7g8l
Mar 27 15:48:09.819: INFO: Got endpoints: latency-svc-sgq68 [748.186676ms]
Mar 27 15:48:09.829: INFO: Created: latency-svc-bfwxk
Mar 27 15:48:09.870: INFO: Got endpoints: latency-svc-vw8db [749.912249ms]
Mar 27 15:48:09.887: INFO: Created: latency-svc-76bf4
Mar 27 15:48:09.920: INFO: Got endpoints: latency-svc-wqnx9 [746.838634ms]
Mar 27 15:48:09.931: INFO: Created: latency-svc-4mmwr
Mar 27 15:48:09.974: INFO: Got endpoints: latency-svc-nhz5b [755.027558ms]
Mar 27 15:48:09.988: INFO: Created: latency-svc-qlgx5
Mar 27 15:48:10.021: INFO: Got endpoints: latency-svc-7b88z [752.80952ms]
Mar 27 15:48:10.032: INFO: Created: latency-svc-nhx9z
Mar 27 15:48:10.070: INFO: Got endpoints: latency-svc-l4dfn [743.286889ms]
Mar 27 15:48:10.084: INFO: Created: latency-svc-kf4sx
Mar 27 15:48:10.120: INFO: Got endpoints: latency-svc-4hgc5 [750.693486ms]
Mar 27 15:48:10.132: INFO: Created: latency-svc-j5ppw
Mar 27 15:48:10.169: INFO: Got endpoints: latency-svc-9g9hf [745.381868ms]
Mar 27 15:48:10.182: INFO: Created: latency-svc-hkttp
Mar 27 15:48:10.217: INFO: Got endpoints: latency-svc-85wsw [748.110631ms]
Mar 27 15:48:10.230: INFO: Created: latency-svc-lrc9d
Mar 27 15:48:10.271: INFO: Got endpoints: latency-svc-8hfrf [749.901569ms]
Mar 27 15:48:10.283: INFO: Created: latency-svc-bplr9
Mar 27 15:48:10.319: INFO: Got endpoints: latency-svc-2fh47 [748.959497ms]
Mar 27 15:48:10.330: INFO: Created: latency-svc-fm7nz
Mar 27 15:48:10.369: INFO: Got endpoints: latency-svc-rbbjx [749.731093ms]
Mar 27 15:48:10.379: INFO: Created: latency-svc-4947l
Mar 27 15:48:10.419: INFO: Got endpoints: latency-svc-2fs44 [749.411659ms]
Mar 27 15:48:10.429: INFO: Created: latency-svc-kc72f
Mar 27 15:48:10.471: INFO: Got endpoints: latency-svc-4wt2k [751.167685ms]
Mar 27 15:48:10.482: INFO: Created: latency-svc-tnvcj
Mar 27 15:48:10.518: INFO: Got endpoints: latency-svc-t7g8l [741.501383ms]
Mar 27 15:48:10.532: INFO: Created: latency-svc-thxbd
Mar 27 15:48:10.571: INFO: Got endpoints: latency-svc-bfwxk [751.478974ms]
Mar 27 15:48:10.585: INFO: Created: latency-svc-wtrxd
Mar 27 15:48:10.619: INFO: Got endpoints: latency-svc-76bf4 [748.958365ms]
Mar 27 15:48:10.631: INFO: Created: latency-svc-5qrhp
Mar 27 15:48:10.670: INFO: Got endpoints: latency-svc-4mmwr [749.932087ms]
Mar 27 15:48:10.681: INFO: Created: latency-svc-vsdwh
Mar 27 15:48:10.719: INFO: Got endpoints: latency-svc-qlgx5 [745.52456ms]
Mar 27 15:48:10.730: INFO: Created: latency-svc-dbkwh
Mar 27 15:48:10.769: INFO: Got endpoints: latency-svc-nhx9z [747.655506ms]
Mar 27 15:48:10.782: INFO: Created: latency-svc-gdb6k
Mar 27 15:48:10.823: INFO: Got endpoints: latency-svc-kf4sx [753.066295ms]
Mar 27 15:48:10.847: INFO: Created: latency-svc-vwx5n
Mar 27 15:48:10.869: INFO: Got endpoints: latency-svc-j5ppw [749.372528ms]
Mar 27 15:48:10.881: INFO: Created: latency-svc-xzbqh
Mar 27 15:48:10.917: INFO: Got endpoints: latency-svc-hkttp [748.382565ms]
Mar 27 15:48:10.928: INFO: Created: latency-svc-xqf5b
Mar 27 15:48:10.968: INFO: Got endpoints: latency-svc-lrc9d [750.584995ms]
Mar 27 15:48:10.980: INFO: Created: latency-svc-lf2jv
Mar 27 15:48:11.018: INFO: Got endpoints: latency-svc-bplr9 [746.931927ms]
Mar 27 15:48:11.029: INFO: Created: latency-svc-7k6zj
Mar 27 15:48:11.070: INFO: Got endpoints: latency-svc-fm7nz [751.433584ms]
Mar 27 15:48:11.081: INFO: Created: latency-svc-shptc
Mar 27 15:48:11.121: INFO: Got endpoints: latency-svc-4947l [751.907033ms]
Mar 27 15:48:11.132: INFO: Created: latency-svc-chz8g
Mar 27 15:48:11.169: INFO: Got endpoints: latency-svc-kc72f [750.784788ms]
Mar 27 15:48:11.182: INFO: Created: latency-svc-pzskc
Mar 27 15:48:11.221: INFO: Got endpoints: latency-svc-tnvcj [749.875032ms]
Mar 27 15:48:11.234: INFO: Created: latency-svc-qzbk7
Mar 27 15:48:11.269: INFO: Got endpoints: latency-svc-thxbd [751.15152ms]
Mar 27 15:48:11.280: INFO: Created: latency-svc-whm94
Mar 27 15:48:11.318: INFO: Got endpoints: latency-svc-wtrxd [747.370569ms]
Mar 27 15:48:11.329: INFO: Created: latency-svc-ffmbt
Mar 27 15:48:11.371: INFO: Got endpoints: latency-svc-5qrhp [751.335863ms]
Mar 27 15:48:11.381: INFO: Created: latency-svc-wj46n
Mar 27 15:48:11.421: INFO: Got endpoints: latency-svc-vsdwh [750.599243ms]
Mar 27 15:48:11.434: INFO: Created: latency-svc-77q5d
Mar 27 15:48:11.473: INFO: Got endpoints: latency-svc-dbkwh [753.96643ms]
Mar 27 15:48:11.483: INFO: Created: latency-svc-2p8j6
Mar 27 15:48:11.523: INFO: Got endpoints: latency-svc-gdb6k [754.669792ms]
Mar 27 15:48:11.534: INFO: Created: latency-svc-clw7b
Mar 27 15:48:11.571: INFO: Got endpoints: latency-svc-vwx5n [748.382165ms]
Mar 27 15:48:11.581: INFO: Created: latency-svc-klqg5
Mar 27 15:48:11.622: INFO: Got endpoints: latency-svc-xzbqh [752.587944ms]
Mar 27 15:48:11.633: INFO: Created: latency-svc-5npql
Mar 27 15:48:11.669: INFO: Got endpoints: latency-svc-xqf5b [751.398195ms]
Mar 27 15:48:11.680: INFO: Created: latency-svc-tvksx
Mar 27 15:48:11.719: INFO: Got endpoints: latency-svc-lf2jv [751.796287ms]
Mar 27 15:48:11.731: INFO: Created: latency-svc-xjkd9
Mar 27 15:48:11.771: INFO: Got endpoints: latency-svc-7k6zj [753.250706ms]
Mar 27 15:48:11.783: INFO: Created: latency-svc-qlt2p
Mar 27 15:48:11.819: INFO: Got endpoints: latency-svc-shptc [748.111749ms]
Mar 27 15:48:11.830: INFO: Created: latency-svc-5k9xf
Mar 27 15:48:11.872: INFO: Got endpoints: latency-svc-chz8g [750.688706ms]
Mar 27 15:48:11.883: INFO: Created: latency-svc-jhzhx
Mar 27 15:48:11.918: INFO: Got endpoints: latency-svc-pzskc [748.614662ms]
Mar 27 15:48:11.930: INFO: Created: latency-svc-46qsf
Mar 27 15:48:11.970: INFO: Got endpoints: latency-svc-qzbk7 [748.879088ms]
Mar 27 15:48:11.980: INFO: Created: latency-svc-2k9th
Mar 27 15:48:12.021: INFO: Got endpoints: latency-svc-whm94 [751.97113ms]
Mar 27 15:48:12.032: INFO: Created: latency-svc-mqcfk
Mar 27 15:48:12.072: INFO: Got endpoints: latency-svc-ffmbt [753.73047ms]
Mar 27 15:48:12.086: INFO: Created: latency-svc-7ccqg
Mar 27 15:48:12.122: INFO: Got endpoints: latency-svc-wj46n [750.726147ms]
Mar 27 15:48:12.131: INFO: Created: latency-svc-89kbw
Mar 27 15:48:12.169: INFO: Got endpoints: latency-svc-77q5d [748.244393ms]
Mar 27 15:48:12.179: INFO: Created: latency-svc-v46t4
Mar 27 15:48:12.219: INFO: Got endpoints: latency-svc-2p8j6 [745.664775ms]
Mar 27 15:48:12.230: INFO: Created: latency-svc-6zjj2
Mar 27 15:48:12.271: INFO: Got endpoints: latency-svc-clw7b [746.928001ms]
Mar 27 15:48:12.282: INFO: Created: latency-svc-7f7k8
Mar 27 15:48:12.320: INFO: Got endpoints: latency-svc-klqg5 [749.331474ms]
Mar 27 15:48:12.333: INFO: Created: latency-svc-h7d92
Mar 27 15:48:12.370: INFO: Got endpoints: latency-svc-5npql [747.869568ms]
Mar 27 15:48:12.383: INFO: Created: latency-svc-f9c55
Mar 27 15:48:12.419: INFO: Got endpoints: latency-svc-tvksx [749.935704ms]
Mar 27 15:48:12.429: INFO: Created: latency-svc-zw529
Mar 27 15:48:12.472: INFO: Got endpoints: latency-svc-xjkd9 [752.151796ms]
Mar 27 15:48:12.482: INFO: Created: latency-svc-c6gvl
Mar 27 15:48:12.520: INFO: Got endpoints: latency-svc-qlt2p [748.560913ms]
Mar 27 15:48:12.531: INFO: Created: latency-svc-xvvw6
Mar 27 15:48:12.575: INFO: Got endpoints: latency-svc-5k9xf [755.553065ms]
Mar 27 15:48:12.586: INFO: Created: latency-svc-pl9bg
Mar 27 15:48:12.619: INFO: Got endpoints: latency-svc-jhzhx [746.627792ms]
Mar 27 15:48:12.632: INFO: Created: latency-svc-zqh9r
Mar 27 15:48:12.670: INFO: Got endpoints: latency-svc-46qsf [751.936433ms]
Mar 27 15:48:12.681: INFO: Created: latency-svc-99f55
Mar 27 15:48:12.721: INFO: Got endpoints: latency-svc-2k9th [750.310658ms]
Mar 27 15:48:12.731: INFO: Created: latency-svc-p7vlx
Mar 27 15:48:12.776: INFO: Got endpoints: latency-svc-mqcfk [754.867305ms]
Mar 27 15:48:12.788: INFO: Created: latency-svc-b7ptg
Mar 27 15:48:12.826: INFO: Got endpoints: latency-svc-7ccqg [753.842543ms]
Mar 27 15:48:12.837: INFO: Created: latency-svc-2x57q
Mar 27 15:48:12.869: INFO: Got endpoints: latency-svc-89kbw [746.991339ms]
Mar 27 15:48:12.879: INFO: Created: latency-svc-pc9mr
Mar 27 15:48:12.920: INFO: Got endpoints: latency-svc-v46t4 [750.730197ms]
Mar 27 15:48:12.932: INFO: Created: latency-svc-42gt8
Mar 27 15:48:12.972: INFO: Got endpoints: latency-svc-6zjj2 [752.201845ms]
Mar 27 15:48:12.988: INFO: Created: latency-svc-hckf8
Mar 27 15:48:13.019: INFO: Got endpoints: latency-svc-7f7k8 [748.222093ms]
Mar 27 15:48:13.030: INFO: Created: latency-svc-v66nz
Mar 27 15:48:13.071: INFO: Got endpoints: latency-svc-h7d92 [750.686609ms]
Mar 27 15:48:13.081: INFO: Created: latency-svc-ml7kx
Mar 27 15:48:13.119: INFO: Got endpoints: latency-svc-f9c55 [748.360061ms]
Mar 27 15:48:13.128: INFO: Created: latency-svc-lx699
Mar 27 15:48:13.169: INFO: Got endpoints: latency-svc-zw529 [750.395616ms]
Mar 27 15:48:13.184: INFO: Created: latency-svc-zrptb
Mar 27 15:48:13.221: INFO: Got endpoints: latency-svc-c6gvl [749.557642ms]
Mar 27 15:48:13.233: INFO: Created: latency-svc-kzml8
Mar 27 15:48:13.273: INFO: Got endpoints: latency-svc-xvvw6 [752.882895ms]
Mar 27 15:48:13.284: INFO: Created: latency-svc-cnbj4
Mar 27 15:48:13.320: INFO: Got endpoints: latency-svc-pl9bg [745.031895ms]
Mar 27 15:48:13.332: INFO: Created: latency-svc-g6mfr
Mar 27 15:48:13.370: INFO: Got endpoints: latency-svc-zqh9r [750.919003ms]
Mar 27 15:48:13.381: INFO: Created: latency-svc-5nzr5
Mar 27 15:48:13.422: INFO: Got endpoints: latency-svc-99f55 [751.20934ms]
Mar 27 15:48:13.434: INFO: Created: latency-svc-plpcd
Mar 27 15:48:13.470: INFO: Got endpoints: latency-svc-p7vlx [749.040868ms]
Mar 27 15:48:13.480: INFO: Created: latency-svc-k2xkt
Mar 27 15:48:13.521: INFO: Got endpoints: latency-svc-b7ptg [745.269522ms]
Mar 27 15:48:13.531: INFO: Created: latency-svc-vsxmz
Mar 27 15:48:13.569: INFO: Got endpoints: latency-svc-2x57q [743.585957ms]
Mar 27 15:48:13.581: INFO: Created: latency-svc-pndlg
Mar 27 15:48:13.621: INFO: Got endpoints: latency-svc-pc9mr [752.272063ms]
Mar 27 15:48:13.634: INFO: Created: latency-svc-4bkbl
Mar 27 15:48:13.671: INFO: Got endpoints: latency-svc-42gt8 [750.845683ms]
Mar 27 15:48:13.685: INFO: Created: latency-svc-m58nx
Mar 27 15:48:13.720: INFO: Got endpoints: latency-svc-hckf8 [748.126919ms]
Mar 27 15:48:13.733: INFO: Created: latency-svc-gwm8r
Mar 27 15:48:13.769: INFO: Got endpoints: latency-svc-v66nz [750.274538ms]
Mar 27 15:48:13.783: INFO: Created: latency-svc-tbgs2
Mar 27 15:48:13.819: INFO: Got endpoints: latency-svc-ml7kx [747.648619ms]
Mar 27 15:48:13.831: INFO: Created: latency-svc-bznwd
Mar 27 15:48:13.872: INFO: Got endpoints: latency-svc-lx699 [752.801289ms]
Mar 27 15:48:13.883: INFO: Created: latency-svc-mzp7m
Mar 27 15:48:13.924: INFO: Got endpoints: latency-svc-zrptb [755.012925ms]
Mar 27 15:48:13.936: INFO: Created: latency-svc-ndnt8
Mar 27 15:48:13.972: INFO: Got endpoints: latency-svc-kzml8 [750.47516ms]
Mar 27 15:48:13.984: INFO: Created: latency-svc-m2gmc
Mar 27 15:48:14.020: INFO: Got endpoints: latency-svc-cnbj4 [747.466869ms]
Mar 27 15:48:14.030: INFO: Created: latency-svc-v7b79
Mar 27 15:48:14.071: INFO: Got endpoints: latency-svc-g6mfr [750.657917ms]
Mar 27 15:48:14.081: INFO: Created: latency-svc-xggm5
Mar 27 15:48:14.127: INFO: Got endpoints: latency-svc-5nzr5 [756.995637ms]
Mar 27 15:48:14.140: INFO: Created: latency-svc-dqbqc
Mar 27 15:48:14.174: INFO: Got endpoints: latency-svc-plpcd [752.176855ms]
Mar 27 15:48:14.187: INFO: Created: latency-svc-g64mj
Mar 27 15:48:14.220: INFO: Got endpoints: latency-svc-k2xkt [750.181146ms]
Mar 27 15:48:14.230: INFO: Created: latency-svc-g5nb9
Mar 27 15:48:14.268: INFO: Got endpoints: latency-svc-vsxmz [746.726812ms]
Mar 27 15:48:14.285: INFO: Created: latency-svc-c2g9h
Mar 27 15:48:14.318: INFO: Got endpoints: latency-svc-pndlg [748.327759ms]
Mar 27 15:48:14.328: INFO: Created: latency-svc-jvgkq
Mar 27 15:48:14.369: INFO: Got endpoints: latency-svc-4bkbl [747.397985ms]
Mar 27 15:48:14.382: INFO: Created: latency-svc-nspkt
Mar 27 15:48:14.422: INFO: Got endpoints: latency-svc-m58nx [751.223129ms]
Mar 27 15:48:14.436: INFO: Created: latency-svc-nbztt
Mar 27 15:48:14.469: INFO: Got endpoints: latency-svc-gwm8r [749.027484ms]
Mar 27 15:48:14.482: INFO: Created: latency-svc-w44fh
Mar 27 15:48:14.523: INFO: Got endpoints: latency-svc-tbgs2 [753.281282ms]
Mar 27 15:48:14.534: INFO: Created: latency-svc-jf7lk
Mar 27 15:48:14.573: INFO: Got endpoints: latency-svc-bznwd [753.32226ms]
Mar 27 15:48:14.584: INFO: Created: latency-svc-gwkgq
Mar 27 15:48:14.625: INFO: Got endpoints: latency-svc-mzp7m [752.891558ms]
Mar 27 15:48:14.641: INFO: Created: latency-svc-p5jkd
Mar 27 15:48:14.670: INFO: Got endpoints: latency-svc-ndnt8 [745.799616ms]
Mar 27 15:48:14.683: INFO: Created: latency-svc-scslw
Mar 27 15:48:14.720: INFO: Got endpoints: latency-svc-m2gmc [748.348583ms]
Mar 27 15:48:14.730: INFO: Created: latency-svc-s8xcb
Mar 27 15:48:14.770: INFO: Got endpoints: latency-svc-v7b79 [749.386487ms]
Mar 27 15:48:14.781: INFO: Created: latency-svc-wfbt6
Mar 27 15:48:14.822: INFO: Got endpoints: latency-svc-xggm5 [751.794653ms]
Mar 27 15:48:14.834: INFO: Created: latency-svc-9bh8n
Mar 27 15:48:14.869: INFO: Got endpoints: latency-svc-dqbqc [742.297161ms]
Mar 27 15:48:14.881: INFO: Created: latency-svc-528d6
Mar 27 15:48:14.920: INFO: Got endpoints: latency-svc-g64mj [745.783298ms]
Mar 27 15:48:14.931: INFO: Created: latency-svc-v9j9p
Mar 27 15:48:14.970: INFO: Got endpoints: latency-svc-g5nb9 [749.460931ms]
Mar 27 15:48:14.979: INFO: Created: latency-svc-jx2f8
Mar 27 15:48:15.020: INFO: Got endpoints: latency-svc-c2g9h [751.894723ms]
Mar 27 15:48:15.031: INFO: Created: latency-svc-t6qhp
Mar 27 15:48:15.072: INFO: Got endpoints: latency-svc-jvgkq [754.335827ms]
Mar 27 15:48:15.086: INFO: Created: latency-svc-xdcfm
Mar 27 15:48:15.122: INFO: Got endpoints: latency-svc-nspkt [753.026951ms]
Mar 27 15:48:15.137: INFO: Created: latency-svc-cq8wd
Mar 27 15:48:15.171: INFO: Got endpoints: latency-svc-nbztt [748.44615ms]
Mar 27 15:48:15.180: INFO: Created: latency-svc-7vlbh
Mar 27 15:48:15.218: INFO: Got endpoints: latency-svc-w44fh [749.544017ms]
Mar 27 15:48:15.230: INFO: Created: latency-svc-v9h9k
Mar 27 15:48:15.269: INFO: Got endpoints: latency-svc-jf7lk [746.3803ms]
Mar 27 15:48:15.322: INFO: Got endpoints: latency-svc-gwkgq [749.381136ms]
Mar 27 15:48:15.372: INFO: Got endpoints: latency-svc-p5jkd [747.69948ms]
Mar 27 15:48:15.419: INFO: Got endpoints: latency-svc-scslw [749.124174ms]
Mar 27 15:48:15.471: INFO: Got endpoints: latency-svc-s8xcb [751.310094ms]
Mar 27 15:48:15.519: INFO: Got endpoints: latency-svc-wfbt6 [748.772247ms]
Mar 27 15:48:15.567: INFO: Got endpoints: latency-svc-9bh8n [744.8828ms]
Mar 27 15:48:15.619: INFO: Got endpoints: latency-svc-528d6 [749.52941ms]
Mar 27 15:48:15.671: INFO: Got endpoints: latency-svc-v9j9p [751.489458ms]
Mar 27 15:48:15.718: INFO: Got endpoints: latency-svc-jx2f8 [748.082236ms]
Mar 27 15:48:15.772: INFO: Got endpoints: latency-svc-t6qhp [751.905796ms]
Mar 27 15:48:15.822: INFO: Got endpoints: latency-svc-xdcfm [749.684006ms]
Mar 27 15:48:15.869: INFO: Got endpoints: latency-svc-cq8wd [747.259317ms]
Mar 27 15:48:15.920: INFO: Got endpoints: latency-svc-7vlbh [748.917084ms]
Mar 27 15:48:15.972: INFO: Got endpoints: latency-svc-v9h9k [753.675326ms]
Mar 27 15:48:15.972: INFO: Latencies: [20.603342ms 27.116545ms 41.87065ms 53.490871ms 58.635268ms 63.621258ms 68.25924ms 75.835121ms 86.287625ms 94.607106ms 102.441806ms 111.796915ms 120.533081ms 125.533461ms 125.727592ms 126.963378ms 127.420349ms 127.664198ms 128.785637ms 129.03705ms 129.726324ms 130.599807ms 131.056497ms 133.553096ms 134.140261ms 134.645711ms 134.912819ms 135.475741ms 138.247675ms 139.017959ms 139.357993ms 142.030749ms 142.426349ms 143.686399ms 145.694408ms 153.599438ms 194.327769ms 239.331181ms 271.346507ms 332.830456ms 354.805168ms 402.530417ms 434.284645ms 481.112518ms 525.153546ms 566.063345ms 605.758006ms 646.463001ms 692.025177ms 735.694076ms 737.326031ms 741.501383ms 742.297161ms 743.286889ms 743.585957ms 744.040556ms 744.8828ms 745.031895ms 745.269522ms 745.381868ms 745.52456ms 745.664775ms 745.783298ms 745.799616ms 746.03724ms 746.249603ms 746.3803ms 746.464793ms 746.627792ms 746.726812ms 746.838634ms 746.844433ms 746.89948ms 746.928001ms 746.931927ms 746.991339ms 746.993343ms 747.188952ms 747.259317ms 747.370569ms 747.397985ms 747.466869ms 747.497284ms 747.648619ms 747.655506ms 747.69948ms 747.869568ms 748.082236ms 748.110631ms 748.111749ms 748.126919ms 748.186676ms 748.222093ms 748.244393ms 748.327759ms 748.348583ms 748.360061ms 748.382165ms 748.382565ms 748.44615ms 748.560913ms 748.614662ms 748.673591ms 748.732735ms 748.772247ms 748.879088ms 748.891686ms 748.917084ms 748.938668ms 748.958365ms 748.959497ms 749.027484ms 749.040868ms 749.124174ms 749.269405ms 749.331474ms 749.372528ms 749.381136ms 749.386487ms 749.411659ms 749.460931ms 749.52941ms 749.544017ms 749.549972ms 749.557642ms 749.656587ms 749.684006ms 749.731093ms 749.736277ms 749.830638ms 749.875032ms 749.901569ms 749.912249ms 749.932087ms 749.935704ms 750.181146ms 750.200412ms 750.274538ms 750.310658ms 750.384772ms 750.395616ms 750.47516ms 750.584995ms 750.599243ms 750.657917ms 750.686609ms 750.688706ms 750.693486ms 750.726147ms 750.730197ms 750.784788ms 750.845683ms 750.919003ms 751.15152ms 751.167685ms 751.20934ms 751.223129ms 751.310094ms 751.335863ms 751.398195ms 751.433584ms 751.478974ms 751.489458ms 751.736563ms 751.794653ms 751.796287ms 751.894723ms 751.905796ms 751.907033ms 751.936433ms 751.97113ms 751.998247ms 752.063685ms 752.151796ms 752.176855ms 752.201845ms 752.272063ms 752.587944ms 752.801289ms 752.80952ms 752.882895ms 752.891558ms 753.026951ms 753.066295ms 753.250706ms 753.281282ms 753.32226ms 753.597854ms 753.675326ms 753.73047ms 753.842543ms 753.96643ms 754.335827ms 754.669792ms 754.867305ms 755.012925ms 755.027558ms 755.553065ms 756.91397ms 756.995637ms]
Mar 27 15:48:15.972: INFO: 50 %ile: 748.560913ms
Mar 27 15:48:15.972: INFO: 90 %ile: 752.882895ms
Mar 27 15:48:15.973: INFO: 99 %ile: 756.91397ms
Mar 27 15:48:15.973: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Mar 27 15:48:15.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6898" for this suite. 03/27/23 15:48:15.982
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":350,"skipped":6441,"failed":0}
------------------------------
• [SLOW TEST] [10.782 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:48:05.213
    Mar 27 15:48:05.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename svc-latency 03/27/23 15:48:05.215
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:48:05.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:48:05.236
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Mar 27 15:48:05.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-6898 03/27/23 15:48:05.24
    I0327 15:48:05.246257      24 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6898, replica count: 1
    I0327 15:48:06.297672      24 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0327 15:48:07.298775      24 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 15:48:07.411: INFO: Created: latency-svc-6pbl7
    Mar 27 15:48:07.421: INFO: Got endpoints: latency-svc-6pbl7 [21.914315ms]
    Mar 27 15:48:07.434: INFO: Created: latency-svc-7wprc
    Mar 27 15:48:07.442: INFO: Created: latency-svc-w9w7t
    Mar 27 15:48:07.442: INFO: Got endpoints: latency-svc-7wprc [20.603342ms]
    Mar 27 15:48:07.449: INFO: Got endpoints: latency-svc-w9w7t [27.116545ms]
    Mar 27 15:48:07.453: INFO: Created: latency-svc-g9zc9
    Mar 27 15:48:07.460: INFO: Created: latency-svc-r5vz2
    Mar 27 15:48:07.464: INFO: Got endpoints: latency-svc-g9zc9 [41.87065ms]
    Mar 27 15:48:07.465: INFO: Created: latency-svc-xp6tm
    Mar 27 15:48:07.475: INFO: Got endpoints: latency-svc-r5vz2 [53.490871ms]
    Mar 27 15:48:07.480: INFO: Created: latency-svc-t8qc9
    Mar 27 15:48:07.480: INFO: Got endpoints: latency-svc-xp6tm [58.635268ms]
    Mar 27 15:48:07.482: INFO: Created: latency-svc-nnktn
    Mar 27 15:48:07.485: INFO: Got endpoints: latency-svc-t8qc9 [63.621258ms]
    Mar 27 15:48:07.489: INFO: Created: latency-svc-6jbt4
    Mar 27 15:48:07.490: INFO: Got endpoints: latency-svc-nnktn [68.25924ms]
    Mar 27 15:48:07.498: INFO: Got endpoints: latency-svc-6jbt4 [75.835121ms]
    Mar 27 15:48:07.502: INFO: Created: latency-svc-ks9tr
    Mar 27 15:48:07.507: INFO: Created: latency-svc-csjcf
    Mar 27 15:48:07.508: INFO: Got endpoints: latency-svc-ks9tr [86.287625ms]
    Mar 27 15:48:07.516: INFO: Created: latency-svc-kn6t2
    Mar 27 15:48:07.516: INFO: Got endpoints: latency-svc-csjcf [94.607106ms]
    Mar 27 15:48:07.524: INFO: Got endpoints: latency-svc-kn6t2 [102.441806ms]
    Mar 27 15:48:07.526: INFO: Created: latency-svc-x6gt5
    Mar 27 15:48:07.534: INFO: Got endpoints: latency-svc-x6gt5 [111.796915ms]
    Mar 27 15:48:07.538: INFO: Created: latency-svc-ncw8b
    Mar 27 15:48:07.540: INFO: Created: latency-svc-lc5gx
    Mar 27 15:48:07.542: INFO: Got endpoints: latency-svc-ncw8b [120.533081ms]
    Mar 27 15:48:07.549: INFO: Got endpoints: latency-svc-lc5gx [126.963378ms]
    Mar 27 15:48:07.554: INFO: Created: latency-svc-nbhqd
    Mar 27 15:48:07.560: INFO: Created: latency-svc-mbtvz
    Mar 27 15:48:07.567: INFO: Got endpoints: latency-svc-nbhqd [145.694408ms]
    Mar 27 15:48:07.570: INFO: Got endpoints: latency-svc-mbtvz [127.664198ms]
    Mar 27 15:48:07.570: INFO: Created: latency-svc-lq4rz
    Mar 27 15:48:07.578: INFO: Got endpoints: latency-svc-lq4rz [129.726324ms]
    Mar 27 15:48:07.584: INFO: Created: latency-svc-lpwf4
    Mar 27 15:48:07.589: INFO: Got endpoints: latency-svc-lpwf4 [125.727592ms]
    Mar 27 15:48:07.590: INFO: Created: latency-svc-vrdfw
    Mar 27 15:48:07.599: INFO: Created: latency-svc-7lgxd
    Mar 27 15:48:07.600: INFO: Got endpoints: latency-svc-vrdfw [125.533461ms]
    Mar 27 15:48:07.609: INFO: Got endpoints: latency-svc-7lgxd [129.03705ms]
    Mar 27 15:48:07.610: INFO: Created: latency-svc-jdjxz
    Mar 27 15:48:07.616: INFO: Got endpoints: latency-svc-jdjxz [130.599807ms]
    Mar 27 15:48:07.617: INFO: Created: latency-svc-r8mp5
    Mar 27 15:48:07.628: INFO: Got endpoints: latency-svc-r8mp5 [138.247675ms]
    Mar 27 15:48:07.628: INFO: Created: latency-svc-wck9x
    Mar 27 15:48:07.631: INFO: Got endpoints: latency-svc-wck9x [133.553096ms]
    Mar 27 15:48:07.640: INFO: Created: latency-svc-6ktfn
    Mar 27 15:48:07.644: INFO: Created: latency-svc-khkp9
    Mar 27 15:48:07.651: INFO: Got endpoints: latency-svc-6ktfn [142.426349ms]
    Mar 27 15:48:07.651: INFO: Got endpoints: latency-svc-khkp9 [134.912819ms]
    Mar 27 15:48:07.655: INFO: Created: latency-svc-sfmlb
    Mar 27 15:48:07.664: INFO: Got endpoints: latency-svc-sfmlb [139.357993ms]
    Mar 27 15:48:07.667: INFO: Created: latency-svc-hws4f
    Mar 27 15:48:07.673: INFO: Got endpoints: latency-svc-hws4f [139.017959ms]
    Mar 27 15:48:07.675: INFO: Created: latency-svc-x46dm
    Mar 27 15:48:07.679: INFO: Created: latency-svc-dv7wv
    Mar 27 15:48:07.686: INFO: Got endpoints: latency-svc-x46dm [143.686399ms]
    Mar 27 15:48:07.691: INFO: Got endpoints: latency-svc-dv7wv [142.030749ms]
    Mar 27 15:48:07.691: INFO: Created: latency-svc-8q8gx
    Mar 27 15:48:07.696: INFO: Got endpoints: latency-svc-8q8gx [128.785637ms]
    Mar 27 15:48:07.699: INFO: Created: latency-svc-sl9f9
    Mar 27 15:48:07.705: INFO: Created: latency-svc-wnz5t
    Mar 27 15:48:07.706: INFO: Got endpoints: latency-svc-sl9f9 [135.475741ms]
    Mar 27 15:48:07.713: INFO: Got endpoints: latency-svc-wnz5t [134.645711ms]
    Mar 27 15:48:07.716: INFO: Created: latency-svc-2kw8v
    Mar 27 15:48:07.723: INFO: Created: latency-svc-cwbrw
    Mar 27 15:48:07.723: INFO: Got endpoints: latency-svc-2kw8v [134.140261ms]
    Mar 27 15:48:07.731: INFO: Created: latency-svc-gkt7z
    Mar 27 15:48:07.732: INFO: Got endpoints: latency-svc-cwbrw [131.056497ms]
    Mar 27 15:48:07.737: INFO: Got endpoints: latency-svc-gkt7z [127.420349ms]
    Mar 27 15:48:07.741: INFO: Created: latency-svc-pb74s
    Mar 27 15:48:07.749: INFO: Created: latency-svc-r5mpg
    Mar 27 15:48:07.754: INFO: Created: latency-svc-222qx
    Mar 27 15:48:07.760: INFO: Created: latency-svc-8n8r2
    Mar 27 15:48:07.767: INFO: Created: latency-svc-crzd4
    Mar 27 15:48:07.770: INFO: Got endpoints: latency-svc-pb74s [153.599438ms]
    Mar 27 15:48:07.774: INFO: Created: latency-svc-nn7b6
    Mar 27 15:48:07.779: INFO: Created: latency-svc-lf9lj
    Mar 27 15:48:07.789: INFO: Created: latency-svc-2wvbl
    Mar 27 15:48:07.796: INFO: Created: latency-svc-657cd
    Mar 27 15:48:07.804: INFO: Created: latency-svc-85gg7
    Mar 27 15:48:07.813: INFO: Created: latency-svc-bh9cq
    Mar 27 15:48:07.822: INFO: Got endpoints: latency-svc-r5mpg [194.327769ms]
    Mar 27 15:48:07.825: INFO: Created: latency-svc-t579k
    Mar 27 15:48:07.834: INFO: Created: latency-svc-tl58w
    Mar 27 15:48:07.849: INFO: Created: latency-svc-v9l68
    Mar 27 15:48:07.859: INFO: Created: latency-svc-dn4fc
    Mar 27 15:48:07.869: INFO: Created: latency-svc-m6tqg
    Mar 27 15:48:07.871: INFO: Got endpoints: latency-svc-222qx [239.331181ms]
    Mar 27 15:48:07.877: INFO: Created: latency-svc-jwv6d
    Mar 27 15:48:07.884: INFO: Created: latency-svc-wfcr5
    Mar 27 15:48:07.922: INFO: Got endpoints: latency-svc-8n8r2 [271.346507ms]
    Mar 27 15:48:07.934: INFO: Created: latency-svc-hwrmd
    Mar 27 15:48:07.984: INFO: Got endpoints: latency-svc-crzd4 [332.830456ms]
    Mar 27 15:48:08.013: INFO: Created: latency-svc-8gzzc
    Mar 27 15:48:08.018: INFO: Got endpoints: latency-svc-nn7b6 [354.805168ms]
    Mar 27 15:48:08.033: INFO: Created: latency-svc-wpkm9
    Mar 27 15:48:08.075: INFO: Got endpoints: latency-svc-lf9lj [402.530417ms]
    Mar 27 15:48:08.090: INFO: Created: latency-svc-696f6
    Mar 27 15:48:08.121: INFO: Got endpoints: latency-svc-2wvbl [434.284645ms]
    Mar 27 15:48:08.139: INFO: Created: latency-svc-h55zf
    Mar 27 15:48:08.172: INFO: Got endpoints: latency-svc-657cd [481.112518ms]
    Mar 27 15:48:08.183: INFO: Created: latency-svc-8ld8f
    Mar 27 15:48:08.221: INFO: Got endpoints: latency-svc-85gg7 [525.153546ms]
    Mar 27 15:48:08.233: INFO: Created: latency-svc-frdws
    Mar 27 15:48:08.272: INFO: Got endpoints: latency-svc-bh9cq [566.063345ms]
    Mar 27 15:48:08.281: INFO: Created: latency-svc-5hvjj
    Mar 27 15:48:08.319: INFO: Got endpoints: latency-svc-t579k [605.758006ms]
    Mar 27 15:48:08.331: INFO: Created: latency-svc-7c2ql
    Mar 27 15:48:08.370: INFO: Got endpoints: latency-svc-tl58w [646.463001ms]
    Mar 27 15:48:08.380: INFO: Created: latency-svc-m7scw
    Mar 27 15:48:08.424: INFO: Got endpoints: latency-svc-v9l68 [692.025177ms]
    Mar 27 15:48:08.436: INFO: Created: latency-svc-nmpdg
    Mar 27 15:48:08.472: INFO: Got endpoints: latency-svc-dn4fc [735.694076ms]
    Mar 27 15:48:08.483: INFO: Created: latency-svc-shxrg
    Mar 27 15:48:08.519: INFO: Got endpoints: latency-svc-m6tqg [749.656587ms]
    Mar 27 15:48:08.531: INFO: Created: latency-svc-t69qd
    Mar 27 15:48:08.569: INFO: Got endpoints: latency-svc-jwv6d [746.844433ms]
    Mar 27 15:48:08.585: INFO: Created: latency-svc-bvn4k
    Mar 27 15:48:08.620: INFO: Got endpoints: latency-svc-wfcr5 [748.938668ms]
    Mar 27 15:48:08.632: INFO: Created: latency-svc-pbg4m
    Mar 27 15:48:08.674: INFO: Got endpoints: latency-svc-hwrmd [751.998247ms]
    Mar 27 15:48:08.684: INFO: Created: latency-svc-qhw6d
    Mar 27 15:48:08.722: INFO: Got endpoints: latency-svc-8gzzc [737.326031ms]
    Mar 27 15:48:08.743: INFO: Created: latency-svc-plc2w
    Mar 27 15:48:08.772: INFO: Got endpoints: latency-svc-wpkm9 [753.597854ms]
    Mar 27 15:48:08.783: INFO: Created: latency-svc-kfjqd
    Mar 27 15:48:08.820: INFO: Got endpoints: latency-svc-696f6 [744.040556ms]
    Mar 27 15:48:08.832: INFO: Created: latency-svc-kl84k
    Mar 27 15:48:08.869: INFO: Got endpoints: latency-svc-h55zf [748.891686ms]
    Mar 27 15:48:08.882: INFO: Created: latency-svc-n5vpg
    Mar 27 15:48:08.922: INFO: Got endpoints: latency-svc-8ld8f [749.830638ms]
    Mar 27 15:48:08.936: INFO: Created: latency-svc-xh7q9
    Mar 27 15:48:08.973: INFO: Got endpoints: latency-svc-frdws [752.063685ms]
    Mar 27 15:48:08.986: INFO: Created: latency-svc-lvnzk
    Mar 27 15:48:09.019: INFO: Got endpoints: latency-svc-5hvjj [747.497284ms]
    Mar 27 15:48:09.030: INFO: Created: latency-svc-kr6pb
    Mar 27 15:48:09.071: INFO: Got endpoints: latency-svc-7c2ql [751.736563ms]
    Mar 27 15:48:09.081: INFO: Created: latency-svc-sgq68
    Mar 27 15:48:09.120: INFO: Got endpoints: latency-svc-m7scw [750.384772ms]
    Mar 27 15:48:09.133: INFO: Created: latency-svc-vw8db
    Mar 27 15:48:09.173: INFO: Got endpoints: latency-svc-nmpdg [749.269405ms]
    Mar 27 15:48:09.189: INFO: Created: latency-svc-wqnx9
    Mar 27 15:48:09.219: INFO: Got endpoints: latency-svc-shxrg [746.03724ms]
    Mar 27 15:48:09.229: INFO: Created: latency-svc-nhz5b
    Mar 27 15:48:09.268: INFO: Got endpoints: latency-svc-t69qd [748.673591ms]
    Mar 27 15:48:09.281: INFO: Created: latency-svc-7b88z
    Mar 27 15:48:09.321: INFO: Got endpoints: latency-svc-bvn4k [746.249603ms]
    Mar 27 15:48:09.338: INFO: Created: latency-svc-l4dfn
    Mar 27 15:48:09.369: INFO: Got endpoints: latency-svc-pbg4m [747.188952ms]
    Mar 27 15:48:09.384: INFO: Created: latency-svc-4hgc5
    Mar 27 15:48:09.423: INFO: Got endpoints: latency-svc-qhw6d [749.549972ms]
    Mar 27 15:48:09.435: INFO: Created: latency-svc-9g9hf
    Mar 27 15:48:09.469: INFO: Got endpoints: latency-svc-plc2w [746.993343ms]
    Mar 27 15:48:09.489: INFO: Created: latency-svc-85wsw
    Mar 27 15:48:09.521: INFO: Got endpoints: latency-svc-kfjqd [748.732735ms]
    Mar 27 15:48:09.534: INFO: Created: latency-svc-8hfrf
    Mar 27 15:48:09.570: INFO: Got endpoints: latency-svc-kl84k [750.200412ms]
    Mar 27 15:48:09.581: INFO: Created: latency-svc-2fh47
    Mar 27 15:48:09.619: INFO: Got endpoints: latency-svc-n5vpg [749.736277ms]
    Mar 27 15:48:09.630: INFO: Created: latency-svc-rbbjx
    Mar 27 15:48:09.669: INFO: Got endpoints: latency-svc-xh7q9 [746.89948ms]
    Mar 27 15:48:09.681: INFO: Created: latency-svc-2fs44
    Mar 27 15:48:09.720: INFO: Got endpoints: latency-svc-lvnzk [746.464793ms]
    Mar 27 15:48:09.735: INFO: Created: latency-svc-4wt2k
    Mar 27 15:48:09.776: INFO: Got endpoints: latency-svc-kr6pb [756.91397ms]
    Mar 27 15:48:09.790: INFO: Created: latency-svc-t7g8l
    Mar 27 15:48:09.819: INFO: Got endpoints: latency-svc-sgq68 [748.186676ms]
    Mar 27 15:48:09.829: INFO: Created: latency-svc-bfwxk
    Mar 27 15:48:09.870: INFO: Got endpoints: latency-svc-vw8db [749.912249ms]
    Mar 27 15:48:09.887: INFO: Created: latency-svc-76bf4
    Mar 27 15:48:09.920: INFO: Got endpoints: latency-svc-wqnx9 [746.838634ms]
    Mar 27 15:48:09.931: INFO: Created: latency-svc-4mmwr
    Mar 27 15:48:09.974: INFO: Got endpoints: latency-svc-nhz5b [755.027558ms]
    Mar 27 15:48:09.988: INFO: Created: latency-svc-qlgx5
    Mar 27 15:48:10.021: INFO: Got endpoints: latency-svc-7b88z [752.80952ms]
    Mar 27 15:48:10.032: INFO: Created: latency-svc-nhx9z
    Mar 27 15:48:10.070: INFO: Got endpoints: latency-svc-l4dfn [743.286889ms]
    Mar 27 15:48:10.084: INFO: Created: latency-svc-kf4sx
    Mar 27 15:48:10.120: INFO: Got endpoints: latency-svc-4hgc5 [750.693486ms]
    Mar 27 15:48:10.132: INFO: Created: latency-svc-j5ppw
    Mar 27 15:48:10.169: INFO: Got endpoints: latency-svc-9g9hf [745.381868ms]
    Mar 27 15:48:10.182: INFO: Created: latency-svc-hkttp
    Mar 27 15:48:10.217: INFO: Got endpoints: latency-svc-85wsw [748.110631ms]
    Mar 27 15:48:10.230: INFO: Created: latency-svc-lrc9d
    Mar 27 15:48:10.271: INFO: Got endpoints: latency-svc-8hfrf [749.901569ms]
    Mar 27 15:48:10.283: INFO: Created: latency-svc-bplr9
    Mar 27 15:48:10.319: INFO: Got endpoints: latency-svc-2fh47 [748.959497ms]
    Mar 27 15:48:10.330: INFO: Created: latency-svc-fm7nz
    Mar 27 15:48:10.369: INFO: Got endpoints: latency-svc-rbbjx [749.731093ms]
    Mar 27 15:48:10.379: INFO: Created: latency-svc-4947l
    Mar 27 15:48:10.419: INFO: Got endpoints: latency-svc-2fs44 [749.411659ms]
    Mar 27 15:48:10.429: INFO: Created: latency-svc-kc72f
    Mar 27 15:48:10.471: INFO: Got endpoints: latency-svc-4wt2k [751.167685ms]
    Mar 27 15:48:10.482: INFO: Created: latency-svc-tnvcj
    Mar 27 15:48:10.518: INFO: Got endpoints: latency-svc-t7g8l [741.501383ms]
    Mar 27 15:48:10.532: INFO: Created: latency-svc-thxbd
    Mar 27 15:48:10.571: INFO: Got endpoints: latency-svc-bfwxk [751.478974ms]
    Mar 27 15:48:10.585: INFO: Created: latency-svc-wtrxd
    Mar 27 15:48:10.619: INFO: Got endpoints: latency-svc-76bf4 [748.958365ms]
    Mar 27 15:48:10.631: INFO: Created: latency-svc-5qrhp
    Mar 27 15:48:10.670: INFO: Got endpoints: latency-svc-4mmwr [749.932087ms]
    Mar 27 15:48:10.681: INFO: Created: latency-svc-vsdwh
    Mar 27 15:48:10.719: INFO: Got endpoints: latency-svc-qlgx5 [745.52456ms]
    Mar 27 15:48:10.730: INFO: Created: latency-svc-dbkwh
    Mar 27 15:48:10.769: INFO: Got endpoints: latency-svc-nhx9z [747.655506ms]
    Mar 27 15:48:10.782: INFO: Created: latency-svc-gdb6k
    Mar 27 15:48:10.823: INFO: Got endpoints: latency-svc-kf4sx [753.066295ms]
    Mar 27 15:48:10.847: INFO: Created: latency-svc-vwx5n
    Mar 27 15:48:10.869: INFO: Got endpoints: latency-svc-j5ppw [749.372528ms]
    Mar 27 15:48:10.881: INFO: Created: latency-svc-xzbqh
    Mar 27 15:48:10.917: INFO: Got endpoints: latency-svc-hkttp [748.382565ms]
    Mar 27 15:48:10.928: INFO: Created: latency-svc-xqf5b
    Mar 27 15:48:10.968: INFO: Got endpoints: latency-svc-lrc9d [750.584995ms]
    Mar 27 15:48:10.980: INFO: Created: latency-svc-lf2jv
    Mar 27 15:48:11.018: INFO: Got endpoints: latency-svc-bplr9 [746.931927ms]
    Mar 27 15:48:11.029: INFO: Created: latency-svc-7k6zj
    Mar 27 15:48:11.070: INFO: Got endpoints: latency-svc-fm7nz [751.433584ms]
    Mar 27 15:48:11.081: INFO: Created: latency-svc-shptc
    Mar 27 15:48:11.121: INFO: Got endpoints: latency-svc-4947l [751.907033ms]
    Mar 27 15:48:11.132: INFO: Created: latency-svc-chz8g
    Mar 27 15:48:11.169: INFO: Got endpoints: latency-svc-kc72f [750.784788ms]
    Mar 27 15:48:11.182: INFO: Created: latency-svc-pzskc
    Mar 27 15:48:11.221: INFO: Got endpoints: latency-svc-tnvcj [749.875032ms]
    Mar 27 15:48:11.234: INFO: Created: latency-svc-qzbk7
    Mar 27 15:48:11.269: INFO: Got endpoints: latency-svc-thxbd [751.15152ms]
    Mar 27 15:48:11.280: INFO: Created: latency-svc-whm94
    Mar 27 15:48:11.318: INFO: Got endpoints: latency-svc-wtrxd [747.370569ms]
    Mar 27 15:48:11.329: INFO: Created: latency-svc-ffmbt
    Mar 27 15:48:11.371: INFO: Got endpoints: latency-svc-5qrhp [751.335863ms]
    Mar 27 15:48:11.381: INFO: Created: latency-svc-wj46n
    Mar 27 15:48:11.421: INFO: Got endpoints: latency-svc-vsdwh [750.599243ms]
    Mar 27 15:48:11.434: INFO: Created: latency-svc-77q5d
    Mar 27 15:48:11.473: INFO: Got endpoints: latency-svc-dbkwh [753.96643ms]
    Mar 27 15:48:11.483: INFO: Created: latency-svc-2p8j6
    Mar 27 15:48:11.523: INFO: Got endpoints: latency-svc-gdb6k [754.669792ms]
    Mar 27 15:48:11.534: INFO: Created: latency-svc-clw7b
    Mar 27 15:48:11.571: INFO: Got endpoints: latency-svc-vwx5n [748.382165ms]
    Mar 27 15:48:11.581: INFO: Created: latency-svc-klqg5
    Mar 27 15:48:11.622: INFO: Got endpoints: latency-svc-xzbqh [752.587944ms]
    Mar 27 15:48:11.633: INFO: Created: latency-svc-5npql
    Mar 27 15:48:11.669: INFO: Got endpoints: latency-svc-xqf5b [751.398195ms]
    Mar 27 15:48:11.680: INFO: Created: latency-svc-tvksx
    Mar 27 15:48:11.719: INFO: Got endpoints: latency-svc-lf2jv [751.796287ms]
    Mar 27 15:48:11.731: INFO: Created: latency-svc-xjkd9
    Mar 27 15:48:11.771: INFO: Got endpoints: latency-svc-7k6zj [753.250706ms]
    Mar 27 15:48:11.783: INFO: Created: latency-svc-qlt2p
    Mar 27 15:48:11.819: INFO: Got endpoints: latency-svc-shptc [748.111749ms]
    Mar 27 15:48:11.830: INFO: Created: latency-svc-5k9xf
    Mar 27 15:48:11.872: INFO: Got endpoints: latency-svc-chz8g [750.688706ms]
    Mar 27 15:48:11.883: INFO: Created: latency-svc-jhzhx
    Mar 27 15:48:11.918: INFO: Got endpoints: latency-svc-pzskc [748.614662ms]
    Mar 27 15:48:11.930: INFO: Created: latency-svc-46qsf
    Mar 27 15:48:11.970: INFO: Got endpoints: latency-svc-qzbk7 [748.879088ms]
    Mar 27 15:48:11.980: INFO: Created: latency-svc-2k9th
    Mar 27 15:48:12.021: INFO: Got endpoints: latency-svc-whm94 [751.97113ms]
    Mar 27 15:48:12.032: INFO: Created: latency-svc-mqcfk
    Mar 27 15:48:12.072: INFO: Got endpoints: latency-svc-ffmbt [753.73047ms]
    Mar 27 15:48:12.086: INFO: Created: latency-svc-7ccqg
    Mar 27 15:48:12.122: INFO: Got endpoints: latency-svc-wj46n [750.726147ms]
    Mar 27 15:48:12.131: INFO: Created: latency-svc-89kbw
    Mar 27 15:48:12.169: INFO: Got endpoints: latency-svc-77q5d [748.244393ms]
    Mar 27 15:48:12.179: INFO: Created: latency-svc-v46t4
    Mar 27 15:48:12.219: INFO: Got endpoints: latency-svc-2p8j6 [745.664775ms]
    Mar 27 15:48:12.230: INFO: Created: latency-svc-6zjj2
    Mar 27 15:48:12.271: INFO: Got endpoints: latency-svc-clw7b [746.928001ms]
    Mar 27 15:48:12.282: INFO: Created: latency-svc-7f7k8
    Mar 27 15:48:12.320: INFO: Got endpoints: latency-svc-klqg5 [749.331474ms]
    Mar 27 15:48:12.333: INFO: Created: latency-svc-h7d92
    Mar 27 15:48:12.370: INFO: Got endpoints: latency-svc-5npql [747.869568ms]
    Mar 27 15:48:12.383: INFO: Created: latency-svc-f9c55
    Mar 27 15:48:12.419: INFO: Got endpoints: latency-svc-tvksx [749.935704ms]
    Mar 27 15:48:12.429: INFO: Created: latency-svc-zw529
    Mar 27 15:48:12.472: INFO: Got endpoints: latency-svc-xjkd9 [752.151796ms]
    Mar 27 15:48:12.482: INFO: Created: latency-svc-c6gvl
    Mar 27 15:48:12.520: INFO: Got endpoints: latency-svc-qlt2p [748.560913ms]
    Mar 27 15:48:12.531: INFO: Created: latency-svc-xvvw6
    Mar 27 15:48:12.575: INFO: Got endpoints: latency-svc-5k9xf [755.553065ms]
    Mar 27 15:48:12.586: INFO: Created: latency-svc-pl9bg
    Mar 27 15:48:12.619: INFO: Got endpoints: latency-svc-jhzhx [746.627792ms]
    Mar 27 15:48:12.632: INFO: Created: latency-svc-zqh9r
    Mar 27 15:48:12.670: INFO: Got endpoints: latency-svc-46qsf [751.936433ms]
    Mar 27 15:48:12.681: INFO: Created: latency-svc-99f55
    Mar 27 15:48:12.721: INFO: Got endpoints: latency-svc-2k9th [750.310658ms]
    Mar 27 15:48:12.731: INFO: Created: latency-svc-p7vlx
    Mar 27 15:48:12.776: INFO: Got endpoints: latency-svc-mqcfk [754.867305ms]
    Mar 27 15:48:12.788: INFO: Created: latency-svc-b7ptg
    Mar 27 15:48:12.826: INFO: Got endpoints: latency-svc-7ccqg [753.842543ms]
    Mar 27 15:48:12.837: INFO: Created: latency-svc-2x57q
    Mar 27 15:48:12.869: INFO: Got endpoints: latency-svc-89kbw [746.991339ms]
    Mar 27 15:48:12.879: INFO: Created: latency-svc-pc9mr
    Mar 27 15:48:12.920: INFO: Got endpoints: latency-svc-v46t4 [750.730197ms]
    Mar 27 15:48:12.932: INFO: Created: latency-svc-42gt8
    Mar 27 15:48:12.972: INFO: Got endpoints: latency-svc-6zjj2 [752.201845ms]
    Mar 27 15:48:12.988: INFO: Created: latency-svc-hckf8
    Mar 27 15:48:13.019: INFO: Got endpoints: latency-svc-7f7k8 [748.222093ms]
    Mar 27 15:48:13.030: INFO: Created: latency-svc-v66nz
    Mar 27 15:48:13.071: INFO: Got endpoints: latency-svc-h7d92 [750.686609ms]
    Mar 27 15:48:13.081: INFO: Created: latency-svc-ml7kx
    Mar 27 15:48:13.119: INFO: Got endpoints: latency-svc-f9c55 [748.360061ms]
    Mar 27 15:48:13.128: INFO: Created: latency-svc-lx699
    Mar 27 15:48:13.169: INFO: Got endpoints: latency-svc-zw529 [750.395616ms]
    Mar 27 15:48:13.184: INFO: Created: latency-svc-zrptb
    Mar 27 15:48:13.221: INFO: Got endpoints: latency-svc-c6gvl [749.557642ms]
    Mar 27 15:48:13.233: INFO: Created: latency-svc-kzml8
    Mar 27 15:48:13.273: INFO: Got endpoints: latency-svc-xvvw6 [752.882895ms]
    Mar 27 15:48:13.284: INFO: Created: latency-svc-cnbj4
    Mar 27 15:48:13.320: INFO: Got endpoints: latency-svc-pl9bg [745.031895ms]
    Mar 27 15:48:13.332: INFO: Created: latency-svc-g6mfr
    Mar 27 15:48:13.370: INFO: Got endpoints: latency-svc-zqh9r [750.919003ms]
    Mar 27 15:48:13.381: INFO: Created: latency-svc-5nzr5
    Mar 27 15:48:13.422: INFO: Got endpoints: latency-svc-99f55 [751.20934ms]
    Mar 27 15:48:13.434: INFO: Created: latency-svc-plpcd
    Mar 27 15:48:13.470: INFO: Got endpoints: latency-svc-p7vlx [749.040868ms]
    Mar 27 15:48:13.480: INFO: Created: latency-svc-k2xkt
    Mar 27 15:48:13.521: INFO: Got endpoints: latency-svc-b7ptg [745.269522ms]
    Mar 27 15:48:13.531: INFO: Created: latency-svc-vsxmz
    Mar 27 15:48:13.569: INFO: Got endpoints: latency-svc-2x57q [743.585957ms]
    Mar 27 15:48:13.581: INFO: Created: latency-svc-pndlg
    Mar 27 15:48:13.621: INFO: Got endpoints: latency-svc-pc9mr [752.272063ms]
    Mar 27 15:48:13.634: INFO: Created: latency-svc-4bkbl
    Mar 27 15:48:13.671: INFO: Got endpoints: latency-svc-42gt8 [750.845683ms]
    Mar 27 15:48:13.685: INFO: Created: latency-svc-m58nx
    Mar 27 15:48:13.720: INFO: Got endpoints: latency-svc-hckf8 [748.126919ms]
    Mar 27 15:48:13.733: INFO: Created: latency-svc-gwm8r
    Mar 27 15:48:13.769: INFO: Got endpoints: latency-svc-v66nz [750.274538ms]
    Mar 27 15:48:13.783: INFO: Created: latency-svc-tbgs2
    Mar 27 15:48:13.819: INFO: Got endpoints: latency-svc-ml7kx [747.648619ms]
    Mar 27 15:48:13.831: INFO: Created: latency-svc-bznwd
    Mar 27 15:48:13.872: INFO: Got endpoints: latency-svc-lx699 [752.801289ms]
    Mar 27 15:48:13.883: INFO: Created: latency-svc-mzp7m
    Mar 27 15:48:13.924: INFO: Got endpoints: latency-svc-zrptb [755.012925ms]
    Mar 27 15:48:13.936: INFO: Created: latency-svc-ndnt8
    Mar 27 15:48:13.972: INFO: Got endpoints: latency-svc-kzml8 [750.47516ms]
    Mar 27 15:48:13.984: INFO: Created: latency-svc-m2gmc
    Mar 27 15:48:14.020: INFO: Got endpoints: latency-svc-cnbj4 [747.466869ms]
    Mar 27 15:48:14.030: INFO: Created: latency-svc-v7b79
    Mar 27 15:48:14.071: INFO: Got endpoints: latency-svc-g6mfr [750.657917ms]
    Mar 27 15:48:14.081: INFO: Created: latency-svc-xggm5
    Mar 27 15:48:14.127: INFO: Got endpoints: latency-svc-5nzr5 [756.995637ms]
    Mar 27 15:48:14.140: INFO: Created: latency-svc-dqbqc
    Mar 27 15:48:14.174: INFO: Got endpoints: latency-svc-plpcd [752.176855ms]
    Mar 27 15:48:14.187: INFO: Created: latency-svc-g64mj
    Mar 27 15:48:14.220: INFO: Got endpoints: latency-svc-k2xkt [750.181146ms]
    Mar 27 15:48:14.230: INFO: Created: latency-svc-g5nb9
    Mar 27 15:48:14.268: INFO: Got endpoints: latency-svc-vsxmz [746.726812ms]
    Mar 27 15:48:14.285: INFO: Created: latency-svc-c2g9h
    Mar 27 15:48:14.318: INFO: Got endpoints: latency-svc-pndlg [748.327759ms]
    Mar 27 15:48:14.328: INFO: Created: latency-svc-jvgkq
    Mar 27 15:48:14.369: INFO: Got endpoints: latency-svc-4bkbl [747.397985ms]
    Mar 27 15:48:14.382: INFO: Created: latency-svc-nspkt
    Mar 27 15:48:14.422: INFO: Got endpoints: latency-svc-m58nx [751.223129ms]
    Mar 27 15:48:14.436: INFO: Created: latency-svc-nbztt
    Mar 27 15:48:14.469: INFO: Got endpoints: latency-svc-gwm8r [749.027484ms]
    Mar 27 15:48:14.482: INFO: Created: latency-svc-w44fh
    Mar 27 15:48:14.523: INFO: Got endpoints: latency-svc-tbgs2 [753.281282ms]
    Mar 27 15:48:14.534: INFO: Created: latency-svc-jf7lk
    Mar 27 15:48:14.573: INFO: Got endpoints: latency-svc-bznwd [753.32226ms]
    Mar 27 15:48:14.584: INFO: Created: latency-svc-gwkgq
    Mar 27 15:48:14.625: INFO: Got endpoints: latency-svc-mzp7m [752.891558ms]
    Mar 27 15:48:14.641: INFO: Created: latency-svc-p5jkd
    Mar 27 15:48:14.670: INFO: Got endpoints: latency-svc-ndnt8 [745.799616ms]
    Mar 27 15:48:14.683: INFO: Created: latency-svc-scslw
    Mar 27 15:48:14.720: INFO: Got endpoints: latency-svc-m2gmc [748.348583ms]
    Mar 27 15:48:14.730: INFO: Created: latency-svc-s8xcb
    Mar 27 15:48:14.770: INFO: Got endpoints: latency-svc-v7b79 [749.386487ms]
    Mar 27 15:48:14.781: INFO: Created: latency-svc-wfbt6
    Mar 27 15:48:14.822: INFO: Got endpoints: latency-svc-xggm5 [751.794653ms]
    Mar 27 15:48:14.834: INFO: Created: latency-svc-9bh8n
    Mar 27 15:48:14.869: INFO: Got endpoints: latency-svc-dqbqc [742.297161ms]
    Mar 27 15:48:14.881: INFO: Created: latency-svc-528d6
    Mar 27 15:48:14.920: INFO: Got endpoints: latency-svc-g64mj [745.783298ms]
    Mar 27 15:48:14.931: INFO: Created: latency-svc-v9j9p
    Mar 27 15:48:14.970: INFO: Got endpoints: latency-svc-g5nb9 [749.460931ms]
    Mar 27 15:48:14.979: INFO: Created: latency-svc-jx2f8
    Mar 27 15:48:15.020: INFO: Got endpoints: latency-svc-c2g9h [751.894723ms]
    Mar 27 15:48:15.031: INFO: Created: latency-svc-t6qhp
    Mar 27 15:48:15.072: INFO: Got endpoints: latency-svc-jvgkq [754.335827ms]
    Mar 27 15:48:15.086: INFO: Created: latency-svc-xdcfm
    Mar 27 15:48:15.122: INFO: Got endpoints: latency-svc-nspkt [753.026951ms]
    Mar 27 15:48:15.137: INFO: Created: latency-svc-cq8wd
    Mar 27 15:48:15.171: INFO: Got endpoints: latency-svc-nbztt [748.44615ms]
    Mar 27 15:48:15.180: INFO: Created: latency-svc-7vlbh
    Mar 27 15:48:15.218: INFO: Got endpoints: latency-svc-w44fh [749.544017ms]
    Mar 27 15:48:15.230: INFO: Created: latency-svc-v9h9k
    Mar 27 15:48:15.269: INFO: Got endpoints: latency-svc-jf7lk [746.3803ms]
    Mar 27 15:48:15.322: INFO: Got endpoints: latency-svc-gwkgq [749.381136ms]
    Mar 27 15:48:15.372: INFO: Got endpoints: latency-svc-p5jkd [747.69948ms]
    Mar 27 15:48:15.419: INFO: Got endpoints: latency-svc-scslw [749.124174ms]
    Mar 27 15:48:15.471: INFO: Got endpoints: latency-svc-s8xcb [751.310094ms]
    Mar 27 15:48:15.519: INFO: Got endpoints: latency-svc-wfbt6 [748.772247ms]
    Mar 27 15:48:15.567: INFO: Got endpoints: latency-svc-9bh8n [744.8828ms]
    Mar 27 15:48:15.619: INFO: Got endpoints: latency-svc-528d6 [749.52941ms]
    Mar 27 15:48:15.671: INFO: Got endpoints: latency-svc-v9j9p [751.489458ms]
    Mar 27 15:48:15.718: INFO: Got endpoints: latency-svc-jx2f8 [748.082236ms]
    Mar 27 15:48:15.772: INFO: Got endpoints: latency-svc-t6qhp [751.905796ms]
    Mar 27 15:48:15.822: INFO: Got endpoints: latency-svc-xdcfm [749.684006ms]
    Mar 27 15:48:15.869: INFO: Got endpoints: latency-svc-cq8wd [747.259317ms]
    Mar 27 15:48:15.920: INFO: Got endpoints: latency-svc-7vlbh [748.917084ms]
    Mar 27 15:48:15.972: INFO: Got endpoints: latency-svc-v9h9k [753.675326ms]
    Mar 27 15:48:15.972: INFO: Latencies: [20.603342ms 27.116545ms 41.87065ms 53.490871ms 58.635268ms 63.621258ms 68.25924ms 75.835121ms 86.287625ms 94.607106ms 102.441806ms 111.796915ms 120.533081ms 125.533461ms 125.727592ms 126.963378ms 127.420349ms 127.664198ms 128.785637ms 129.03705ms 129.726324ms 130.599807ms 131.056497ms 133.553096ms 134.140261ms 134.645711ms 134.912819ms 135.475741ms 138.247675ms 139.017959ms 139.357993ms 142.030749ms 142.426349ms 143.686399ms 145.694408ms 153.599438ms 194.327769ms 239.331181ms 271.346507ms 332.830456ms 354.805168ms 402.530417ms 434.284645ms 481.112518ms 525.153546ms 566.063345ms 605.758006ms 646.463001ms 692.025177ms 735.694076ms 737.326031ms 741.501383ms 742.297161ms 743.286889ms 743.585957ms 744.040556ms 744.8828ms 745.031895ms 745.269522ms 745.381868ms 745.52456ms 745.664775ms 745.783298ms 745.799616ms 746.03724ms 746.249603ms 746.3803ms 746.464793ms 746.627792ms 746.726812ms 746.838634ms 746.844433ms 746.89948ms 746.928001ms 746.931927ms 746.991339ms 746.993343ms 747.188952ms 747.259317ms 747.370569ms 747.397985ms 747.466869ms 747.497284ms 747.648619ms 747.655506ms 747.69948ms 747.869568ms 748.082236ms 748.110631ms 748.111749ms 748.126919ms 748.186676ms 748.222093ms 748.244393ms 748.327759ms 748.348583ms 748.360061ms 748.382165ms 748.382565ms 748.44615ms 748.560913ms 748.614662ms 748.673591ms 748.732735ms 748.772247ms 748.879088ms 748.891686ms 748.917084ms 748.938668ms 748.958365ms 748.959497ms 749.027484ms 749.040868ms 749.124174ms 749.269405ms 749.331474ms 749.372528ms 749.381136ms 749.386487ms 749.411659ms 749.460931ms 749.52941ms 749.544017ms 749.549972ms 749.557642ms 749.656587ms 749.684006ms 749.731093ms 749.736277ms 749.830638ms 749.875032ms 749.901569ms 749.912249ms 749.932087ms 749.935704ms 750.181146ms 750.200412ms 750.274538ms 750.310658ms 750.384772ms 750.395616ms 750.47516ms 750.584995ms 750.599243ms 750.657917ms 750.686609ms 750.688706ms 750.693486ms 750.726147ms 750.730197ms 750.784788ms 750.845683ms 750.919003ms 751.15152ms 751.167685ms 751.20934ms 751.223129ms 751.310094ms 751.335863ms 751.398195ms 751.433584ms 751.478974ms 751.489458ms 751.736563ms 751.794653ms 751.796287ms 751.894723ms 751.905796ms 751.907033ms 751.936433ms 751.97113ms 751.998247ms 752.063685ms 752.151796ms 752.176855ms 752.201845ms 752.272063ms 752.587944ms 752.801289ms 752.80952ms 752.882895ms 752.891558ms 753.026951ms 753.066295ms 753.250706ms 753.281282ms 753.32226ms 753.597854ms 753.675326ms 753.73047ms 753.842543ms 753.96643ms 754.335827ms 754.669792ms 754.867305ms 755.012925ms 755.027558ms 755.553065ms 756.91397ms 756.995637ms]
    Mar 27 15:48:15.972: INFO: 50 %ile: 748.560913ms
    Mar 27 15:48:15.972: INFO: 90 %ile: 752.882895ms
    Mar 27 15:48:15.973: INFO: 99 %ile: 756.91397ms
    Mar 27 15:48:15.973: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Mar 27 15:48:15.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-6898" for this suite. 03/27/23 15:48:15.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:48:15.998
Mar 27 15:48:15.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename csistoragecapacity 03/27/23 15:48:16
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:48:16.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:48:16.017
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 03/27/23 15:48:16.02
STEP: getting /apis/storage.k8s.io 03/27/23 15:48:16.023
STEP: getting /apis/storage.k8s.io/v1 03/27/23 15:48:16.024
STEP: creating 03/27/23 15:48:16.025
STEP: watching 03/27/23 15:48:16.043
Mar 27 15:48:16.043: INFO: starting watch
STEP: getting 03/27/23 15:48:16.054
STEP: listing in namespace 03/27/23 15:48:16.059
STEP: listing across namespaces 03/27/23 15:48:16.062
STEP: patching 03/27/23 15:48:16.065
STEP: updating 03/27/23 15:48:16.071
Mar 27 15:48:16.076: INFO: waiting for watch events with expected annotations in namespace
Mar 27 15:48:16.076: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 03/27/23 15:48:16.078
STEP: deleting a collection 03/27/23 15:48:16.091
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Mar 27 15:48:16.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-68" for this suite. 03/27/23 15:48:16.112
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":351,"skipped":6456,"failed":0}
------------------------------
• [0.121 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:48:15.998
    Mar 27 15:48:15.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename csistoragecapacity 03/27/23 15:48:16
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:48:16.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:48:16.017
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 03/27/23 15:48:16.02
    STEP: getting /apis/storage.k8s.io 03/27/23 15:48:16.023
    STEP: getting /apis/storage.k8s.io/v1 03/27/23 15:48:16.024
    STEP: creating 03/27/23 15:48:16.025
    STEP: watching 03/27/23 15:48:16.043
    Mar 27 15:48:16.043: INFO: starting watch
    STEP: getting 03/27/23 15:48:16.054
    STEP: listing in namespace 03/27/23 15:48:16.059
    STEP: listing across namespaces 03/27/23 15:48:16.062
    STEP: patching 03/27/23 15:48:16.065
    STEP: updating 03/27/23 15:48:16.071
    Mar 27 15:48:16.076: INFO: waiting for watch events with expected annotations in namespace
    Mar 27 15:48:16.076: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 03/27/23 15:48:16.078
    STEP: deleting a collection 03/27/23 15:48:16.091
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Mar 27 15:48:16.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-68" for this suite. 03/27/23 15:48:16.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:48:16.126
Mar 27 15:48:16.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename container-probe 03/27/23 15:48:16.127
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:48:16.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:48:16.145
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-aa1d0d0b-e176-445f-8c1a-231173744c5a in namespace container-probe-8656 03/27/23 15:48:16.148
Mar 27 15:48:16.157: INFO: Waiting up to 5m0s for pod "busybox-aa1d0d0b-e176-445f-8c1a-231173744c5a" in namespace "container-probe-8656" to be "not pending"
Mar 27 15:48:16.160: INFO: Pod "busybox-aa1d0d0b-e176-445f-8c1a-231173744c5a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.011918ms
Mar 27 15:48:18.165: INFO: Pod "busybox-aa1d0d0b-e176-445f-8c1a-231173744c5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008062291s
Mar 27 15:48:20.164: INFO: Pod "busybox-aa1d0d0b-e176-445f-8c1a-231173744c5a": Phase="Running", Reason="", readiness=true. Elapsed: 4.007611832s
Mar 27 15:48:20.164: INFO: Pod "busybox-aa1d0d0b-e176-445f-8c1a-231173744c5a" satisfied condition "not pending"
Mar 27 15:48:20.164: INFO: Started pod busybox-aa1d0d0b-e176-445f-8c1a-231173744c5a in namespace container-probe-8656
STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 15:48:20.164
Mar 27 15:48:20.168: INFO: Initial restart count of pod busybox-aa1d0d0b-e176-445f-8c1a-231173744c5a is 0
STEP: deleting the pod 03/27/23 15:52:21.338
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 27 15:52:21.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8656" for this suite. 03/27/23 15:52:21.542
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":352,"skipped":6473,"failed":0}
------------------------------
• [SLOW TEST] [245.626 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:48:16.126
    Mar 27 15:48:16.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename container-probe 03/27/23 15:48:16.127
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:48:16.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:48:16.145
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-aa1d0d0b-e176-445f-8c1a-231173744c5a in namespace container-probe-8656 03/27/23 15:48:16.148
    Mar 27 15:48:16.157: INFO: Waiting up to 5m0s for pod "busybox-aa1d0d0b-e176-445f-8c1a-231173744c5a" in namespace "container-probe-8656" to be "not pending"
    Mar 27 15:48:16.160: INFO: Pod "busybox-aa1d0d0b-e176-445f-8c1a-231173744c5a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.011918ms
    Mar 27 15:48:18.165: INFO: Pod "busybox-aa1d0d0b-e176-445f-8c1a-231173744c5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008062291s
    Mar 27 15:48:20.164: INFO: Pod "busybox-aa1d0d0b-e176-445f-8c1a-231173744c5a": Phase="Running", Reason="", readiness=true. Elapsed: 4.007611832s
    Mar 27 15:48:20.164: INFO: Pod "busybox-aa1d0d0b-e176-445f-8c1a-231173744c5a" satisfied condition "not pending"
    Mar 27 15:48:20.164: INFO: Started pod busybox-aa1d0d0b-e176-445f-8c1a-231173744c5a in namespace container-probe-8656
    STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 15:48:20.164
    Mar 27 15:48:20.168: INFO: Initial restart count of pod busybox-aa1d0d0b-e176-445f-8c1a-231173744c5a is 0
    STEP: deleting the pod 03/27/23 15:52:21.338
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 27 15:52:21.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8656" for this suite. 03/27/23 15:52:21.542
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:52:21.753
Mar 27 15:52:21.753: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename init-container 03/27/23 15:52:21.754
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:52:21.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:52:21.925
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 03/27/23 15:52:21.928
Mar 27 15:52:21.928: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 27 15:52:28.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-612" for this suite. 03/27/23 15:52:28.878
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":353,"skipped":6477,"failed":0}
------------------------------
• [SLOW TEST] [7.133 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:52:21.753
    Mar 27 15:52:21.753: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename init-container 03/27/23 15:52:21.754
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:52:21.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:52:21.925
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 03/27/23 15:52:21.928
    Mar 27 15:52:21.928: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 27 15:52:28.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-612" for this suite. 03/27/23 15:52:28.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:52:28.887
Mar 27 15:52:28.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename webhook 03/27/23 15:52:28.888
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:52:28.904
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:52:28.909
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/27/23 15:52:28.924
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:52:29.334
STEP: Deploying the webhook pod 03/27/23 15:52:29.341
STEP: Wait for the deployment to be ready 03/27/23 15:52:29.355
Mar 27 15:52:29.365: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 03/27/23 15:52:31.378
STEP: Verifying the service has paired with the endpoint 03/27/23 15:52:31.388
Mar 27 15:52:32.389: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 03/27/23 15:52:32.393
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/27/23 15:52:32.394
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/27/23 15:52:32.394
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/27/23 15:52:32.394
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/27/23 15:52:32.396
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/27/23 15:52:32.396
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/27/23 15:52:32.397
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 27 15:52:32.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9283" for this suite. 03/27/23 15:52:32.405
STEP: Destroying namespace "webhook-9283-markers" for this suite. 03/27/23 15:52:32.41
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":354,"skipped":6482,"failed":0}
------------------------------
• [3.576 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:52:28.887
    Mar 27 15:52:28.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename webhook 03/27/23 15:52:28.888
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:52:28.904
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:52:28.909
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/27/23 15:52:28.924
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 15:52:29.334
    STEP: Deploying the webhook pod 03/27/23 15:52:29.341
    STEP: Wait for the deployment to be ready 03/27/23 15:52:29.355
    Mar 27 15:52:29.365: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 03/27/23 15:52:31.378
    STEP: Verifying the service has paired with the endpoint 03/27/23 15:52:31.388
    Mar 27 15:52:32.389: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 03/27/23 15:52:32.393
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/27/23 15:52:32.394
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/27/23 15:52:32.394
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/27/23 15:52:32.394
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/27/23 15:52:32.396
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/27/23 15:52:32.396
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/27/23 15:52:32.397
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 27 15:52:32.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9283" for this suite. 03/27/23 15:52:32.405
    STEP: Destroying namespace "webhook-9283-markers" for this suite. 03/27/23 15:52:32.41
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:52:32.463
Mar 27 15:52:32.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename services 03/27/23 15:52:32.464
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:52:32.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:52:32.482
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-4819 03/27/23 15:52:32.485
STEP: creating service affinity-clusterip-transition in namespace services-4819 03/27/23 15:52:32.485
STEP: creating replication controller affinity-clusterip-transition in namespace services-4819 03/27/23 15:52:32.495
I0327 15:52:32.504367      24 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4819, replica count: 3
I0327 15:52:35.555947      24 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0327 15:52:38.556221      24 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 15:52:38.563: INFO: Creating new exec pod
Mar 27 15:52:38.568: INFO: Waiting up to 5m0s for pod "execpod-affinitygcx56" in namespace "services-4819" to be "running"
Mar 27 15:52:38.575: INFO: Pod "execpod-affinitygcx56": Phase="Pending", Reason="", readiness=false. Elapsed: 6.642427ms
Mar 27 15:52:40.584: INFO: Pod "execpod-affinitygcx56": Phase="Running", Reason="", readiness=true. Elapsed: 2.015653839s
Mar 27 15:52:40.584: INFO: Pod "execpod-affinitygcx56" satisfied condition "running"
Mar 27 15:52:41.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-4819 exec execpod-affinitygcx56 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Mar 27 15:52:41.771: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar 27 15:52:41.771: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 15:52:41.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-4819 exec execpod-affinitygcx56 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.23.172 80'
Mar 27 15:52:41.948: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.23.172 80\nConnection to 10.240.23.172 80 port [tcp/http] succeeded!\n"
Mar 27 15:52:41.949: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 27 15:52:41.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-4819 exec execpod-affinitygcx56 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.23.172:80/ ; done'
Mar 27 15:52:42.250: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n"
Mar 27 15:52:42.250: INFO: stdout: "\naffinity-clusterip-transition-rn2jp\naffinity-clusterip-transition-fj9vn\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-rn2jp\naffinity-clusterip-transition-fj9vn\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-rn2jp\naffinity-clusterip-transition-fj9vn\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-rn2jp\naffinity-clusterip-transition-fj9vn\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-rn2jp\naffinity-clusterip-transition-fj9vn\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-rn2jp"
Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-rn2jp
Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-fj9vn
Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-rn2jp
Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-fj9vn
Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-rn2jp
Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-fj9vn
Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-rn2jp
Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-fj9vn
Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-rn2jp
Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-fj9vn
Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-rn2jp
Mar 27 15:52:42.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-4819 exec execpod-affinitygcx56 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.23.172:80/ ; done'
Mar 27 15:52:42.563: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n"
Mar 27 15:52:42.563: INFO: stdout: "\naffinity-clusterip-transition-fj9vn\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl"
Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-fj9vn
Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
Mar 27 15:52:42.563: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4819, will wait for the garbage collector to delete the pods 03/27/23 15:52:42.575
Mar 27 15:52:42.636: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.239174ms
Mar 27 15:52:42.737: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.244614ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 27 15:52:45.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4819" for this suite. 03/27/23 15:52:45.062
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":355,"skipped":6483,"failed":0}
------------------------------
• [SLOW TEST] [12.606 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:52:32.463
    Mar 27 15:52:32.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename services 03/27/23 15:52:32.464
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:52:32.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:52:32.482
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-4819 03/27/23 15:52:32.485
    STEP: creating service affinity-clusterip-transition in namespace services-4819 03/27/23 15:52:32.485
    STEP: creating replication controller affinity-clusterip-transition in namespace services-4819 03/27/23 15:52:32.495
    I0327 15:52:32.504367      24 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4819, replica count: 3
    I0327 15:52:35.555947      24 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0327 15:52:38.556221      24 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 15:52:38.563: INFO: Creating new exec pod
    Mar 27 15:52:38.568: INFO: Waiting up to 5m0s for pod "execpod-affinitygcx56" in namespace "services-4819" to be "running"
    Mar 27 15:52:38.575: INFO: Pod "execpod-affinitygcx56": Phase="Pending", Reason="", readiness=false. Elapsed: 6.642427ms
    Mar 27 15:52:40.584: INFO: Pod "execpod-affinitygcx56": Phase="Running", Reason="", readiness=true. Elapsed: 2.015653839s
    Mar 27 15:52:40.584: INFO: Pod "execpod-affinitygcx56" satisfied condition "running"
    Mar 27 15:52:41.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-4819 exec execpod-affinitygcx56 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Mar 27 15:52:41.771: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Mar 27 15:52:41.771: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 15:52:41.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-4819 exec execpod-affinitygcx56 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.23.172 80'
    Mar 27 15:52:41.948: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.23.172 80\nConnection to 10.240.23.172 80 port [tcp/http] succeeded!\n"
    Mar 27 15:52:41.949: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 27 15:52:41.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-4819 exec execpod-affinitygcx56 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.23.172:80/ ; done'
    Mar 27 15:52:42.250: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n"
    Mar 27 15:52:42.250: INFO: stdout: "\naffinity-clusterip-transition-rn2jp\naffinity-clusterip-transition-fj9vn\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-rn2jp\naffinity-clusterip-transition-fj9vn\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-rn2jp\naffinity-clusterip-transition-fj9vn\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-rn2jp\naffinity-clusterip-transition-fj9vn\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-rn2jp\naffinity-clusterip-transition-fj9vn\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-rn2jp"
    Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-rn2jp
    Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-fj9vn
    Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-rn2jp
    Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-fj9vn
    Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-rn2jp
    Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-fj9vn
    Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-rn2jp
    Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-fj9vn
    Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-rn2jp
    Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-fj9vn
    Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.250: INFO: Received response from host: affinity-clusterip-transition-rn2jp
    Mar 27 15:52:42.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1396699515 --namespace=services-4819 exec execpod-affinitygcx56 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.23.172:80/ ; done'
    Mar 27 15:52:42.563: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.23.172:80/\n"
    Mar 27 15:52:42.563: INFO: stdout: "\naffinity-clusterip-transition-fj9vn\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl\naffinity-clusterip-transition-6slkl"
    Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-fj9vn
    Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.563: INFO: Received response from host: affinity-clusterip-transition-6slkl
    Mar 27 15:52:42.563: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4819, will wait for the garbage collector to delete the pods 03/27/23 15:52:42.575
    Mar 27 15:52:42.636: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.239174ms
    Mar 27 15:52:42.737: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.244614ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 27 15:52:45.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4819" for this suite. 03/27/23 15:52:45.062
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:52:45.071
Mar 27 15:52:45.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename replication-controller 03/27/23 15:52:45.072
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:52:45.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:52:45.09
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 03/27/23 15:52:45.096
STEP: When the matched label of one of its pods change 03/27/23 15:52:45.104
Mar 27 15:52:45.108: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 27 15:52:50.112: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 03/27/23 15:52:50.125
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar 27 15:52:51.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1622" for this suite. 03/27/23 15:52:51.14
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":356,"skipped":6500,"failed":0}
------------------------------
• [SLOW TEST] [6.076 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:52:45.071
    Mar 27 15:52:45.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename replication-controller 03/27/23 15:52:45.072
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:52:45.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:52:45.09
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 03/27/23 15:52:45.096
    STEP: When the matched label of one of its pods change 03/27/23 15:52:45.104
    Mar 27 15:52:45.108: INFO: Pod name pod-release: Found 0 pods out of 1
    Mar 27 15:52:50.112: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/27/23 15:52:50.125
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar 27 15:52:51.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1622" for this suite. 03/27/23 15:52:51.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:52:51.148
Mar 27 15:52:51.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename container-runtime 03/27/23 15:52:51.15
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:52:51.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:52:51.167
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 03/27/23 15:52:51.169
STEP: wait for the container to reach Succeeded 03/27/23 15:52:51.18
STEP: get the container status 03/27/23 15:52:57.211
STEP: the container should be terminated 03/27/23 15:52:57.214
STEP: the termination message should be set 03/27/23 15:52:57.214
Mar 27 15:52:57.214: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/27/23 15:52:57.214
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar 27 15:52:57.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5926" for this suite. 03/27/23 15:52:57.24
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":357,"skipped":6533,"failed":0}
------------------------------
• [SLOW TEST] [6.099 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:52:51.148
    Mar 27 15:52:51.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename container-runtime 03/27/23 15:52:51.15
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:52:51.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:52:51.167
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 03/27/23 15:52:51.169
    STEP: wait for the container to reach Succeeded 03/27/23 15:52:51.18
    STEP: get the container status 03/27/23 15:52:57.211
    STEP: the container should be terminated 03/27/23 15:52:57.214
    STEP: the termination message should be set 03/27/23 15:52:57.214
    Mar 27 15:52:57.214: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/27/23 15:52:57.214
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar 27 15:52:57.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-5926" for this suite. 03/27/23 15:52:57.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:52:57.25
Mar 27 15:52:57.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename runtimeclass 03/27/23 15:52:57.251
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:52:57.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:52:57.274
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Mar 27 15:52:57.294: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6945 to be scheduled
Mar 27 15:52:57.298: INFO: 1 pods are not scheduled: [runtimeclass-6945/test-runtimeclass-runtimeclass-6945-preconfigured-handler-x2zdw(b56044af-b5f8-4258-b8d6-446c5e7ec4be)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar 27 15:52:59.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6945" for this suite. 03/27/23 15:52:59.313
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":358,"skipped":6625,"failed":0}
------------------------------
• [2.070 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:52:57.25
    Mar 27 15:52:57.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename runtimeclass 03/27/23 15:52:57.251
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:52:57.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:52:57.274
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Mar 27 15:52:57.294: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6945 to be scheduled
    Mar 27 15:52:57.298: INFO: 1 pods are not scheduled: [runtimeclass-6945/test-runtimeclass-runtimeclass-6945-preconfigured-handler-x2zdw(b56044af-b5f8-4258-b8d6-446c5e7ec4be)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar 27 15:52:59.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-6945" for this suite. 03/27/23 15:52:59.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:52:59.323
Mar 27 15:52:59.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename tables 03/27/23 15:52:59.324
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:52:59.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:52:59.391
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Mar 27 15:52:59.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-2473" for this suite. 03/27/23 15:52:59.401
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":359,"skipped":6639,"failed":0}
------------------------------
• [0.085 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:52:59.323
    Mar 27 15:52:59.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename tables 03/27/23 15:52:59.324
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:52:59.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:52:59.391
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Mar 27 15:52:59.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-2473" for this suite. 03/27/23 15:52:59.401
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:52:59.408
Mar 27 15:52:59.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename container-probe 03/27/23 15:52:59.409
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:52:59.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:52:59.423
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-c82b2371-b1df-407b-95b9-e10fb807e3e5 in namespace container-probe-7394 03/27/23 15:52:59.425
Mar 27 15:52:59.439: INFO: Waiting up to 5m0s for pod "busybox-c82b2371-b1df-407b-95b9-e10fb807e3e5" in namespace "container-probe-7394" to be "not pending"
Mar 27 15:52:59.451: INFO: Pod "busybox-c82b2371-b1df-407b-95b9-e10fb807e3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.405135ms
Mar 27 15:53:01.456: INFO: Pod "busybox-c82b2371-b1df-407b-95b9-e10fb807e3e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.017075539s
Mar 27 15:53:01.456: INFO: Pod "busybox-c82b2371-b1df-407b-95b9-e10fb807e3e5" satisfied condition "not pending"
Mar 27 15:53:01.456: INFO: Started pod busybox-c82b2371-b1df-407b-95b9-e10fb807e3e5 in namespace container-probe-7394
STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 15:53:01.456
Mar 27 15:53:01.459: INFO: Initial restart count of pod busybox-c82b2371-b1df-407b-95b9-e10fb807e3e5 is 0
Mar 27 15:53:51.607: INFO: Restart count of pod container-probe-7394/busybox-c82b2371-b1df-407b-95b9-e10fb807e3e5 is now 1 (50.148311597s elapsed)
STEP: deleting the pod 03/27/23 15:53:51.607
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 27 15:53:51.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7394" for this suite. 03/27/23 15:53:51.635
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":360,"skipped":6642,"failed":0}
------------------------------
• [SLOW TEST] [52.234 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:52:59.408
    Mar 27 15:52:59.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename container-probe 03/27/23 15:52:59.409
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:52:59.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:52:59.423
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-c82b2371-b1df-407b-95b9-e10fb807e3e5 in namespace container-probe-7394 03/27/23 15:52:59.425
    Mar 27 15:52:59.439: INFO: Waiting up to 5m0s for pod "busybox-c82b2371-b1df-407b-95b9-e10fb807e3e5" in namespace "container-probe-7394" to be "not pending"
    Mar 27 15:52:59.451: INFO: Pod "busybox-c82b2371-b1df-407b-95b9-e10fb807e3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.405135ms
    Mar 27 15:53:01.456: INFO: Pod "busybox-c82b2371-b1df-407b-95b9-e10fb807e3e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.017075539s
    Mar 27 15:53:01.456: INFO: Pod "busybox-c82b2371-b1df-407b-95b9-e10fb807e3e5" satisfied condition "not pending"
    Mar 27 15:53:01.456: INFO: Started pod busybox-c82b2371-b1df-407b-95b9-e10fb807e3e5 in namespace container-probe-7394
    STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 15:53:01.456
    Mar 27 15:53:01.459: INFO: Initial restart count of pod busybox-c82b2371-b1df-407b-95b9-e10fb807e3e5 is 0
    Mar 27 15:53:51.607: INFO: Restart count of pod container-probe-7394/busybox-c82b2371-b1df-407b-95b9-e10fb807e3e5 is now 1 (50.148311597s elapsed)
    STEP: deleting the pod 03/27/23 15:53:51.607
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 27 15:53:51.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7394" for this suite. 03/27/23 15:53:51.635
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:53:51.643
Mar 27 15:53:51.643: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename endpointslicemirroring 03/27/23 15:53:51.644
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:53:51.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:53:51.668
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 03/27/23 15:53:51.685
Mar 27 15:53:51.695: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 03/27/23 15:53:53.7
Mar 27 15:53:53.707: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 03/27/23 15:53:55.712
Mar 27 15:53:55.722: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Mar 27 15:53:57.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-1100" for this suite. 03/27/23 15:53:57.732
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":361,"skipped":6655,"failed":0}
------------------------------
• [SLOW TEST] [6.095 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:53:51.643
    Mar 27 15:53:51.643: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename endpointslicemirroring 03/27/23 15:53:51.644
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:53:51.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:53:51.668
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 03/27/23 15:53:51.685
    Mar 27 15:53:51.695: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 03/27/23 15:53:53.7
    Mar 27 15:53:53.707: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 03/27/23 15:53:55.712
    Mar 27 15:53:55.722: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Mar 27 15:53:57.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-1100" for this suite. 03/27/23 15:53:57.732
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/27/23 15:53:57.74
Mar 27 15:53:57.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
STEP: Building a namespace api object, basename emptydir 03/27/23 15:53:57.741
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:53:57.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:53:57.761
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/27/23 15:53:57.765
Mar 27 15:53:57.773: INFO: Waiting up to 5m0s for pod "pod-33a3d9c7-3345-465e-bf18-463931eb84b3" in namespace "emptydir-9905" to be "Succeeded or Failed"
Mar 27 15:53:57.776: INFO: Pod "pod-33a3d9c7-3345-465e-bf18-463931eb84b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.632012ms
Mar 27 15:53:59.780: INFO: Pod "pod-33a3d9c7-3345-465e-bf18-463931eb84b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.006951172s
Mar 27 15:54:01.782: INFO: Pod "pod-33a3d9c7-3345-465e-bf18-463931eb84b3": Phase="Running", Reason="", readiness=false. Elapsed: 4.009164233s
Mar 27 15:54:03.782: INFO: Pod "pod-33a3d9c7-3345-465e-bf18-463931eb84b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008439452s
STEP: Saw pod success 03/27/23 15:54:03.782
Mar 27 15:54:03.782: INFO: Pod "pod-33a3d9c7-3345-465e-bf18-463931eb84b3" satisfied condition "Succeeded or Failed"
Mar 27 15:54:03.786: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-33a3d9c7-3345-465e-bf18-463931eb84b3 container test-container: <nil>
STEP: delete the pod 03/27/23 15:54:03.797
Mar 27 15:54:03.812: INFO: Waiting for pod pod-33a3d9c7-3345-465e-bf18-463931eb84b3 to disappear
Mar 27 15:54:03.815: INFO: Pod pod-33a3d9c7-3345-465e-bf18-463931eb84b3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 27 15:54:03.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9905" for this suite. 03/27/23 15:54:03.819
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":362,"skipped":6658,"failed":0}
------------------------------
• [SLOW TEST] [6.087 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/27/23 15:53:57.74
    Mar 27 15:53:57.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1396699515
    STEP: Building a namespace api object, basename emptydir 03/27/23 15:53:57.741
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 15:53:57.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 15:53:57.761
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/27/23 15:53:57.765
    Mar 27 15:53:57.773: INFO: Waiting up to 5m0s for pod "pod-33a3d9c7-3345-465e-bf18-463931eb84b3" in namespace "emptydir-9905" to be "Succeeded or Failed"
    Mar 27 15:53:57.776: INFO: Pod "pod-33a3d9c7-3345-465e-bf18-463931eb84b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.632012ms
    Mar 27 15:53:59.780: INFO: Pod "pod-33a3d9c7-3345-465e-bf18-463931eb84b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.006951172s
    Mar 27 15:54:01.782: INFO: Pod "pod-33a3d9c7-3345-465e-bf18-463931eb84b3": Phase="Running", Reason="", readiness=false. Elapsed: 4.009164233s
    Mar 27 15:54:03.782: INFO: Pod "pod-33a3d9c7-3345-465e-bf18-463931eb84b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008439452s
    STEP: Saw pod success 03/27/23 15:54:03.782
    Mar 27 15:54:03.782: INFO: Pod "pod-33a3d9c7-3345-465e-bf18-463931eb84b3" satisfied condition "Succeeded or Failed"
    Mar 27 15:54:03.786: INFO: Trying to get logs from node k8s-conformance-1249712-tests-worker-m5xqt-57d9685bdd-dncrc pod pod-33a3d9c7-3345-465e-bf18-463931eb84b3 container test-container: <nil>
    STEP: delete the pod 03/27/23 15:54:03.797
    Mar 27 15:54:03.812: INFO: Waiting for pod pod-33a3d9c7-3345-465e-bf18-463931eb84b3 to disappear
    Mar 27 15:54:03.815: INFO: Pod pod-33a3d9c7-3345-465e-bf18-463931eb84b3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 27 15:54:03.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9905" for this suite. 03/27/23 15:54:03.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
Mar 27 15:54:03.836: INFO: Running AfterSuite actions on all nodes
Mar 27 15:54:03.836: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Mar 27 15:54:03.836: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Mar 27 15:54:03.836: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Mar 27 15:54:03.836: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Mar 27 15:54:03.836: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Mar 27 15:54:03.836: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Mar 27 15:54:03.836: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Mar 27 15:54:03.836: INFO: Running AfterSuite actions on node 1
Mar 27 15:54:03.836: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.001 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Mar 27 15:54:03.836: INFO: Running AfterSuite actions on all nodes
    Mar 27 15:54:03.836: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Mar 27 15:54:03.836: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Mar 27 15:54:03.836: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Mar 27 15:54:03.836: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Mar 27 15:54:03.836: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Mar 27 15:54:03.836: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Mar 27 15:54:03.836: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Mar 27 15:54:03.836: INFO: Running AfterSuite actions on node 1
    Mar 27 15:54:03.836: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.087 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 6479.378 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h47m59.653053569s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

